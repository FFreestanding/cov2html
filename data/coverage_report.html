<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Combined Coverage Report</title>
<style>

:root {
    --bg-color: #fff;
    --text-color: #333;
    --sidebar-bg: #f5f5f5;
    --sidebar-hover: #e0e0e0;
    --line-highlight: #90EE90;
    --line-number-color: #888;
    --link-color: #0066cc;
    --border-color: #ddd;
    --toggle-color: #555;
    --good-color: #4caf50;
    --medium-color: #ff9800;
    --bad-color: #f44336;
    --header-bg: #f0f0f0;
}

@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #1e1e1e;
        --text-color: #e0e0e0;
        --sidebar-bg: #252525;
        --sidebar-hover: #333;
        --line-highlight: #2d4f2d;
        --line-number-color: #888;
        --link-color: #4b98e0;
        --border-color: #444;
        --toggle-color: #aaa;
        --good-color: #4caf50;
        --medium-color: #ff9800;
        --bad-color: #f44336;
        --header-bg: #2a2a2a;
    }
}

* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    color: var(--text-color);
    background: var(--bg-color);
    display: flex;
    height: 100vh;
    overflow: hidden;
    margin: 0;
}

.sidebar {
    width: 300px;
    height: 100vh;
    overflow: auto;
    padding: 15px;
    background-color: var(--sidebar-bg);
    border-right: 1px solid var(--border-color);
    position: relative;
}

.content {
    flex-grow: 1;
    height: 100vh;
    overflow: auto;
    padding: 15px;
}

.coverage-header, .file-header {
    padding-bottom: 15px;
    margin-bottom: 15px;
    border-bottom: 1px solid var(--border-color);
}

.coverage-summary {
    margin-top: 8px;
    font-size: 14px;
}

.coverage-good { color: var(--good-color); }
.coverage-medium { color: var(--medium-color); }
.coverage-bad { color: var(--bad-color); }

.directory {
    margin: 4px 0;
}

.file-entry {
    margin: 4px 0;
    padding-left: 3px;
}

.file-entry.active .file-link {
    background-color: var(--sidebar-hover);
    font-weight: bold;
}

.file-link {
    text-decoration: none;
    color: var(--link-color);
    display: block;
    padding: 4px 8px;
    border-radius: 3px;
    transition: background-color 0.2s;
}

.file-link:hover {
    background-color: var(--sidebar-hover);
}

.coverage-badge {
    font-size: 0.85em;
    margin-left: 5px;
}

.tree-toggle {
    cursor: pointer;
    user-select: none;
    padding: 4px 8px;
    border-radius: 3px;
    transition: background-color 0.2s;
    position: relative;
    font-weight: 500;
}

.tree-toggle:hover {
    background-color: var(--sidebar-hover);
}

.tree-toggle::before {
    content: 'â–¶';
    display: inline-block;
    margin-right: 5px;
    font-size: 0.9em;
    transition: transform 0.2s;
    color: var(--toggle-color);
}

.tree-toggle.expanded::before {
    transform: rotate(90deg);
}

.tree-child {
    margin-left: 15px;
    display: none;
    border-left: 1px solid var(--border-color);
    padding-left: 10px;
}

.tree-child.expanded {
    display: block;
}

.source-code {
    margin: 0;
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
    background-color: var(--bg-color);
    line-height: 1.5;
    overflow-x: auto;
    tab-size: 4;
}

.line {
    display: flex;
    white-space: pre;
}

.line.covered {
    background-color: var(--line-highlight);
}

.line-number {
    color: var(--line-number-color);
    padding: 0 12px;
    margin-right: 12px;
    text-align: right;
    user-select: none;
    border-right: 1px solid var(--border-color);
    min-width: 40px;
}

.line-content {
    flex: 1;
}

.include-brackets {
    color: var(--text-color);
}

.welcome {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    height: 100%;
}

.welcome h1 {
    margin-bottom: 20px;
}
</style>
</head>
<body>
<div id="sidebar" class="sidebar">
<div class="coverage-header">
<h2>Coverage Report</h2>
<div class="coverage-summary">Overall: <span class="coverage-bad">0.8%</span> (2781 of 333487 lines)</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">arch/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">x86/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">entry/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">vsyscall/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_entry_vsyscall_vsyscall_64_c')" class="file-link" data-id="arch_x86_entry_vsyscall_vsyscall_64_c">vsyscall_64.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">include/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">asm/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">fpu/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_fpu_xstate_h')" class="file-link" data-id="arch_x86_include_asm_fpu_xstate_h">xstate.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">trace/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_trace_fpu_h')" class="file-link" data-id="arch_x86_include_asm_trace_fpu_h">fpu.h <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_atomic_h')" class="file-link" data-id="arch_x86_include_asm_atomic_h">atomic.h <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_cpufeature_h')" class="file-link" data-id="arch_x86_include_asm_cpufeature_h">cpufeature.h <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_desc_h')" class="file-link" data-id="arch_x86_include_asm_desc_h">desc.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_insn_h')" class="file-link" data-id="arch_x86_include_asm_insn_h">insn.h <span class="coverage-badge coverage-bad">(1.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_irqflags_h')" class="file-link" data-id="arch_x86_include_asm_irqflags_h">irqflags.h <span class="coverage-badge coverage-bad">(2.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_jump_label_h')" class="file-link" data-id="arch_x86_include_asm_jump_label_h">jump_label.h <span class="coverage-badge coverage-bad">(5.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_mmu_context_h')" class="file-link" data-id="arch_x86_include_asm_mmu_context_h">mmu_context.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_page_64_h')" class="file-link" data-id="arch_x86_include_asm_page_64_h">page_64.h <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_pgalloc_h')" class="file-link" data-id="arch_x86_include_asm_pgalloc_h">pgalloc.h <span class="coverage-badge coverage-bad">(1.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_pgtable_h')" class="file-link" data-id="arch_x86_include_asm_pgtable_h">pgtable.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_pgtable_64_h')" class="file-link" data-id="arch_x86_include_asm_pgtable_64_h">pgtable_64.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_pkru_h')" class="file-link" data-id="arch_x86_include_asm_pkru_h">pkru.h <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_sync_core_h')" class="file-link" data-id="arch_x86_include_asm_sync_core_h">sync_core.h <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_include_asm_text-patching_h')" class="file-link" data-id="arch_x86_include_asm_text-patching_h">text-patching.h <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">kernel/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">cpu/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">mtrr/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_cpu_mtrr_generic_c')" class="file-link" data-id="arch_x86_kernel_cpu_mtrr_generic_c">generic.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_cpu_aperfmperf_c')" class="file-link" data-id="arch_x86_kernel_cpu_aperfmperf_c">aperfmperf.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">fpu/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_fpu_context_h')" class="file-link" data-id="arch_x86_kernel_fpu_context_h">context.h <span class="coverage-badge coverage-bad">(6.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_fpu_core_c')" class="file-link" data-id="arch_x86_kernel_fpu_core_c">core.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_fpu_internal_h')" class="file-link" data-id="arch_x86_kernel_fpu_internal_h">internal.h <span class="coverage-badge coverage-bad">(3.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_fpu_xstate_c')" class="file-link" data-id="arch_x86_kernel_fpu_xstate_c">xstate.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_fpu_xstate_h')" class="file-link" data-id="arch_x86_kernel_fpu_xstate_h">xstate.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_alternative_c')" class="file-link" data-id="arch_x86_kernel_alternative_c">alternative.c <span class="coverage-badge coverage-bad">(1.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_jump_label_c')" class="file-link" data-id="arch_x86_kernel_jump_label_c">jump_label.c <span class="coverage-badge coverage-bad">(6.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_kvmclock_c')" class="file-link" data-id="arch_x86_kernel_kvmclock_c">kvmclock.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_ldt_c')" class="file-link" data-id="arch_x86_kernel_ldt_c">ldt.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_process_c')" class="file-link" data-id="arch_x86_kernel_process_c">process.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_kernel_tsc_c')" class="file-link" data-id="arch_x86_kernel_tsc_c">tsc.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">lib/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_lib_inat_c')" class="file-link" data-id="arch_x86_lib_inat_c">inat.c <span class="coverage-badge coverage-bad">(3.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_lib_insn_c')" class="file-link" data-id="arch_x86_lib_insn_c">insn.c <span class="coverage-badge coverage-bad">(4.8%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">mm/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_mm_extable_c')" class="file-link" data-id="arch_x86_mm_extable_c">extable.c <span class="coverage-badge coverage-bad">(1.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_mm_fault_c')" class="file-link" data-id="arch_x86_mm_fault_c">fault.c <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_mm_pgtable_c')" class="file-link" data-id="arch_x86_mm_pgtable_c">pgtable.c <span class="coverage-badge coverage-bad">(1.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_mm_physaddr_c')" class="file-link" data-id="arch_x86_mm_physaddr_c">physaddr.c <span class="coverage-badge coverage-bad">(4.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('arch_x86_mm_pti_c')" class="file-link" data-id="arch_x86_mm_pti_c">pti.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">block/</div>
<div class="tree-child expanded">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('block_blk-cgroup_c')" class="file-link" data-id="block_blk-cgroup_c">blk-cgroup.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">drivers/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">char/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('drivers_char_random_c')" class="file-link" data-id="drivers_char_random_c">random.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">fs/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">ext4/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ext4_block_validity_c')" class="file-link" data-id="fs_ext4_block_validity_c">block_validity.c <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ext4_ext4_h')" class="file-link" data-id="fs_ext4_ext4_h">ext4.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ext4_extents_status_c')" class="file-link" data-id="fs_ext4_extents_status_c">extents_status.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ext4_inode_c')" class="file-link" data-id="fs_ext4_inode_c">inode.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ext4_namei_c')" class="file-link" data-id="fs_ext4_namei_c">namei.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_anon_inodes_c')" class="file-link" data-id="fs_anon_inodes_c">anon_inodes.c <span class="coverage-badge coverage-bad">(1.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_buffer_c')" class="file-link" data-id="fs_buffer_c">buffer.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_dcache_c')" class="file-link" data-id="fs_dcache_c">dcache.c <span class="coverage-badge coverage-bad">(2.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_fcntl_c')" class="file-link" data-id="fs_fcntl_c">fcntl.c <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_file_c')" class="file-link" data-id="fs_file_c">file.c <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_file_table_c')" class="file-link" data-id="fs_file_table_c">file_table.c <span class="coverage-badge coverage-bad">(3.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_inode_c')" class="file-link" data-id="fs_inode_c">inode.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_ioctl_c')" class="file-link" data-id="fs_ioctl_c">ioctl.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_mnt_idmapping_c')" class="file-link" data-id="fs_mnt_idmapping_c">mnt_idmapping.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_namei_c')" class="file-link" data-id="fs_namei_c">namei.c <span class="coverage-badge coverage-bad">(2.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_namespace_c')" class="file-link" data-id="fs_namespace_c">namespace.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_nsfs_c')" class="file-link" data-id="fs_nsfs_c">nsfs.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_select_c')" class="file-link" data-id="fs_select_c">select.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('fs_userfaultfd_c')" class="file-link" data-id="fs_userfaultfd_c">userfaultfd.c <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">include/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">asm-generic/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_asm-generic_pgalloc_h')" class="file-link" data-id="include_asm-generic_pgalloc_h">pgalloc.h <span class="coverage-badge coverage-bad">(1.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_asm-generic_rwonce_h')" class="file-link" data-id="include_asm-generic_rwonce_h">rwonce.h <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">linux/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">atomic/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_atomic_atomic-arch-fallback_h')" class="file-link" data-id="include_linux_atomic_atomic-arch-fallback_h">atomic-arch-fallback.h <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_atomic_atomic-instrumented_h')" class="file-link" data-id="include_linux_atomic_atomic-instrumented_h">atomic-instrumented.h <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">sched/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_sched_mm_h')" class="file-link" data-id="include_linux_sched_mm_h">mm.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_sched_signal_h')" class="file-link" data-id="include_linux_sched_signal_h">signal.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_sched_task_h')" class="file-link" data-id="include_linux_sched_task_h">task.h <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_sched_user_h')" class="file-link" data-id="include_linux_sched_user_h">user.h <span class="coverage-badge coverage-bad">(3.6%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_audit_h')" class="file-link" data-id="include_linux_audit_h">audit.h <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_bit_spinlock_h')" class="file-link" data-id="include_linux_bit_spinlock_h">bit_spinlock.h <span class="coverage-badge coverage-bad">(2.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_bpf_h')" class="file-link" data-id="include_linux_bpf_h">bpf.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_bpf_verifier_h')" class="file-link" data-id="include_linux_bpf_verifier_h">bpf_verifier.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_bpfptr_h')" class="file-link" data-id="include_linux_bpfptr_h">bpfptr.h <span class="coverage-badge coverage-bad">(5.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_bsearch_h')" class="file-link" data-id="include_linux_bsearch_h">bsearch.h <span class="coverage-badge coverage-bad">(6.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_buffer_head_h')" class="file-link" data-id="include_linux_buffer_head_h">buffer_head.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_cpumask_h')" class="file-link" data-id="include_linux_cpumask_h">cpumask.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_cred_h')" class="file-link" data-id="include_linux_cred_h">cred.h <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_dcache_h')" class="file-link" data-id="include_linux_dcache_h">dcache.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_delayacct_h')" class="file-link" data-id="include_linux_delayacct_h">delayacct.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_device_cgroup_h')" class="file-link" data-id="include_linux_device_cgroup_h">device_cgroup.h <span class="coverage-badge coverage-bad">(1.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_file_h')" class="file-link" data-id="include_linux_file_h">file.h <span class="coverage-badge coverage-bad">(3.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_file_ref_h')" class="file-link" data-id="include_linux_file_ref_h">file_ref.h <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_find_h')" class="file-link" data-id="include_linux_find_h">find.h <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_fs_h')" class="file-link" data-id="include_linux_fs_h">fs.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_fscrypt_h')" class="file-link" data-id="include_linux_fscrypt_h">fscrypt.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_fsnotify_backend_h')" class="file-link" data-id="include_linux_fsnotify_backend_h">fsnotify_backend.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_gfp_h')" class="file-link" data-id="include_linux_gfp_h">gfp.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_idr_h')" class="file-link" data-id="include_linux_idr_h">idr.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_jbd2_h')" class="file-link" data-id="include_linux_jbd2_h">jbd2.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_jump_label_h')" class="file-link" data-id="include_linux_jump_label_h">jump_label.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_kernel_h')" class="file-link" data-id="include_linux_kernel_h">kernel.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_kprobes_h')" class="file-link" data-id="include_linux_kprobes_h">kprobes.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_license_h')" class="file-link" data-id="include_linux_license_h">license.h <span class="coverage-badge coverage-bad">(13.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_list_h')" class="file-link" data-id="include_linux_list_h">list.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_list_bl_h')" class="file-link" data-id="include_linux_list_bl_h">list_bl.h <span class="coverage-badge coverage-bad">(2.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_log2_h')" class="file-link" data-id="include_linux_log2_h">log2.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_maple_tree_h')" class="file-link" data-id="include_linux_maple_tree_h">maple_tree.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_memcontrol_h')" class="file-link" data-id="include_linux_memcontrol_h">memcontrol.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mempolicy_h')" class="file-link" data-id="include_linux_mempolicy_h">mempolicy.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mm_h')" class="file-link" data-id="include_linux_mm_h">mm.h <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mm_inline_h')" class="file-link" data-id="include_linux_mm_inline_h">mm_inline.h <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mmap_lock_h')" class="file-link" data-id="include_linux_mmap_lock_h">mmap_lock.h <span class="coverage-badge coverage-bad">(4.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mmu_notifier_h')" class="file-link" data-id="include_linux_mmu_notifier_h">mmu_notifier.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_mmzone_h')" class="file-link" data-id="include_linux_mmzone_h">mmzone.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_nodemask_h')" class="file-link" data-id="include_linux_nodemask_h">nodemask.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_page-flags_h')" class="file-link" data-id="include_linux_page-flags_h">page-flags.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_page_ref_h')" class="file-link" data-id="include_linux_page_ref_h">page_ref.h <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_pagemap_h')" class="file-link" data-id="include_linux_pagemap_h">pagemap.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_percpu-rwsem_h')" class="file-link" data-id="include_linux_percpu-rwsem_h">percpu-rwsem.h <span class="coverage-badge coverage-bad">(1.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_perf_event_h')" class="file-link" data-id="include_linux_perf_event_h">perf_event.h <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_pgtable_h')" class="file-link" data-id="include_linux_pgtable_h">pgtable.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_pid_h')" class="file-link" data-id="include_linux_pid_h">pid.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_preempt_h')" class="file-link" data-id="include_linux_preempt_h">preempt.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_rbtree_h')" class="file-link" data-id="include_linux_rbtree_h">rbtree.h <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_rbtree_augmented_h')" class="file-link" data-id="include_linux_rbtree_augmented_h">rbtree_augmented.h <span class="coverage-badge coverage-bad">(4.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_rculist_bl_h')" class="file-link" data-id="include_linux_rculist_bl_h">rculist_bl.h <span class="coverage-badge coverage-bad">(2.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_rcupdate_h')" class="file-link" data-id="include_linux_rcupdate_h">rcupdate.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_refcount_h')" class="file-link" data-id="include_linux_refcount_h">refcount.h <span class="coverage-badge coverage-bad">(1.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_rwsem_h')" class="file-link" data-id="include_linux_rwsem_h">rwsem.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_sched_h')" class="file-link" data-id="include_linux_sched_h">sched.h <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_seqlock_h')" class="file-link" data-id="include_linux_seqlock_h">seqlock.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_slab_h')" class="file-link" data-id="include_linux_slab_h">slab.h <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_swapops_h')" class="file-link" data-id="include_linux_swapops_h">swapops.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_topology_h')" class="file-link" data-id="include_linux_topology_h">topology.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_uaccess_h')" class="file-link" data-id="include_linux_uaccess_h">uaccess.h <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_vmstat_h')" class="file-link" data-id="include_linux_vmstat_h">vmstat.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_wait_h')" class="file-link" data-id="include_linux_wait_h">wait.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_linux_xarray_h')" class="file-link" data-id="include_linux_xarray_h">xarray.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">net/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_net_tcx_h')" class="file-link" data-id="include_net_tcx_h">tcx.h <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">trace/</div>
<div class="tree-child">
<div class="directory">
<div class="tree-toggle">events/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_block_h')" class="file-link" data-id="include_trace_events_block_h">block.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_csd_h')" class="file-link" data-id="include_trace_events_csd_h">csd.h <span class="coverage-badge coverage-bad">(2.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_ext4_h')" class="file-link" data-id="include_trace_events_ext4_h">ext4.h <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_kmem_h')" class="file-link" data-id="include_trace_events_kmem_h">kmem.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_maple_tree_h')" class="file-link" data-id="include_trace_events_maple_tree_h">maple_tree.h <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_pagemap_h')" class="file-link" data-id="include_trace_events_pagemap_h">pagemap.h <span class="coverage-badge coverage-bad">(1.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_percpu_h')" class="file-link" data-id="include_trace_events_percpu_h">percpu.h <span class="coverage-badge coverage-bad">(2.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_timer_h')" class="file-link" data-id="include_trace_events_timer_h">timer.h <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_vmalloc_h')" class="file-link" data-id="include_trace_events_vmalloc_h">vmalloc.h <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('include_trace_events_workqueue_h')" class="file-link" data-id="include_trace_events_workqueue_h">workqueue.h <span class="coverage-badge coverage-bad">(1.5%)</span></a></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">kernel/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">bpf/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_arena_c')" class="file-link" data-id="kernel_bpf_arena_c">arena.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_arraymap_c')" class="file-link" data-id="kernel_bpf_arraymap_c">arraymap.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_bloom_filter_c')" class="file-link" data-id="kernel_bpf_bloom_filter_c">bloom_filter.c <span class="coverage-badge coverage-bad">(5.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_bpf_local_storage_c')" class="file-link" data-id="kernel_bpf_bpf_local_storage_c">bpf_local_storage.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_bpf_lru_list_c')" class="file-link" data-id="kernel_bpf_bpf_lru_list_c">bpf_lru_list.c <span class="coverage-badge coverage-bad">(1.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_btf_c')" class="file-link" data-id="kernel_bpf_btf_c">btf.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_core_c')" class="file-link" data-id="kernel_bpf_core_c">core.c <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_cpumap_c')" class="file-link" data-id="kernel_bpf_cpumap_c">cpumap.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_devmap_c')" class="file-link" data-id="kernel_bpf_devmap_c">devmap.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_disasm_c')" class="file-link" data-id="kernel_bpf_disasm_c">disasm.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_hashtab_c')" class="file-link" data-id="kernel_bpf_hashtab_c">hashtab.c <span class="coverage-badge coverage-bad">(2.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_inode_c')" class="file-link" data-id="kernel_bpf_inode_c">inode.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_log_c')" class="file-link" data-id="kernel_bpf_log_c">log.c <span class="coverage-badge coverage-bad">(7.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_lpm_trie_c')" class="file-link" data-id="kernel_bpf_lpm_trie_c">lpm_trie.c <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_memalloc_c')" class="file-link" data-id="kernel_bpf_memalloc_c">memalloc.c <span class="coverage-badge coverage-bad">(4.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_mprog_c')" class="file-link" data-id="kernel_bpf_mprog_c">mprog.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_net_namespace_c')" class="file-link" data-id="kernel_bpf_net_namespace_c">net_namespace.c <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_offload_c')" class="file-link" data-id="kernel_bpf_offload_c">offload.c <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_percpu_freelist_c')" class="file-link" data-id="kernel_bpf_percpu_freelist_c">percpu_freelist.c <span class="coverage-badge coverage-bad">(6.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_queue_stack_maps_c')" class="file-link" data-id="kernel_bpf_queue_stack_maps_c">queue_stack_maps.c <span class="coverage-badge coverage-bad">(3.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_reuseport_array_c')" class="file-link" data-id="kernel_bpf_reuseport_array_c">reuseport_array.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_ringbuf_c')" class="file-link" data-id="kernel_bpf_ringbuf_c">ringbuf.c <span class="coverage-badge coverage-bad">(1.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_stackmap_c')" class="file-link" data-id="kernel_bpf_stackmap_c">stackmap.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_syscall_c')" class="file-link" data-id="kernel_bpf_syscall_c">syscall.c <span class="coverage-badge coverage-bad">(5.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_tcx_c')" class="file-link" data-id="kernel_bpf_tcx_c">tcx.c <span class="coverage-badge coverage-bad">(2.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_tnum_c')" class="file-link" data-id="kernel_bpf_tnum_c">tnum.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_token_c')" class="file-link" data-id="kernel_bpf_token_c">token.c <span class="coverage-badge coverage-bad">(5.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_bpf_verifier_c')" class="file-link" data-id="kernel_bpf_verifier_c">verifier.c <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">cgroup/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_cgroup_cpuset_c')" class="file-link" data-id="kernel_cgroup_cpuset_c">cpuset.c <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_cgroup_rstat_c')" class="file-link" data-id="kernel_cgroup_rstat_c">rstat.c <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">events/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_events_core_c')" class="file-link" data-id="kernel_events_core_c">core.c <span class="coverage-badge coverage-bad">(0.0%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">printk/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_printk_printk_safe_c')" class="file-link" data-id="kernel_printk_printk_safe_c">printk_safe.c <span class="coverage-badge coverage-bad">(2.2%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle">time/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_time_hrtimer_c')" class="file-link" data-id="kernel_time_hrtimer_c">hrtimer.c <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_time_timekeeping_c')" class="file-link" data-id="kernel_time_timekeeping_c">timekeeping.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_capability_c')" class="file-link" data-id="kernel_capability_c">capability.c <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_cpu_c')" class="file-link" data-id="kernel_cpu_c">cpu.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_fuzzer_dev_c')" class="file-link" data-id="kernel_fuzzer_dev_c">fuzzer_dev.c <span class="coverage-badge coverage-bad">(3.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_irq_work_c')" class="file-link" data-id="kernel_irq_work_c">irq_work.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_jump_label_c')" class="file-link" data-id="kernel_jump_label_c">jump_label.c <span class="coverage-badge coverage-bad">(1.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_kprobes_c')" class="file-link" data-id="kernel_kprobes_c">kprobes.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_kthread_c')" class="file-link" data-id="kernel_kthread_c">kthread.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_pid_c')" class="file-link" data-id="kernel_pid_c">pid.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_smp_c')" class="file-link" data-id="kernel_smp_c">smp.c <span class="coverage-badge coverage-bad">(1.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_task_work_c')" class="file-link" data-id="kernel_task_work_c">task_work.c <span class="coverage-badge coverage-bad">(2.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_user_c')" class="file-link" data-id="kernel_user_c">user.c <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('kernel_workqueue_c')" class="file-link" data-id="kernel_workqueue_c">workqueue.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">lib/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">crypto/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_crypto_chacha_c')" class="file-link" data-id="lib_crypto_chacha_c">chacha.c <span class="coverage-badge coverage-bad">(3.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_crypto_sha1_c')" class="file-link" data-id="lib_crypto_sha1_c">sha1.c <span class="coverage-badge coverage-bad">(5.0%)</span></a></div>
</div>
</div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_bitmap_c')" class="file-link" data-id="lib_bitmap_c">bitmap.c <span class="coverage-badge coverage-bad">(0.9%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_bsearch_c')" class="file-link" data-id="lib_bsearch_c">bsearch.c <span class="coverage-badge coverage-bad">(5.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_cpumask_c')" class="file-link" data-id="lib_cpumask_c">cpumask.c <span class="coverage-badge coverage-bad">(1.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_errseq_c')" class="file-link" data-id="lib_errseq_c">errseq.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_extable_c')" class="file-link" data-id="lib_extable_c">extable.c <span class="coverage-badge coverage-bad">(1.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_idr_c')" class="file-link" data-id="lib_idr_c">idr.c <span class="coverage-badge coverage-bad">(1.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_llist_c')" class="file-link" data-id="lib_llist_c">llist.c <span class="coverage-badge coverage-bad">(1.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_lockref_c')" class="file-link" data-id="lib_lockref_c">lockref.c <span class="coverage-badge coverage-bad">(3.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_maple_tree_c')" class="file-link" data-id="lib_maple_tree_c">maple_tree.c <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_percpu_counter_c')" class="file-link" data-id="lib_percpu_counter_c">percpu_counter.c <span class="coverage-badge coverage-bad">(2.0%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_radix-tree_c')" class="file-link" data-id="lib_radix-tree_c">radix-tree.c <span class="coverage-badge coverage-bad">(3.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_refcount_c')" class="file-link" data-id="lib_refcount_c">refcount.c <span class="coverage-badge coverage-bad">(2.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_sort_c')" class="file-link" data-id="lib_sort_c">sort.c <span class="coverage-badge coverage-bad">(6.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_strncpy_from_user_c')" class="file-link" data-id="lib_strncpy_from_user_c">strncpy_from_user.c <span class="coverage-badge coverage-bad">(3.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_timerqueue_c')" class="file-link" data-id="lib_timerqueue_c">timerqueue.c <span class="coverage-badge coverage-bad">(7.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_usercopy_c')" class="file-link" data-id="lib_usercopy_c">usercopy.c <span class="coverage-badge coverage-bad">(11.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_vsprintf_c')" class="file-link" data-id="lib_vsprintf_c">vsprintf.c <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('lib_xarray_c')" class="file-link" data-id="lib_xarray_c">xarray.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">mm/</div>
<div class="tree-child expanded">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_backing-dev_c')" class="file-link" data-id="mm_backing-dev_c">backing-dev.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_filemap_c')" class="file-link" data-id="mm_filemap_c">filemap.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_internal_h')" class="file-link" data-id="mm_internal_h">internal.h <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_interval_tree_c')" class="file-link" data-id="mm_interval_tree_c">interval_tree.c <span class="coverage-badge coverage-bad">(2.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_list_lru_c')" class="file-link" data-id="mm_list_lru_c">list_lru.c <span class="coverage-badge coverage-bad">(0.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_memory_c')" class="file-link" data-id="mm_memory_c">memory.c <span class="coverage-badge coverage-bad">(1.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_mempolicy_c')" class="file-link" data-id="mm_mempolicy_c">mempolicy.c <span class="coverage-badge coverage-bad">(0.6%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_mmap_c')" class="file-link" data-id="mm_mmap_c">mmap.c <span class="coverage-badge coverage-bad">(0.2%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_percpu-vm_c')" class="file-link" data-id="mm_percpu-vm_c">percpu-vm.c <span class="coverage-badge coverage-bad">(3.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_percpu_c')" class="file-link" data-id="mm_percpu_c">percpu.c <span class="coverage-badge coverage-bad">(4.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_pgalloc-track_h')" class="file-link" data-id="mm_pgalloc-track_h">pgalloc-track.h <span class="coverage-badge coverage-bad">(7.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_pgtable-generic_c')" class="file-link" data-id="mm_pgtable-generic_c">pgtable-generic.c <span class="coverage-badge coverage-bad">(2.7%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_rmap_c')" class="file-link" data-id="mm_rmap_c">rmap.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_swap_c')" class="file-link" data-id="mm_swap_c">swap.c <span class="coverage-badge coverage-bad">(2.8%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_util_c')" class="file-link" data-id="mm_util_c">util.c <span class="coverage-badge coverage-bad">(1.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_vma_c')" class="file-link" data-id="mm_vma_c">vma.c <span class="coverage-badge coverage-bad">(0.5%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('mm_vmalloc_c')" class="file-link" data-id="mm_vmalloc_c">vmalloc.c <span class="coverage-badge coverage-bad">(3.3%)</span></a></div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">net/</div>
<div class="tree-child expanded">
<div class="directory">
<div class="tree-toggle">core/</div>
<div class="tree-child">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('net_core_dev_c')" class="file-link" data-id="net_core_dev_c">dev.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('net_core_net_namespace_c')" class="file-link" data-id="net_core_net_namespace_c">net_namespace.c <span class="coverage-badge coverage-bad">(0.3%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('net_core_rtnetlink_c')" class="file-link" data-id="net_core_rtnetlink_c">rtnetlink.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('net_core_sock_map_c')" class="file-link" data-id="net_core_sock_map_c">sock_map.c <span class="coverage-badge coverage-bad">(0.8%)</span></a></div>
</div>
</div>
</div>
</div>
<div class="directory">
<div class="tree-toggle expanded">security/</div>
<div class="tree-child expanded">
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('security_commoncap_c')" class="file-link" data-id="security_commoncap_c">commoncap.c <span class="coverage-badge coverage-bad">(0.1%)</span></a></div>
<div class="file-entry"><a href="javascript:void(0)" onclick="showFile('security_security_c')" class="file-link" data-id="security_security_c">security.c <span class="coverage-badge coverage-bad">(0.4%)</span></a></div>
</div>
</div>
</div>
<div id="content" class="content">
<div id="welcome" class="welcome">
<h1>Coverage Report</h1>
<p>Select a file from the sidebar to view coverage details.</p>
<p>Generated with FFFuzzer coverage tool</p>
</div>
<div id="file_arch_x86_include_asm_irqflags_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_topology_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_cpu_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_sched_signal_h" class="file-content" style="display:none;"></div>
<div id="file_fs_ext4_block_validity_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_workqueue_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_kprobes_c" class="file-content" style="display:none;"></div>
<div id="file_fs_anon_inodes_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_pgtable_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mmzone_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_ldt_c" class="file-content" style="display:none;"></div>
<div id="file_include_asm-generic_pgalloc_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_swapops_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_jump_label_c" class="file-content" style="display:none;"></div>
<div id="file_fs_userfaultfd_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mempolicy_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_vmalloc_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mmu_notifier_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_disasm_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_vmstat_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_page_64_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_hashtab_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_lpm_trie_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_cpumask_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_mprog_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_sched_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_process_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_offload_c" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_pagemap_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_arena_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_memalloc_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_find_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_kmem_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_token_c" class="file-content" style="display:none;"></div>
<div id="file_lib_bsearch_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_irq_work_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_rbtree_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_fsnotify_backend_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_cpufeature_h" class="file-content" style="display:none;"></div>
<div id="file_mm_swap_c" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_ext4_h" class="file-content" style="display:none;"></div>
<div id="file_mm_vmalloc_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_list_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_time_timekeeping_c" class="file-content" style="display:none;"></div>
<div id="file_lib_errseq_c" class="file-content" style="display:none;"></div>
<div id="file_fs_namei_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_desc_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_bsearch_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_fpu_xstate_h" class="file-content" style="display:none;"></div>
<div id="file_security_commoncap_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_buffer_head_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_alternative_c" class="file-content" style="display:none;"></div>
<div id="file_security_security_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_pid_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_tcx_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_rbtree_augmented_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_cpu_mtrr_generic_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_rculist_bl_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_uaccess_h" class="file-content" style="display:none;"></div>
<div id="file_net_core_net_namespace_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_jump_label_h" class="file-content" style="display:none;"></div>
<div id="file_net_core_dev_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_file_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mm_h" class="file-content" style="display:none;"></div>
<div id="file_fs_nsfs_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_verifier_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_cgroup_cpuset_c" class="file-content" style="display:none;"></div>
<div id="file_lib_xarray_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mmap_lock_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_time_hrtimer_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_dcache_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_bpf_local_storage_c" class="file-content" style="display:none;"></div>
<div id="file_fs_ext4_inode_c" class="file-content" style="display:none;"></div>
<div id="file_fs_buffer_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_list_bl_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_bpf_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_fs_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_mm_inline_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_syscall_c" class="file-content" style="display:none;"></div>
<div id="file_lib_usercopy_c" class="file-content" style="display:none;"></div>
<div id="file_fs_select_c" class="file-content" style="display:none;"></div>
<div id="file_lib_timerqueue_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_kernel_h" class="file-content" style="display:none;"></div>
<div id="file_lib_maple_tree_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_insn_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_rwsem_h" class="file-content" style="display:none;"></div>
<div id="file_include_net_tcx_h" class="file-content" style="display:none;"></div>
<div id="file_include_asm-generic_rwonce_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_mm_pti_c" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_block_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_tsc_c" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_timer_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_cgroup_rstat_c" class="file-content" style="display:none;"></div>
<div id="file_mm_backing-dev_c" class="file-content" style="display:none;"></div>
<div id="file_fs_file_c" class="file-content" style="display:none;"></div>
<div id="file_lib_cpumask_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_cpumap_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_atomic_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_refcount_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_idr_h" class="file-content" style="display:none;"></div>
<div id="file_fs_ext4_ext4_h" class="file-content" style="display:none;"></div>
<div id="file_mm_util_c" class="file-content" style="display:none;"></div>
<div id="file_lib_sort_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_events_core_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_ringbuf_c" class="file-content" style="display:none;"></div>
<div id="file_fs_inode_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_wait_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_bpf_verifier_h" class="file-content" style="display:none;"></div>
<div id="file_drivers_char_random_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_fpu_core_c" class="file-content" style="display:none;"></div>
<div id="file_mm_pgalloc-track_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_trace_fpu_h" class="file-content" style="display:none;"></div>
<div id="file_mm_mmap_c" class="file-content" style="display:none;"></div>
<div id="file_lib_extable_c" class="file-content" style="display:none;"></div>
<div id="file_fs_mnt_idmapping_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_maple_tree_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_bpfptr_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_jump_label_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_mm_fault_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_mm_pgtable_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_sched_user_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_slab_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_sched_task_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_bpf_lru_list_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_jump_label_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_reuseport_array_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_license_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_inode_c" class="file-content" style="display:none;"></div>
<div id="file_mm_percpu-vm_c" class="file-content" style="display:none;"></div>
<div id="file_lib_bitmap_c" class="file-content" style="display:none;"></div>
<div id="file_lib_strncpy_from_user_c" class="file-content" style="display:none;"></div>
<div id="file_net_core_sock_map_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_log_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_cpu_aperfmperf_c" class="file-content" style="display:none;"></div>
<div id="file_net_core_rtnetlink_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_capability_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_kprobes_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_page_ref_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_user_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_file_ref_h" class="file-content" style="display:none;"></div>
<div id="file_lib_idr_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_fscrypt_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_text-patching_h" class="file-content" style="display:none;"></div>
<div id="file_mm_interval_tree_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_percpu_freelist_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_core_c" class="file-content" style="display:none;"></div>
<div id="file_fs_ext4_namei_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_kvmclock_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_stackmap_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_mm_physaddr_c" class="file-content" style="display:none;"></div>
<div id="file_mm_list_lru_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_atomic_atomic-arch-fallback_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_percpu_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_lib_insn_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_atomic_atomic-instrumented_h" class="file-content" style="display:none;"></div>
<div id="file_mm_mempolicy_c" class="file-content" style="display:none;"></div>
<div id="file_lib_refcount_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_percpu-rwsem_h" class="file-content" style="display:none;"></div>
<div id="file_mm_filemap_c" class="file-content" style="display:none;"></div>
<div id="file_lib_crypto_chacha_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_queue_stack_maps_c" class="file-content" style="display:none;"></div>
<div id="file_fs_namespace_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_sync_core_h" class="file-content" style="display:none;"></div>
<div id="file_fs_ext4_extents_status_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_audit_h" class="file-content" style="display:none;"></div>
<div id="file_lib_radix-tree_c" class="file-content" style="display:none;"></div>
<div id="file_block_blk-cgroup_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_smp_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_pagemap_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_delayacct_h" class="file-content" style="display:none;"></div>
<div id="file_mm_percpu_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_kthread_c" class="file-content" style="display:none;"></div>
<div id="file_mm_vma_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_devmap_c" class="file-content" style="display:none;"></div>
<div id="file_fs_dcache_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_pkru_h" class="file-content" style="display:none;"></div>
<div id="file_mm_memory_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_tnum_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_perf_event_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_fpu_xstate_h" class="file-content" style="display:none;"></div>
<div id="file_lib_vsprintf_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_pgalloc_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_mm_extable_c" class="file-content" style="display:none;"></div>
<div id="file_fs_ioctl_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_device_cgroup_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_fpu_xstate_c" class="file-content" style="display:none;"></div>
<div id="file_mm_internal_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_workqueue_h" class="file-content" style="display:none;"></div>
<div id="file_lib_crypto_sha1_c" class="file-content" style="display:none;"></div>
<div id="file_mm_rmap_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_btf_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_fuzzer_dev_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_pid_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_net_namespace_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_fpu_internal_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_sched_mm_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_pgtable_64_h" class="file-content" style="display:none;"></div>
<div id="file_mm_pgtable-generic_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_lib_inat_c" class="file-content" style="display:none;"></div>
<div id="file_lib_lockref_c" class="file-content" style="display:none;"></div>
<div id="file_fs_file_table_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_preempt_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_seqlock_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_bit_spinlock_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_rcupdate_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_xarray_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_memcontrol_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_log2_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_page-flags_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_kernel_fpu_context_h" class="file-content" style="display:none;"></div>
<div id="file_lib_percpu_counter_c" class="file-content" style="display:none;"></div>
<div id="file_fs_fcntl_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_printk_printk_safe_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_cred_h" class="file-content" style="display:none;"></div>
<div id="file_kernel_task_work_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_arraymap_c" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_entry_vsyscall_vsyscall_64_c" class="file-content" style="display:none;"></div>
<div id="file_kernel_bpf_bloom_filter_c" class="file-content" style="display:none;"></div>
<div id="file_include_linux_nodemask_h" class="file-content" style="display:none;"></div>
<div id="file_arch_x86_include_asm_mmu_context_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_csd_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_pgtable_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_jbd2_h" class="file-content" style="display:none;"></div>
<div id="file_include_trace_events_maple_tree_h" class="file-content" style="display:none;"></div>
<div id="file_include_linux_gfp_h" class="file-content" style="display:none;"></div>
<div id="file_lib_llist_c" class="file-content" style="display:none;"></div>
</div>
<script>
const fileData = {
  "arch_x86_include_asm_irqflags_h": {
    path: "arch/x86/include/asm/irqflags.h",
    covered: [97, 154, 42, 155],
    totalLines: 159,
    coveredCount: 4,
    coveragePct: 2.5,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _X86_IRQFLAGS_H_",
        "#define _X86_IRQFLAGS_H_",
        "",
        "#include <asm/processor-flags.h>",
        "",
        "#ifndef __ASSEMBLY__",
        "",
        "#include <asm/nospec-branch.h>",
        "",
        "/*",
        " * Interrupt control:",
        " */",
        "",
        "/* Declaration required for gcc < 4.9 to prevent -Werror=missing-prototypes */",
        "extern inline unsigned long native_save_fl(void);",
        "extern __always_inline unsigned long native_save_fl(void)",
        "{",
        "	unsigned long flags;",
        "",
        "	/*",
        "	 * \"=rm\" is safe here, because \"pop\" adjusts the stack before",
        "	 * it evaluates its effective address -- this is part of the",
        "	 * documented behavior of the \"pop\" instruction.",
        "	 */",
        "	asm volatile(\"# __raw_save_flags\\n\\t\"",
        "		     \"pushf ; pop %0\"",
        "		     : \"=rm\" (flags)",
        "		     : /* no input */",
        "		     : \"memory\");",
        "",
        "	return flags;",
        "}",
        "",
        "static __always_inline void native_irq_disable(void)",
        "{",
        "	asm volatile(\"cli\": : :\"memory\");",
        "}",
        "",
        "static __always_inline void native_irq_enable(void)",
        "{",
        "	asm volatile(\"sti\": : :\"memory\");",
        "}",
        "",
        "static __always_inline void native_safe_halt(void)",
        "{",
        "	mds_idle_clear_cpu_buffers();",
        "	asm volatile(\"sti; hlt\": : :\"memory\");",
        "}",
        "",
        "static __always_inline void native_halt(void)",
        "{",
        "	mds_idle_clear_cpu_buffers();",
        "	asm volatile(\"hlt\": : :\"memory\");",
        "}",
        "",
        "static __always_inline int native_irqs_disabled_flags(unsigned long flags)",
        "{",
        "	return !(flags & X86_EFLAGS_IF);",
        "}",
        "",
        "static __always_inline unsigned long native_local_irq_save(void)",
        "{",
        "	unsigned long flags = native_save_fl();",
        "",
        "	native_irq_disable();",
        "",
        "	return flags;",
        "}",
        "",
        "static __always_inline void native_local_irq_restore(unsigned long flags)",
        "{",
        "	if (!native_irqs_disabled_flags(flags))",
        "		native_irq_enable();",
        "}",
        "",
        "#endif",
        "",
        "#ifdef CONFIG_PARAVIRT_XXL",
        "#include <asm/paravirt.h>",
        "#else",
        "#ifndef __ASSEMBLY__",
        "#include <linux/types.h>",
        "",
        "static __always_inline unsigned long arch_local_save_flags(void)",
        "{",
        "	return native_save_fl();",
        "}",
        "",
        "static __always_inline void arch_local_irq_disable(void)",
        "{",
        "	native_irq_disable();",
        "}",
        "",
        "static __always_inline void arch_local_irq_enable(void)",
        "{",
        "	native_irq_enable();",
        "}",
        "",
        "/*",
        " * Used in the idle loop; sti takes one instruction cycle",
        " * to complete:",
        " */",
        "static __always_inline void arch_safe_halt(void)",
        "{",
        "	native_safe_halt();",
        "}",
        "",
        "/*",
        " * Used when interrupts are already enabled or to",
        " * shutdown the processor:",
        " */",
        "static __always_inline void halt(void)",
        "{",
        "	native_halt();",
        "}",
        "",
        "/*",
        " * For spinlocks, etc:",
        " */",
        "static __always_inline unsigned long arch_local_irq_save(void)",
        "{",
        "	unsigned long flags = arch_local_save_flags();",
        "	arch_local_irq_disable();",
        "	return flags;",
        "}",
        "#else",
        "",
        "#ifdef CONFIG_X86_64",
        "#ifdef CONFIG_DEBUG_ENTRY",
        "#define SAVE_FLAGS		pushfq; popq %rax",
        "#endif",
        "",
        "#endif",
        "",
        "#endif /* __ASSEMBLY__ */",
        "#endif /* CONFIG_PARAVIRT_XXL */",
        "",
        "#ifndef __ASSEMBLY__",
        "static __always_inline int arch_irqs_disabled_flags(unsigned long flags)",
        "{",
        "	return !(flags & X86_EFLAGS_IF);",
        "}",
        "",
        "static __always_inline int arch_irqs_disabled(void)",
        "{",
        "	unsigned long flags = arch_local_save_flags();",
        "",
        "	return arch_irqs_disabled_flags(flags);",
        "}",
        "",
        "static __always_inline void arch_local_irq_restore(unsigned long flags)",
        "{",
        "	if (!arch_irqs_disabled_flags(flags))",
        "		arch_local_irq_enable();",
        "}",
        "#endif /* !__ASSEMBLY__ */",
        "",
        "#endif"
    ]
  },
  "include_linux_topology_h": {
    path: "include/linux/topology.h",
    covered: [89],
    totalLines: 282,
    coveredCount: 1,
    coveragePct: 0.4,
    source: [
        "/*",
        " * include/linux/topology.h",
        " *",
        " * Written by: Matthew Dobson, IBM Corporation",
        " *",
        " * Copyright (C) 2002, IBM Corp.",
        " *",
        " * All rights reserved.",
        " *",
        " * This program is free software; you can redistribute it and/or modify",
        " * it under the terms of the GNU General Public License as published by",
        " * the Free Software Foundation; either version 2 of the License, or",
        " * (at your option) any later version.",
        " *",
        " * This program is distributed in the hope that it will be useful, but",
        " * WITHOUT ANY WARRANTY; without even the implied warranty of",
        " * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or",
        " * NON INFRINGEMENT.  See the GNU General Public License for more",
        " * details.",
        " *",
        " * You should have received a copy of the GNU General Public License",
        " * along with this program; if not, write to the Free Software",
        " * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.",
        " *",
        " * Send feedback to <colpatch@us.ibm.com>",
        " */",
        "#ifndef _LINUX_TOPOLOGY_H",
        "#define _LINUX_TOPOLOGY_H",
        "",
        "#include <linux/arch_topology.h>",
        "#include <linux/cpumask.h>",
        "#include <linux/bitops.h>",
        "#include <linux/mmzone.h>",
        "#include <linux/smp.h>",
        "#include <linux/percpu.h>",
        "#include <asm/topology.h>",
        "",
        "#ifndef nr_cpus_node",
        "#define nr_cpus_node(node) cpumask_weight(cpumask_of_node(node))",
        "#endif",
        "",
        "#define for_each_node_with_cpus(node)			\\",
        "	for_each_online_node(node)			\\",
        "		if (nr_cpus_node(node))",
        "",
        "int arch_update_cpu_topology(void);",
        "",
        "/* Conform to ACPI 2.0 SLIT distance definitions */",
        "#define LOCAL_DISTANCE		10",
        "#define REMOTE_DISTANCE		20",
        "#define DISTANCE_BITS           8",
        "#ifndef node_distance",
        "#define node_distance(from,to)	((from) == (to) ? LOCAL_DISTANCE : REMOTE_DISTANCE)",
        "#endif",
        "#ifndef RECLAIM_DISTANCE",
        "/*",
        " * If the distance between nodes in a system is larger than RECLAIM_DISTANCE",
        " * (in whatever arch specific measurement units returned by node_distance())",
        " * and node_reclaim_mode is enabled then the VM will only call node_reclaim()",
        " * on nodes within this distance.",
        " */",
        "#define RECLAIM_DISTANCE 30",
        "#endif",
        "",
        "/*",
        " * The following tunable allows platforms to override the default node",
        " * reclaim distance (RECLAIM_DISTANCE) if remote memory accesses are",
        " * sufficiently fast that the default value actually hurts",
        " * performance.",
        " *",
        " * AMD EPYC machines use this because even though the 2-hop distance",
        " * is 32 (3.2x slower than a local memory access) performance actually",
        " * *improves* if allowed to reclaim memory and load balance tasks",
        " * between NUMA nodes 2-hops apart.",
        " */",
        "extern int __read_mostly node_reclaim_distance;",
        "",
        "#ifndef PENALTY_FOR_NODE_WITH_CPUS",
        "#define PENALTY_FOR_NODE_WITH_CPUS	(1)",
        "#endif",
        "",
        "#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID",
        "DECLARE_PER_CPU(int, numa_node);",
        "",
        "#ifndef numa_node_id",
        "/* Returns the number of the current Node. */",
        "static inline int numa_node_id(void)",
        "{",
        "	return raw_cpu_read(numa_node);",
        "}",
        "#endif",
        "",
        "#ifndef cpu_to_node",
        "static inline int cpu_to_node(int cpu)",
        "{",
        "	return per_cpu(numa_node, cpu);",
        "}",
        "#endif",
        "",
        "#ifndef set_numa_node",
        "static inline void set_numa_node(int node)",
        "{",
        "	this_cpu_write(numa_node, node);",
        "}",
        "#endif",
        "",
        "#ifndef set_cpu_numa_node",
        "static inline void set_cpu_numa_node(int cpu, int node)",
        "{",
        "	per_cpu(numa_node, cpu) = node;",
        "}",
        "#endif",
        "",
        "#else	/* !CONFIG_USE_PERCPU_NUMA_NODE_ID */",
        "",
        "/* Returns the number of the current Node. */",
        "#ifndef numa_node_id",
        "static inline int numa_node_id(void)",
        "{",
        "	return cpu_to_node(raw_smp_processor_id());",
        "}",
        "#endif",
        "",
        "#endif	/* [!]CONFIG_USE_PERCPU_NUMA_NODE_ID */",
        "",
        "#ifdef CONFIG_HAVE_MEMORYLESS_NODES",
        "",
        "/*",
        " * N.B., Do NOT reference the '_numa_mem_' per cpu variable directly.",
        " * It will not be defined when CONFIG_HAVE_MEMORYLESS_NODES is not defined.",
        " * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem().",
        " */",
        "DECLARE_PER_CPU(int, _numa_mem_);",
        "",
        "#ifndef set_numa_mem",
        "static inline void set_numa_mem(int node)",
        "{",
        "	this_cpu_write(_numa_mem_, node);",
        "}",
        "#endif",
        "",
        "#ifndef numa_mem_id",
        "/* Returns the number of the nearest Node with memory */",
        "static inline int numa_mem_id(void)",
        "{",
        "	return raw_cpu_read(_numa_mem_);",
        "}",
        "#endif",
        "",
        "#ifndef cpu_to_mem",
        "static inline int cpu_to_mem(int cpu)",
        "{",
        "	return per_cpu(_numa_mem_, cpu);",
        "}",
        "#endif",
        "",
        "#ifndef set_cpu_numa_mem",
        "static inline void set_cpu_numa_mem(int cpu, int node)",
        "{",
        "	per_cpu(_numa_mem_, cpu) = node;",
        "}",
        "#endif",
        "",
        "#else	/* !CONFIG_HAVE_MEMORYLESS_NODES */",
        "",
        "#ifndef numa_mem_id",
        "/* Returns the number of the nearest Node with memory */",
        "static inline int numa_mem_id(void)",
        "{",
        "	return numa_node_id();",
        "}",
        "#endif",
        "",
        "#ifndef cpu_to_mem",
        "static inline int cpu_to_mem(int cpu)",
        "{",
        "	return cpu_to_node(cpu);",
        "}",
        "#endif",
        "",
        "#endif	/* [!]CONFIG_HAVE_MEMORYLESS_NODES */",
        "",
        "#if defined(topology_die_id) && defined(topology_die_cpumask)",
        "#define TOPOLOGY_DIE_SYSFS",
        "#endif",
        "#if defined(topology_cluster_id) && defined(topology_cluster_cpumask)",
        "#define TOPOLOGY_CLUSTER_SYSFS",
        "#endif",
        "#if defined(topology_book_id) && defined(topology_book_cpumask)",
        "#define TOPOLOGY_BOOK_SYSFS",
        "#endif",
        "#if defined(topology_drawer_id) && defined(topology_drawer_cpumask)",
        "#define TOPOLOGY_DRAWER_SYSFS",
        "#endif",
        "",
        "#ifndef topology_physical_package_id",
        "#define topology_physical_package_id(cpu)	((void)(cpu), -1)",
        "#endif",
        "#ifndef topology_die_id",
        "#define topology_die_id(cpu)			((void)(cpu), -1)",
        "#endif",
        "#ifndef topology_cluster_id",
        "#define topology_cluster_id(cpu)		((void)(cpu), -1)",
        "#endif",
        "#ifndef topology_core_id",
        "#define topology_core_id(cpu)			((void)(cpu), 0)",
        "#endif",
        "#ifndef topology_book_id",
        "#define topology_book_id(cpu)			((void)(cpu), -1)",
        "#endif",
        "#ifndef topology_drawer_id",
        "#define topology_drawer_id(cpu)			((void)(cpu), -1)",
        "#endif",
        "#ifndef topology_ppin",
        "#define topology_ppin(cpu)			((void)(cpu), 0ull)",
        "#endif",
        "#ifndef topology_sibling_cpumask",
        "#define topology_sibling_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "#ifndef topology_core_cpumask",
        "#define topology_core_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "#ifndef topology_cluster_cpumask",
        "#define topology_cluster_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "#ifndef topology_die_cpumask",
        "#define topology_die_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "#ifndef topology_book_cpumask",
        "#define topology_book_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "#ifndef topology_drawer_cpumask",
        "#define topology_drawer_cpumask(cpu)		cpumask_of(cpu)",
        "#endif",
        "",
        "#if defined(CONFIG_SCHED_SMT) && !defined(cpu_smt_mask)",
        "static inline const struct cpumask *cpu_smt_mask(int cpu)",
        "{",
        "	return topology_sibling_cpumask(cpu);",
        "}",
        "#endif",
        "",
        "static inline const struct cpumask *cpu_cpu_mask(int cpu)",
        "{",
        "	return cpumask_of_node(cpu_to_node(cpu));",
        "}",
        "",
        "#ifdef CONFIG_NUMA",
        "int sched_numa_find_nth_cpu(const struct cpumask *cpus, int cpu, int node);",
        "extern const struct cpumask *sched_numa_hop_mask(unsigned int node, unsigned int hops);",
        "#else",
        "static __always_inline int sched_numa_find_nth_cpu(const struct cpumask *cpus, int cpu, int node)",
        "{",
        "	return cpumask_nth_and(cpu, cpus, cpu_online_mask);",
        "}",
        "",
        "static inline const struct cpumask *",
        "sched_numa_hop_mask(unsigned int node, unsigned int hops)",
        "{",
        "	return ERR_PTR(-EOPNOTSUPP);",
        "}",
        "#endif	/* CONFIG_NUMA */",
        "",
        "/**",
        " * for_each_numa_hop_mask - iterate over cpumasks of increasing NUMA distance",
        " *                          from a given node.",
        " * @mask: the iteration variable.",
        " * @node: the NUMA node to start the search from.",
        " *",
        " * Requires rcu_lock to be held.",
        " *",
        " * Yields cpu_online_mask for @node == NUMA_NO_NODE.",
        " */",
        "#define for_each_numa_hop_mask(mask, node)				       \\",
        "	for (unsigned int __hops = 0;					       \\",
        "	     mask = (node != NUMA_NO_NODE || __hops) ?			       \\",
        "		     sched_numa_hop_mask(node, __hops) :		       \\",
        "		     cpu_online_mask,					       \\",
        "	     !IS_ERR_OR_NULL(mask);					       \\",
        "	     __hops++)",
        "",
        "#endif /* _LINUX_TOPOLOGY_H */"
    ]
  },
  "kernel_cpu_c": {
    path: "kernel/cpu.c",
    covered: [3218, 490, 489, 501, 502, 517],
    totalLines: 3236,
    coveredCount: 6,
    coveragePct: 0.2,
    source: [
        "/* CPU control.",
        " * (C) 2001, 2002, 2003, 2004 Rusty Russell",
        " *",
        " * This code is licenced under the GPL.",
        " */",
        "#include <linux/sched/mm.h>",
        "#include <linux/proc_fs.h>",
        "#include <linux/smp.h>",
        "#include <linux/init.h>",
        "#include <linux/notifier.h>",
        "#include <linux/sched/signal.h>",
        "#include <linux/sched/hotplug.h>",
        "#include <linux/sched/isolation.h>",
        "#include <linux/sched/task.h>",
        "#include <linux/sched/smt.h>",
        "#include <linux/unistd.h>",
        "#include <linux/cpu.h>",
        "#include <linux/oom.h>",
        "#include <linux/rcupdate.h>",
        "#include <linux/delay.h>",
        "#include <linux/export.h>",
        "#include <linux/bug.h>",
        "#include <linux/kthread.h>",
        "#include <linux/stop_machine.h>",
        "#include <linux/mutex.h>",
        "#include <linux/gfp.h>",
        "#include <linux/suspend.h>",
        "#include <linux/lockdep.h>",
        "#include <linux/tick.h>",
        "#include <linux/irq.h>",
        "#include <linux/nmi.h>",
        "#include <linux/smpboot.h>",
        "#include <linux/relay.h>",
        "#include <linux/slab.h>",
        "#include <linux/scs.h>",
        "#include <linux/percpu-rwsem.h>",
        "#include <linux/cpuset.h>",
        "#include <linux/random.h>",
        "#include <linux/cc_platform.h>",
        "",
        "#include <trace/events/power.h>",
        "#define CREATE_TRACE_POINTS",
        "#include <trace/events/cpuhp.h>",
        "",
        "#include \"smpboot.h\"",
        "",
        "/**",
        " * struct cpuhp_cpu_state - Per cpu hotplug state storage",
        " * @state:	The current cpu state",
        " * @target:	The target state",
        " * @fail:	Current CPU hotplug callback state",
        " * @thread:	Pointer to the hotplug thread",
        " * @should_run:	Thread should execute",
        " * @rollback:	Perform a rollback",
        " * @single:	Single callback invocation",
        " * @bringup:	Single callback bringup or teardown selector",
        " * @node:	Remote CPU node; for multi-instance, do a",
        " *		single entry callback for install/remove",
        " * @last:	For multi-instance rollback, remember how far we got",
        " * @cb_state:	The state for a single callback (install/uninstall)",
        " * @result:	Result of the operation",
        " * @ap_sync_state:	State for AP synchronization",
        " * @done_up:	Signal completion to the issuer of the task for cpu-up",
        " * @done_down:	Signal completion to the issuer of the task for cpu-down",
        " */",
        "struct cpuhp_cpu_state {",
        "	enum cpuhp_state	state;",
        "	enum cpuhp_state	target;",
        "	enum cpuhp_state	fail;",
        "#ifdef CONFIG_SMP",
        "	struct task_struct	*thread;",
        "	bool			should_run;",
        "	bool			rollback;",
        "	bool			single;",
        "	bool			bringup;",
        "	struct hlist_node	*node;",
        "	struct hlist_node	*last;",
        "	enum cpuhp_state	cb_state;",
        "	int			result;",
        "	atomic_t		ap_sync_state;",
        "	struct completion	done_up;",
        "	struct completion	done_down;",
        "#endif",
        "};",
        "",
        "static DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state) = {",
        "	.fail = CPUHP_INVALID,",
        "};",
        "",
        "#ifdef CONFIG_SMP",
        "cpumask_t cpus_booted_once_mask;",
        "#endif",
        "",
        "#if defined(CONFIG_LOCKDEP) && defined(CONFIG_SMP)",
        "static struct lockdep_map cpuhp_state_up_map =",
        "	STATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-up\", &cpuhp_state_up_map);",
        "static struct lockdep_map cpuhp_state_down_map =",
        "	STATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-down\", &cpuhp_state_down_map);",
        "",
        "",
        "static inline void cpuhp_lock_acquire(bool bringup)",
        "{",
        "	lock_map_acquire(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);",
        "}",
        "",
        "static inline void cpuhp_lock_release(bool bringup)",
        "{",
        "	lock_map_release(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);",
        "}",
        "#else",
        "",
        "static inline void cpuhp_lock_acquire(bool bringup) { }",
        "static inline void cpuhp_lock_release(bool bringup) { }",
        "",
        "#endif",
        "",
        "/**",
        " * struct cpuhp_step - Hotplug state machine step",
        " * @name:	Name of the step",
        " * @startup:	Startup function of the step",
        " * @teardown:	Teardown function of the step",
        " * @cant_stop:	Bringup/teardown can't be stopped at this step",
        " * @multi_instance:	State has multiple instances which get added afterwards",
        " */",
        "struct cpuhp_step {",
        "	const char		*name;",
        "	union {",
        "		int		(*single)(unsigned int cpu);",
        "		int		(*multi)(unsigned int cpu,",
        "					 struct hlist_node *node);",
        "	} startup;",
        "	union {",
        "		int		(*single)(unsigned int cpu);",
        "		int		(*multi)(unsigned int cpu,",
        "					 struct hlist_node *node);",
        "	} teardown;",
        "	/* private: */",
        "	struct hlist_head	list;",
        "	/* public: */",
        "	bool			cant_stop;",
        "	bool			multi_instance;",
        "};",
        "",
        "static DEFINE_MUTEX(cpuhp_state_mutex);",
        "static struct cpuhp_step cpuhp_hp_states[];",
        "",
        "static struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)",
        "{",
        "	return cpuhp_hp_states + state;",
        "}",
        "",
        "static bool cpuhp_step_empty(bool bringup, struct cpuhp_step *step)",
        "{",
        "	return bringup ? !step->startup.single : !step->teardown.single;",
        "}",
        "",
        "/**",
        " * cpuhp_invoke_callback - Invoke the callbacks for a given state",
        " * @cpu:	The cpu for which the callback should be invoked",
        " * @state:	The state to do callbacks for",
        " * @bringup:	True if the bringup callback should be invoked",
        " * @node:	For multi-instance, do a single entry callback for install/remove",
        " * @lastp:	For multi-instance rollback, remember how far we got",
        " *",
        " * Called from cpu hotplug and from the state register machinery.",
        " *",
        " * Return: %0 on success or a negative errno code",
        " */",
        "static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,",
        "				 bool bringup, struct hlist_node *node,",
        "				 struct hlist_node **lastp)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	struct cpuhp_step *step = cpuhp_get_step(state);",
        "	int (*cbm)(unsigned int cpu, struct hlist_node *node);",
        "	int (*cb)(unsigned int cpu);",
        "	int ret, cnt;",
        "",
        "	if (st->fail == state) {",
        "		st->fail = CPUHP_INVALID;",
        "		return -EAGAIN;",
        "	}",
        "",
        "	if (cpuhp_step_empty(bringup, step)) {",
        "		WARN_ON_ONCE(1);",
        "		return 0;",
        "	}",
        "",
        "	if (!step->multi_instance) {",
        "		WARN_ON_ONCE(lastp && *lastp);",
        "		cb = bringup ? step->startup.single : step->teardown.single;",
        "",
        "		trace_cpuhp_enter(cpu, st->target, state, cb);",
        "		ret = cb(cpu);",
        "		trace_cpuhp_exit(cpu, st->state, state, ret);",
        "		return ret;",
        "	}",
        "	cbm = bringup ? step->startup.multi : step->teardown.multi;",
        "",
        "	/* Single invocation for instance add/remove */",
        "	if (node) {",
        "		WARN_ON_ONCE(lastp && *lastp);",
        "		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);",
        "		ret = cbm(cpu, node);",
        "		trace_cpuhp_exit(cpu, st->state, state, ret);",
        "		return ret;",
        "	}",
        "",
        "	/* State transition. Invoke on all instances */",
        "	cnt = 0;",
        "	hlist_for_each(node, &step->list) {",
        "		if (lastp && node == *lastp)",
        "			break;",
        "",
        "		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);",
        "		ret = cbm(cpu, node);",
        "		trace_cpuhp_exit(cpu, st->state, state, ret);",
        "		if (ret) {",
        "			if (!lastp)",
        "				goto err;",
        "",
        "			*lastp = node;",
        "			return ret;",
        "		}",
        "		cnt++;",
        "	}",
        "	if (lastp)",
        "		*lastp = NULL;",
        "	return 0;",
        "err:",
        "	/* Rollback the instances if one failed */",
        "	cbm = !bringup ? step->startup.multi : step->teardown.multi;",
        "	if (!cbm)",
        "		return ret;",
        "",
        "	hlist_for_each(node, &step->list) {",
        "		if (!cnt--)",
        "			break;",
        "",
        "		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);",
        "		ret = cbm(cpu, node);",
        "		trace_cpuhp_exit(cpu, st->state, state, ret);",
        "		/*",
        "		 * Rollback must not fail,",
        "		 */",
        "		WARN_ON_ONCE(ret);",
        "	}",
        "	return ret;",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "static bool cpuhp_is_ap_state(enum cpuhp_state state)",
        "{",
        "	/*",
        "	 * The extra check for CPUHP_TEARDOWN_CPU is only for documentation",
        "	 * purposes as that state is handled explicitly in cpu_down.",
        "	 */",
        "	return state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;",
        "}",
        "",
        "static inline void wait_for_ap_thread(struct cpuhp_cpu_state *st, bool bringup)",
        "{",
        "	struct completion *done = bringup ? &st->done_up : &st->done_down;",
        "	wait_for_completion(done);",
        "}",
        "",
        "static inline void complete_ap_thread(struct cpuhp_cpu_state *st, bool bringup)",
        "{",
        "	struct completion *done = bringup ? &st->done_up : &st->done_down;",
        "	complete(done);",
        "}",
        "",
        "/*",
        " * The former STARTING/DYING states, ran with IRQs disabled and must not fail.",
        " */",
        "static bool cpuhp_is_atomic_state(enum cpuhp_state state)",
        "{",
        "	return CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;",
        "}",
        "",
        "/* Synchronization state management */",
        "enum cpuhp_sync_state {",
        "	SYNC_STATE_DEAD,",
        "	SYNC_STATE_KICKED,",
        "	SYNC_STATE_SHOULD_DIE,",
        "	SYNC_STATE_ALIVE,",
        "	SYNC_STATE_SHOULD_ONLINE,",
        "	SYNC_STATE_ONLINE,",
        "};",
        "",
        "#ifdef CONFIG_HOTPLUG_CORE_SYNC",
        "/**",
        " * cpuhp_ap_update_sync_state - Update synchronization state during bringup/teardown",
        " * @state:	The synchronization state to set",
        " *",
        " * No synchronization point. Just update of the synchronization state, but implies",
        " * a full barrier so that the AP changes are visible before the control CPU proceeds.",
        " */",
        "static inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)",
        "{",
        "	atomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);",
        "",
        "	(void)atomic_xchg(st, state);",
        "}",
        "",
        "void __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }",
        "",
        "static bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,",
        "				      enum cpuhp_sync_state next_state)",
        "{",
        "	atomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);",
        "	ktime_t now, end, start = ktime_get();",
        "	int sync;",
        "",
        "	end = start + 10ULL * NSEC_PER_SEC;",
        "",
        "	sync = atomic_read(st);",
        "	while (1) {",
        "		if (sync == state) {",
        "			if (!atomic_try_cmpxchg(st, &sync, next_state))",
        "				continue;",
        "			return true;",
        "		}",
        "",
        "		now = ktime_get();",
        "		if (now > end) {",
        "			/* Timeout. Leave the state unchanged */",
        "			return false;",
        "		} else if (now - start < NSEC_PER_MSEC) {",
        "			/* Poll for one millisecond */",
        "			arch_cpuhp_sync_state_poll();",
        "		} else {",
        "			usleep_range(USEC_PER_MSEC, 2 * USEC_PER_MSEC);",
        "		}",
        "		sync = atomic_read(st);",
        "	}",
        "	return true;",
        "}",
        "#else  /* CONFIG_HOTPLUG_CORE_SYNC */",
        "static inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state) { }",
        "#endif /* !CONFIG_HOTPLUG_CORE_SYNC */",
        "",
        "#ifdef CONFIG_HOTPLUG_CORE_SYNC_DEAD",
        "/**",
        " * cpuhp_ap_report_dead - Update synchronization state to DEAD",
        " *",
        " * No synchronization point. Just update of the synchronization state.",
        " */",
        "void cpuhp_ap_report_dead(void)",
        "{",
        "	cpuhp_ap_update_sync_state(SYNC_STATE_DEAD);",
        "}",
        "",
        "void __weak arch_cpuhp_cleanup_dead_cpu(unsigned int cpu) { }",
        "",
        "/*",
        " * Late CPU shutdown synchronization point. Cannot use cpuhp_state::done_down",
        " * because the AP cannot issue complete() at this stage.",
        " */",
        "static void cpuhp_bp_sync_dead(unsigned int cpu)",
        "{",
        "	atomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);",
        "	int sync = atomic_read(st);",
        "",
        "	do {",
        "		/* CPU can have reported dead already. Don't overwrite that! */",
        "		if (sync == SYNC_STATE_DEAD)",
        "			break;",
        "	} while (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_SHOULD_DIE));",
        "",
        "	if (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {",
        "		/* CPU reached dead state. Invoke the cleanup function */",
        "		arch_cpuhp_cleanup_dead_cpu(cpu);",
        "		return;",
        "	}",
        "",
        "	/* No further action possible. Emit message and give up. */",
        "	pr_err(\"CPU%u failed to report dead state\\n\", cpu);",
        "}",
        "#else /* CONFIG_HOTPLUG_CORE_SYNC_DEAD */",
        "static inline void cpuhp_bp_sync_dead(unsigned int cpu) { }",
        "#endif /* !CONFIG_HOTPLUG_CORE_SYNC_DEAD */",
        "",
        "#ifdef CONFIG_HOTPLUG_CORE_SYNC_FULL",
        "/**",
        " * cpuhp_ap_sync_alive - Synchronize AP with the control CPU once it is alive",
        " *",
        " * Updates the AP synchronization state to SYNC_STATE_ALIVE and waits",
        " * for the BP to release it.",
        " */",
        "void cpuhp_ap_sync_alive(void)",
        "{",
        "	atomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);",
        "",
        "	cpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);",
        "",
        "	/* Wait for the control CPU to release it. */",
        "	while (atomic_read(st) != SYNC_STATE_SHOULD_ONLINE)",
        "		cpu_relax();",
        "}",
        "",
        "static bool cpuhp_can_boot_ap(unsigned int cpu)",
        "{",
        "	atomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);",
        "	int sync = atomic_read(st);",
        "",
        "again:",
        "	switch (sync) {",
        "	case SYNC_STATE_DEAD:",
        "		/* CPU is properly dead */",
        "		break;",
        "	case SYNC_STATE_KICKED:",
        "		/* CPU did not come up in previous attempt */",
        "		break;",
        "	case SYNC_STATE_ALIVE:",
        "		/* CPU is stuck cpuhp_ap_sync_alive(). */",
        "		break;",
        "	default:",
        "		/* CPU failed to report online or dead and is in limbo state. */",
        "		return false;",
        "	}",
        "",
        "	/* Prepare for booting */",
        "	if (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_KICKED))",
        "		goto again;",
        "",
        "	return true;",
        "}",
        "",
        "void __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }",
        "",
        "/*",
        " * Early CPU bringup synchronization point. Cannot use cpuhp_state::done_up",
        " * because the AP cannot issue complete() so early in the bringup.",
        " */",
        "static int cpuhp_bp_sync_alive(unsigned int cpu)",
        "{",
        "	int ret = 0;",
        "",
        "	if (!IS_ENABLED(CONFIG_HOTPLUG_CORE_SYNC_FULL))",
        "		return 0;",
        "",
        "	if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {",
        "		pr_err(\"CPU%u failed to report alive state\\n\", cpu);",
        "		ret = -EIO;",
        "	}",
        "",
        "	/* Let the architecture cleanup the kick alive mechanics. */",
        "	arch_cpuhp_cleanup_kick_cpu(cpu);",
        "	return ret;",
        "}",
        "#else /* CONFIG_HOTPLUG_CORE_SYNC_FULL */",
        "static inline int cpuhp_bp_sync_alive(unsigned int cpu) { return 0; }",
        "static inline bool cpuhp_can_boot_ap(unsigned int cpu) { return true; }",
        "#endif /* !CONFIG_HOTPLUG_CORE_SYNC_FULL */",
        "",
        "/* Serializes the updates to cpu_online_mask, cpu_present_mask */",
        "static DEFINE_MUTEX(cpu_add_remove_lock);",
        "bool cpuhp_tasks_frozen;",
        "EXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);",
        "",
        "/*",
        " * The following two APIs (cpu_maps_update_begin/done) must be used when",
        " * attempting to serialize the updates to cpu_online_mask & cpu_present_mask.",
        " */",
        "void cpu_maps_update_begin(void)",
        "{",
        "	mutex_lock(&cpu_add_remove_lock);",
        "}",
        "",
        "void cpu_maps_update_done(void)",
        "{",
        "	mutex_unlock(&cpu_add_remove_lock);",
        "}",
        "",
        "/*",
        " * If set, cpu_up and cpu_down will return -EBUSY and do nothing.",
        " * Should always be manipulated under cpu_add_remove_lock",
        " */",
        "static int cpu_hotplug_disabled;",
        "",
        "#ifdef CONFIG_HOTPLUG_CPU",
        "",
        "DEFINE_STATIC_PERCPU_RWSEM(cpu_hotplug_lock);",
        "",
        "static bool cpu_hotplug_offline_disabled __ro_after_init;",
        "",
        "void cpus_read_lock(void)",
        "{",
        "	percpu_down_read(&cpu_hotplug_lock);",
        "}",
        "EXPORT_SYMBOL_GPL(cpus_read_lock);",
        "",
        "int cpus_read_trylock(void)",
        "{",
        "	return percpu_down_read_trylock(&cpu_hotplug_lock);",
        "}",
        "EXPORT_SYMBOL_GPL(cpus_read_trylock);",
        "",
        "void cpus_read_unlock(void)",
        "{",
        "	percpu_up_read(&cpu_hotplug_lock);",
        "}",
        "EXPORT_SYMBOL_GPL(cpus_read_unlock);",
        "",
        "void cpus_write_lock(void)",
        "{",
        "	percpu_down_write(&cpu_hotplug_lock);",
        "}",
        "",
        "void cpus_write_unlock(void)",
        "{",
        "	percpu_up_write(&cpu_hotplug_lock);",
        "}",
        "",
        "void lockdep_assert_cpus_held(void)",
        "{",
        "	/*",
        "	 * We can't have hotplug operations before userspace starts running,",
        "	 * and some init codepaths will knowingly not take the hotplug lock.",
        "	 * This is all valid, so mute lockdep until it makes sense to report",
        "	 * unheld locks.",
        "	 */",
        "	if (system_state < SYSTEM_RUNNING)",
        "		return;",
        "",
        "	percpu_rwsem_assert_held(&cpu_hotplug_lock);",
        "}",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "int lockdep_is_cpus_held(void)",
        "{",
        "	return percpu_rwsem_is_held(&cpu_hotplug_lock);",
        "}",
        "#endif",
        "",
        "static void lockdep_acquire_cpus_lock(void)",
        "{",
        "	rwsem_acquire(&cpu_hotplug_lock.dep_map, 0, 0, _THIS_IP_);",
        "}",
        "",
        "static void lockdep_release_cpus_lock(void)",
        "{",
        "	rwsem_release(&cpu_hotplug_lock.dep_map, _THIS_IP_);",
        "}",
        "",
        "/* Declare CPU offlining not supported */",
        "void cpu_hotplug_disable_offlining(void)",
        "{",
        "	cpu_maps_update_begin();",
        "	cpu_hotplug_offline_disabled = true;",
        "	cpu_maps_update_done();",
        "}",
        "",
        "/*",
        " * Wait for currently running CPU hotplug operations to complete (if any) and",
        " * disable future CPU hotplug (from sysfs). The 'cpu_add_remove_lock' protects",
        " * the 'cpu_hotplug_disabled' flag. The same lock is also acquired by the",
        " * hotplug path before performing hotplug operations. So acquiring that lock",
        " * guarantees mutual exclusion from any currently running hotplug operations.",
        " */",
        "void cpu_hotplug_disable(void)",
        "{",
        "	cpu_maps_update_begin();",
        "	cpu_hotplug_disabled++;",
        "	cpu_maps_update_done();",
        "}",
        "EXPORT_SYMBOL_GPL(cpu_hotplug_disable);",
        "",
        "static void __cpu_hotplug_enable(void)",
        "{",
        "	if (WARN_ONCE(!cpu_hotplug_disabled, \"Unbalanced cpu hotplug enable\\n\"))",
        "		return;",
        "	cpu_hotplug_disabled--;",
        "}",
        "",
        "void cpu_hotplug_enable(void)",
        "{",
        "	cpu_maps_update_begin();",
        "	__cpu_hotplug_enable();",
        "	cpu_maps_update_done();",
        "}",
        "EXPORT_SYMBOL_GPL(cpu_hotplug_enable);",
        "",
        "#else",
        "",
        "static void lockdep_acquire_cpus_lock(void)",
        "{",
        "}",
        "",
        "static void lockdep_release_cpus_lock(void)",
        "{",
        "}",
        "",
        "#endif	/* CONFIG_HOTPLUG_CPU */",
        "",
        "/*",
        " * Architectures that need SMT-specific errata handling during SMT hotplug",
        " * should override this.",
        " */",
        "void __weak arch_smt_update(void) { }",
        "",
        "#ifdef CONFIG_HOTPLUG_SMT",
        "",
        "enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;",
        "static unsigned int cpu_smt_max_threads __ro_after_init;",
        "unsigned int cpu_smt_num_threads __read_mostly = UINT_MAX;",
        "",
        "void __init cpu_smt_disable(bool force)",
        "{",
        "	if (!cpu_smt_possible())",
        "		return;",
        "",
        "	if (force) {",
        "		pr_info(\"SMT: Force disabled\\n\");",
        "		cpu_smt_control = CPU_SMT_FORCE_DISABLED;",
        "	} else {",
        "		pr_info(\"SMT: disabled\\n\");",
        "		cpu_smt_control = CPU_SMT_DISABLED;",
        "	}",
        "	cpu_smt_num_threads = 1;",
        "}",
        "",
        "/*",
        " * The decision whether SMT is supported can only be done after the full",
        " * CPU identification. Called from architecture code.",
        " */",
        "void __init cpu_smt_set_num_threads(unsigned int num_threads,",
        "				    unsigned int max_threads)",
        "{",
        "	WARN_ON(!num_threads || (num_threads > max_threads));",
        "",
        "	if (max_threads == 1)",
        "		cpu_smt_control = CPU_SMT_NOT_SUPPORTED;",
        "",
        "	cpu_smt_max_threads = max_threads;",
        "",
        "	/*",
        "	 * If SMT has been disabled via the kernel command line or SMT is",
        "	 * not supported, set cpu_smt_num_threads to 1 for consistency.",
        "	 * If enabled, take the architecture requested number of threads",
        "	 * to bring up into account.",
        "	 */",
        "	if (cpu_smt_control != CPU_SMT_ENABLED)",
        "		cpu_smt_num_threads = 1;",
        "	else if (num_threads < cpu_smt_num_threads)",
        "		cpu_smt_num_threads = num_threads;",
        "}",
        "",
        "static int __init smt_cmdline_disable(char *str)",
        "{",
        "	cpu_smt_disable(str && !strcmp(str, \"force\"));",
        "	return 0;",
        "}",
        "early_param(\"nosmt\", smt_cmdline_disable);",
        "",
        "/*",
        " * For Archicture supporting partial SMT states check if the thread is allowed.",
        " * Otherwise this has already been checked through cpu_smt_max_threads when",
        " * setting the SMT level.",
        " */",
        "static inline bool cpu_smt_thread_allowed(unsigned int cpu)",
        "{",
        "#ifdef CONFIG_SMT_NUM_THREADS_DYNAMIC",
        "	return topology_smt_thread_allowed(cpu);",
        "#else",
        "	return true;",
        "#endif",
        "}",
        "",
        "static inline bool cpu_bootable(unsigned int cpu)",
        "{",
        "	if (cpu_smt_control == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))",
        "		return true;",
        "",
        "	/* All CPUs are bootable if controls are not configured */",
        "	if (cpu_smt_control == CPU_SMT_NOT_IMPLEMENTED)",
        "		return true;",
        "",
        "	/* All CPUs are bootable if CPU is not SMT capable */",
        "	if (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)",
        "		return true;",
        "",
        "	if (topology_is_primary_thread(cpu))",
        "		return true;",
        "",
        "	/*",
        "	 * On x86 it's required to boot all logical CPUs at least once so",
        "	 * that the init code can get a chance to set CR4.MCE on each",
        "	 * CPU. Otherwise, a broadcasted MCE observing CR4.MCE=0b on any",
        "	 * core will shutdown the machine.",
        "	 */",
        "	return !cpumask_test_cpu(cpu, &cpus_booted_once_mask);",
        "}",
        "",
        "/* Returns true if SMT is supported and not forcefully (irreversibly) disabled */",
        "bool cpu_smt_possible(void)",
        "{",
        "	return cpu_smt_control != CPU_SMT_FORCE_DISABLED &&",
        "		cpu_smt_control != CPU_SMT_NOT_SUPPORTED;",
        "}",
        "EXPORT_SYMBOL_GPL(cpu_smt_possible);",
        "",
        "#else",
        "static inline bool cpu_bootable(unsigned int cpu) { return true; }",
        "#endif",
        "",
        "static inline enum cpuhp_state",
        "cpuhp_set_state(int cpu, struct cpuhp_cpu_state *st, enum cpuhp_state target)",
        "{",
        "	enum cpuhp_state prev_state = st->state;",
        "	bool bringup = st->state < target;",
        "",
        "	st->rollback = false;",
        "	st->last = NULL;",
        "",
        "	st->target = target;",
        "	st->single = false;",
        "	st->bringup = bringup;",
        "	if (cpu_dying(cpu) != !bringup)",
        "		set_cpu_dying(cpu, !bringup);",
        "",
        "	return prev_state;",
        "}",
        "",
        "static inline void",
        "cpuhp_reset_state(int cpu, struct cpuhp_cpu_state *st,",
        "		  enum cpuhp_state prev_state)",
        "{",
        "	bool bringup = !st->bringup;",
        "",
        "	st->target = prev_state;",
        "",
        "	/*",
        "	 * Already rolling back. No need invert the bringup value or to change",
        "	 * the current state.",
        "	 */",
        "	if (st->rollback)",
        "		return;",
        "",
        "	st->rollback = true;",
        "",
        "	/*",
        "	 * If we have st->last we need to undo partial multi_instance of this",
        "	 * state first. Otherwise start undo at the previous state.",
        "	 */",
        "	if (!st->last) {",
        "		if (st->bringup)",
        "			st->state--;",
        "		else",
        "			st->state++;",
        "	}",
        "",
        "	st->bringup = bringup;",
        "	if (cpu_dying(cpu) != !bringup)",
        "		set_cpu_dying(cpu, !bringup);",
        "}",
        "",
        "/* Regular hotplug invocation of the AP hotplug thread */",
        "static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)",
        "{",
        "	if (!st->single && st->state == st->target)",
        "		return;",
        "",
        "	st->result = 0;",
        "	/*",
        "	 * Make sure the above stores are visible before should_run becomes",
        "	 * true. Paired with the mb() above in cpuhp_thread_fun()",
        "	 */",
        "	smp_mb();",
        "	st->should_run = true;",
        "	wake_up_process(st->thread);",
        "	wait_for_ap_thread(st, st->bringup);",
        "}",
        "",
        "static int cpuhp_kick_ap(int cpu, struct cpuhp_cpu_state *st,",
        "			 enum cpuhp_state target)",
        "{",
        "	enum cpuhp_state prev_state;",
        "	int ret;",
        "",
        "	prev_state = cpuhp_set_state(cpu, st, target);",
        "	__cpuhp_kick_ap(st);",
        "	if ((ret = st->result)) {",
        "		cpuhp_reset_state(cpu, st, prev_state);",
        "		__cpuhp_kick_ap(st);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "static int bringup_wait_for_ap_online(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "",
        "	/* Wait for the CPU to reach CPUHP_AP_ONLINE_IDLE */",
        "	wait_for_ap_thread(st, true);",
        "	if (WARN_ON_ONCE((!cpu_online(cpu))))",
        "		return -ECANCELED;",
        "",
        "	/* Unpark the hotplug thread of the target cpu */",
        "	kthread_unpark(st->thread);",
        "",
        "	/*",
        "	 * SMT soft disabling on X86 requires to bring the CPU out of the",
        "	 * BIOS 'wait for SIPI' state in order to set the CR4.MCE bit.  The",
        "	 * CPU marked itself as booted_once in notify_cpu_starting() so the",
        "	 * cpu_bootable() check will now return false if this is not the",
        "	 * primary sibling.",
        "	 */",
        "	if (!cpu_bootable(cpu))",
        "		return -ECANCELED;",
        "	return 0;",
        "}",
        "",
        "#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP",
        "static int cpuhp_kick_ap_alive(unsigned int cpu)",
        "{",
        "	if (!cpuhp_can_boot_ap(cpu))",
        "		return -EAGAIN;",
        "",
        "	return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));",
        "}",
        "",
        "static int cpuhp_bringup_ap(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	int ret;",
        "",
        "	/*",
        "	 * Some architectures have to walk the irq descriptors to",
        "	 * setup the vector space for the cpu which comes online.",
        "	 * Prevent irq alloc/free across the bringup.",
        "	 */",
        "	irq_lock_sparse();",
        "",
        "	ret = cpuhp_bp_sync_alive(cpu);",
        "	if (ret)",
        "		goto out_unlock;",
        "",
        "	ret = bringup_wait_for_ap_online(cpu);",
        "	if (ret)",
        "		goto out_unlock;",
        "",
        "	irq_unlock_sparse();",
        "",
        "	if (st->target <= CPUHP_AP_ONLINE_IDLE)",
        "		return 0;",
        "",
        "	return cpuhp_kick_ap(cpu, st, st->target);",
        "",
        "out_unlock:",
        "	irq_unlock_sparse();",
        "	return ret;",
        "}",
        "#else",
        "static int bringup_cpu(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	struct task_struct *idle = idle_thread_get(cpu);",
        "	int ret;",
        "",
        "	if (!cpuhp_can_boot_ap(cpu))",
        "		return -EAGAIN;",
        "",
        "	/*",
        "	 * Some architectures have to walk the irq descriptors to",
        "	 * setup the vector space for the cpu which comes online.",
        "	 *",
        "	 * Prevent irq alloc/free across the bringup by acquiring the",
        "	 * sparse irq lock. Hold it until the upcoming CPU completes the",
        "	 * startup in cpuhp_online_idle() which allows to avoid",
        "	 * intermediate synchronization points in the architecture code.",
        "	 */",
        "	irq_lock_sparse();",
        "",
        "	ret = __cpu_up(cpu, idle);",
        "	if (ret)",
        "		goto out_unlock;",
        "",
        "	ret = cpuhp_bp_sync_alive(cpu);",
        "	if (ret)",
        "		goto out_unlock;",
        "",
        "	ret = bringup_wait_for_ap_online(cpu);",
        "	if (ret)",
        "		goto out_unlock;",
        "",
        "	irq_unlock_sparse();",
        "",
        "	if (st->target <= CPUHP_AP_ONLINE_IDLE)",
        "		return 0;",
        "",
        "	return cpuhp_kick_ap(cpu, st, st->target);",
        "",
        "out_unlock:",
        "	irq_unlock_sparse();",
        "	return ret;",
        "}",
        "#endif",
        "",
        "static int finish_cpu(unsigned int cpu)",
        "{",
        "	struct task_struct *idle = idle_thread_get(cpu);",
        "	struct mm_struct *mm = idle->active_mm;",
        "",
        "	/*",
        "	 * idle_task_exit() will have switched to &init_mm, now",
        "	 * clean up any remaining active_mm state.",
        "	 */",
        "	if (mm != &init_mm)",
        "		idle->active_mm = &init_mm;",
        "	mmdrop_lazy_tlb(mm);",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Hotplug state machine related functions",
        " */",
        "",
        "/*",
        " * Get the next state to run. Empty ones will be skipped. Returns true if a",
        " * state must be run.",
        " *",
        " * st->state will be modified ahead of time, to match state_to_run, as if it",
        " * has already ran.",
        " */",
        "static bool cpuhp_next_state(bool bringup,",
        "			     enum cpuhp_state *state_to_run,",
        "			     struct cpuhp_cpu_state *st,",
        "			     enum cpuhp_state target)",
        "{",
        "	do {",
        "		if (bringup) {",
        "			if (st->state >= target)",
        "				return false;",
        "",
        "			*state_to_run = ++st->state;",
        "		} else {",
        "			if (st->state <= target)",
        "				return false;",
        "",
        "			*state_to_run = st->state--;",
        "		}",
        "",
        "		if (!cpuhp_step_empty(bringup, cpuhp_get_step(*state_to_run)))",
        "			break;",
        "	} while (true);",
        "",
        "	return true;",
        "}",
        "",
        "static int __cpuhp_invoke_callback_range(bool bringup,",
        "					 unsigned int cpu,",
        "					 struct cpuhp_cpu_state *st,",
        "					 enum cpuhp_state target,",
        "					 bool nofail)",
        "{",
        "	enum cpuhp_state state;",
        "	int ret = 0;",
        "",
        "	while (cpuhp_next_state(bringup, &state, st, target)) {",
        "		int err;",
        "",
        "		err = cpuhp_invoke_callback(cpu, state, bringup, NULL, NULL);",
        "		if (!err)",
        "			continue;",
        "",
        "		if (nofail) {",
        "			pr_warn(\"CPU %u %s state %s (%d) failed (%d)\\n\",",
        "				cpu, bringup ? \"UP\" : \"DOWN\",",
        "				cpuhp_get_step(st->state)->name,",
        "				st->state, err);",
        "			ret = -1;",
        "		} else {",
        "			ret = err;",
        "			break;",
        "		}",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "static inline int cpuhp_invoke_callback_range(bool bringup,",
        "					      unsigned int cpu,",
        "					      struct cpuhp_cpu_state *st,",
        "					      enum cpuhp_state target)",
        "{",
        "	return __cpuhp_invoke_callback_range(bringup, cpu, st, target, false);",
        "}",
        "",
        "static inline void cpuhp_invoke_callback_range_nofail(bool bringup,",
        "						      unsigned int cpu,",
        "						      struct cpuhp_cpu_state *st,",
        "						      enum cpuhp_state target)",
        "{",
        "	__cpuhp_invoke_callback_range(bringup, cpu, st, target, true);",
        "}",
        "",
        "static inline bool can_rollback_cpu(struct cpuhp_cpu_state *st)",
        "{",
        "	if (IS_ENABLED(CONFIG_HOTPLUG_CPU))",
        "		return true;",
        "	/*",
        "	 * When CPU hotplug is disabled, then taking the CPU down is not",
        "	 * possible because takedown_cpu() and the architecture and",
        "	 * subsystem specific mechanisms are not available. So the CPU",
        "	 * which would be completely unplugged again needs to stay around",
        "	 * in the current state.",
        "	 */",
        "	return st->state <= CPUHP_BRINGUP_CPU;",
        "}",
        "",
        "static int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,",
        "			      enum cpuhp_state target)",
        "{",
        "	enum cpuhp_state prev_state = st->state;",
        "	int ret = 0;",
        "",
        "	ret = cpuhp_invoke_callback_range(true, cpu, st, target);",
        "	if (ret) {",
        "		pr_debug(\"CPU UP failed (%d) CPU %u state %s (%d)\\n\",",
        "			 ret, cpu, cpuhp_get_step(st->state)->name,",
        "			 st->state);",
        "",
        "		cpuhp_reset_state(cpu, st, prev_state);",
        "		if (can_rollback_cpu(st))",
        "			WARN_ON(cpuhp_invoke_callback_range(false, cpu, st,",
        "							    prev_state));",
        "	}",
        "	return ret;",
        "}",
        "",
        "/*",
        " * The cpu hotplug threads manage the bringup and teardown of the cpus",
        " */",
        "static int cpuhp_should_run(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);",
        "",
        "	return st->should_run;",
        "}",
        "",
        "/*",
        " * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke",
        " * callbacks when a state gets [un]installed at runtime.",
        " *",
        " * Each invocation of this function by the smpboot thread does a single AP",
        " * state callback.",
        " *",
        " * It has 3 modes of operation:",
        " *  - single: runs st->cb_state",
        " *  - up:     runs ++st->state, while st->state < st->target",
        " *  - down:   runs st->state--, while st->state > st->target",
        " *",
        " * When complete or on error, should_run is cleared and the completion is fired.",
        " */",
        "static void cpuhp_thread_fun(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);",
        "	bool bringup = st->bringup;",
        "	enum cpuhp_state state;",
        "",
        "	if (WARN_ON_ONCE(!st->should_run))",
        "		return;",
        "",
        "	/*",
        "	 * ACQUIRE for the cpuhp_should_run() load of ->should_run. Ensures",
        "	 * that if we see ->should_run we also see the rest of the state.",
        "	 */",
        "	smp_mb();",
        "",
        "	/*",
        "	 * The BP holds the hotplug lock, but we're now running on the AP,",
        "	 * ensure that anybody asserting the lock is held, will actually find",
        "	 * it so.",
        "	 */",
        "	lockdep_acquire_cpus_lock();",
        "	cpuhp_lock_acquire(bringup);",
        "",
        "	if (st->single) {",
        "		state = st->cb_state;",
        "		st->should_run = false;",
        "	} else {",
        "		st->should_run = cpuhp_next_state(bringup, &state, st, st->target);",
        "		if (!st->should_run)",
        "			goto end;",
        "	}",
        "",
        "	WARN_ON_ONCE(!cpuhp_is_ap_state(state));",
        "",
        "	if (cpuhp_is_atomic_state(state)) {",
        "		local_irq_disable();",
        "		st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);",
        "		local_irq_enable();",
        "",
        "		/*",
        "		 * STARTING/DYING must not fail!",
        "		 */",
        "		WARN_ON_ONCE(st->result);",
        "	} else {",
        "		st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);",
        "	}",
        "",
        "	if (st->result) {",
        "		/*",
        "		 * If we fail on a rollback, we're up a creek without no",
        "		 * paddle, no way forward, no way back. We loose, thanks for",
        "		 * playing.",
        "		 */",
        "		WARN_ON_ONCE(st->rollback);",
        "		st->should_run = false;",
        "	}",
        "",
        "end:",
        "	cpuhp_lock_release(bringup);",
        "	lockdep_release_cpus_lock();",
        "",
        "	if (!st->should_run)",
        "		complete_ap_thread(st, bringup);",
        "}",
        "",
        "/* Invoke a single callback on a remote cpu */",
        "static int",
        "cpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,",
        "			 struct hlist_node *node)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	int ret;",
        "",
        "	if (!cpu_online(cpu))",
        "		return 0;",
        "",
        "	cpuhp_lock_acquire(false);",
        "	cpuhp_lock_release(false);",
        "",
        "	cpuhp_lock_acquire(true);",
        "	cpuhp_lock_release(true);",
        "",
        "	/*",
        "	 * If we are up and running, use the hotplug thread. For early calls",
        "	 * we invoke the thread function directly.",
        "	 */",
        "	if (!st->thread)",
        "		return cpuhp_invoke_callback(cpu, state, bringup, node, NULL);",
        "",
        "	st->rollback = false;",
        "	st->last = NULL;",
        "",
        "	st->node = node;",
        "	st->bringup = bringup;",
        "	st->cb_state = state;",
        "	st->single = true;",
        "",
        "	__cpuhp_kick_ap(st);",
        "",
        "	/*",
        "	 * If we failed and did a partial, do a rollback.",
        "	 */",
        "	if ((ret = st->result) && st->last) {",
        "		st->rollback = true;",
        "		st->bringup = !bringup;",
        "",
        "		__cpuhp_kick_ap(st);",
        "	}",
        "",
        "	/*",
        "	 * Clean up the leftovers so the next hotplug operation wont use stale",
        "	 * data.",
        "	 */",
        "	st->node = st->last = NULL;",
        "	return ret;",
        "}",
        "",
        "static int cpuhp_kick_ap_work(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	enum cpuhp_state prev_state = st->state;",
        "	int ret;",
        "",
        "	cpuhp_lock_acquire(false);",
        "	cpuhp_lock_release(false);",
        "",
        "	cpuhp_lock_acquire(true);",
        "	cpuhp_lock_release(true);",
        "",
        "	trace_cpuhp_enter(cpu, st->target, prev_state, cpuhp_kick_ap_work);",
        "	ret = cpuhp_kick_ap(cpu, st, st->target);",
        "	trace_cpuhp_exit(cpu, st->state, prev_state, ret);",
        "",
        "	return ret;",
        "}",
        "",
        "static struct smp_hotplug_thread cpuhp_threads = {",
        "	.store			= &cpuhp_state.thread,",
        "	.thread_should_run	= cpuhp_should_run,",
        "	.thread_fn		= cpuhp_thread_fun,",
        "	.thread_comm		= \"cpuhp/%u\",",
        "	.selfparking		= true,",
        "};",
        "",
        "static __init void cpuhp_init_state(void)",
        "{",
        "	struct cpuhp_cpu_state *st;",
        "	int cpu;",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		init_completion(&st->done_up);",
        "		init_completion(&st->done_down);",
        "	}",
        "}",
        "",
        "void __init cpuhp_threads_init(void)",
        "{",
        "	cpuhp_init_state();",
        "	BUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));",
        "	kthread_unpark(this_cpu_read(cpuhp_state.thread));",
        "}",
        "",
        "#ifdef CONFIG_HOTPLUG_CPU",
        "#ifndef arch_clear_mm_cpumask_cpu",
        "#define arch_clear_mm_cpumask_cpu(cpu, mm) cpumask_clear_cpu(cpu, mm_cpumask(mm))",
        "#endif",
        "",
        "/**",
        " * clear_tasks_mm_cpumask - Safely clear tasks' mm_cpumask for a CPU",
        " * @cpu: a CPU id",
        " *",
        " * This function walks all processes, finds a valid mm struct for each one and",
        " * then clears a corresponding bit in mm's cpumask.  While this all sounds",
        " * trivial, there are various non-obvious corner cases, which this function",
        " * tries to solve in a safe manner.",
        " *",
        " * Also note that the function uses a somewhat relaxed locking scheme, so it may",
        " * be called only for an already offlined CPU.",
        " */",
        "void clear_tasks_mm_cpumask(int cpu)",
        "{",
        "	struct task_struct *p;",
        "",
        "	/*",
        "	 * This function is called after the cpu is taken down and marked",
        "	 * offline, so its not like new tasks will ever get this cpu set in",
        "	 * their mm mask. -- Peter Zijlstra",
        "	 * Thus, we may use rcu_read_lock() here, instead of grabbing",
        "	 * full-fledged tasklist_lock.",
        "	 */",
        "	WARN_ON(cpu_online(cpu));",
        "	rcu_read_lock();",
        "	for_each_process(p) {",
        "		struct task_struct *t;",
        "",
        "		/*",
        "		 * Main thread might exit, but other threads may still have",
        "		 * a valid mm. Find one.",
        "		 */",
        "		t = find_lock_task_mm(p);",
        "		if (!t)",
        "			continue;",
        "		arch_clear_mm_cpumask_cpu(cpu, t->mm);",
        "		task_unlock(t);",
        "	}",
        "	rcu_read_unlock();",
        "}",
        "",
        "/* Take this CPU down. */",
        "static int take_cpu_down(void *_param)",
        "{",
        "	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);",
        "	enum cpuhp_state target = max((int)st->target, CPUHP_AP_OFFLINE);",
        "	int err, cpu = smp_processor_id();",
        "",
        "	/* Ensure this CPU doesn't handle any more interrupts. */",
        "	err = __cpu_disable();",
        "	if (err < 0)",
        "		return err;",
        "",
        "	/*",
        "	 * Must be called from CPUHP_TEARDOWN_CPU, which means, as we are going",
        "	 * down, that the current state is CPUHP_TEARDOWN_CPU - 1.",
        "	 */",
        "	WARN_ON(st->state != (CPUHP_TEARDOWN_CPU - 1));",
        "",
        "	/*",
        "	 * Invoke the former CPU_DYING callbacks. DYING must not fail!",
        "	 */",
        "	cpuhp_invoke_callback_range_nofail(false, cpu, st, target);",
        "",
        "	/* Park the stopper thread */",
        "	stop_machine_park(cpu);",
        "	return 0;",
        "}",
        "",
        "static int takedown_cpu(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	int err;",
        "",
        "	/* Park the smpboot threads */",
        "	kthread_park(st->thread);",
        "",
        "	/*",
        "	 * Prevent irq alloc/free while the dying cpu reorganizes the",
        "	 * interrupt affinities.",
        "	 */",
        "	irq_lock_sparse();",
        "",
        "	/*",
        "	 * So now all preempt/rcu users must observe !cpu_active().",
        "	 */",
        "	err = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));",
        "	if (err) {",
        "		/* CPU refused to die */",
        "		irq_unlock_sparse();",
        "		/* Unpark the hotplug thread so we can rollback there */",
        "		kthread_unpark(st->thread);",
        "		return err;",
        "	}",
        "	BUG_ON(cpu_online(cpu));",
        "",
        "	/*",
        "	 * The teardown callback for CPUHP_AP_SCHED_STARTING will have removed",
        "	 * all runnable tasks from the CPU, there's only the idle task left now",
        "	 * that the migration thread is done doing the stop_machine thing.",
        "	 *",
        "	 * Wait for the stop thread to go away.",
        "	 */",
        "	wait_for_ap_thread(st, false);",
        "	BUG_ON(st->state != CPUHP_AP_IDLE_DEAD);",
        "",
        "	/* Interrupts are moved away from the dying cpu, reenable alloc/free */",
        "	irq_unlock_sparse();",
        "",
        "	hotplug_cpu__broadcast_tick_pull(cpu);",
        "	/* This actually kills the CPU. */",
        "	__cpu_die(cpu);",
        "",
        "	cpuhp_bp_sync_dead(cpu);",
        "",
        "	lockdep_cleanup_dead_cpu(cpu, idle_thread_get(cpu));",
        "",
        "	/*",
        "	 * Callbacks must be re-integrated right away to the RCU state machine.",
        "	 * Otherwise an RCU callback could block a further teardown function",
        "	 * waiting for its completion.",
        "	 */",
        "	rcutree_migrate_callbacks(cpu);",
        "",
        "	return 0;",
        "}",
        "",
        "static void cpuhp_complete_idle_dead(void *arg)",
        "{",
        "	struct cpuhp_cpu_state *st = arg;",
        "",
        "	complete_ap_thread(st, false);",
        "}",
        "",
        "void cpuhp_report_idle_dead(void)",
        "{",
        "	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);",
        "",
        "	BUG_ON(st->state != CPUHP_AP_OFFLINE);",
        "	tick_assert_timekeeping_handover();",
        "	rcutree_report_cpu_dead();",
        "	st->state = CPUHP_AP_IDLE_DEAD;",
        "	/*",
        "	 * We cannot call complete after rcutree_report_cpu_dead() so we delegate it",
        "	 * to an online cpu.",
        "	 */",
        "	smp_call_function_single(cpumask_first(cpu_online_mask),",
        "				 cpuhp_complete_idle_dead, st, 0);",
        "}",
        "",
        "static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,",
        "				enum cpuhp_state target)",
        "{",
        "	enum cpuhp_state prev_state = st->state;",
        "	int ret = 0;",
        "",
        "	ret = cpuhp_invoke_callback_range(false, cpu, st, target);",
        "	if (ret) {",
        "		pr_debug(\"CPU DOWN failed (%d) CPU %u state %s (%d)\\n\",",
        "			 ret, cpu, cpuhp_get_step(st->state)->name,",
        "			 st->state);",
        "",
        "		cpuhp_reset_state(cpu, st, prev_state);",
        "",
        "		if (st->state < prev_state)",
        "			WARN_ON(cpuhp_invoke_callback_range(true, cpu, st,",
        "							    prev_state));",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "/* Requires cpu_add_remove_lock to be held */",
        "static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,",
        "			   enum cpuhp_state target)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	int prev_state, ret = 0;",
        "",
        "	if (num_online_cpus() == 1)",
        "		return -EBUSY;",
        "",
        "	if (!cpu_present(cpu))",
        "		return -EINVAL;",
        "",
        "	cpus_write_lock();",
        "",
        "	cpuhp_tasks_frozen = tasks_frozen;",
        "",
        "	prev_state = cpuhp_set_state(cpu, st, target);",
        "	/*",
        "	 * If the current CPU state is in the range of the AP hotplug thread,",
        "	 * then we need to kick the thread.",
        "	 */",
        "	if (st->state > CPUHP_TEARDOWN_CPU) {",
        "		st->target = max((int)target, CPUHP_TEARDOWN_CPU);",
        "		ret = cpuhp_kick_ap_work(cpu);",
        "		/*",
        "		 * The AP side has done the error rollback already. Just",
        "		 * return the error code..",
        "		 */",
        "		if (ret)",
        "			goto out;",
        "",
        "		/*",
        "		 * We might have stopped still in the range of the AP hotplug",
        "		 * thread. Nothing to do anymore.",
        "		 */",
        "		if (st->state > CPUHP_TEARDOWN_CPU)",
        "			goto out;",
        "",
        "		st->target = target;",
        "	}",
        "	/*",
        "	 * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need",
        "	 * to do the further cleanups.",
        "	 */",
        "	ret = cpuhp_down_callbacks(cpu, st, target);",
        "	if (ret && st->state < prev_state) {",
        "		if (st->state == CPUHP_TEARDOWN_CPU) {",
        "			cpuhp_reset_state(cpu, st, prev_state);",
        "			__cpuhp_kick_ap(st);",
        "		} else {",
        "			WARN(1, \"DEAD callback error for CPU%d\", cpu);",
        "		}",
        "	}",
        "",
        "out:",
        "	cpus_write_unlock();",
        "	/*",
        "	 * Do post unplug cleanup. This is still protected against",
        "	 * concurrent CPU hotplug via cpu_add_remove_lock.",
        "	 */",
        "	lockup_detector_cleanup();",
        "	arch_smt_update();",
        "	return ret;",
        "}",
        "",
        "struct cpu_down_work {",
        "	unsigned int		cpu;",
        "	enum cpuhp_state	target;",
        "};",
        "",
        "static long __cpu_down_maps_locked(void *arg)",
        "{",
        "	struct cpu_down_work *work = arg;",
        "",
        "	return _cpu_down(work->cpu, 0, work->target);",
        "}",
        "",
        "static int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)",
        "{",
        "	struct cpu_down_work work = { .cpu = cpu, .target = target, };",
        "",
        "	/*",
        "	 * If the platform does not support hotplug, report it explicitly to",
        "	 * differentiate it from a transient offlining failure.",
        "	 */",
        "	if (cpu_hotplug_offline_disabled)",
        "		return -EOPNOTSUPP;",
        "	if (cpu_hotplug_disabled)",
        "		return -EBUSY;",
        "",
        "	/*",
        "	 * Ensure that the control task does not run on the to be offlined",
        "	 * CPU to prevent a deadlock against cfs_b->period_timer.",
        "	 * Also keep at least one housekeeping cpu onlined to avoid generating",
        "	 * an empty sched_domain span.",
        "	 */",
        "	for_each_cpu_and(cpu, cpu_online_mask, housekeeping_cpumask(HK_TYPE_DOMAIN)) {",
        "		if (cpu != work.cpu)",
        "			return work_on_cpu(cpu, __cpu_down_maps_locked, &work);",
        "	}",
        "	return -EBUSY;",
        "}",
        "",
        "static int cpu_down(unsigned int cpu, enum cpuhp_state target)",
        "{",
        "	int err;",
        "",
        "	cpu_maps_update_begin();",
        "	err = cpu_down_maps_locked(cpu, target);",
        "	cpu_maps_update_done();",
        "	return err;",
        "}",
        "",
        "/**",
        " * cpu_device_down - Bring down a cpu device",
        " * @dev: Pointer to the cpu device to offline",
        " *",
        " * This function is meant to be used by device core cpu subsystem only.",
        " *",
        " * Other subsystems should use remove_cpu() instead.",
        " *",
        " * Return: %0 on success or a negative errno code",
        " */",
        "int cpu_device_down(struct device *dev)",
        "{",
        "	return cpu_down(dev->id, CPUHP_OFFLINE);",
        "}",
        "",
        "int remove_cpu(unsigned int cpu)",
        "{",
        "	int ret;",
        "",
        "	lock_device_hotplug();",
        "	ret = device_offline(get_cpu_device(cpu));",
        "	unlock_device_hotplug();",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(remove_cpu);",
        "",
        "void smp_shutdown_nonboot_cpus(unsigned int primary_cpu)",
        "{",
        "	unsigned int cpu;",
        "	int error;",
        "",
        "	cpu_maps_update_begin();",
        "",
        "	/*",
        "	 * Make certain the cpu I'm about to reboot on is online.",
        "	 *",
        "	 * This is inline to what migrate_to_reboot_cpu() already do.",
        "	 */",
        "	if (!cpu_online(primary_cpu))",
        "		primary_cpu = cpumask_first(cpu_online_mask);",
        "",
        "	for_each_online_cpu(cpu) {",
        "		if (cpu == primary_cpu)",
        "			continue;",
        "",
        "		error = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);",
        "		if (error) {",
        "			pr_err(\"Failed to offline CPU%d - error=%d\",",
        "				cpu, error);",
        "			break;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * Ensure all but the reboot CPU are offline.",
        "	 */",
        "	BUG_ON(num_online_cpus() > 1);",
        "",
        "	/*",
        "	 * Make sure the CPUs won't be enabled by someone else after this",
        "	 * point. Kexec will reboot to a new kernel shortly resetting",
        "	 * everything along the way.",
        "	 */",
        "	cpu_hotplug_disabled++;",
        "",
        "	cpu_maps_update_done();",
        "}",
        "",
        "#else",
        "#define takedown_cpu		NULL",
        "#endif /*CONFIG_HOTPLUG_CPU*/",
        "",
        "/**",
        " * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU",
        " * @cpu: cpu that just started",
        " *",
        " * It must be called by the arch code on the new cpu, before the new cpu",
        " * enables interrupts and before the \"boot\" cpu returns from __cpu_up().",
        " */",
        "void notify_cpu_starting(unsigned int cpu)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	enum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);",
        "",
        "	rcutree_report_cpu_starting(cpu);	/* Enables RCU usage on this CPU. */",
        "	cpumask_set_cpu(cpu, &cpus_booted_once_mask);",
        "",
        "	/*",
        "	 * STARTING must not fail!",
        "	 */",
        "	cpuhp_invoke_callback_range_nofail(true, cpu, st, target);",
        "}",
        "",
        "/*",
        " * Called from the idle task. Wake up the controlling task which brings the",
        " * hotplug thread of the upcoming CPU up and then delegates the rest of the",
        " * online bringup to the hotplug thread.",
        " */",
        "void cpuhp_online_idle(enum cpuhp_state state)",
        "{",
        "	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);",
        "",
        "	/* Happens for the boot cpu */",
        "	if (state != CPUHP_AP_ONLINE_IDLE)",
        "		return;",
        "",
        "	cpuhp_ap_update_sync_state(SYNC_STATE_ONLINE);",
        "",
        "	/*",
        "	 * Unpark the stopper thread before we start the idle loop (and start",
        "	 * scheduling); this ensures the stopper task is always available.",
        "	 */",
        "	stop_machine_unpark(smp_processor_id());",
        "",
        "	st->state = CPUHP_AP_ONLINE_IDLE;",
        "	complete_ap_thread(st, true);",
        "}",
        "",
        "/* Requires cpu_add_remove_lock to be held */",
        "static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "	struct task_struct *idle;",
        "	int ret = 0;",
        "",
        "	cpus_write_lock();",
        "",
        "	if (!cpu_present(cpu)) {",
        "		ret = -EINVAL;",
        "		goto out;",
        "	}",
        "",
        "	/*",
        "	 * The caller of cpu_up() might have raced with another",
        "	 * caller. Nothing to do.",
        "	 */",
        "	if (st->state >= target)",
        "		goto out;",
        "",
        "	if (st->state == CPUHP_OFFLINE) {",
        "		/* Let it fail before we try to bring the cpu up */",
        "		idle = idle_thread_get(cpu);",
        "		if (IS_ERR(idle)) {",
        "			ret = PTR_ERR(idle);",
        "			goto out;",
        "		}",
        "",
        "		/*",
        "		 * Reset stale stack state from the last time this CPU was online.",
        "		 */",
        "		scs_task_reset(idle);",
        "		kasan_unpoison_task_stack(idle);",
        "	}",
        "",
        "	cpuhp_tasks_frozen = tasks_frozen;",
        "",
        "	cpuhp_set_state(cpu, st, target);",
        "	/*",
        "	 * If the current CPU state is in the range of the AP hotplug thread,",
        "	 * then we need to kick the thread once more.",
        "	 */",
        "	if (st->state > CPUHP_BRINGUP_CPU) {",
        "		ret = cpuhp_kick_ap_work(cpu);",
        "		/*",
        "		 * The AP side has done the error rollback already. Just",
        "		 * return the error code..",
        "		 */",
        "		if (ret)",
        "			goto out;",
        "	}",
        "",
        "	/*",
        "	 * Try to reach the target state. We max out on the BP at",
        "	 * CPUHP_BRINGUP_CPU. After that the AP hotplug thread is",
        "	 * responsible for bringing it up to the target state.",
        "	 */",
        "	target = min((int)target, CPUHP_BRINGUP_CPU);",
        "	ret = cpuhp_up_callbacks(cpu, st, target);",
        "out:",
        "	cpus_write_unlock();",
        "	arch_smt_update();",
        "	return ret;",
        "}",
        "",
        "static int cpu_up(unsigned int cpu, enum cpuhp_state target)",
        "{",
        "	int err = 0;",
        "",
        "	if (!cpu_possible(cpu)) {",
        "		pr_err(\"can't online cpu %d because it is not configured as may-hotadd at boot time\\n\",",
        "		       cpu);",
        "		return -EINVAL;",
        "	}",
        "",
        "	err = try_online_node(cpu_to_node(cpu));",
        "	if (err)",
        "		return err;",
        "",
        "	cpu_maps_update_begin();",
        "",
        "	if (cpu_hotplug_disabled) {",
        "		err = -EBUSY;",
        "		goto out;",
        "	}",
        "	if (!cpu_bootable(cpu)) {",
        "		err = -EPERM;",
        "		goto out;",
        "	}",
        "",
        "	err = _cpu_up(cpu, 0, target);",
        "out:",
        "	cpu_maps_update_done();",
        "	return err;",
        "}",
        "",
        "/**",
        " * cpu_device_up - Bring up a cpu device",
        " * @dev: Pointer to the cpu device to online",
        " *",
        " * This function is meant to be used by device core cpu subsystem only.",
        " *",
        " * Other subsystems should use add_cpu() instead.",
        " *",
        " * Return: %0 on success or a negative errno code",
        " */",
        "int cpu_device_up(struct device *dev)",
        "{",
        "	return cpu_up(dev->id, CPUHP_ONLINE);",
        "}",
        "",
        "int add_cpu(unsigned int cpu)",
        "{",
        "	int ret;",
        "",
        "	lock_device_hotplug();",
        "	ret = device_online(get_cpu_device(cpu));",
        "	unlock_device_hotplug();",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(add_cpu);",
        "",
        "/**",
        " * bringup_hibernate_cpu - Bring up the CPU that we hibernated on",
        " * @sleep_cpu: The cpu we hibernated on and should be brought up.",
        " *",
        " * On some architectures like arm64, we can hibernate on any CPU, but on",
        " * wake up the CPU we hibernated on might be offline as a side effect of",
        " * using maxcpus= for example.",
        " *",
        " * Return: %0 on success or a negative errno code",
        " */",
        "int bringup_hibernate_cpu(unsigned int sleep_cpu)",
        "{",
        "	int ret;",
        "",
        "	if (!cpu_online(sleep_cpu)) {",
        "		pr_info(\"Hibernated on a CPU that is offline! Bringing CPU up.\\n\");",
        "		ret = cpu_up(sleep_cpu, CPUHP_ONLINE);",
        "		if (ret) {",
        "			pr_err(\"Failed to bring hibernate-CPU up!\\n\");",
        "			return ret;",
        "		}",
        "	}",
        "	return 0;",
        "}",
        "",
        "static void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int ncpus,",
        "				      enum cpuhp_state target)",
        "{",
        "	unsigned int cpu;",
        "",
        "	for_each_cpu(cpu, mask) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "",
        "		if (cpu_up(cpu, target) && can_rollback_cpu(st)) {",
        "			/*",
        "			 * If this failed then cpu_up() might have only",
        "			 * rolled back to CPUHP_BP_KICK_AP for the final",
        "			 * online. Clean it up. NOOP if already rolled back.",
        "			 */",
        "			WARN_ON(cpuhp_invoke_callback_range(false, cpu, st, CPUHP_OFFLINE));",
        "		}",
        "",
        "		if (!--ncpus)",
        "			break;",
        "	}",
        "}",
        "",
        "#ifdef CONFIG_HOTPLUG_PARALLEL",
        "static bool __cpuhp_parallel_bringup __ro_after_init = true;",
        "",
        "static int __init parallel_bringup_parse_param(char *arg)",
        "{",
        "	return kstrtobool(arg, &__cpuhp_parallel_bringup);",
        "}",
        "early_param(\"cpuhp.parallel\", parallel_bringup_parse_param);",
        "",
        "#ifdef CONFIG_HOTPLUG_SMT",
        "static inline bool cpuhp_smt_aware(void)",
        "{",
        "	return cpu_smt_max_threads > 1;",
        "}",
        "",
        "static inline const struct cpumask *cpuhp_get_primary_thread_mask(void)",
        "{",
        "	return cpu_primary_thread_mask;",
        "}",
        "#else",
        "static inline bool cpuhp_smt_aware(void)",
        "{",
        "	return false;",
        "}",
        "static inline const struct cpumask *cpuhp_get_primary_thread_mask(void)",
        "{",
        "	return cpu_none_mask;",
        "}",
        "#endif",
        "",
        "bool __weak arch_cpuhp_init_parallel_bringup(void)",
        "{",
        "	return true;",
        "}",
        "",
        "/*",
        " * On architectures which have enabled parallel bringup this invokes all BP",
        " * prepare states for each of the to be onlined APs first. The last state",
        " * sends the startup IPI to the APs. The APs proceed through the low level",
        " * bringup code in parallel and then wait for the control CPU to release",
        " * them one by one for the final onlining procedure.",
        " *",
        " * This avoids waiting for each AP to respond to the startup IPI in",
        " * CPUHP_BRINGUP_CPU.",
        " */",
        "static bool __init cpuhp_bringup_cpus_parallel(unsigned int ncpus)",
        "{",
        "	const struct cpumask *mask = cpu_present_mask;",
        "",
        "	if (__cpuhp_parallel_bringup)",
        "		__cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();",
        "	if (!__cpuhp_parallel_bringup)",
        "		return false;",
        "",
        "	if (cpuhp_smt_aware()) {",
        "		const struct cpumask *pmask = cpuhp_get_primary_thread_mask();",
        "		static struct cpumask tmp_mask __initdata;",
        "",
        "		/*",
        "		 * X86 requires to prevent that SMT siblings stopped while",
        "		 * the primary thread does a microcode update for various",
        "		 * reasons. Bring the primary threads up first.",
        "		 */",
        "		cpumask_and(&tmp_mask, mask, pmask);",
        "		cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_BP_KICK_AP);",
        "		cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_ONLINE);",
        "		/* Account for the online CPUs */",
        "		ncpus -= num_online_cpus();",
        "		if (!ncpus)",
        "			return true;",
        "		/* Create the mask for secondary CPUs */",
        "		cpumask_andnot(&tmp_mask, mask, pmask);",
        "		mask = &tmp_mask;",
        "	}",
        "",
        "	/* Bring the not-yet started CPUs up */",
        "	cpuhp_bringup_mask(mask, ncpus, CPUHP_BP_KICK_AP);",
        "	cpuhp_bringup_mask(mask, ncpus, CPUHP_ONLINE);",
        "	return true;",
        "}",
        "#else",
        "static inline bool cpuhp_bringup_cpus_parallel(unsigned int ncpus) { return false; }",
        "#endif /* CONFIG_HOTPLUG_PARALLEL */",
        "",
        "void __init bringup_nonboot_cpus(unsigned int max_cpus)",
        "{",
        "	if (!max_cpus)",
        "		return;",
        "",
        "	/* Try parallel bringup optimization if enabled */",
        "	if (cpuhp_bringup_cpus_parallel(max_cpus))",
        "		return;",
        "",
        "	/* Full per CPU serialized bringup */",
        "	cpuhp_bringup_mask(cpu_present_mask, max_cpus, CPUHP_ONLINE);",
        "}",
        "",
        "#ifdef CONFIG_PM_SLEEP_SMP",
        "static cpumask_var_t frozen_cpus;",
        "",
        "int freeze_secondary_cpus(int primary)",
        "{",
        "	int cpu, error = 0;",
        "",
        "	cpu_maps_update_begin();",
        "	if (primary == -1) {",
        "		primary = cpumask_first(cpu_online_mask);",
        "		if (!housekeeping_cpu(primary, HK_TYPE_TIMER))",
        "			primary = housekeeping_any_cpu(HK_TYPE_TIMER);",
        "	} else {",
        "		if (!cpu_online(primary))",
        "			primary = cpumask_first(cpu_online_mask);",
        "	}",
        "",
        "	/*",
        "	 * We take down all of the non-boot CPUs in one shot to avoid races",
        "	 * with the userspace trying to use the CPU hotplug at the same time",
        "	 */",
        "	cpumask_clear(frozen_cpus);",
        "",
        "	pr_info(\"Disabling non-boot CPUs ...\\n\");",
        "	for (cpu = nr_cpu_ids - 1; cpu >= 0; cpu--) {",
        "		if (!cpu_online(cpu) || cpu == primary)",
        "			continue;",
        "",
        "		if (pm_wakeup_pending()) {",
        "			pr_info(\"Wakeup pending. Abort CPU freeze\\n\");",
        "			error = -EBUSY;",
        "			break;",
        "		}",
        "",
        "		trace_suspend_resume(TPS(\"CPU_OFF\"), cpu, true);",
        "		error = _cpu_down(cpu, 1, CPUHP_OFFLINE);",
        "		trace_suspend_resume(TPS(\"CPU_OFF\"), cpu, false);",
        "		if (!error)",
        "			cpumask_set_cpu(cpu, frozen_cpus);",
        "		else {",
        "			pr_err(\"Error taking CPU%d down: %d\\n\", cpu, error);",
        "			break;",
        "		}",
        "	}",
        "",
        "	if (!error)",
        "		BUG_ON(num_online_cpus() > 1);",
        "	else",
        "		pr_err(\"Non-boot CPUs are not disabled\\n\");",
        "",
        "	/*",
        "	 * Make sure the CPUs won't be enabled by someone else. We need to do",
        "	 * this even in case of failure as all freeze_secondary_cpus() users are",
        "	 * supposed to do thaw_secondary_cpus() on the failure path.",
        "	 */",
        "	cpu_hotplug_disabled++;",
        "",
        "	cpu_maps_update_done();",
        "	return error;",
        "}",
        "",
        "void __weak arch_thaw_secondary_cpus_begin(void)",
        "{",
        "}",
        "",
        "void __weak arch_thaw_secondary_cpus_end(void)",
        "{",
        "}",
        "",
        "void thaw_secondary_cpus(void)",
        "{",
        "	int cpu, error;",
        "",
        "	/* Allow everyone to use the CPU hotplug again */",
        "	cpu_maps_update_begin();",
        "	__cpu_hotplug_enable();",
        "	if (cpumask_empty(frozen_cpus))",
        "		goto out;",
        "",
        "	pr_info(\"Enabling non-boot CPUs ...\\n\");",
        "",
        "	arch_thaw_secondary_cpus_begin();",
        "",
        "	for_each_cpu(cpu, frozen_cpus) {",
        "		trace_suspend_resume(TPS(\"CPU_ON\"), cpu, true);",
        "		error = _cpu_up(cpu, 1, CPUHP_ONLINE);",
        "		trace_suspend_resume(TPS(\"CPU_ON\"), cpu, false);",
        "		if (!error) {",
        "			pr_info(\"CPU%d is up\\n\", cpu);",
        "			continue;",
        "		}",
        "		pr_warn(\"Error taking CPU%d up: %d\\n\", cpu, error);",
        "	}",
        "",
        "	arch_thaw_secondary_cpus_end();",
        "",
        "	cpumask_clear(frozen_cpus);",
        "out:",
        "	cpu_maps_update_done();",
        "}",
        "",
        "static int __init alloc_frozen_cpus(void)",
        "{",
        "	if (!alloc_cpumask_var(&frozen_cpus, GFP_KERNEL|__GFP_ZERO))",
        "		return -ENOMEM;",
        "	return 0;",
        "}",
        "core_initcall(alloc_frozen_cpus);",
        "",
        "/*",
        " * When callbacks for CPU hotplug notifications are being executed, we must",
        " * ensure that the state of the system with respect to the tasks being frozen",
        " * or not, as reported by the notification, remains unchanged *throughout the",
        " * duration* of the execution of the callbacks.",
        " * Hence we need to prevent the freezer from racing with regular CPU hotplug.",
        " *",
        " * This synchronization is implemented by mutually excluding regular CPU",
        " * hotplug and Suspend/Hibernate call paths by hooking onto the Suspend/",
        " * Hibernate notifications.",
        " */",
        "static int",
        "cpu_hotplug_pm_callback(struct notifier_block *nb,",
        "			unsigned long action, void *ptr)",
        "{",
        "	switch (action) {",
        "",
        "	case PM_SUSPEND_PREPARE:",
        "	case PM_HIBERNATION_PREPARE:",
        "		cpu_hotplug_disable();",
        "		break;",
        "",
        "	case PM_POST_SUSPEND:",
        "	case PM_POST_HIBERNATION:",
        "		cpu_hotplug_enable();",
        "		break;",
        "",
        "	default:",
        "		return NOTIFY_DONE;",
        "	}",
        "",
        "	return NOTIFY_OK;",
        "}",
        "",
        "",
        "static int __init cpu_hotplug_pm_sync_init(void)",
        "{",
        "	/*",
        "	 * cpu_hotplug_pm_callback has higher priority than x86",
        "	 * bsp_pm_callback which depends on cpu_hotplug_pm_callback",
        "	 * to disable cpu hotplug to avoid cpu hotplug race.",
        "	 */",
        "	pm_notifier(cpu_hotplug_pm_callback, 0);",
        "	return 0;",
        "}",
        "core_initcall(cpu_hotplug_pm_sync_init);",
        "",
        "#endif /* CONFIG_PM_SLEEP_SMP */",
        "",
        "int __boot_cpu_id;",
        "",
        "#endif /* CONFIG_SMP */",
        "",
        "/* Boot processor state steps */",
        "static struct cpuhp_step cpuhp_hp_states[] = {",
        "	[CPUHP_OFFLINE] = {",
        "		.name			= \"offline\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= NULL,",
        "	},",
        "#ifdef CONFIG_SMP",
        "	[CPUHP_CREATE_THREADS]= {",
        "		.name			= \"threads:prepare\",",
        "		.startup.single		= smpboot_create_threads,",
        "		.teardown.single	= NULL,",
        "		.cant_stop		= true,",
        "	},",
        "	[CPUHP_PERF_PREPARE] = {",
        "		.name			= \"perf:prepare\",",
        "		.startup.single		= perf_event_init_cpu,",
        "		.teardown.single	= perf_event_exit_cpu,",
        "	},",
        "	[CPUHP_RANDOM_PREPARE] = {",
        "		.name			= \"random:prepare\",",
        "		.startup.single		= random_prepare_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_WORKQUEUE_PREP] = {",
        "		.name			= \"workqueue:prepare\",",
        "		.startup.single		= workqueue_prepare_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_HRTIMERS_PREPARE] = {",
        "		.name			= \"hrtimers:prepare\",",
        "		.startup.single		= hrtimers_prepare_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_SMPCFD_PREPARE] = {",
        "		.name			= \"smpcfd:prepare\",",
        "		.startup.single		= smpcfd_prepare_cpu,",
        "		.teardown.single	= smpcfd_dead_cpu,",
        "	},",
        "	[CPUHP_RELAY_PREPARE] = {",
        "		.name			= \"relay:prepare\",",
        "		.startup.single		= relay_prepare_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_RCUTREE_PREP] = {",
        "		.name			= \"RCU/tree:prepare\",",
        "		.startup.single		= rcutree_prepare_cpu,",
        "		.teardown.single	= rcutree_dead_cpu,",
        "	},",
        "	/*",
        "	 * On the tear-down path, timers_dead_cpu() must be invoked",
        "	 * before blk_mq_queue_reinit_notify() from notify_dead(),",
        "	 * otherwise a RCU stall occurs.",
        "	 */",
        "	[CPUHP_TIMERS_PREPARE] = {",
        "		.name			= \"timers:prepare\",",
        "		.startup.single		= timers_prepare_cpu,",
        "		.teardown.single	= timers_dead_cpu,",
        "	},",
        "",
        "#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP",
        "	/*",
        "	 * Kicks the AP alive. AP will wait in cpuhp_ap_sync_alive() until",
        "	 * the next step will release it.",
        "	 */",
        "	[CPUHP_BP_KICK_AP] = {",
        "		.name			= \"cpu:kick_ap\",",
        "		.startup.single		= cpuhp_kick_ap_alive,",
        "	},",
        "",
        "	/*",
        "	 * Waits for the AP to reach cpuhp_ap_sync_alive() and then",
        "	 * releases it for the complete bringup.",
        "	 */",
        "	[CPUHP_BRINGUP_CPU] = {",
        "		.name			= \"cpu:bringup\",",
        "		.startup.single		= cpuhp_bringup_ap,",
        "		.teardown.single	= finish_cpu,",
        "		.cant_stop		= true,",
        "	},",
        "#else",
        "	/*",
        "	 * All-in-one CPU bringup state which includes the kick alive.",
        "	 */",
        "	[CPUHP_BRINGUP_CPU] = {",
        "		.name			= \"cpu:bringup\",",
        "		.startup.single		= bringup_cpu,",
        "		.teardown.single	= finish_cpu,",
        "		.cant_stop		= true,",
        "	},",
        "#endif",
        "	/* Final state before CPU kills itself */",
        "	[CPUHP_AP_IDLE_DEAD] = {",
        "		.name			= \"idle:dead\",",
        "	},",
        "	/*",
        "	 * Last state before CPU enters the idle loop to die. Transient state",
        "	 * for synchronization.",
        "	 */",
        "	[CPUHP_AP_OFFLINE] = {",
        "		.name			= \"ap:offline\",",
        "		.cant_stop		= true,",
        "	},",
        "	/* First state is scheduler control. Interrupts are disabled */",
        "	[CPUHP_AP_SCHED_STARTING] = {",
        "		.name			= \"sched:starting\",",
        "		.startup.single		= sched_cpu_starting,",
        "		.teardown.single	= sched_cpu_dying,",
        "	},",
        "	[CPUHP_AP_RCUTREE_DYING] = {",
        "		.name			= \"RCU/tree:dying\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= rcutree_dying_cpu,",
        "	},",
        "	[CPUHP_AP_SMPCFD_DYING] = {",
        "		.name			= \"smpcfd:dying\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= smpcfd_dying_cpu,",
        "	},",
        "	[CPUHP_AP_HRTIMERS_DYING] = {",
        "		.name			= \"hrtimers:dying\",",
        "		.startup.single		= hrtimers_cpu_starting,",
        "		.teardown.single	= hrtimers_cpu_dying,",
        "	},",
        "	[CPUHP_AP_TICK_DYING] = {",
        "		.name			= \"tick:dying\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= tick_cpu_dying,",
        "	},",
        "	/* Entry state on starting. Interrupts enabled from here on. Transient",
        "	 * state for synchronsization */",
        "	[CPUHP_AP_ONLINE] = {",
        "		.name			= \"ap:online\",",
        "	},",
        "	/*",
        "	 * Handled on control processor until the plugged processor manages",
        "	 * this itself.",
        "	 */",
        "	[CPUHP_TEARDOWN_CPU] = {",
        "		.name			= \"cpu:teardown\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= takedown_cpu,",
        "		.cant_stop		= true,",
        "	},",
        "",
        "	[CPUHP_AP_SCHED_WAIT_EMPTY] = {",
        "		.name			= \"sched:waitempty\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= sched_cpu_wait_empty,",
        "	},",
        "",
        "	/* Handle smpboot threads park/unpark */",
        "	[CPUHP_AP_SMPBOOT_THREADS] = {",
        "		.name			= \"smpboot/threads:online\",",
        "		.startup.single		= smpboot_unpark_threads,",
        "		.teardown.single	= smpboot_park_threads,",
        "	},",
        "	[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {",
        "		.name			= \"irq/affinity:online\",",
        "		.startup.single		= irq_affinity_online_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_AP_PERF_ONLINE] = {",
        "		.name			= \"perf:online\",",
        "		.startup.single		= perf_event_init_cpu,",
        "		.teardown.single	= perf_event_exit_cpu,",
        "	},",
        "	[CPUHP_AP_WATCHDOG_ONLINE] = {",
        "		.name			= \"lockup_detector:online\",",
        "		.startup.single		= lockup_detector_online_cpu,",
        "		.teardown.single	= lockup_detector_offline_cpu,",
        "	},",
        "	[CPUHP_AP_WORKQUEUE_ONLINE] = {",
        "		.name			= \"workqueue:online\",",
        "		.startup.single		= workqueue_online_cpu,",
        "		.teardown.single	= workqueue_offline_cpu,",
        "	},",
        "	[CPUHP_AP_RANDOM_ONLINE] = {",
        "		.name			= \"random:online\",",
        "		.startup.single		= random_online_cpu,",
        "		.teardown.single	= NULL,",
        "	},",
        "	[CPUHP_AP_RCUTREE_ONLINE] = {",
        "		.name			= \"RCU/tree:online\",",
        "		.startup.single		= rcutree_online_cpu,",
        "		.teardown.single	= rcutree_offline_cpu,",
        "	},",
        "#endif",
        "	/*",
        "	 * The dynamically registered state space is here",
        "	 */",
        "",
        "#ifdef CONFIG_SMP",
        "	/* Last state is scheduler control setting the cpu active */",
        "	[CPUHP_AP_ACTIVE] = {",
        "		.name			= \"sched:active\",",
        "		.startup.single		= sched_cpu_activate,",
        "		.teardown.single	= sched_cpu_deactivate,",
        "	},",
        "#endif",
        "",
        "	/* CPU is fully up and running. */",
        "	[CPUHP_ONLINE] = {",
        "		.name			= \"online\",",
        "		.startup.single		= NULL,",
        "		.teardown.single	= NULL,",
        "	},",
        "};",
        "",
        "/* Sanity check for callbacks */",
        "static int cpuhp_cb_check(enum cpuhp_state state)",
        "{",
        "	if (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)",
        "		return -EINVAL;",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Returns a free for dynamic slot assignment of the Online state. The states",
        " * are protected by the cpuhp_slot_states mutex and an empty slot is identified",
        " * by having no name assigned.",
        " */",
        "static int cpuhp_reserve_state(enum cpuhp_state state)",
        "{",
        "	enum cpuhp_state i, end;",
        "	struct cpuhp_step *step;",
        "",
        "	switch (state) {",
        "	case CPUHP_AP_ONLINE_DYN:",
        "		step = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;",
        "		end = CPUHP_AP_ONLINE_DYN_END;",
        "		break;",
        "	case CPUHP_BP_PREPARE_DYN:",
        "		step = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;",
        "		end = CPUHP_BP_PREPARE_DYN_END;",
        "		break;",
        "	default:",
        "		return -EINVAL;",
        "	}",
        "",
        "	for (i = state; i <= end; i++, step++) {",
        "		if (!step->name)",
        "			return i;",
        "	}",
        "	WARN(1, \"No more dynamic states available for CPU hotplug\\n\");",
        "	return -ENOSPC;",
        "}",
        "",
        "static int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,",
        "				 int (*startup)(unsigned int cpu),",
        "				 int (*teardown)(unsigned int cpu),",
        "				 bool multi_instance)",
        "{",
        "	/* (Un)Install the callbacks for further cpu hotplug operations */",
        "	struct cpuhp_step *sp;",
        "	int ret = 0;",
        "",
        "	/*",
        "	 * If name is NULL, then the state gets removed.",
        "	 *",
        "	 * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on",
        "	 * the first allocation from these dynamic ranges, so the removal",
        "	 * would trigger a new allocation and clear the wrong (already",
        "	 * empty) state, leaving the callbacks of the to be cleared state",
        "	 * dangling, which causes wreckage on the next hotplug operation.",
        "	 */",
        "	if (name && (state == CPUHP_AP_ONLINE_DYN ||",
        "		     state == CPUHP_BP_PREPARE_DYN)) {",
        "		ret = cpuhp_reserve_state(state);",
        "		if (ret < 0)",
        "			return ret;",
        "		state = ret;",
        "	}",
        "	sp = cpuhp_get_step(state);",
        "	if (name && sp->name)",
        "		return -EBUSY;",
        "",
        "	sp->startup.single = startup;",
        "	sp->teardown.single = teardown;",
        "	sp->name = name;",
        "	sp->multi_instance = multi_instance;",
        "	INIT_HLIST_HEAD(&sp->list);",
        "	return ret;",
        "}",
        "",
        "static void *cpuhp_get_teardown_cb(enum cpuhp_state state)",
        "{",
        "	return cpuhp_get_step(state)->teardown.single;",
        "}",
        "",
        "/*",
        " * Call the startup/teardown function for a step either on the AP or",
        " * on the current CPU.",
        " */",
        "static int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,",
        "			    struct hlist_node *node)",
        "{",
        "	struct cpuhp_step *sp = cpuhp_get_step(state);",
        "	int ret;",
        "",
        "	/*",
        "	 * If there's nothing to do, we done.",
        "	 * Relies on the union for multi_instance.",
        "	 */",
        "	if (cpuhp_step_empty(bringup, sp))",
        "		return 0;",
        "	/*",
        "	 * The non AP bound callbacks can fail on bringup. On teardown",
        "	 * e.g. module removal we crash for now.",
        "	 */",
        "#ifdef CONFIG_SMP",
        "	if (cpuhp_is_ap_state(state))",
        "		ret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);",
        "	else",
        "		ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);",
        "#else",
        "	ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);",
        "#endif",
        "	BUG_ON(ret && !bringup);",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Called from __cpuhp_setup_state on a recoverable failure.",
        " *",
        " * Note: The teardown callbacks for rollback are not allowed to fail!",
        " */",
        "static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,",
        "				   struct hlist_node *node)",
        "{",
        "	int cpu;",
        "",
        "	/* Roll back the already executed steps on the other cpus */",
        "	for_each_present_cpu(cpu) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		int cpustate = st->state;",
        "",
        "		if (cpu >= failedcpu)",
        "			break;",
        "",
        "		/* Did we invoke the startup call on that cpu ? */",
        "		if (cpustate >= state)",
        "			cpuhp_issue_call(cpu, state, false, node);",
        "	}",
        "}",
        "",
        "int __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,",
        "					  struct hlist_node *node,",
        "					  bool invoke)",
        "{",
        "	struct cpuhp_step *sp;",
        "	int cpu;",
        "	int ret;",
        "",
        "	lockdep_assert_cpus_held();",
        "",
        "	sp = cpuhp_get_step(state);",
        "	if (sp->multi_instance == false)",
        "		return -EINVAL;",
        "",
        "	mutex_lock(&cpuhp_state_mutex);",
        "",
        "	if (!invoke || !sp->startup.multi)",
        "		goto add_node;",
        "",
        "	/*",
        "	 * Try to call the startup callback for each present cpu",
        "	 * depending on the hotplug state of the cpu.",
        "	 */",
        "	for_each_present_cpu(cpu) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		int cpustate = st->state;",
        "",
        "		if (cpustate < state)",
        "			continue;",
        "",
        "		ret = cpuhp_issue_call(cpu, state, true, node);",
        "		if (ret) {",
        "			if (sp->teardown.multi)",
        "				cpuhp_rollback_install(cpu, state, node);",
        "			goto unlock;",
        "		}",
        "	}",
        "add_node:",
        "	ret = 0;",
        "	hlist_add_head(node, &sp->list);",
        "unlock:",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	return ret;",
        "}",
        "",
        "int __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,",
        "			       bool invoke)",
        "{",
        "	int ret;",
        "",
        "	cpus_read_lock();",
        "	ret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);",
        "	cpus_read_unlock();",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);",
        "",
        "/**",
        " * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state",
        " * @state:		The state to setup",
        " * @name:		Name of the step",
        " * @invoke:		If true, the startup function is invoked for cpus where",
        " *			cpu state >= @state",
        " * @startup:		startup callback function",
        " * @teardown:		teardown callback function",
        " * @multi_instance:	State is set up for multiple instances which get",
        " *			added afterwards.",
        " *",
        " * The caller needs to hold cpus read locked while calling this function.",
        " * Return:",
        " *   On success:",
        " *      Positive state number if @state is CPUHP_AP_ONLINE_DYN or CPUHP_BP_PREPARE_DYN;",
        " *      0 for all other states",
        " *   On failure: proper (negative) error code",
        " */",
        "int __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,",
        "				   const char *name, bool invoke,",
        "				   int (*startup)(unsigned int cpu),",
        "				   int (*teardown)(unsigned int cpu),",
        "				   bool multi_instance)",
        "{",
        "	int cpu, ret = 0;",
        "	bool dynstate;",
        "",
        "	lockdep_assert_cpus_held();",
        "",
        "	if (cpuhp_cb_check(state) || !name)",
        "		return -EINVAL;",
        "",
        "	mutex_lock(&cpuhp_state_mutex);",
        "",
        "	ret = cpuhp_store_callbacks(state, name, startup, teardown,",
        "				    multi_instance);",
        "",
        "	dynstate = state == CPUHP_AP_ONLINE_DYN || state == CPUHP_BP_PREPARE_DYN;",
        "	if (ret > 0 && dynstate) {",
        "		state = ret;",
        "		ret = 0;",
        "	}",
        "",
        "	if (ret || !invoke || !startup)",
        "		goto out;",
        "",
        "	/*",
        "	 * Try to call the startup callback for each present cpu",
        "	 * depending on the hotplug state of the cpu.",
        "	 */",
        "	for_each_present_cpu(cpu) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		int cpustate = st->state;",
        "",
        "		if (cpustate < state)",
        "			continue;",
        "",
        "		ret = cpuhp_issue_call(cpu, state, true, NULL);",
        "		if (ret) {",
        "			if (teardown)",
        "				cpuhp_rollback_install(cpu, state, NULL);",
        "			cpuhp_store_callbacks(state, NULL, NULL, NULL, false);",
        "			goto out;",
        "		}",
        "	}",
        "out:",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	/*",
        "	 * If the requested state is CPUHP_AP_ONLINE_DYN or CPUHP_BP_PREPARE_DYN,",
        "	 * return the dynamically allocated state in case of success.",
        "	 */",
        "	if (!ret && dynstate)",
        "		return state;",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);",
        "",
        "int __cpuhp_setup_state(enum cpuhp_state state,",
        "			const char *name, bool invoke,",
        "			int (*startup)(unsigned int cpu),",
        "			int (*teardown)(unsigned int cpu),",
        "			bool multi_instance)",
        "{",
        "	int ret;",
        "",
        "	cpus_read_lock();",
        "	ret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,",
        "					     teardown, multi_instance);",
        "	cpus_read_unlock();",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(__cpuhp_setup_state);",
        "",
        "int __cpuhp_state_remove_instance(enum cpuhp_state state,",
        "				  struct hlist_node *node, bool invoke)",
        "{",
        "	struct cpuhp_step *sp = cpuhp_get_step(state);",
        "	int cpu;",
        "",
        "	BUG_ON(cpuhp_cb_check(state));",
        "",
        "	if (!sp->multi_instance)",
        "		return -EINVAL;",
        "",
        "	cpus_read_lock();",
        "	mutex_lock(&cpuhp_state_mutex);",
        "",
        "	if (!invoke || !cpuhp_get_teardown_cb(state))",
        "		goto remove;",
        "	/*",
        "	 * Call the teardown callback for each present cpu depending",
        "	 * on the hotplug state of the cpu. This function is not",
        "	 * allowed to fail currently!",
        "	 */",
        "	for_each_present_cpu(cpu) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		int cpustate = st->state;",
        "",
        "		if (cpustate >= state)",
        "			cpuhp_issue_call(cpu, state, false, node);",
        "	}",
        "",
        "remove:",
        "	hlist_del(node);",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	cpus_read_unlock();",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);",
        "",
        "/**",
        " * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state",
        " * @state:	The state to remove",
        " * @invoke:	If true, the teardown function is invoked for cpus where",
        " *		cpu state >= @state",
        " *",
        " * The caller needs to hold cpus read locked while calling this function.",
        " * The teardown callback is currently not allowed to fail. Think",
        " * about module removal!",
        " */",
        "void __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)",
        "{",
        "	struct cpuhp_step *sp = cpuhp_get_step(state);",
        "	int cpu;",
        "",
        "	BUG_ON(cpuhp_cb_check(state));",
        "",
        "	lockdep_assert_cpus_held();",
        "",
        "	mutex_lock(&cpuhp_state_mutex);",
        "	if (sp->multi_instance) {",
        "		WARN(!hlist_empty(&sp->list),",
        "		     \"Error: Removing state %d which has instances left.\\n\",",
        "		     state);",
        "		goto remove;",
        "	}",
        "",
        "	if (!invoke || !cpuhp_get_teardown_cb(state))",
        "		goto remove;",
        "",
        "	/*",
        "	 * Call the teardown callback for each present cpu depending",
        "	 * on the hotplug state of the cpu. This function is not",
        "	 * allowed to fail currently!",
        "	 */",
        "	for_each_present_cpu(cpu) {",
        "		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);",
        "		int cpustate = st->state;",
        "",
        "		if (cpustate >= state)",
        "			cpuhp_issue_call(cpu, state, false, NULL);",
        "	}",
        "remove:",
        "	cpuhp_store_callbacks(state, NULL, NULL, NULL, false);",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "}",
        "EXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);",
        "",
        "void __cpuhp_remove_state(enum cpuhp_state state, bool invoke)",
        "{",
        "	cpus_read_lock();",
        "	__cpuhp_remove_state_cpuslocked(state, invoke);",
        "	cpus_read_unlock();",
        "}",
        "EXPORT_SYMBOL(__cpuhp_remove_state);",
        "",
        "#ifdef CONFIG_HOTPLUG_SMT",
        "static void cpuhp_offline_cpu_device(unsigned int cpu)",
        "{",
        "	struct device *dev = get_cpu_device(cpu);",
        "",
        "	dev->offline = true;",
        "	/* Tell user space about the state change */",
        "	kobject_uevent(&dev->kobj, KOBJ_OFFLINE);",
        "}",
        "",
        "static void cpuhp_online_cpu_device(unsigned int cpu)",
        "{",
        "	struct device *dev = get_cpu_device(cpu);",
        "",
        "	dev->offline = false;",
        "	/* Tell user space about the state change */",
        "	kobject_uevent(&dev->kobj, KOBJ_ONLINE);",
        "}",
        "",
        "int cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)",
        "{",
        "	int cpu, ret = 0;",
        "",
        "	cpu_maps_update_begin();",
        "	for_each_online_cpu(cpu) {",
        "		if (topology_is_primary_thread(cpu))",
        "			continue;",
        "		/*",
        "		 * Disable can be called with CPU_SMT_ENABLED when changing",
        "		 * from a higher to lower number of SMT threads per core.",
        "		 */",
        "		if (ctrlval == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))",
        "			continue;",
        "		ret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);",
        "		if (ret)",
        "			break;",
        "		/*",
        "		 * As this needs to hold the cpu maps lock it's impossible",
        "		 * to call device_offline() because that ends up calling",
        "		 * cpu_down() which takes cpu maps lock. cpu maps lock",
        "		 * needs to be held as this might race against in kernel",
        "		 * abusers of the hotplug machinery (thermal management).",
        "		 *",
        "		 * So nothing would update device:offline state. That would",
        "		 * leave the sysfs entry stale and prevent onlining after",
        "		 * smt control has been changed to 'off' again. This is",
        "		 * called under the sysfs hotplug lock, so it is properly",
        "		 * serialized against the regular offline usage.",
        "		 */",
        "		cpuhp_offline_cpu_device(cpu);",
        "	}",
        "	if (!ret)",
        "		cpu_smt_control = ctrlval;",
        "	cpu_maps_update_done();",
        "	return ret;",
        "}",
        "",
        "/* Check if the core a CPU belongs to is online */",
        "#if !defined(topology_is_core_online)",
        "static inline bool topology_is_core_online(unsigned int cpu)",
        "{",
        "	return true;",
        "}",
        "#endif",
        "",
        "int cpuhp_smt_enable(void)",
        "{",
        "	int cpu, ret = 0;",
        "",
        "	cpu_maps_update_begin();",
        "	cpu_smt_control = CPU_SMT_ENABLED;",
        "	for_each_present_cpu(cpu) {",
        "		/* Skip online CPUs and CPUs on offline nodes */",
        "		if (cpu_online(cpu) || !node_online(cpu_to_node(cpu)))",
        "			continue;",
        "		if (!cpu_smt_thread_allowed(cpu) || !topology_is_core_online(cpu))",
        "			continue;",
        "		ret = _cpu_up(cpu, 0, CPUHP_ONLINE);",
        "		if (ret)",
        "			break;",
        "		/* See comment in cpuhp_smt_disable() */",
        "		cpuhp_online_cpu_device(cpu);",
        "	}",
        "	cpu_maps_update_done();",
        "	return ret;",
        "}",
        "#endif",
        "",
        "#if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)",
        "static ssize_t state_show(struct device *dev,",
        "			  struct device_attribute *attr, char *buf)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);",
        "",
        "	return sprintf(buf, \"%d\\n\", st->state);",
        "}",
        "static DEVICE_ATTR_RO(state);",
        "",
        "static ssize_t target_store(struct device *dev, struct device_attribute *attr,",
        "			    const char *buf, size_t count)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);",
        "	struct cpuhp_step *sp;",
        "	int target, ret;",
        "",
        "	ret = kstrtoint(buf, 10, &target);",
        "	if (ret)",
        "		return ret;",
        "",
        "#ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL",
        "	if (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)",
        "		return -EINVAL;",
        "#else",
        "	if (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)",
        "		return -EINVAL;",
        "#endif",
        "",
        "	ret = lock_device_hotplug_sysfs();",
        "	if (ret)",
        "		return ret;",
        "",
        "	mutex_lock(&cpuhp_state_mutex);",
        "	sp = cpuhp_get_step(target);",
        "	ret = !sp->name || sp->cant_stop ? -EINVAL : 0;",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	if (ret)",
        "		goto out;",
        "",
        "	if (st->state < target)",
        "		ret = cpu_up(dev->id, target);",
        "	else if (st->state > target)",
        "		ret = cpu_down(dev->id, target);",
        "	else if (WARN_ON(st->target != target))",
        "		st->target = target;",
        "out:",
        "	unlock_device_hotplug();",
        "	return ret ? ret : count;",
        "}",
        "",
        "static ssize_t target_show(struct device *dev,",
        "			   struct device_attribute *attr, char *buf)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);",
        "",
        "	return sprintf(buf, \"%d\\n\", st->target);",
        "}",
        "static DEVICE_ATTR_RW(target);",
        "",
        "static ssize_t fail_store(struct device *dev, struct device_attribute *attr,",
        "			  const char *buf, size_t count)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);",
        "	struct cpuhp_step *sp;",
        "	int fail, ret;",
        "",
        "	ret = kstrtoint(buf, 10, &fail);",
        "	if (ret)",
        "		return ret;",
        "",
        "	if (fail == CPUHP_INVALID) {",
        "		st->fail = fail;",
        "		return count;",
        "	}",
        "",
        "	if (fail < CPUHP_OFFLINE || fail > CPUHP_ONLINE)",
        "		return -EINVAL;",
        "",
        "	/*",
        "	 * Cannot fail STARTING/DYING callbacks.",
        "	 */",
        "	if (cpuhp_is_atomic_state(fail))",
        "		return -EINVAL;",
        "",
        "	/*",
        "	 * DEAD callbacks cannot fail...",
        "	 * ... neither can CPUHP_BRINGUP_CPU during hotunplug. The latter",
        "	 * triggering STARTING callbacks, a failure in this state would",
        "	 * hinder rollback.",
        "	 */",
        "	if (fail <= CPUHP_BRINGUP_CPU && st->state > CPUHP_BRINGUP_CPU)",
        "		return -EINVAL;",
        "",
        "	/*",
        "	 * Cannot fail anything that doesn't have callbacks.",
        "	 */",
        "	mutex_lock(&cpuhp_state_mutex);",
        "	sp = cpuhp_get_step(fail);",
        "	if (!sp->startup.single && !sp->teardown.single)",
        "		ret = -EINVAL;",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	if (ret)",
        "		return ret;",
        "",
        "	st->fail = fail;",
        "",
        "	return count;",
        "}",
        "",
        "static ssize_t fail_show(struct device *dev,",
        "			 struct device_attribute *attr, char *buf)",
        "{",
        "	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);",
        "",
        "	return sprintf(buf, \"%d\\n\", st->fail);",
        "}",
        "",
        "static DEVICE_ATTR_RW(fail);",
        "",
        "static struct attribute *cpuhp_cpu_attrs[] = {",
        "	&dev_attr_state.attr,",
        "	&dev_attr_target.attr,",
        "	&dev_attr_fail.attr,",
        "	NULL",
        "};",
        "",
        "static const struct attribute_group cpuhp_cpu_attr_group = {",
        "	.attrs = cpuhp_cpu_attrs,",
        "	.name = \"hotplug\",",
        "};",
        "",
        "static ssize_t states_show(struct device *dev,",
        "				 struct device_attribute *attr, char *buf)",
        "{",
        "	ssize_t cur, res = 0;",
        "	int i;",
        "",
        "	mutex_lock(&cpuhp_state_mutex);",
        "	for (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {",
        "		struct cpuhp_step *sp = cpuhp_get_step(i);",
        "",
        "		if (sp->name) {",
        "			cur = sprintf(buf, \"%3d: %s\\n\", i, sp->name);",
        "			buf += cur;",
        "			res += cur;",
        "		}",
        "	}",
        "	mutex_unlock(&cpuhp_state_mutex);",
        "	return res;",
        "}",
        "static DEVICE_ATTR_RO(states);",
        "",
        "static struct attribute *cpuhp_cpu_root_attrs[] = {",
        "	&dev_attr_states.attr,",
        "	NULL",
        "};",
        "",
        "static const struct attribute_group cpuhp_cpu_root_attr_group = {",
        "	.attrs = cpuhp_cpu_root_attrs,",
        "	.name = \"hotplug\",",
        "};",
        "",
        "#ifdef CONFIG_HOTPLUG_SMT",
        "",
        "static bool cpu_smt_num_threads_valid(unsigned int threads)",
        "{",
        "	if (IS_ENABLED(CONFIG_SMT_NUM_THREADS_DYNAMIC))",
        "		return threads >= 1 && threads <= cpu_smt_max_threads;",
        "	return threads == 1 || threads == cpu_smt_max_threads;",
        "}",
        "",
        "static ssize_t",
        "__store_smt_control(struct device *dev, struct device_attribute *attr,",
        "		    const char *buf, size_t count)",
        "{",
        "	int ctrlval, ret, num_threads, orig_threads;",
        "	bool force_off;",
        "",
        "	if (cpu_smt_control == CPU_SMT_FORCE_DISABLED)",
        "		return -EPERM;",
        "",
        "	if (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)",
        "		return -ENODEV;",
        "",
        "	if (sysfs_streq(buf, \"on\")) {",
        "		ctrlval = CPU_SMT_ENABLED;",
        "		num_threads = cpu_smt_max_threads;",
        "	} else if (sysfs_streq(buf, \"off\")) {",
        "		ctrlval = CPU_SMT_DISABLED;",
        "		num_threads = 1;",
        "	} else if (sysfs_streq(buf, \"forceoff\")) {",
        "		ctrlval = CPU_SMT_FORCE_DISABLED;",
        "		num_threads = 1;",
        "	} else if (kstrtoint(buf, 10, &num_threads) == 0) {",
        "		if (num_threads == 1)",
        "			ctrlval = CPU_SMT_DISABLED;",
        "		else if (cpu_smt_num_threads_valid(num_threads))",
        "			ctrlval = CPU_SMT_ENABLED;",
        "		else",
        "			return -EINVAL;",
        "	} else {",
        "		return -EINVAL;",
        "	}",
        "",
        "	ret = lock_device_hotplug_sysfs();",
        "	if (ret)",
        "		return ret;",
        "",
        "	orig_threads = cpu_smt_num_threads;",
        "	cpu_smt_num_threads = num_threads;",
        "",
        "	force_off = ctrlval != cpu_smt_control && ctrlval == CPU_SMT_FORCE_DISABLED;",
        "",
        "	if (num_threads > orig_threads)",
        "		ret = cpuhp_smt_enable();",
        "	else if (num_threads < orig_threads || force_off)",
        "		ret = cpuhp_smt_disable(ctrlval);",
        "",
        "	unlock_device_hotplug();",
        "	return ret ? ret : count;",
        "}",
        "",
        "#else /* !CONFIG_HOTPLUG_SMT */",
        "static ssize_t",
        "__store_smt_control(struct device *dev, struct device_attribute *attr,",
        "		    const char *buf, size_t count)",
        "{",
        "	return -ENODEV;",
        "}",
        "#endif /* CONFIG_HOTPLUG_SMT */",
        "",
        "static const char *smt_states[] = {",
        "	[CPU_SMT_ENABLED]		= \"on\",",
        "	[CPU_SMT_DISABLED]		= \"off\",",
        "	[CPU_SMT_FORCE_DISABLED]	= \"forceoff\",",
        "	[CPU_SMT_NOT_SUPPORTED]		= \"notsupported\",",
        "	[CPU_SMT_NOT_IMPLEMENTED]	= \"notimplemented\",",
        "};",
        "",
        "static ssize_t control_show(struct device *dev,",
        "			    struct device_attribute *attr, char *buf)",
        "{",
        "	const char *state = smt_states[cpu_smt_control];",
        "",
        "#ifdef CONFIG_HOTPLUG_SMT",
        "	/*",
        "	 * If SMT is enabled but not all threads are enabled then show the",
        "	 * number of threads. If all threads are enabled show \"on\". Otherwise",
        "	 * show the state name.",
        "	 */",
        "	if (cpu_smt_control == CPU_SMT_ENABLED &&",
        "	    cpu_smt_num_threads != cpu_smt_max_threads)",
        "		return sysfs_emit(buf, \"%d\\n\", cpu_smt_num_threads);",
        "#endif",
        "",
        "	return sysfs_emit(buf, \"%s\\n\", state);",
        "}",
        "",
        "static ssize_t control_store(struct device *dev, struct device_attribute *attr,",
        "			     const char *buf, size_t count)",
        "{",
        "	return __store_smt_control(dev, attr, buf, count);",
        "}",
        "static DEVICE_ATTR_RW(control);",
        "",
        "static ssize_t active_show(struct device *dev,",
        "			   struct device_attribute *attr, char *buf)",
        "{",
        "	return sysfs_emit(buf, \"%d\\n\", sched_smt_active());",
        "}",
        "static DEVICE_ATTR_RO(active);",
        "",
        "static struct attribute *cpuhp_smt_attrs[] = {",
        "	&dev_attr_control.attr,",
        "	&dev_attr_active.attr,",
        "	NULL",
        "};",
        "",
        "static const struct attribute_group cpuhp_smt_attr_group = {",
        "	.attrs = cpuhp_smt_attrs,",
        "	.name = \"smt\",",
        "};",
        "",
        "static int __init cpu_smt_sysfs_init(void)",
        "{",
        "	struct device *dev_root;",
        "	int ret = -ENODEV;",
        "",
        "	dev_root = bus_get_dev_root(&cpu_subsys);",
        "	if (dev_root) {",
        "		ret = sysfs_create_group(&dev_root->kobj, &cpuhp_smt_attr_group);",
        "		put_device(dev_root);",
        "	}",
        "	return ret;",
        "}",
        "",
        "static int __init cpuhp_sysfs_init(void)",
        "{",
        "	struct device *dev_root;",
        "	int cpu, ret;",
        "",
        "	ret = cpu_smt_sysfs_init();",
        "	if (ret)",
        "		return ret;",
        "",
        "	dev_root = bus_get_dev_root(&cpu_subsys);",
        "	if (dev_root) {",
        "		ret = sysfs_create_group(&dev_root->kobj, &cpuhp_cpu_root_attr_group);",
        "		put_device(dev_root);",
        "		if (ret)",
        "			return ret;",
        "	}",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		struct device *dev = get_cpu_device(cpu);",
        "",
        "		if (!dev)",
        "			continue;",
        "		ret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);",
        "		if (ret)",
        "			return ret;",
        "	}",
        "	return 0;",
        "}",
        "device_initcall(cpuhp_sysfs_init);",
        "#endif /* CONFIG_SYSFS && CONFIG_HOTPLUG_CPU */",
        "",
        "/*",
        " * cpu_bit_bitmap[] is a special, \"compressed\" data structure that",
        " * represents all NR_CPUS bits binary values of 1<<nr.",
        " *",
        " * It is used by cpumask_of() to get a constant address to a CPU",
        " * mask value that has a single bit set only.",
        " */",
        "",
        "/* cpu_bit_bitmap[0] is empty - so we can back into it */",
        "#define MASK_DECLARE_1(x)	[x+1][0] = (1UL << (x))",
        "#define MASK_DECLARE_2(x)	MASK_DECLARE_1(x), MASK_DECLARE_1(x+1)",
        "#define MASK_DECLARE_4(x)	MASK_DECLARE_2(x), MASK_DECLARE_2(x+2)",
        "#define MASK_DECLARE_8(x)	MASK_DECLARE_4(x), MASK_DECLARE_4(x+4)",
        "",
        "const unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {",
        "",
        "	MASK_DECLARE_8(0),	MASK_DECLARE_8(8),",
        "	MASK_DECLARE_8(16),	MASK_DECLARE_8(24),",
        "#if BITS_PER_LONG > 32",
        "	MASK_DECLARE_8(32),	MASK_DECLARE_8(40),",
        "	MASK_DECLARE_8(48),	MASK_DECLARE_8(56),",
        "#endif",
        "};",
        "EXPORT_SYMBOL_GPL(cpu_bit_bitmap);",
        "",
        "const DECLARE_BITMAP(cpu_all_bits, NR_CPUS) = CPU_BITS_ALL;",
        "EXPORT_SYMBOL(cpu_all_bits);",
        "",
        "#ifdef CONFIG_INIT_ALL_POSSIBLE",
        "struct cpumask __cpu_possible_mask __ro_after_init",
        "	= {CPU_BITS_ALL};",
        "#else",
        "struct cpumask __cpu_possible_mask __ro_after_init;",
        "#endif",
        "EXPORT_SYMBOL(__cpu_possible_mask);",
        "",
        "struct cpumask __cpu_online_mask __read_mostly;",
        "EXPORT_SYMBOL(__cpu_online_mask);",
        "",
        "struct cpumask __cpu_enabled_mask __read_mostly;",
        "EXPORT_SYMBOL(__cpu_enabled_mask);",
        "",
        "struct cpumask __cpu_present_mask __read_mostly;",
        "EXPORT_SYMBOL(__cpu_present_mask);",
        "",
        "struct cpumask __cpu_active_mask __read_mostly;",
        "EXPORT_SYMBOL(__cpu_active_mask);",
        "",
        "struct cpumask __cpu_dying_mask __read_mostly;",
        "EXPORT_SYMBOL(__cpu_dying_mask);",
        "",
        "atomic_t __num_online_cpus __read_mostly;",
        "EXPORT_SYMBOL(__num_online_cpus);",
        "",
        "void init_cpu_present(const struct cpumask *src)",
        "{",
        "	cpumask_copy(&__cpu_present_mask, src);",
        "}",
        "",
        "void init_cpu_possible(const struct cpumask *src)",
        "{",
        "	cpumask_copy(&__cpu_possible_mask, src);",
        "}",
        "",
        "void init_cpu_online(const struct cpumask *src)",
        "{",
        "	cpumask_copy(&__cpu_online_mask, src);",
        "}",
        "",
        "void set_cpu_online(unsigned int cpu, bool online)",
        "{",
        "	/*",
        "	 * atomic_inc/dec() is required to handle the horrid abuse of this",
        "	 * function by the reboot and kexec code which invoke it from",
        "	 * IPI/NMI broadcasts when shutting down CPUs. Invocation from",
        "	 * regular CPU hotplug is properly serialized.",
        "	 *",
        "	 * Note, that the fact that __num_online_cpus is of type atomic_t",
        "	 * does not protect readers which are not serialized against",
        "	 * concurrent hotplug operations.",
        "	 */",
        "	if (online) {",
        "		if (!cpumask_test_and_set_cpu(cpu, &__cpu_online_mask))",
        "			atomic_inc(&__num_online_cpus);",
        "	} else {",
        "		if (cpumask_test_and_clear_cpu(cpu, &__cpu_online_mask))",
        "			atomic_dec(&__num_online_cpus);",
        "	}",
        "}",
        "",
        "/*",
        " * Activate the first processor.",
        " */",
        "void __init boot_cpu_init(void)",
        "{",
        "	int cpu = smp_processor_id();",
        "",
        "	/* Mark the boot cpu \"present\", \"online\" etc for SMP and UP case */",
        "	set_cpu_online(cpu, true);",
        "	set_cpu_active(cpu, true);",
        "	set_cpu_present(cpu, true);",
        "	set_cpu_possible(cpu, true);",
        "",
        "#ifdef CONFIG_SMP",
        "	__boot_cpu_id = cpu;",
        "#endif",
        "}",
        "",
        "/*",
        " * Must be called _AFTER_ setting up the per_cpu areas",
        " */",
        "void __init boot_cpu_hotplug_init(void)",
        "{",
        "#ifdef CONFIG_SMP",
        "	cpumask_set_cpu(smp_processor_id(), &cpus_booted_once_mask);",
        "	atomic_set(this_cpu_ptr(&cpuhp_state.ap_sync_state), SYNC_STATE_ONLINE);",
        "#endif",
        "	this_cpu_write(cpuhp_state.state, CPUHP_ONLINE);",
        "	this_cpu_write(cpuhp_state.target, CPUHP_ONLINE);",
        "}",
        "",
        "#ifdef CONFIG_CPU_MITIGATIONS",
        "/*",
        " * These are used for a global \"mitigations=\" cmdline option for toggling",
        " * optional CPU mitigations.",
        " */",
        "enum cpu_mitigations {",
        "	CPU_MITIGATIONS_OFF,",
        "	CPU_MITIGATIONS_AUTO,",
        "	CPU_MITIGATIONS_AUTO_NOSMT,",
        "};",
        "",
        "static enum cpu_mitigations cpu_mitigations __ro_after_init = CPU_MITIGATIONS_AUTO;",
        "",
        "static int __init mitigations_parse_cmdline(char *arg)",
        "{",
        "	if (!strcmp(arg, \"off\"))",
        "		cpu_mitigations = CPU_MITIGATIONS_OFF;",
        "	else if (!strcmp(arg, \"auto\"))",
        "		cpu_mitigations = CPU_MITIGATIONS_AUTO;",
        "	else if (!strcmp(arg, \"auto,nosmt\"))",
        "		cpu_mitigations = CPU_MITIGATIONS_AUTO_NOSMT;",
        "	else",
        "		pr_crit(\"Unsupported mitigations=%s, system may still be vulnerable\\n\",",
        "			arg);",
        "",
        "	return 0;",
        "}",
        "",
        "/* mitigations=off */",
        "bool cpu_mitigations_off(void)",
        "{",
        "	return cpu_mitigations == CPU_MITIGATIONS_OFF;",
        "}",
        "EXPORT_SYMBOL_GPL(cpu_mitigations_off);",
        "",
        "/* mitigations=auto,nosmt */",
        "bool cpu_mitigations_auto_nosmt(void)",
        "{",
        "	return cpu_mitigations == CPU_MITIGATIONS_AUTO_NOSMT;",
        "}",
        "EXPORT_SYMBOL_GPL(cpu_mitigations_auto_nosmt);",
        "#else",
        "static int __init mitigations_parse_cmdline(char *arg)",
        "{",
        "	pr_crit(\"Kernel compiled without mitigations, ignoring 'mitigations'; system may still be vulnerable\\n\");",
        "	return 0;",
        "}",
        "#endif",
        "early_param(\"mitigations\", mitigations_parse_cmdline);"
    ]
  },
  "include_linux_sched_signal_h": {
    path: "include/linux/sched/signal.h",
    covered: [400, 422],
    totalLines: 782,
    coveredCount: 2,
    coveragePct: 0.3,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_SCHED_SIGNAL_H",
        "#define _LINUX_SCHED_SIGNAL_H",
        "",
        "#include <linux/rculist.h>",
        "#include <linux/signal.h>",
        "#include <linux/sched.h>",
        "#include <linux/sched/jobctl.h>",
        "#include <linux/sched/task.h>",
        "#include <linux/cred.h>",
        "#include <linux/refcount.h>",
        "#include <linux/pid.h>",
        "#include <linux/posix-timers.h>",
        "#include <linux/mm_types.h>",
        "#include <asm/ptrace.h>",
        "",
        "/*",
        " * Types defining task->signal and task->sighand and APIs using them:",
        " */",
        "",
        "struct sighand_struct {",
        "	spinlock_t		siglock;",
        "	refcount_t		count;",
        "	wait_queue_head_t	signalfd_wqh;",
        "	struct k_sigaction	action[_NSIG];",
        "};",
        "",
        "/*",
        " * Per-process accounting stats:",
        " */",
        "struct pacct_struct {",
        "	int			ac_flag;",
        "	long			ac_exitcode;",
        "	unsigned long		ac_mem;",
        "	u64			ac_utime, ac_stime;",
        "	unsigned long		ac_minflt, ac_majflt;",
        "};",
        "",
        "struct cpu_itimer {",
        "	u64 expires;",
        "	u64 incr;",
        "};",
        "",
        "/*",
        " * This is the atomic variant of task_cputime, which can be used for",
        " * storing and updating task_cputime statistics without locking.",
        " */",
        "struct task_cputime_atomic {",
        "	atomic64_t utime;",
        "	atomic64_t stime;",
        "	atomic64_t sum_exec_runtime;",
        "};",
        "",
        "#define INIT_CPUTIME_ATOMIC \\",
        "	(struct task_cputime_atomic) {				\\",
        "		.utime = ATOMIC64_INIT(0),			\\",
        "		.stime = ATOMIC64_INIT(0),			\\",
        "		.sum_exec_runtime = ATOMIC64_INIT(0),		\\",
        "	}",
        "/**",
        " * struct thread_group_cputimer - thread group interval timer counts",
        " * @cputime_atomic:	atomic thread group interval timers.",
        " *",
        " * This structure contains the version of task_cputime, above, that is",
        " * used for thread group CPU timer calculations.",
        " */",
        "struct thread_group_cputimer {",
        "	struct task_cputime_atomic cputime_atomic;",
        "};",
        "",
        "struct multiprocess_signals {",
        "	sigset_t signal;",
        "	struct hlist_node node;",
        "};",
        "",
        "struct core_thread {",
        "	struct task_struct *task;",
        "	struct core_thread *next;",
        "};",
        "",
        "struct core_state {",
        "	atomic_t nr_threads;",
        "	struct core_thread dumper;",
        "	struct completion startup;",
        "};",
        "",
        "/*",
        " * NOTE! \"signal_struct\" does not have its own",
        " * locking, because a shared signal_struct always",
        " * implies a shared sighand_struct, so locking",
        " * sighand_struct is always a proper superset of",
        " * the locking of signal_struct.",
        " */",
        "struct signal_struct {",
        "	refcount_t		sigcnt;",
        "	atomic_t		live;",
        "	int			nr_threads;",
        "	int			quick_threads;",
        "	struct list_head	thread_head;",
        "",
        "	wait_queue_head_t	wait_chldexit;	/* for wait4() */",
        "",
        "	/* current thread group signal load-balancing target: */",
        "	struct task_struct	*curr_target;",
        "",
        "	/* shared signal handling: */",
        "	struct sigpending	shared_pending;",
        "",
        "	/* For collecting multiprocess signals during fork */",
        "	struct hlist_head	multiprocess;",
        "",
        "	/* thread group exit support */",
        "	int			group_exit_code;",
        "	/* notify group_exec_task when notify_count is less or equal to 0 */",
        "	int			notify_count;",
        "	struct task_struct	*group_exec_task;",
        "",
        "	/* thread group stop support, overloads group_exit_code too */",
        "	int			group_stop_count;",
        "	unsigned int		flags; /* see SIGNAL_* flags below */",
        "",
        "	struct core_state *core_state; /* coredumping support */",
        "",
        "	/*",
        "	 * PR_SET_CHILD_SUBREAPER marks a process, like a service",
        "	 * manager, to re-parent orphan (double-forking) child processes",
        "	 * to this process instead of 'init'. The service manager is",
        "	 * able to receive SIGCHLD signals and is able to investigate",
        "	 * the process until it calls wait(). All children of this",
        "	 * process will inherit a flag if they should look for a",
        "	 * child_subreaper process at exit.",
        "	 */",
        "	unsigned int		is_child_subreaper:1;",
        "	unsigned int		has_child_subreaper:1;",
        "",
        "#ifdef CONFIG_POSIX_TIMERS",
        "",
        "	/* POSIX.1b Interval Timers */",
        "	unsigned int		next_posix_timer_id;",
        "	struct hlist_head	posix_timers;",
        "	struct hlist_head	ignored_posix_timers;",
        "",
        "	/* ITIMER_REAL timer for the process */",
        "	struct hrtimer real_timer;",
        "	ktime_t it_real_incr;",
        "",
        "	/*",
        "	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use",
        "	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these",
        "	 * values are defined to 0 and 1 respectively",
        "	 */",
        "	struct cpu_itimer it[2];",
        "",
        "	/*",
        "	 * Thread group totals for process CPU timers.",
        "	 * See thread_group_cputimer(), et al, for details.",
        "	 */",
        "	struct thread_group_cputimer cputimer;",
        "",
        "#endif",
        "	/* Empty if CONFIG_POSIX_TIMERS=n */",
        "	struct posix_cputimers posix_cputimers;",
        "",
        "	/* PID/PID hash table linkage. */",
        "	struct pid *pids[PIDTYPE_MAX];",
        "",
        "#ifdef CONFIG_NO_HZ_FULL",
        "	atomic_t tick_dep_mask;",
        "#endif",
        "",
        "	struct pid *tty_old_pgrp;",
        "",
        "	/* boolean value for session group leader */",
        "	int leader;",
        "",
        "	struct tty_struct *tty; /* NULL if no tty */",
        "",
        "#ifdef CONFIG_SCHED_AUTOGROUP",
        "	struct autogroup *autogroup;",
        "#endif",
        "	/*",
        "	 * Cumulative resource counters for dead threads in the group,",
        "	 * and for reaped dead child processes forked by this group.",
        "	 * Live threads maintain their own counters and add to these",
        "	 * in __exit_signal, except for the group leader.",
        "	 */",
        "	seqlock_t stats_lock;",
        "	u64 utime, stime, cutime, cstime;",
        "	u64 gtime;",
        "	u64 cgtime;",
        "	struct prev_cputime prev_cputime;",
        "	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;",
        "	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;",
        "	unsigned long inblock, oublock, cinblock, coublock;",
        "	unsigned long maxrss, cmaxrss;",
        "	struct task_io_accounting ioac;",
        "",
        "	/*",
        "	 * Cumulative ns of schedule CPU time fo dead threads in the",
        "	 * group, not including a zombie group leader, (This only differs",
        "	 * from jiffies_to_ns(utime + stime) if sched_clock uses something",
        "	 * other than jiffies.)",
        "	 */",
        "	unsigned long long sum_sched_runtime;",
        "",
        "	/*",
        "	 * We don't bother to synchronize most readers of this at all,",
        "	 * because there is no reader checking a limit that actually needs",
        "	 * to get both rlim_cur and rlim_max atomically, and either one",
        "	 * alone is a single word that can safely be read normally.",
        "	 * getrlimit/setrlimit use task_lock(current->group_leader) to",
        "	 * protect this instead of the siglock, because they really",
        "	 * have no need to disable irqs.",
        "	 */",
        "	struct rlimit rlim[RLIM_NLIMITS];",
        "",
        "#ifdef CONFIG_BSD_PROCESS_ACCT",
        "	struct pacct_struct pacct;	/* per-process accounting information */",
        "#endif",
        "#ifdef CONFIG_TASKSTATS",
        "	struct taskstats *stats;",
        "#endif",
        "#ifdef CONFIG_AUDIT",
        "	unsigned audit_tty;",
        "	struct tty_audit_buf *tty_audit_buf;",
        "#endif",
        "",
        "	/*",
        "	 * Thread is the potential origin of an oom condition; kill first on",
        "	 * oom",
        "	 */",
        "	bool oom_flag_origin;",
        "	short oom_score_adj;		/* OOM kill score adjustment */",
        "	short oom_score_adj_min;	/* OOM kill score adjustment min value.",
        "					 * Only settable by CAP_SYS_RESOURCE. */",
        "	struct mm_struct *oom_mm;	/* recorded mm when the thread group got",
        "					 * killed by the oom killer */",
        "",
        "	struct mutex cred_guard_mutex;	/* guard against foreign influences on",
        "					 * credential calculations",
        "					 * (notably. ptrace)",
        "					 * Deprecated do not use in new code.",
        "					 * Use exec_update_lock instead.",
        "					 */",
        "	struct rw_semaphore exec_update_lock;	/* Held while task_struct is",
        "						 * being updated during exec,",
        "						 * and may have inconsistent",
        "						 * permissions.",
        "						 */",
        "} __randomize_layout;",
        "",
        "/*",
        " * Bits in flags field of signal_struct.",
        " */",
        "#define SIGNAL_STOP_STOPPED	0x00000001 /* job control stop in effect */",
        "#define SIGNAL_STOP_CONTINUED	0x00000002 /* SIGCONT since WCONTINUED reap */",
        "#define SIGNAL_GROUP_EXIT	0x00000004 /* group exit in progress */",
        "/*",
        " * Pending notifications to parent.",
        " */",
        "#define SIGNAL_CLD_STOPPED	0x00000010",
        "#define SIGNAL_CLD_CONTINUED	0x00000020",
        "#define SIGNAL_CLD_MASK		(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)",
        "",
        "#define SIGNAL_UNKILLABLE	0x00000040 /* for init: ignore fatal signals */",
        "",
        "#define SIGNAL_STOP_MASK (SIGNAL_CLD_MASK | SIGNAL_STOP_STOPPED | \\",
        "			  SIGNAL_STOP_CONTINUED)",
        "",
        "static inline void signal_set_stop_flags(struct signal_struct *sig,",
        "					 unsigned int flags)",
        "{",
        "	WARN_ON(sig->flags & SIGNAL_GROUP_EXIT);",
        "	sig->flags = (sig->flags & ~SIGNAL_STOP_MASK) | flags;",
        "}",
        "",
        "extern void flush_signals(struct task_struct *);",
        "extern void ignore_signals(struct task_struct *);",
        "extern void flush_signal_handlers(struct task_struct *, int force_default);",
        "extern int dequeue_signal(sigset_t *mask, kernel_siginfo_t *info, enum pid_type *type);",
        "",
        "static inline int kernel_dequeue_signal(void)",
        "{",
        "	struct task_struct *task = current;",
        "	kernel_siginfo_t __info;",
        "	enum pid_type __type;",
        "	int ret;",
        "",
        "	spin_lock_irq(&task->sighand->siglock);",
        "	ret = dequeue_signal(&task->blocked, &__info, &__type);",
        "	spin_unlock_irq(&task->sighand->siglock);",
        "",
        "	return ret;",
        "}",
        "",
        "static inline void kernel_signal_stop(void)",
        "{",
        "	spin_lock_irq(&current->sighand->siglock);",
        "	if (current->jobctl & JOBCTL_STOP_DEQUEUED) {",
        "		current->jobctl |= JOBCTL_STOPPED;",
        "		set_special_state(TASK_STOPPED);",
        "	}",
        "	spin_unlock_irq(&current->sighand->siglock);",
        "",
        "	schedule();",
        "}",
        "",
        "int force_sig_fault_to_task(int sig, int code, void __user *addr,",
        "			    struct task_struct *t);",
        "int force_sig_fault(int sig, int code, void __user *addr);",
        "int send_sig_fault(int sig, int code, void __user *addr, struct task_struct *t);",
        "",
        "int force_sig_mceerr(int code, void __user *, short);",
        "int send_sig_mceerr(int code, void __user *, short, struct task_struct *);",
        "",
        "int force_sig_bnderr(void __user *addr, void __user *lower, void __user *upper);",
        "int force_sig_pkuerr(void __user *addr, u32 pkey);",
        "int send_sig_perf(void __user *addr, u32 type, u64 sig_data);",
        "",
        "int force_sig_ptrace_errno_trap(int errno, void __user *addr);",
        "int force_sig_fault_trapno(int sig, int code, void __user *addr, int trapno);",
        "int send_sig_fault_trapno(int sig, int code, void __user *addr, int trapno,",
        "			struct task_struct *t);",
        "int force_sig_seccomp(int syscall, int reason, bool force_coredump);",
        "",
        "extern int send_sig_info(int, struct kernel_siginfo *, struct task_struct *);",
        "extern void force_sigsegv(int sig);",
        "extern int force_sig_info(struct kernel_siginfo *);",
        "extern int __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp);",
        "extern int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid);",
        "extern int kill_pid_usb_asyncio(int sig, int errno, sigval_t addr, struct pid *,",
        "				const struct cred *);",
        "extern int kill_pgrp(struct pid *pid, int sig, int priv);",
        "extern int kill_pid(struct pid *pid, int sig, int priv);",
        "extern __must_check bool do_notify_parent(struct task_struct *, int);",
        "extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);",
        "extern void force_sig(int);",
        "extern void force_fatal_sig(int);",
        "extern void force_exit_sig(int);",
        "extern int send_sig(int, struct task_struct *, int);",
        "extern int zap_other_threads(struct task_struct *p);",
        "extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);",
        "",
        "static inline void clear_notify_signal(void)",
        "{",
        "	clear_thread_flag(TIF_NOTIFY_SIGNAL);",
        "	smp_mb__after_atomic();",
        "}",
        "",
        "/*",
        " * Returns 'true' if kick_process() is needed to force a transition from",
        " * user -> kernel to guarantee expedient run of TWA_SIGNAL based task_work.",
        " */",
        "static inline bool __set_notify_signal(struct task_struct *task)",
        "{",
        "	return !test_and_set_tsk_thread_flag(task, TIF_NOTIFY_SIGNAL) &&",
        "	       !wake_up_state(task, TASK_INTERRUPTIBLE);",
        "}",
        "",
        "/*",
        " * Called to break out of interruptible wait loops, and enter the",
        " * exit_to_user_mode_loop().",
        " */",
        "static inline void set_notify_signal(struct task_struct *task)",
        "{",
        "	if (__set_notify_signal(task))",
        "		kick_process(task);",
        "}",
        "",
        "static inline int restart_syscall(void)",
        "{",
        "	set_tsk_thread_flag(current, TIF_SIGPENDING);",
        "	return -ERESTARTNOINTR;",
        "}",
        "",
        "static inline int task_sigpending(struct task_struct *p)",
        "{",
        "	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));",
        "}",
        "",
        "static inline int signal_pending(struct task_struct *p)",
        "{",
        "	/*",
        "	 * TIF_NOTIFY_SIGNAL isn't really a signal, but it requires the same",
        "	 * behavior in terms of ensuring that we break out of wait loops",
        "	 * so that notify signal callbacks can be processed.",
        "	 */",
        "	if (unlikely(test_tsk_thread_flag(p, TIF_NOTIFY_SIGNAL)))",
        "		return 1;",
        "	return task_sigpending(p);",
        "}",
        "",
        "static inline int __fatal_signal_pending(struct task_struct *p)",
        "{",
        "	return unlikely(sigismember(&p->pending.signal, SIGKILL));",
        "}",
        "",
        "static inline int fatal_signal_pending(struct task_struct *p)",
        "{",
        "	return task_sigpending(p) && __fatal_signal_pending(p);",
        "}",
        "",
        "static inline int signal_pending_state(unsigned int state, struct task_struct *p)",
        "{",
        "	if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))",
        "		return 0;",
        "	if (!signal_pending(p))",
        "		return 0;",
        "",
        "	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);",
        "}",
        "",
        "/*",
        " * This should only be used in fault handlers to decide whether we",
        " * should stop the current fault routine to handle the signals",
        " * instead, especially with the case where we've got interrupted with",
        " * a VM_FAULT_RETRY.",
        " */",
        "static inline bool fault_signal_pending(vm_fault_t fault_flags,",
        "					struct pt_regs *regs)",
        "{",
        "	return unlikely((fault_flags & VM_FAULT_RETRY) &&",
        "			(fatal_signal_pending(current) ||",
        "			 (user_mode(regs) && signal_pending(current))));",
        "}",
        "",
        "/*",
        " * Reevaluate whether the task has signals pending delivery.",
        " * Wake the task if so.",
        " * This is required every time the blocked sigset_t changes.",
        " * callers must hold sighand->siglock.",
        " */",
        "extern void recalc_sigpending(void);",
        "extern void calculate_sigpending(void);",
        "",
        "extern void signal_wake_up_state(struct task_struct *t, unsigned int state);",
        "",
        "static inline void signal_wake_up(struct task_struct *t, bool fatal)",
        "{",
        "	unsigned int state = 0;",
        "	if (fatal && !(t->jobctl & JOBCTL_PTRACE_FROZEN)) {",
        "		t->jobctl &= ~(JOBCTL_STOPPED | JOBCTL_TRACED);",
        "		state = TASK_WAKEKILL | __TASK_TRACED;",
        "	}",
        "	signal_wake_up_state(t, state);",
        "}",
        "static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)",
        "{",
        "	unsigned int state = 0;",
        "	if (resume) {",
        "		t->jobctl &= ~JOBCTL_TRACED;",
        "		state = __TASK_TRACED;",
        "	}",
        "	signal_wake_up_state(t, state);",
        "}",
        "",
        "void task_join_group_stop(struct task_struct *task);",
        "",
        "#ifdef TIF_RESTORE_SIGMASK",
        "/*",
        " * Legacy restore_sigmask accessors.  These are inefficient on",
        " * SMP architectures because they require atomic operations.",
        " */",
        "",
        "/**",
        " * set_restore_sigmask() - make sure saved_sigmask processing gets done",
        " *",
        " * This sets TIF_RESTORE_SIGMASK and ensures that the arch signal code",
        " * will run before returning to user mode, to process the flag.  For",
        " * all callers, TIF_SIGPENDING is already set or it's no harm to set",
        " * it.  TIF_RESTORE_SIGMASK need not be in the set of bits that the",
        " * arch code will notice on return to user mode, in case those bits",
        " * are scarce.  We set TIF_SIGPENDING here to ensure that the arch",
        " * signal code always gets run when TIF_RESTORE_SIGMASK is set.",
        " */",
        "static inline void set_restore_sigmask(void)",
        "{",
        "	set_thread_flag(TIF_RESTORE_SIGMASK);",
        "}",
        "",
        "static inline void clear_tsk_restore_sigmask(struct task_struct *task)",
        "{",
        "	clear_tsk_thread_flag(task, TIF_RESTORE_SIGMASK);",
        "}",
        "",
        "static inline void clear_restore_sigmask(void)",
        "{",
        "	clear_thread_flag(TIF_RESTORE_SIGMASK);",
        "}",
        "static inline bool test_tsk_restore_sigmask(struct task_struct *task)",
        "{",
        "	return test_tsk_thread_flag(task, TIF_RESTORE_SIGMASK);",
        "}",
        "static inline bool test_restore_sigmask(void)",
        "{",
        "	return test_thread_flag(TIF_RESTORE_SIGMASK);",
        "}",
        "static inline bool test_and_clear_restore_sigmask(void)",
        "{",
        "	return test_and_clear_thread_flag(TIF_RESTORE_SIGMASK);",
        "}",
        "",
        "#else	/* TIF_RESTORE_SIGMASK */",
        "",
        "/* Higher-quality implementation, used if TIF_RESTORE_SIGMASK doesn't exist. */",
        "static inline void set_restore_sigmask(void)",
        "{",
        "	current->restore_sigmask = true;",
        "}",
        "static inline void clear_tsk_restore_sigmask(struct task_struct *task)",
        "{",
        "	task->restore_sigmask = false;",
        "}",
        "static inline void clear_restore_sigmask(void)",
        "{",
        "	current->restore_sigmask = false;",
        "}",
        "static inline bool test_restore_sigmask(void)",
        "{",
        "	return current->restore_sigmask;",
        "}",
        "static inline bool test_tsk_restore_sigmask(struct task_struct *task)",
        "{",
        "	return task->restore_sigmask;",
        "}",
        "static inline bool test_and_clear_restore_sigmask(void)",
        "{",
        "	if (!current->restore_sigmask)",
        "		return false;",
        "	current->restore_sigmask = false;",
        "	return true;",
        "}",
        "#endif",
        "",
        "static inline void restore_saved_sigmask(void)",
        "{",
        "	if (test_and_clear_restore_sigmask())",
        "		__set_current_blocked(&current->saved_sigmask);",
        "}",
        "",
        "extern int set_user_sigmask(const sigset_t __user *umask, size_t sigsetsize);",
        "",
        "static inline void restore_saved_sigmask_unless(bool interrupted)",
        "{",
        "	if (interrupted)",
        "		WARN_ON(!signal_pending(current));",
        "	else",
        "		restore_saved_sigmask();",
        "}",
        "",
        "static inline sigset_t *sigmask_to_save(void)",
        "{",
        "	sigset_t *res = &current->blocked;",
        "	if (unlikely(test_restore_sigmask()))",
        "		res = &current->saved_sigmask;",
        "	return res;",
        "}",
        "",
        "static inline int kill_cad_pid(int sig, int priv)",
        "{",
        "	return kill_pid(cad_pid, sig, priv);",
        "}",
        "",
        "/* These can be the second arg to send_sig_info/send_group_sig_info.  */",
        "#define SEND_SIG_NOINFO ((struct kernel_siginfo *) 0)",
        "#define SEND_SIG_PRIV	((struct kernel_siginfo *) 1)",
        "",
        "static inline int __on_sig_stack(unsigned long sp)",
        "{",
        "#ifdef CONFIG_STACK_GROWSUP",
        "	return sp >= current->sas_ss_sp &&",
        "		sp - current->sas_ss_sp < current->sas_ss_size;",
        "#else",
        "	return sp > current->sas_ss_sp &&",
        "		sp - current->sas_ss_sp <= current->sas_ss_size;",
        "#endif",
        "}",
        "",
        "/*",
        " * True if we are on the alternate signal stack.",
        " */",
        "static inline int on_sig_stack(unsigned long sp)",
        "{",
        "	/*",
        "	 * If the signal stack is SS_AUTODISARM then, by construction, we",
        "	 * can't be on the signal stack unless user code deliberately set",
        "	 * SS_AUTODISARM when we were already on it.",
        "	 *",
        "	 * This improves reliability: if user state gets corrupted such that",
        "	 * the stack pointer points very close to the end of the signal stack,",
        "	 * then this check will enable the signal to be handled anyway.",
        "	 */",
        "	if (current->sas_ss_flags & SS_AUTODISARM)",
        "		return 0;",
        "",
        "	return __on_sig_stack(sp);",
        "}",
        "",
        "static inline int sas_ss_flags(unsigned long sp)",
        "{",
        "	if (!current->sas_ss_size)",
        "		return SS_DISABLE;",
        "",
        "	return on_sig_stack(sp) ? SS_ONSTACK : 0;",
        "}",
        "",
        "static inline void sas_ss_reset(struct task_struct *p)",
        "{",
        "	p->sas_ss_sp = 0;",
        "	p->sas_ss_size = 0;",
        "	p->sas_ss_flags = SS_DISABLE;",
        "}",
        "",
        "static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)",
        "{",
        "	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))",
        "#ifdef CONFIG_STACK_GROWSUP",
        "		return current->sas_ss_sp;",
        "#else",
        "		return current->sas_ss_sp + current->sas_ss_size;",
        "#endif",
        "	return sp;",
        "}",
        "",
        "extern void __cleanup_sighand(struct sighand_struct *);",
        "extern void flush_itimer_signals(void);",
        "",
        "#define tasklist_empty() \\",
        "	list_empty(&init_task.tasks)",
        "",
        "#define next_task(p) \\",
        "	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)",
        "",
        "#define for_each_process(p) \\",
        "	for (p = &init_task ; (p = next_task(p)) != &init_task ; )",
        "",
        "extern bool current_is_single_threaded(void);",
        "",
        "/*",
        " * Without tasklist/siglock it is only rcu-safe if g can't exit/exec,",
        " * otherwise next_thread(t) will never reach g after list_del_rcu(g).",
        " */",
        "#define while_each_thread(g, t) \\",
        "	while ((t = next_thread(t)) != g)",
        "",
        "#define for_other_threads(p, t)	\\",
        "	for (t = p; (t = next_thread(t)) != p; )",
        "",
        "#define __for_each_thread(signal, t)	\\",
        "	list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node, \\",
        "		lockdep_is_held(&tasklist_lock))",
        "",
        "#define for_each_thread(p, t)		\\",
        "	__for_each_thread((p)->signal, t)",
        "",
        "/* Careful: this is a double loop, 'break' won't work as expected. */",
        "#define for_each_process_thread(p, t)	\\",
        "	for_each_process(p) for_each_thread(p, t)",
        "",
        "typedef int (*proc_visitor)(struct task_struct *p, void *data);",
        "void walk_process_tree(struct task_struct *top, proc_visitor, void *);",
        "",
        "static inline",
        "struct pid *task_pid_type(struct task_struct *task, enum pid_type type)",
        "{",
        "	struct pid *pid;",
        "	if (type == PIDTYPE_PID)",
        "		pid = task_pid(task);",
        "	else",
        "		pid = task->signal->pids[type];",
        "	return pid;",
        "}",
        "",
        "static inline struct pid *task_tgid(struct task_struct *task)",
        "{",
        "	return task->signal->pids[PIDTYPE_TGID];",
        "}",
        "",
        "/*",
        " * Without tasklist or RCU lock it is not safe to dereference",
        " * the result of task_pgrp/task_session even if task == current,",
        " * we can race with another thread doing sys_setsid/sys_setpgid.",
        " */",
        "static inline struct pid *task_pgrp(struct task_struct *task)",
        "{",
        "	return task->signal->pids[PIDTYPE_PGID];",
        "}",
        "",
        "static inline struct pid *task_session(struct task_struct *task)",
        "{",
        "	return task->signal->pids[PIDTYPE_SID];",
        "}",
        "",
        "static inline int get_nr_threads(struct task_struct *task)",
        "{",
        "	return task->signal->nr_threads;",
        "}",
        "",
        "static inline bool thread_group_leader(struct task_struct *p)",
        "{",
        "	return p->exit_signal >= 0;",
        "}",
        "",
        "static inline",
        "bool same_thread_group(struct task_struct *p1, struct task_struct *p2)",
        "{",
        "	return p1->signal == p2->signal;",
        "}",
        "",
        "/*",
        " * returns NULL if p is the last thread in the thread group",
        " */",
        "static inline struct task_struct *__next_thread(struct task_struct *p)",
        "{",
        "	return list_next_or_null_rcu(&p->signal->thread_head,",
        "					&p->thread_node,",
        "					struct task_struct,",
        "					thread_node);",
        "}",
        "",
        "static inline struct task_struct *next_thread(struct task_struct *p)",
        "{",
        "	return __next_thread(p) ?: p->group_leader;",
        "}",
        "",
        "static inline int thread_group_empty(struct task_struct *p)",
        "{",
        "	return thread_group_leader(p) &&",
        "	       list_is_last(&p->thread_node, &p->signal->thread_head);",
        "}",
        "",
        "#define delay_group_leader(p) \\",
        "		(thread_group_leader(p) && !thread_group_empty(p))",
        "",
        "extern struct sighand_struct *__lock_task_sighand(struct task_struct *task,",
        "							unsigned long *flags);",
        "",
        "static inline struct sighand_struct *lock_task_sighand(struct task_struct *task,",
        "						       unsigned long *flags)",
        "{",
        "	struct sighand_struct *ret;",
        "",
        "	ret = __lock_task_sighand(task, flags);",
        "	(void)__cond_lock(&task->sighand->siglock, ret);",
        "	return ret;",
        "}",
        "",
        "static inline void unlock_task_sighand(struct task_struct *task,",
        "						unsigned long *flags)",
        "{",
        "	spin_unlock_irqrestore(&task->sighand->siglock, *flags);",
        "}",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "extern void lockdep_assert_task_sighand_held(struct task_struct *task);",
        "#else",
        "static inline void lockdep_assert_task_sighand_held(struct task_struct *task) { }",
        "#endif",
        "",
        "static inline unsigned long task_rlimit(const struct task_struct *task,",
        "		unsigned int limit)",
        "{",
        "	return READ_ONCE(task->signal->rlim[limit].rlim_cur);",
        "}",
        "",
        "static inline unsigned long task_rlimit_max(const struct task_struct *task,",
        "		unsigned int limit)",
        "{",
        "	return READ_ONCE(task->signal->rlim[limit].rlim_max);",
        "}",
        "",
        "static inline unsigned long rlimit(unsigned int limit)",
        "{",
        "	return task_rlimit(current, limit);",
        "}",
        "",
        "static inline unsigned long rlimit_max(unsigned int limit)",
        "{",
        "	return task_rlimit_max(current, limit);",
        "}",
        "",
        "#endif /* _LINUX_SCHED_SIGNAL_H */"
    ]
  },
  "fs_ext4_block_validity_c": {
    path: "fs/ext4/block_validity.c",
    covered: [323, 325, 345, 320, 346, 306],
    totalLines: 371,
    coveredCount: 6,
    coveragePct: 1.6,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/*",
        " *  linux/fs/ext4/block_validity.c",
        " *",
        " * Copyright (C) 2009",
        " * Theodore Ts'o (tytso@mit.edu)",
        " *",
        " * Track which blocks in the filesystem are metadata blocks that",
        " * should never be used as data blocks by files or directories.",
        " */",
        "",
        "#include <linux/time.h>",
        "#include <linux/fs.h>",
        "#include <linux/namei.h>",
        "#include <linux/quotaops.h>",
        "#include <linux/buffer_head.h>",
        "#include <linux/swap.h>",
        "#include <linux/pagemap.h>",
        "#include <linux/blkdev.h>",
        "#include <linux/slab.h>",
        "#include \"ext4.h\"",
        "",
        "struct ext4_system_zone {",
        "	struct rb_node	node;",
        "	ext4_fsblk_t	start_blk;",
        "	unsigned int	count;",
        "	u32		ino;",
        "};",
        "",
        "static struct kmem_cache *ext4_system_zone_cachep;",
        "",
        "int __init ext4_init_system_zone(void)",
        "{",
        "	ext4_system_zone_cachep = KMEM_CACHE(ext4_system_zone, 0);",
        "	if (ext4_system_zone_cachep == NULL)",
        "		return -ENOMEM;",
        "	return 0;",
        "}",
        "",
        "void ext4_exit_system_zone(void)",
        "{",
        "	rcu_barrier();",
        "	kmem_cache_destroy(ext4_system_zone_cachep);",
        "}",
        "",
        "static inline int can_merge(struct ext4_system_zone *entry1,",
        "		     struct ext4_system_zone *entry2)",
        "{",
        "	if ((entry1->start_blk + entry1->count) == entry2->start_blk &&",
        "	    entry1->ino == entry2->ino)",
        "		return 1;",
        "	return 0;",
        "}",
        "",
        "static void release_system_zone(struct ext4_system_blocks *system_blks)",
        "{",
        "	struct ext4_system_zone	*entry, *n;",
        "",
        "	rbtree_postorder_for_each_entry_safe(entry, n,",
        "				&system_blks->root, node)",
        "		kmem_cache_free(ext4_system_zone_cachep, entry);",
        "}",
        "",
        "/*",
        " * Mark a range of blocks as belonging to the \"system zone\" --- that",
        " * is, filesystem metadata blocks which should never be used by",
        " * inodes.",
        " */",
        "static int add_system_zone(struct ext4_system_blocks *system_blks,",
        "			   ext4_fsblk_t start_blk,",
        "			   unsigned int count, u32 ino)",
        "{",
        "	struct ext4_system_zone *new_entry, *entry;",
        "	struct rb_node **n = &system_blks->root.rb_node, *node;",
        "	struct rb_node *parent = NULL, *new_node;",
        "",
        "	while (*n) {",
        "		parent = *n;",
        "		entry = rb_entry(parent, struct ext4_system_zone, node);",
        "		if (start_blk < entry->start_blk)",
        "			n = &(*n)->rb_left;",
        "		else if (start_blk >= (entry->start_blk + entry->count))",
        "			n = &(*n)->rb_right;",
        "		else	/* Unexpected overlap of system zones. */",
        "			return -EFSCORRUPTED;",
        "	}",
        "",
        "	new_entry = kmem_cache_alloc(ext4_system_zone_cachep,",
        "				     GFP_KERNEL);",
        "	if (!new_entry)",
        "		return -ENOMEM;",
        "	new_entry->start_blk = start_blk;",
        "	new_entry->count = count;",
        "	new_entry->ino = ino;",
        "	new_node = &new_entry->node;",
        "",
        "	rb_link_node(new_node, parent, n);",
        "	rb_insert_color(new_node, &system_blks->root);",
        "",
        "	/* Can we merge to the left? */",
        "	node = rb_prev(new_node);",
        "	if (node) {",
        "		entry = rb_entry(node, struct ext4_system_zone, node);",
        "		if (can_merge(entry, new_entry)) {",
        "			new_entry->start_blk = entry->start_blk;",
        "			new_entry->count += entry->count;",
        "			rb_erase(node, &system_blks->root);",
        "			kmem_cache_free(ext4_system_zone_cachep, entry);",
        "		}",
        "	}",
        "",
        "	/* Can we merge to the right? */",
        "	node = rb_next(new_node);",
        "	if (node) {",
        "		entry = rb_entry(node, struct ext4_system_zone, node);",
        "		if (can_merge(new_entry, entry)) {",
        "			new_entry->count += entry->count;",
        "			rb_erase(node, &system_blks->root);",
        "			kmem_cache_free(ext4_system_zone_cachep, entry);",
        "		}",
        "	}",
        "	return 0;",
        "}",
        "",
        "static void debug_print_tree(struct ext4_sb_info *sbi)",
        "{",
        "	struct rb_node *node;",
        "	struct ext4_system_zone *entry;",
        "	struct ext4_system_blocks *system_blks;",
        "	int first = 1;",
        "",
        "	printk(KERN_INFO \"System zones: \");",
        "	rcu_read_lock();",
        "	system_blks = rcu_dereference(sbi->s_system_blks);",
        "	node = rb_first(&system_blks->root);",
        "	while (node) {",
        "		entry = rb_entry(node, struct ext4_system_zone, node);",
        "		printk(KERN_CONT \"%s%llu-%llu\", first ? \"\" : \", \",",
        "		       entry->start_blk, entry->start_blk + entry->count - 1);",
        "		first = 0;",
        "		node = rb_next(node);",
        "	}",
        "	rcu_read_unlock();",
        "	printk(KERN_CONT \"\\n\");",
        "}",
        "",
        "static int ext4_protect_reserved_inode(struct super_block *sb,",
        "				       struct ext4_system_blocks *system_blks,",
        "				       u32 ino)",
        "{",
        "	struct inode *inode;",
        "	struct ext4_sb_info *sbi = EXT4_SB(sb);",
        "	struct ext4_map_blocks map;",
        "	u32 i = 0, num;",
        "	int err = 0, n;",
        "",
        "	if ((ino < EXT4_ROOT_INO) ||",
        "	    (ino > le32_to_cpu(sbi->s_es->s_inodes_count)))",
        "		return -EINVAL;",
        "	inode = ext4_iget(sb, ino, EXT4_IGET_SPECIAL);",
        "	if (IS_ERR(inode))",
        "		return PTR_ERR(inode);",
        "	num = (inode->i_size + sb->s_blocksize - 1) >> sb->s_blocksize_bits;",
        "	while (i < num) {",
        "		cond_resched();",
        "		map.m_lblk = i;",
        "		map.m_len = num - i;",
        "		n = ext4_map_blocks(NULL, inode, &map, 0);",
        "		if (n < 0) {",
        "			err = n;",
        "			break;",
        "		}",
        "		if (n == 0) {",
        "			i++;",
        "		} else {",
        "			err = add_system_zone(system_blks, map.m_pblk, n, ino);",
        "			if (err < 0) {",
        "				if (err == -EFSCORRUPTED) {",
        "					EXT4_ERROR_INODE_ERR(inode, -err,",
        "						\"blocks %llu-%llu from inode overlap system zone\",",
        "						map.m_pblk,",
        "						map.m_pblk + map.m_len - 1);",
        "				}",
        "				break;",
        "			}",
        "			i += n;",
        "		}",
        "	}",
        "	iput(inode);",
        "	return err;",
        "}",
        "",
        "static void ext4_destroy_system_zone(struct rcu_head *rcu)",
        "{",
        "	struct ext4_system_blocks *system_blks;",
        "",
        "	system_blks = container_of(rcu, struct ext4_system_blocks, rcu);",
        "	release_system_zone(system_blks);",
        "	kfree(system_blks);",
        "}",
        "",
        "/*",
        " * Build system zone rbtree which is used for block validity checking.",
        " *",
        " * The update of system_blks pointer in this function is protected by",
        " * sb->s_umount semaphore. However we have to be careful as we can be",
        " * racing with ext4_inode_block_valid() calls reading system_blks rbtree",
        " * protected only by RCU. That's why we first build the rbtree and then",
        " * swap it in place.",
        " */",
        "int ext4_setup_system_zone(struct super_block *sb)",
        "{",
        "	ext4_group_t ngroups = ext4_get_groups_count(sb);",
        "	struct ext4_sb_info *sbi = EXT4_SB(sb);",
        "	struct ext4_system_blocks *system_blks;",
        "	struct ext4_group_desc *gdp;",
        "	ext4_group_t i;",
        "	int ret;",
        "",
        "	system_blks = kzalloc(sizeof(*system_blks), GFP_KERNEL);",
        "	if (!system_blks)",
        "		return -ENOMEM;",
        "",
        "	for (i=0; i < ngroups; i++) {",
        "		unsigned int meta_blks = ext4_num_base_meta_blocks(sb, i);",
        "",
        "		cond_resched();",
        "		if (meta_blks != 0) {",
        "			ret = add_system_zone(system_blks,",
        "					ext4_group_first_block_no(sb, i),",
        "					meta_blks, 0);",
        "			if (ret)",
        "				goto err;",
        "		}",
        "		gdp = ext4_get_group_desc(sb, i, NULL);",
        "		ret = add_system_zone(system_blks,",
        "				ext4_block_bitmap(sb, gdp), 1, 0);",
        "		if (ret)",
        "			goto err;",
        "		ret = add_system_zone(system_blks,",
        "				ext4_inode_bitmap(sb, gdp), 1, 0);",
        "		if (ret)",
        "			goto err;",
        "		ret = add_system_zone(system_blks,",
        "				ext4_inode_table(sb, gdp),",
        "				sbi->s_itb_per_group, 0);",
        "		if (ret)",
        "			goto err;",
        "	}",
        "	if (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {",
        "		ret = ext4_protect_reserved_inode(sb, system_blks,",
        "				le32_to_cpu(sbi->s_es->s_journal_inum));",
        "		if (ret)",
        "			goto err;",
        "	}",
        "",
        "	/*",
        "	 * System blks rbtree complete, announce it once to prevent racing",
        "	 * with ext4_inode_block_valid() accessing the rbtree at the same",
        "	 * time.",
        "	 */",
        "	rcu_assign_pointer(sbi->s_system_blks, system_blks);",
        "",
        "	if (test_opt(sb, DEBUG))",
        "		debug_print_tree(sbi);",
        "	return 0;",
        "err:",
        "	release_system_zone(system_blks);",
        "	kfree(system_blks);",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Called when the filesystem is unmounted or when remounting it with",
        " * noblock_validity specified.",
        " *",
        " * The update of system_blks pointer in this function is protected by",
        " * sb->s_umount semaphore. However we have to be careful as we can be",
        " * racing with ext4_inode_block_valid() calls reading system_blks rbtree",
        " * protected only by RCU. So we first clear the system_blks pointer and",
        " * then free the rbtree only after RCU grace period expires.",
        " */",
        "void ext4_release_system_zone(struct super_block *sb)",
        "{",
        "	struct ext4_system_blocks *system_blks;",
        "",
        "	system_blks = rcu_dereference_protected(EXT4_SB(sb)->s_system_blks,",
        "					lockdep_is_held(&sb->s_umount));",
        "	rcu_assign_pointer(EXT4_SB(sb)->s_system_blks, NULL);",
        "",
        "	if (system_blks)",
        "		call_rcu(&system_blks->rcu, ext4_destroy_system_zone);",
        "}",
        "",
        "int ext4_sb_block_valid(struct super_block *sb, struct inode *inode,",
        "				ext4_fsblk_t start_blk, unsigned int count)",
        "{",
        "	struct ext4_sb_info *sbi = EXT4_SB(sb);",
        "	struct ext4_system_blocks *system_blks;",
        "	struct ext4_system_zone *entry;",
        "	struct rb_node *n;",
        "	int ret = 1;",
        "",
        "	if ((start_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||",
        "	    (start_blk + count < start_blk) ||",
        "	    (start_blk + count > ext4_blocks_count(sbi->s_es)))",
        "		return 0;",
        "",
        "	/*",
        "	 * Lock the system zone to prevent it being released concurrently",
        "	 * when doing a remount which inverse current \"[no]block_validity\"",
        "	 * mount option.",
        "	 */",
        "	rcu_read_lock();",
        "	system_blks = rcu_dereference(sbi->s_system_blks);",
        "	if (system_blks == NULL)",
        "		goto out_rcu;",
        "",
        "	n = system_blks->root.rb_node;",
        "	while (n) {",
        "		entry = rb_entry(n, struct ext4_system_zone, node);",
        "		if (start_blk + count - 1 < entry->start_blk)",
        "			n = n->rb_left;",
        "		else if (start_blk >= (entry->start_blk + entry->count))",
        "			n = n->rb_right;",
        "		else {",
        "			ret = 0;",
        "			if (inode)",
        "				ret = (entry->ino == inode->i_ino);",
        "			break;",
        "		}",
        "	}",
        "out_rcu:",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Returns 1 if the passed-in block region (start_blk,",
        " * start_blk+count) is valid; 0 if some part of the block region",
        " * overlaps with some other filesystem metadata blocks.",
        " */",
        "int ext4_inode_block_valid(struct inode *inode, ext4_fsblk_t start_blk,",
        "			  unsigned int count)",
        "{",
        "	return ext4_sb_block_valid(inode->i_sb, inode, start_blk, count);",
        "}",
        "",
        "int ext4_check_blockref(const char *function, unsigned int line,",
        "			struct inode *inode, __le32 *p, unsigned int max)",
        "{",
        "	__le32 *bref = p;",
        "	unsigned int blk;",
        "",
        "	if (ext4_has_feature_journal(inode->i_sb) &&",
        "	    (inode->i_ino ==",
        "	     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))",
        "		return 0;",
        "",
        "	while (bref < p+max) {",
        "		blk = le32_to_cpu(*bref++);",
        "		if (blk &&",
        "		    unlikely(!ext4_inode_block_valid(inode, blk, 1))) {",
        "			ext4_error_inode(inode, function, line, blk,",
        "					 \"invalid block\");",
        "			return -EFSCORRUPTED;",
        "		}",
        "	}",
        "	return 0;",
        "}",
        ""
    ]
  },
  "kernel_workqueue_c": {
    path: "kernel/workqueue.c",
    covered: [1240, 2309, 788, 1772, 1234, 2338, 890, 2256, 1714, 2281, 893, 2388, 2282, 1555, 2188, 2321, 2186, 2394, 2390, 2336, 2179, 2382, 2237, 1626, 2335, 1267, 795, 2263, 1711],
    totalLines: 8056,
    coveredCount: 29,
    coveragePct: 0.4,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " * kernel/workqueue.c - generic async execution with shared worker pool",
        " *",
        " * Copyright (C) 2002		Ingo Molnar",
        " *",
        " *   Derived from the taskqueue/keventd code by:",
        " *     David Woodhouse <dwmw2@infradead.org>",
        " *     Andrew Morton",
        " *     Kai Petzke <wpp@marie.physik.tu-berlin.de>",
        " *     Theodore Ts'o <tytso@mit.edu>",
        " *",
        " * Made to use alloc_percpu by Christoph Lameter.",
        " *",
        " * Copyright (C) 2010		SUSE Linux Products GmbH",
        " * Copyright (C) 2010		Tejun Heo <tj@kernel.org>",
        " *",
        " * This is the generic async execution mechanism.  Work items as are",
        " * executed in process context.  The worker pool is shared and",
        " * automatically managed.  There are two worker pools for each CPU (one for",
        " * normal work items and the other for high priority ones) and some extra",
        " * pools for workqueues which are not bound to any specific CPU - the",
        " * number of these backing pools is dynamic.",
        " *",
        " * Please read Documentation/core-api/workqueue.rst for details.",
        " */",
        "",
        "#include <linux/export.h>",
        "#include <linux/kernel.h>",
        "#include <linux/sched.h>",
        "#include <linux/init.h>",
        "#include <linux/interrupt.h>",
        "#include <linux/signal.h>",
        "#include <linux/completion.h>",
        "#include <linux/workqueue.h>",
        "#include <linux/slab.h>",
        "#include <linux/cpu.h>",
        "#include <linux/notifier.h>",
        "#include <linux/kthread.h>",
        "#include <linux/hardirq.h>",
        "#include <linux/mempolicy.h>",
        "#include <linux/freezer.h>",
        "#include <linux/debug_locks.h>",
        "#include <linux/lockdep.h>",
        "#include <linux/idr.h>",
        "#include <linux/jhash.h>",
        "#include <linux/hashtable.h>",
        "#include <linux/rculist.h>",
        "#include <linux/nodemask.h>",
        "#include <linux/moduleparam.h>",
        "#include <linux/uaccess.h>",
        "#include <linux/sched/isolation.h>",
        "#include <linux/sched/debug.h>",
        "#include <linux/nmi.h>",
        "#include <linux/kvm_para.h>",
        "#include <linux/delay.h>",
        "#include <linux/irq_work.h>",
        "",
        "#include \"workqueue_internal.h\"",
        "",
        "enum worker_pool_flags {",
        "	/*",
        "	 * worker_pool flags",
        "	 *",
        "	 * A bound pool is either associated or disassociated with its CPU.",
        "	 * While associated (!DISASSOCIATED), all workers are bound to the",
        "	 * CPU and none has %WORKER_UNBOUND set and concurrency management",
        "	 * is in effect.",
        "	 *",
        "	 * While DISASSOCIATED, the cpu may be offline and all workers have",
        "	 * %WORKER_UNBOUND set and concurrency management disabled, and may",
        "	 * be executing on any CPU.  The pool behaves as an unbound one.",
        "	 *",
        "	 * Note that DISASSOCIATED should be flipped only while holding",
        "	 * wq_pool_attach_mutex to avoid changing binding state while",
        "	 * worker_attach_to_pool() is in progress.",
        "	 *",
        "	 * As there can only be one concurrent BH execution context per CPU, a",
        "	 * BH pool is per-CPU and always DISASSOCIATED.",
        "	 */",
        "	POOL_BH			= 1 << 0,	/* is a BH pool */",
        "	POOL_MANAGER_ACTIVE	= 1 << 1,	/* being managed */",
        "	POOL_DISASSOCIATED	= 1 << 2,	/* cpu can't serve workers */",
        "	POOL_BH_DRAINING	= 1 << 3,	/* draining after CPU offline */",
        "};",
        "",
        "enum worker_flags {",
        "	/* worker flags */",
        "	WORKER_DIE		= 1 << 1,	/* die die die */",
        "	WORKER_IDLE		= 1 << 2,	/* is idle */",
        "	WORKER_PREP		= 1 << 3,	/* preparing to run works */",
        "	WORKER_CPU_INTENSIVE	= 1 << 6,	/* cpu intensive */",
        "	WORKER_UNBOUND		= 1 << 7,	/* worker is unbound */",
        "	WORKER_REBOUND		= 1 << 8,	/* worker was rebound */",
        "",
        "	WORKER_NOT_RUNNING	= WORKER_PREP | WORKER_CPU_INTENSIVE |",
        "				  WORKER_UNBOUND | WORKER_REBOUND,",
        "};",
        "",
        "enum work_cancel_flags {",
        "	WORK_CANCEL_DELAYED	= 1 << 0,	/* canceling a delayed_work */",
        "	WORK_CANCEL_DISABLE	= 1 << 1,	/* canceling to disable */",
        "};",
        "",
        "enum wq_internal_consts {",
        "	NR_STD_WORKER_POOLS	= 2,		/* # standard pools per cpu */",
        "",
        "	UNBOUND_POOL_HASH_ORDER	= 6,		/* hashed by pool->attrs */",
        "	BUSY_WORKER_HASH_ORDER	= 6,		/* 64 pointers */",
        "",
        "	MAX_IDLE_WORKERS_RATIO	= 4,		/* 1/4 of busy can be idle */",
        "	IDLE_WORKER_TIMEOUT	= 300 * HZ,	/* keep idle ones for 5 mins */",
        "",
        "	MAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,",
        "						/* call for help after 10ms",
        "						   (min two ticks) */",
        "	MAYDAY_INTERVAL		= HZ / 10,	/* and then every 100ms */",
        "	CREATE_COOLDOWN		= HZ,		/* time to breath after fail */",
        "",
        "	/*",
        "	 * Rescue workers are used only on emergencies and shared by",
        "	 * all cpus.  Give MIN_NICE.",
        "	 */",
        "	RESCUER_NICE_LEVEL	= MIN_NICE,",
        "	HIGHPRI_NICE_LEVEL	= MIN_NICE,",
        "",
        "	WQ_NAME_LEN		= 32,",
        "	WORKER_ID_LEN		= 10 + WQ_NAME_LEN, /* \"kworker/R-\" + WQ_NAME_LEN */",
        "};",
        "",
        "/*",
        " * We don't want to trap softirq for too long. See MAX_SOFTIRQ_TIME and",
        " * MAX_SOFTIRQ_RESTART in kernel/softirq.c. These are macros because",
        " * msecs_to_jiffies() can't be an initializer.",
        " */",
        "#define BH_WORKER_JIFFIES	msecs_to_jiffies(2)",
        "#define BH_WORKER_RESTARTS	10",
        "",
        "/*",
        " * Structure fields follow one of the following exclusion rules.",
        " *",
        " * I: Modifiable by initialization/destruction paths and read-only for",
        " *    everyone else.",
        " *",
        " * P: Preemption protected.  Disabling preemption is enough and should",
        " *    only be modified and accessed from the local cpu.",
        " *",
        " * L: pool->lock protected.  Access with pool->lock held.",
        " *",
        " * LN: pool->lock and wq_node_nr_active->lock protected for writes. Either for",
        " *     reads.",
        " *",
        " * K: Only modified by worker while holding pool->lock. Can be safely read by",
        " *    self, while holding pool->lock or from IRQ context if %current is the",
        " *    kworker.",
        " *",
        " * S: Only modified by worker self.",
        " *",
        " * A: wq_pool_attach_mutex protected.",
        " *",
        " * PL: wq_pool_mutex protected.",
        " *",
        " * PR: wq_pool_mutex protected for writes.  RCU protected for reads.",
        " *",
        " * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.",
        " *",
        " * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or",
        " *      RCU for reads.",
        " *",
        " * WQ: wq->mutex protected.",
        " *",
        " * WR: wq->mutex protected for writes.  RCU protected for reads.",
        " *",
        " * WO: wq->mutex protected for writes. Updated with WRITE_ONCE() and can be read",
        " *     with READ_ONCE() without locking.",
        " *",
        " * MD: wq_mayday_lock protected.",
        " *",
        " * WD: Used internally by the watchdog.",
        " */",
        "",
        "/* struct worker is defined in workqueue_internal.h */",
        "",
        "struct worker_pool {",
        "	raw_spinlock_t		lock;		/* the pool lock */",
        "	int			cpu;		/* I: the associated cpu */",
        "	int			node;		/* I: the associated node ID */",
        "	int			id;		/* I: pool ID */",
        "	unsigned int		flags;		/* L: flags */",
        "",
        "	unsigned long		watchdog_ts;	/* L: watchdog timestamp */",
        "	bool			cpu_stall;	/* WD: stalled cpu bound pool */",
        "",
        "	/*",
        "	 * The counter is incremented in a process context on the associated CPU",
        "	 * w/ preemption disabled, and decremented or reset in the same context",
        "	 * but w/ pool->lock held. The readers grab pool->lock and are",
        "	 * guaranteed to see if the counter reached zero.",
        "	 */",
        "	int			nr_running;",
        "",
        "	struct list_head	worklist;	/* L: list of pending works */",
        "",
        "	int			nr_workers;	/* L: total number of workers */",
        "	int			nr_idle;	/* L: currently idle workers */",
        "",
        "	struct list_head	idle_list;	/* L: list of idle workers */",
        "	struct timer_list	idle_timer;	/* L: worker idle timeout */",
        "	struct work_struct      idle_cull_work; /* L: worker idle cleanup */",
        "",
        "	struct timer_list	mayday_timer;	  /* L: SOS timer for workers */",
        "",
        "	/* a workers is either on busy_hash or idle_list, or the manager */",
        "	DECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);",
        "						/* L: hash of busy workers */",
        "",
        "	struct worker		*manager;	/* L: purely informational */",
        "	struct list_head	workers;	/* A: attached workers */",
        "",
        "	struct ida		worker_ida;	/* worker IDs for task name */",
        "",
        "	struct workqueue_attrs	*attrs;		/* I: worker attributes */",
        "	struct hlist_node	hash_node;	/* PL: unbound_pool_hash node */",
        "	int			refcnt;		/* PL: refcnt for unbound pools */",
        "",
        "	/*",
        "	 * Destruction of pool is RCU protected to allow dereferences",
        "	 * from get_work_pool().",
        "	 */",
        "	struct rcu_head		rcu;",
        "};",
        "",
        "/*",
        " * Per-pool_workqueue statistics. These can be monitored using",
        " * tools/workqueue/wq_monitor.py.",
        " */",
        "enum pool_workqueue_stats {",
        "	PWQ_STAT_STARTED,	/* work items started execution */",
        "	PWQ_STAT_COMPLETED,	/* work items completed execution */",
        "	PWQ_STAT_CPU_TIME,	/* total CPU time consumed */",
        "	PWQ_STAT_CPU_INTENSIVE,	/* wq_cpu_intensive_thresh_us violations */",
        "	PWQ_STAT_CM_WAKEUP,	/* concurrency-management worker wakeups */",
        "	PWQ_STAT_REPATRIATED,	/* unbound workers brought back into scope */",
        "	PWQ_STAT_MAYDAY,	/* maydays to rescuer */",
        "	PWQ_STAT_RESCUED,	/* linked work items executed by rescuer */",
        "",
        "	PWQ_NR_STATS,",
        "};",
        "",
        "/*",
        " * The per-pool workqueue.  While queued, bits below WORK_PWQ_SHIFT",
        " * of work_struct->data are used for flags and the remaining high bits",
        " * point to the pwq; thus, pwqs need to be aligned at two's power of the",
        " * number of flag bits.",
        " */",
        "struct pool_workqueue {",
        "	struct worker_pool	*pool;		/* I: the associated pool */",
        "	struct workqueue_struct *wq;		/* I: the owning workqueue */",
        "	int			work_color;	/* L: current color */",
        "	int			flush_color;	/* L: flushing color */",
        "	int			refcnt;		/* L: reference count */",
        "	int			nr_in_flight[WORK_NR_COLORS];",
        "						/* L: nr of in_flight works */",
        "	bool			plugged;	/* L: execution suspended */",
        "",
        "	/*",
        "	 * nr_active management and WORK_STRUCT_INACTIVE:",
        "	 *",
        "	 * When pwq->nr_active >= max_active, new work item is queued to",
        "	 * pwq->inactive_works instead of pool->worklist and marked with",
        "	 * WORK_STRUCT_INACTIVE.",
        "	 *",
        "	 * All work items marked with WORK_STRUCT_INACTIVE do not participate in",
        "	 * nr_active and all work items in pwq->inactive_works are marked with",
        "	 * WORK_STRUCT_INACTIVE. But not all WORK_STRUCT_INACTIVE work items are",
        "	 * in pwq->inactive_works. Some of them are ready to run in",
        "	 * pool->worklist or worker->scheduled. Those work itmes are only struct",
        "	 * wq_barrier which is used for flush_work() and should not participate",
        "	 * in nr_active. For non-barrier work item, it is marked with",
        "	 * WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.",
        "	 */",
        "	int			nr_active;	/* L: nr of active works */",
        "	struct list_head	inactive_works;	/* L: inactive works */",
        "	struct list_head	pending_node;	/* LN: node on wq_node_nr_active->pending_pwqs */",
        "	struct list_head	pwqs_node;	/* WR: node on wq->pwqs */",
        "	struct list_head	mayday_node;	/* MD: node on wq->maydays */",
        "",
        "	u64			stats[PWQ_NR_STATS];",
        "",
        "	/*",
        "	 * Release of unbound pwq is punted to a kthread_worker. See put_pwq()",
        "	 * and pwq_release_workfn() for details. pool_workqueue itself is also",
        "	 * RCU protected so that the first pwq can be determined without",
        "	 * grabbing wq->mutex.",
        "	 */",
        "	struct kthread_work	release_work;",
        "	struct rcu_head		rcu;",
        "} __aligned(1 << WORK_STRUCT_PWQ_SHIFT);",
        "",
        "/*",
        " * Structure used to wait for workqueue flush.",
        " */",
        "struct wq_flusher {",
        "	struct list_head	list;		/* WQ: list of flushers */",
        "	int			flush_color;	/* WQ: flush color waiting for */",
        "	struct completion	done;		/* flush completion */",
        "};",
        "",
        "struct wq_device;",
        "",
        "/*",
        " * Unlike in a per-cpu workqueue where max_active limits its concurrency level",
        " * on each CPU, in an unbound workqueue, max_active applies to the whole system.",
        " * As sharing a single nr_active across multiple sockets can be very expensive,",
        " * the counting and enforcement is per NUMA node.",
        " *",
        " * The following struct is used to enforce per-node max_active. When a pwq wants",
        " * to start executing a work item, it should increment ->nr using",
        " * tryinc_node_nr_active(). If acquisition fails due to ->nr already being over",
        " * ->max, the pwq is queued on ->pending_pwqs. As in-flight work items finish",
        " * and decrement ->nr, node_activate_pending_pwq() activates the pending pwqs in",
        " * round-robin order.",
        " */",
        "struct wq_node_nr_active {",
        "	int			max;		/* per-node max_active */",
        "	atomic_t		nr;		/* per-node nr_active */",
        "	raw_spinlock_t		lock;		/* nests inside pool locks */",
        "	struct list_head	pending_pwqs;	/* LN: pwqs with inactive works */",
        "};",
        "",
        "/*",
        " * The externally visible workqueue.  It relays the issued work items to",
        " * the appropriate worker_pool through its pool_workqueues.",
        " */",
        "struct workqueue_struct {",
        "	struct list_head	pwqs;		/* WR: all pwqs of this wq */",
        "	struct list_head	list;		/* PR: list of all workqueues */",
        "",
        "	struct mutex		mutex;		/* protects this wq */",
        "	int			work_color;	/* WQ: current work color */",
        "	int			flush_color;	/* WQ: current flush color */",
        "	atomic_t		nr_pwqs_to_flush; /* flush in progress */",
        "	struct wq_flusher	*first_flusher;	/* WQ: first flusher */",
        "	struct list_head	flusher_queue;	/* WQ: flush waiters */",
        "	struct list_head	flusher_overflow; /* WQ: flush overflow list */",
        "",
        "	struct list_head	maydays;	/* MD: pwqs requesting rescue */",
        "	struct worker		*rescuer;	/* MD: rescue worker */",
        "",
        "	int			nr_drainers;	/* WQ: drain in progress */",
        "",
        "	/* See alloc_workqueue() function comment for info on min/max_active */",
        "	int			max_active;	/* WO: max active works */",
        "	int			min_active;	/* WO: min active works */",
        "	int			saved_max_active; /* WQ: saved max_active */",
        "	int			saved_min_active; /* WQ: saved min_active */",
        "",
        "	struct workqueue_attrs	*unbound_attrs;	/* PW: only for unbound wqs */",
        "	struct pool_workqueue __rcu *dfl_pwq;   /* PW: only for unbound wqs */",
        "",
        "#ifdef CONFIG_SYSFS",
        "	struct wq_device	*wq_dev;	/* I: for sysfs interface */",
        "#endif",
        "#ifdef CONFIG_LOCKDEP",
        "	char			*lock_name;",
        "	struct lock_class_key	key;",
        "	struct lockdep_map	__lockdep_map;",
        "	struct lockdep_map	*lockdep_map;",
        "#endif",
        "	char			name[WQ_NAME_LEN]; /* I: workqueue name */",
        "",
        "	/*",
        "	 * Destruction of workqueue_struct is RCU protected to allow walking",
        "	 * the workqueues list without grabbing wq_pool_mutex.",
        "	 * This is used to dump all workqueues from sysrq.",
        "	 */",
        "	struct rcu_head		rcu;",
        "",
        "	/* hot fields used during command issue, aligned to cacheline */",
        "	unsigned int		flags ____cacheline_aligned; /* WQ: WQ_* flags */",
        "	struct pool_workqueue __rcu * __percpu *cpu_pwq; /* I: per-cpu pwqs */",
        "	struct wq_node_nr_active *node_nr_active[]; /* I: per-node nr_active */",
        "};",
        "",
        "/*",
        " * Each pod type describes how CPUs should be grouped for unbound workqueues.",
        " * See the comment above workqueue_attrs->affn_scope.",
        " */",
        "struct wq_pod_type {",
        "	int			nr_pods;	/* number of pods */",
        "	cpumask_var_t		*pod_cpus;	/* pod -> cpus */",
        "	int			*pod_node;	/* pod -> node */",
        "	int			*cpu_pod;	/* cpu -> pod */",
        "};",
        "",
        "struct work_offq_data {",
        "	u32			pool_id;",
        "	u32			disable;",
        "	u32			flags;",
        "};",
        "",
        "static const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {",
        "	[WQ_AFFN_DFL]		= \"default\",",
        "	[WQ_AFFN_CPU]		= \"cpu\",",
        "	[WQ_AFFN_SMT]		= \"smt\",",
        "	[WQ_AFFN_CACHE]		= \"cache\",",
        "	[WQ_AFFN_NUMA]		= \"numa\",",
        "	[WQ_AFFN_SYSTEM]	= \"system\",",
        "};",
        "",
        "/*",
        " * Per-cpu work items which run for longer than the following threshold are",
        " * automatically considered CPU intensive and excluded from concurrency",
        " * management to prevent them from noticeably delaying other per-cpu work items.",
        " * ULONG_MAX indicates that the user hasn't overridden it with a boot parameter.",
        " * The actual value is initialized in wq_cpu_intensive_thresh_init().",
        " */",
        "static unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;",
        "module_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);",
        "#ifdef CONFIG_WQ_CPU_INTENSIVE_REPORT",
        "static unsigned int wq_cpu_intensive_warning_thresh = 4;",
        "module_param_named(cpu_intensive_warning_thresh, wq_cpu_intensive_warning_thresh, uint, 0644);",
        "#endif",
        "",
        "/* see the comment above the definition of WQ_POWER_EFFICIENT */",
        "static bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);",
        "module_param_named(power_efficient, wq_power_efficient, bool, 0444);",
        "",
        "static bool wq_online;			/* can kworkers be created yet? */",
        "static bool wq_topo_initialized __read_mostly = false;",
        "",
        "static struct kmem_cache *pwq_cache;",
        "",
        "static struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];",
        "static enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;",
        "",
        "/* buf for wq_update_unbound_pod_attrs(), protected by CPU hotplug exclusion */",
        "static struct workqueue_attrs *unbound_wq_update_pwq_attrs_buf;",
        "",
        "static DEFINE_MUTEX(wq_pool_mutex);	/* protects pools and workqueues list */",
        "static DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */",
        "static DEFINE_RAW_SPINLOCK(wq_mayday_lock);	/* protects wq->maydays list */",
        "/* wait for manager to go away */",
        "static struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);",
        "",
        "static LIST_HEAD(workqueues);		/* PR: list of all workqueues */",
        "static bool workqueue_freezing;		/* PL: have wqs started freezing? */",
        "",
        "/* PL: mirror the cpu_online_mask excluding the CPU in the midst of hotplugging */",
        "static cpumask_var_t wq_online_cpumask;",
        "",
        "/* PL&A: allowable cpus for unbound wqs and work items */",
        "static cpumask_var_t wq_unbound_cpumask;",
        "",
        "/* PL: user requested unbound cpumask via sysfs */",
        "static cpumask_var_t wq_requested_unbound_cpumask;",
        "",
        "/* PL: isolated cpumask to be excluded from unbound cpumask */",
        "static cpumask_var_t wq_isolated_cpumask;",
        "",
        "/* for further constrain wq_unbound_cpumask by cmdline parameter*/",
        "static struct cpumask wq_cmdline_cpumask __initdata;",
        "",
        "/* CPU where unbound work was last round robin scheduled from this CPU */",
        "static DEFINE_PER_CPU(int, wq_rr_cpu_last);",
        "",
        "/*",
        " * Local execution of unbound work items is no longer guaranteed.  The",
        " * following always forces round-robin CPU selection on unbound work items",
        " * to uncover usages which depend on it.",
        " */",
        "#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU",
        "static bool wq_debug_force_rr_cpu = true;",
        "#else",
        "static bool wq_debug_force_rr_cpu = false;",
        "#endif",
        "module_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);",
        "",
        "/* to raise softirq for the BH worker pools on other CPUs */",
        "static DEFINE_PER_CPU_SHARED_ALIGNED(struct irq_work [NR_STD_WORKER_POOLS], bh_pool_irq_works);",
        "",
        "/* the BH worker pools */",
        "static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], bh_worker_pools);",
        "",
        "/* the per-cpu worker pools */",
        "static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);",
        "",
        "static DEFINE_IDR(worker_pool_idr);	/* PR: idr of all pools */",
        "",
        "/* PL: hash of all unbound pools keyed by pool->attrs */",
        "static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);",
        "",
        "/* I: attributes used when instantiating standard unbound pools on demand */",
        "static struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];",
        "",
        "/* I: attributes used when instantiating ordered pools on demand */",
        "static struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];",
        "",
        "/*",
        " * I: kthread_worker to release pwq's. pwq release needs to be bounced to a",
        " * process context while holding a pool lock. Bounce to a dedicated kthread",
        " * worker to avoid A-A deadlocks.",
        " */",
        "static struct kthread_worker *pwq_release_worker __ro_after_init;",
        "",
        "struct workqueue_struct *system_wq __ro_after_init;",
        "EXPORT_SYMBOL(system_wq);",
        "struct workqueue_struct *system_highpri_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_highpri_wq);",
        "struct workqueue_struct *system_long_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_long_wq);",
        "struct workqueue_struct *system_unbound_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_unbound_wq);",
        "struct workqueue_struct *system_freezable_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_freezable_wq);",
        "struct workqueue_struct *system_power_efficient_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_power_efficient_wq);",
        "struct workqueue_struct *system_freezable_power_efficient_wq __ro_after_init;",
        "EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);",
        "struct workqueue_struct *system_bh_wq;",
        "EXPORT_SYMBOL_GPL(system_bh_wq);",
        "struct workqueue_struct *system_bh_highpri_wq;",
        "EXPORT_SYMBOL_GPL(system_bh_highpri_wq);",
        "",
        "static int worker_thread(void *__worker);",
        "static void workqueue_sysfs_unregister(struct workqueue_struct *wq);",
        "static void show_pwq(struct pool_workqueue *pwq);",
        "static void show_one_worker_pool(struct worker_pool *pool);",
        "",
        "#define CREATE_TRACE_POINTS",
        "#include <trace/events/workqueue.h>",
        "",
        "#define assert_rcu_or_pool_mutex()					\\",
        "	RCU_LOCKDEP_WARN(!rcu_read_lock_any_held() &&			\\",
        "			 !lockdep_is_held(&wq_pool_mutex),		\\",
        "			 \"RCU or wq_pool_mutex should be held\")",
        "",
        "#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)			\\",
        "	RCU_LOCKDEP_WARN(!rcu_read_lock_any_held() &&			\\",
        "			 !lockdep_is_held(&wq->mutex) &&		\\",
        "			 !lockdep_is_held(&wq_pool_mutex),		\\",
        "			 \"RCU, wq->mutex or wq_pool_mutex should be held\")",
        "",
        "#define for_each_bh_worker_pool(pool, cpu)				\\",
        "	for ((pool) = &per_cpu(bh_worker_pools, cpu)[0];		\\",
        "	     (pool) < &per_cpu(bh_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\",
        "	     (pool)++)",
        "",
        "#define for_each_cpu_worker_pool(pool, cpu)				\\",
        "	for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];		\\",
        "	     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\",
        "	     (pool)++)",
        "",
        "/**",
        " * for_each_pool - iterate through all worker_pools in the system",
        " * @pool: iteration cursor",
        " * @pi: integer used for iteration",
        " *",
        " * This must be called either with wq_pool_mutex held or RCU read",
        " * locked.  If the pool needs to be used beyond the locking in effect, the",
        " * caller is responsible for guaranteeing that the pool stays online.",
        " *",
        " * The if/else clause exists only for the lockdep assertion and can be",
        " * ignored.",
        " */",
        "#define for_each_pool(pool, pi)						\\",
        "	idr_for_each_entry(&worker_pool_idr, pool, pi)			\\",
        "		if (({ assert_rcu_or_pool_mutex(); false; })) { }	\\",
        "		else",
        "",
        "/**",
        " * for_each_pool_worker - iterate through all workers of a worker_pool",
        " * @worker: iteration cursor",
        " * @pool: worker_pool to iterate workers of",
        " *",
        " * This must be called with wq_pool_attach_mutex.",
        " *",
        " * The if/else clause exists only for the lockdep assertion and can be",
        " * ignored.",
        " */",
        "#define for_each_pool_worker(worker, pool)				\\",
        "	list_for_each_entry((worker), &(pool)->workers, node)		\\",
        "		if (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\",
        "		else",
        "",
        "/**",
        " * for_each_pwq - iterate through all pool_workqueues of the specified workqueue",
        " * @pwq: iteration cursor",
        " * @wq: the target workqueue",
        " *",
        " * This must be called either with wq->mutex held or RCU read locked.",
        " * If the pwq needs to be used beyond the locking in effect, the caller is",
        " * responsible for guaranteeing that the pwq stays online.",
        " *",
        " * The if/else clause exists only for the lockdep assertion and can be",
        " * ignored.",
        " */",
        "#define for_each_pwq(pwq, wq)						\\",
        "	list_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,		\\",
        "				 lockdep_is_held(&(wq->mutex)))",
        "",
        "#ifdef CONFIG_DEBUG_OBJECTS_WORK",
        "",
        "static const struct debug_obj_descr work_debug_descr;",
        "",
        "static void *work_debug_hint(void *addr)",
        "{",
        "	return ((struct work_struct *) addr)->func;",
        "}",
        "",
        "static bool work_is_static_object(void *addr)",
        "{",
        "	struct work_struct *work = addr;",
        "",
        "	return test_bit(WORK_STRUCT_STATIC_BIT, work_data_bits(work));",
        "}",
        "",
        "/*",
        " * fixup_init is called when:",
        " * - an active object is initialized",
        " */",
        "static bool work_fixup_init(void *addr, enum debug_obj_state state)",
        "{",
        "	struct work_struct *work = addr;",
        "",
        "	switch (state) {",
        "	case ODEBUG_STATE_ACTIVE:",
        "		cancel_work_sync(work);",
        "		debug_object_init(work, &work_debug_descr);",
        "		return true;",
        "	default:",
        "		return false;",
        "	}",
        "}",
        "",
        "/*",
        " * fixup_free is called when:",
        " * - an active object is freed",
        " */",
        "static bool work_fixup_free(void *addr, enum debug_obj_state state)",
        "{",
        "	struct work_struct *work = addr;",
        "",
        "	switch (state) {",
        "	case ODEBUG_STATE_ACTIVE:",
        "		cancel_work_sync(work);",
        "		debug_object_free(work, &work_debug_descr);",
        "		return true;",
        "	default:",
        "		return false;",
        "	}",
        "}",
        "",
        "static const struct debug_obj_descr work_debug_descr = {",
        "	.name		= \"work_struct\",",
        "	.debug_hint	= work_debug_hint,",
        "	.is_static_object = work_is_static_object,",
        "	.fixup_init	= work_fixup_init,",
        "	.fixup_free	= work_fixup_free,",
        "};",
        "",
        "static inline void debug_work_activate(struct work_struct *work)",
        "{",
        "	debug_object_activate(work, &work_debug_descr);",
        "}",
        "",
        "static inline void debug_work_deactivate(struct work_struct *work)",
        "{",
        "	debug_object_deactivate(work, &work_debug_descr);",
        "}",
        "",
        "void __init_work(struct work_struct *work, int onstack)",
        "{",
        "	if (onstack)",
        "		debug_object_init_on_stack(work, &work_debug_descr);",
        "	else",
        "		debug_object_init(work, &work_debug_descr);",
        "}",
        "EXPORT_SYMBOL_GPL(__init_work);",
        "",
        "void destroy_work_on_stack(struct work_struct *work)",
        "{",
        "	debug_object_free(work, &work_debug_descr);",
        "}",
        "EXPORT_SYMBOL_GPL(destroy_work_on_stack);",
        "",
        "void destroy_delayed_work_on_stack(struct delayed_work *work)",
        "{",
        "	destroy_timer_on_stack(&work->timer);",
        "	debug_object_free(&work->work, &work_debug_descr);",
        "}",
        "EXPORT_SYMBOL_GPL(destroy_delayed_work_on_stack);",
        "",
        "#else",
        "static inline void debug_work_activate(struct work_struct *work) { }",
        "static inline void debug_work_deactivate(struct work_struct *work) { }",
        "#endif",
        "",
        "/**",
        " * worker_pool_assign_id - allocate ID and assign it to @pool",
        " * @pool: the pool pointer of interest",
        " *",
        " * Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned",
        " * successfully, -errno on failure.",
        " */",
        "static int worker_pool_assign_id(struct worker_pool *pool)",
        "{",
        "	int ret;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	ret = idr_alloc(&worker_pool_idr, pool, 0, WORK_OFFQ_POOL_NONE,",
        "			GFP_KERNEL);",
        "	if (ret >= 0) {",
        "		pool->id = ret;",
        "		return 0;",
        "	}",
        "	return ret;",
        "}",
        "",
        "static struct pool_workqueue __rcu **",
        "unbound_pwq_slot(struct workqueue_struct *wq, int cpu)",
        "{",
        "       if (cpu >= 0)",
        "               return per_cpu_ptr(wq->cpu_pwq, cpu);",
        "       else",
        "               return &wq->dfl_pwq;",
        "}",
        "",
        "/* @cpu < 0 for dfl_pwq */",
        "static struct pool_workqueue *unbound_pwq(struct workqueue_struct *wq, int cpu)",
        "{",
        "	return rcu_dereference_check(*unbound_pwq_slot(wq, cpu),",
        "				     lockdep_is_held(&wq_pool_mutex) ||",
        "				     lockdep_is_held(&wq->mutex));",
        "}",
        "",
        "/**",
        " * unbound_effective_cpumask - effective cpumask of an unbound workqueue",
        " * @wq: workqueue of interest",
        " *",
        " * @wq->unbound_attrs->cpumask contains the cpumask requested by the user which",
        " * is masked with wq_unbound_cpumask to determine the effective cpumask. The",
        " * default pwq is always mapped to the pool with the current effective cpumask.",
        " */",
        "static struct cpumask *unbound_effective_cpumask(struct workqueue_struct *wq)",
        "{",
        "	return unbound_pwq(wq, -1)->pool->attrs->__pod_cpumask;",
        "}",
        "",
        "static unsigned int work_color_to_flags(int color)",
        "{",
        "	return color << WORK_STRUCT_COLOR_SHIFT;",
        "}",
        "",
        "static int get_work_color(unsigned long work_data)",
        "{",
        "	return (work_data >> WORK_STRUCT_COLOR_SHIFT) &",
        "		((1 << WORK_STRUCT_COLOR_BITS) - 1);",
        "}",
        "",
        "static int work_next_color(int color)",
        "{",
        "	return (color + 1) % WORK_NR_COLORS;",
        "}",
        "",
        "static unsigned long pool_offq_flags(struct worker_pool *pool)",
        "{",
        "	return (pool->flags & POOL_BH) ? WORK_OFFQ_BH : 0;",
        "}",
        "",
        "/*",
        " * While queued, %WORK_STRUCT_PWQ is set and non flag bits of a work's data",
        " * contain the pointer to the queued pwq.  Once execution starts, the flag",
        " * is cleared and the high bits contain OFFQ flags and pool ID.",
        " *",
        " * set_work_pwq(), set_work_pool_and_clear_pending() and mark_work_canceling()",
        " * can be used to set the pwq, pool or clear work->data. These functions should",
        " * only be called while the work is owned - ie. while the PENDING bit is set.",
        " *",
        " * get_work_pool() and get_work_pwq() can be used to obtain the pool or pwq",
        " * corresponding to a work.  Pool is available once the work has been",
        " * queued anywhere after initialization until it is sync canceled.  pwq is",
        " * available only while the work item is queued.",
        " */",
        "static inline void set_work_data(struct work_struct *work, unsigned long data)",
        "{",
        "	WARN_ON_ONCE(!work_pending(work));",
        "	atomic_long_set(&work->data, data | work_static(work));",
        "}",
        "",
        "static void set_work_pwq(struct work_struct *work, struct pool_workqueue *pwq,",
        "			 unsigned long flags)",
        "{",
        "	set_work_data(work, (unsigned long)pwq | WORK_STRUCT_PENDING |",
        "		      WORK_STRUCT_PWQ | flags);",
        "}",
        "",
        "static void set_work_pool_and_keep_pending(struct work_struct *work,",
        "					   int pool_id, unsigned long flags)",
        "{",
        "	set_work_data(work, ((unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT) |",
        "		      WORK_STRUCT_PENDING | flags);",
        "}",
        "",
        "static void set_work_pool_and_clear_pending(struct work_struct *work,",
        "					    int pool_id, unsigned long flags)",
        "{",
        "	/*",
        "	 * The following wmb is paired with the implied mb in",
        "	 * test_and_set_bit(PENDING) and ensures all updates to @work made",
        "	 * here are visible to and precede any updates by the next PENDING",
        "	 * owner.",
        "	 */",
        "	smp_wmb();",
        "	set_work_data(work, ((unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT) |",
        "		      flags);",
        "	/*",
        "	 * The following mb guarantees that previous clear of a PENDING bit",
        "	 * will not be reordered with any speculative LOADS or STORES from",
        "	 * work->current_func, which is executed afterwards.  This possible",
        "	 * reordering can lead to a missed execution on attempt to queue",
        "	 * the same @work.  E.g. consider this case:",
        "	 *",
        "	 *   CPU#0                         CPU#1",
        "	 *   ----------------------------  --------------------------------",
        "	 *",
        "	 * 1  STORE event_indicated",
        "	 * 2  queue_work_on() {",
        "	 * 3    test_and_set_bit(PENDING)",
        "	 * 4 }                             set_..._and_clear_pending() {",
        "	 * 5                                 set_work_data() # clear bit",
        "	 * 6                                 smp_mb()",
        "	 * 7                               work->current_func() {",
        "	 * 8				      LOAD event_indicated",
        "	 *				   }",
        "	 *",
        "	 * Without an explicit full barrier speculative LOAD on line 8 can",
        "	 * be executed before CPU#0 does STORE on line 1.  If that happens,",
        "	 * CPU#0 observes the PENDING bit is still set and new execution of",
        "	 * a @work is not queued in a hope, that CPU#1 will eventually",
        "	 * finish the queued @work.  Meanwhile CPU#1 does not see",
        "	 * event_indicated is set, because speculative LOAD was executed",
        "	 * before actual STORE.",
        "	 */",
        "	smp_mb();",
        "}",
        "",
        "static inline struct pool_workqueue *work_struct_pwq(unsigned long data)",
        "{",
        "	return (struct pool_workqueue *)(data & WORK_STRUCT_PWQ_MASK);",
        "}",
        "",
        "static struct pool_workqueue *get_work_pwq(struct work_struct *work)",
        "{",
        "	unsigned long data = atomic_long_read(&work->data);",
        "",
        "	if (data & WORK_STRUCT_PWQ)",
        "		return work_struct_pwq(data);",
        "	else",
        "		return NULL;",
        "}",
        "",
        "/**",
        " * get_work_pool - return the worker_pool a given work was associated with",
        " * @work: the work item of interest",
        " *",
        " * Pools are created and destroyed under wq_pool_mutex, and allows read",
        " * access under RCU read lock.  As such, this function should be",
        " * called under wq_pool_mutex or inside of a rcu_read_lock() region.",
        " *",
        " * All fields of the returned pool are accessible as long as the above",
        " * mentioned locking is in effect.  If the returned pool needs to be used",
        " * beyond the critical section, the caller is responsible for ensuring the",
        " * returned pool is and stays online.",
        " *",
        " * Return: The worker_pool @work was last associated with.  %NULL if none.",
        " */",
        "static struct worker_pool *get_work_pool(struct work_struct *work)",
        "{",
        "	unsigned long data = atomic_long_read(&work->data);",
        "	int pool_id;",
        "",
        "	assert_rcu_or_pool_mutex();",
        "",
        "	if (data & WORK_STRUCT_PWQ)",
        "		return work_struct_pwq(data)->pool;",
        "",
        "	pool_id = data >> WORK_OFFQ_POOL_SHIFT;",
        "	if (pool_id == WORK_OFFQ_POOL_NONE)",
        "		return NULL;",
        "",
        "	return idr_find(&worker_pool_idr, pool_id);",
        "}",
        "",
        "static unsigned long shift_and_mask(unsigned long v, u32 shift, u32 bits)",
        "{",
        "	return (v >> shift) & ((1U << bits) - 1);",
        "}",
        "",
        "static void work_offqd_unpack(struct work_offq_data *offqd, unsigned long data)",
        "{",
        "	WARN_ON_ONCE(data & WORK_STRUCT_PWQ);",
        "",
        "	offqd->pool_id = shift_and_mask(data, WORK_OFFQ_POOL_SHIFT,",
        "					WORK_OFFQ_POOL_BITS);",
        "	offqd->disable = shift_and_mask(data, WORK_OFFQ_DISABLE_SHIFT,",
        "					WORK_OFFQ_DISABLE_BITS);",
        "	offqd->flags = data & WORK_OFFQ_FLAG_MASK;",
        "}",
        "",
        "static unsigned long work_offqd_pack_flags(struct work_offq_data *offqd)",
        "{",
        "	return ((unsigned long)offqd->disable << WORK_OFFQ_DISABLE_SHIFT) |",
        "		((unsigned long)offqd->flags);",
        "}",
        "",
        "/*",
        " * Policy functions.  These define the policies on how the global worker",
        " * pools are managed.  Unless noted otherwise, these functions assume that",
        " * they're being called with pool->lock held.",
        " */",
        "",
        "/*",
        " * Need to wake up a worker?  Called from anything but currently",
        " * running workers.",
        " *",
        " * Note that, because unbound workers never contribute to nr_running, this",
        " * function will always return %true for unbound pools as long as the",
        " * worklist isn't empty.",
        " */",
        "static bool need_more_worker(struct worker_pool *pool)",
        "{",
        "	return !list_empty(&pool->worklist) && !pool->nr_running;",
        "}",
        "",
        "/* Can I start working?  Called from busy but !running workers. */",
        "static bool may_start_working(struct worker_pool *pool)",
        "{",
        "	return pool->nr_idle;",
        "}",
        "",
        "/* Do I need to keep working?  Called from currently running workers. */",
        "static bool keep_working(struct worker_pool *pool)",
        "{",
        "	return !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
        "}",
        "",
        "/* Do we need a new worker?  Called from manager. */",
        "static bool need_to_create_worker(struct worker_pool *pool)",
        "{",
        "	return need_more_worker(pool) && !may_start_working(pool);",
        "}",
        "",
        "/* Do we have too many workers and should some go away? */",
        "static bool too_many_workers(struct worker_pool *pool)",
        "{",
        "	bool managing = pool->flags & POOL_MANAGER_ACTIVE;",
        "	int nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
        "	int nr_busy = pool->nr_workers - nr_idle;",
        "",
        "	return nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
        "}",
        "",
        "/**",
        " * worker_set_flags - set worker flags and adjust nr_running accordingly",
        " * @worker: self",
        " * @flags: flags to set",
        " *",
        " * Set @flags in @worker->flags and adjust nr_running accordingly.",
        " */",
        "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	/* If transitioning into NOT_RUNNING, adjust nr_running. */",
        "	if ((flags & WORKER_NOT_RUNNING) &&",
        "	    !(worker->flags & WORKER_NOT_RUNNING)) {",
        "		pool->nr_running--;",
        "	}",
        "",
        "	worker->flags |= flags;",
        "}",
        "",
        "/**",
        " * worker_clr_flags - clear worker flags and adjust nr_running accordingly",
        " * @worker: self",
        " * @flags: flags to clear",
        " *",
        " * Clear @flags in @worker->flags and adjust nr_running accordingly.",
        " */",
        "static inline void worker_clr_flags(struct worker *worker, unsigned int flags)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "	unsigned int oflags = worker->flags;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	worker->flags &= ~flags;",
        "",
        "	/*",
        "	 * If transitioning out of NOT_RUNNING, increment nr_running.  Note",
        "	 * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask",
        "	 * of multiple flags, not a single flag.",
        "	 */",
        "	if ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))",
        "		if (!(worker->flags & WORKER_NOT_RUNNING))",
        "			pool->nr_running++;",
        "}",
        "",
        "/* Return the first idle worker.  Called with pool->lock held. */",
        "static struct worker *first_idle_worker(struct worker_pool *pool)",
        "{",
        "	if (unlikely(list_empty(&pool->idle_list)))",
        "		return NULL;",
        "",
        "	return list_first_entry(&pool->idle_list, struct worker, entry);",
        "}",
        "",
        "/**",
        " * worker_enter_idle - enter idle state",
        " * @worker: worker which is entering idle state",
        " *",
        " * @worker is entering idle state.  Update stats and idle timer if",
        " * necessary.",
        " *",
        " * LOCKING:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void worker_enter_idle(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	if (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||",
        "	    WARN_ON_ONCE(!list_empty(&worker->entry) &&",
        "			 (worker->hentry.next || worker->hentry.pprev)))",
        "		return;",
        "",
        "	/* can't use worker_set_flags(), also called from create_worker() */",
        "	worker->flags |= WORKER_IDLE;",
        "	pool->nr_idle++;",
        "	worker->last_active = jiffies;",
        "",
        "	/* idle_list is LIFO */",
        "	list_add(&worker->entry, &pool->idle_list);",
        "",
        "	if (too_many_workers(pool) && !timer_pending(&pool->idle_timer))",
        "		mod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);",
        "",
        "	/* Sanity check nr_running. */",
        "	WARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);",
        "}",
        "",
        "/**",
        " * worker_leave_idle - leave idle state",
        " * @worker: worker which is leaving idle state",
        " *",
        " * @worker is leaving idle state.  Update stats.",
        " *",
        " * LOCKING:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void worker_leave_idle(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	if (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))",
        "		return;",
        "	worker_clr_flags(worker, WORKER_IDLE);",
        "	pool->nr_idle--;",
        "	list_del_init(&worker->entry);",
        "}",
        "",
        "/**",
        " * find_worker_executing_work - find worker which is executing a work",
        " * @pool: pool of interest",
        " * @work: work to find worker for",
        " *",
        " * Find a worker which is executing @work on @pool by searching",
        " * @pool->busy_hash which is keyed by the address of @work.  For a worker",
        " * to match, its current execution should match the address of @work and",
        " * its work function.  This is to avoid unwanted dependency between",
        " * unrelated work executions through a work item being recycled while still",
        " * being executed.",
        " *",
        " * This is a bit tricky.  A work item may be freed once its execution",
        " * starts and nothing prevents the freed area from being recycled for",
        " * another work item.  If the same work item address ends up being reused",
        " * before the original execution finishes, workqueue will identify the",
        " * recycled work item as currently executing and make it wait until the",
        " * current execution finishes, introducing an unwanted dependency.",
        " *",
        " * This function checks the work item address and work function to avoid",
        " * false positives.  Note that this isn't complete as one may construct a",
        " * work function which can introduce dependency onto itself through a",
        " * recycled work item.  Well, if somebody wants to shoot oneself in the",
        " * foot that badly, there's only so much we can do, and if such deadlock",
        " * actually occurs, it should be easy to locate the culprit work function.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " *",
        " * Return:",
        " * Pointer to worker which is executing @work if found, %NULL",
        " * otherwise.",
        " */",
        "static struct worker *find_worker_executing_work(struct worker_pool *pool,",
        "						 struct work_struct *work)",
        "{",
        "	struct worker *worker;",
        "",
        "	hash_for_each_possible(pool->busy_hash, worker, hentry,",
        "			       (unsigned long)work)",
        "		if (worker->current_work == work &&",
        "		    worker->current_func == work->func)",
        "			return worker;",
        "",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * move_linked_works - move linked works to a list",
        " * @work: start of series of works to be scheduled",
        " * @head: target list to append @work to",
        " * @nextp: out parameter for nested worklist walking",
        " *",
        " * Schedule linked works starting from @work to @head. Work series to be",
        " * scheduled starts at @work and includes any consecutive work with",
        " * WORK_STRUCT_LINKED set in its predecessor. See assign_work() for details on",
        " * @nextp.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void move_linked_works(struct work_struct *work, struct list_head *head,",
        "			      struct work_struct **nextp)",
        "{",
        "	struct work_struct *n;",
        "",
        "	/*",
        "	 * Linked worklist will always end before the end of the list,",
        "	 * use NULL for list head.",
        "	 */",
        "	list_for_each_entry_safe_from(work, n, NULL, entry) {",
        "		list_move_tail(&work->entry, head);",
        "		if (!(*work_data_bits(work) & WORK_STRUCT_LINKED))",
        "			break;",
        "	}",
        "",
        "	/*",
        "	 * If we're already inside safe list traversal and have moved",
        "	 * multiple works to the scheduled queue, the next position",
        "	 * needs to be updated.",
        "	 */",
        "	if (nextp)",
        "		*nextp = n;",
        "}",
        "",
        "/**",
        " * assign_work - assign a work item and its linked work items to a worker",
        " * @work: work to assign",
        " * @worker: worker to assign to",
        " * @nextp: out parameter for nested worklist walking",
        " *",
        " * Assign @work and its linked work items to @worker. If @work is already being",
        " * executed by another worker in the same pool, it'll be punted there.",
        " *",
        " * If @nextp is not NULL, it's updated to point to the next work of the last",
        " * scheduled work. This allows assign_work() to be nested inside",
        " * list_for_each_entry_safe().",
        " *",
        " * Returns %true if @work was successfully assigned to @worker. %false if @work",
        " * was punted to another worker already executing it.",
        " */",
        "static bool assign_work(struct work_struct *work, struct worker *worker,",
        "			struct work_struct **nextp)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "	struct worker *collision;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	/*",
        "	 * A single work shouldn't be executed concurrently by multiple workers.",
        "	 * __queue_work() ensures that @work doesn't jump to a different pool",
        "	 * while still running in the previous pool. Here, we should ensure that",
        "	 * @work is not executed concurrently by multiple workers from the same",
        "	 * pool. Check whether anyone is already processing the work. If so,",
        "	 * defer the work to the currently executing one.",
        "	 */",
        "	collision = find_worker_executing_work(pool, work);",
        "	if (unlikely(collision)) {",
        "		move_linked_works(work, &collision->scheduled, nextp);",
        "		return false;",
        "	}",
        "",
        "	move_linked_works(work, &worker->scheduled, nextp);",
        "	return true;",
        "}",
        "",
        "static struct irq_work *bh_pool_irq_work(struct worker_pool *pool)",
        "{",
        "	int high = pool->attrs->nice == HIGHPRI_NICE_LEVEL ? 1 : 0;",
        "",
        "	return &per_cpu(bh_pool_irq_works, pool->cpu)[high];",
        "}",
        "",
        "static void kick_bh_pool(struct worker_pool *pool)",
        "{",
        "#ifdef CONFIG_SMP",
        "	/* see drain_dead_softirq_workfn() for BH_DRAINING */",
        "	if (unlikely(pool->cpu != smp_processor_id() &&",
        "		     !(pool->flags & POOL_BH_DRAINING))) {",
        "		irq_work_queue_on(bh_pool_irq_work(pool), pool->cpu);",
        "		return;",
        "	}",
        "#endif",
        "	if (pool->attrs->nice == HIGHPRI_NICE_LEVEL)",
        "		raise_softirq_irqoff(HI_SOFTIRQ);",
        "	else",
        "		raise_softirq_irqoff(TASKLET_SOFTIRQ);",
        "}",
        "",
        "/**",
        " * kick_pool - wake up an idle worker if necessary",
        " * @pool: pool to kick",
        " *",
        " * @pool may have pending work items. Wake up worker if necessary. Returns",
        " * whether a worker was woken up.",
        " */",
        "static bool kick_pool(struct worker_pool *pool)",
        "{",
        "	struct worker *worker = first_idle_worker(pool);",
        "	struct task_struct *p;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	if (!need_more_worker(pool) || !worker)",
        "		return false;",
        "",
        "	if (pool->flags & POOL_BH) {",
        "		kick_bh_pool(pool);",
        "		return true;",
        "	}",
        "",
        "	p = worker->task;",
        "",
        "#ifdef CONFIG_SMP",
        "	/*",
        "	 * Idle @worker is about to execute @work and waking up provides an",
        "	 * opportunity to migrate @worker at a lower cost by setting the task's",
        "	 * wake_cpu field. Let's see if we want to move @worker to improve",
        "	 * execution locality.",
        "	 *",
        "	 * We're waking the worker that went idle the latest and there's some",
        "	 * chance that @worker is marked idle but hasn't gone off CPU yet. If",
        "	 * so, setting the wake_cpu won't do anything. As this is a best-effort",
        "	 * optimization and the race window is narrow, let's leave as-is for",
        "	 * now. If this becomes pronounced, we can skip over workers which are",
        "	 * still on cpu when picking an idle worker.",
        "	 *",
        "	 * If @pool has non-strict affinity, @worker might have ended up outside",
        "	 * its affinity scope. Repatriate.",
        "	 */",
        "	if (!pool->attrs->affn_strict &&",
        "	    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {",
        "		struct work_struct *work = list_first_entry(&pool->worklist,",
        "						struct work_struct, entry);",
        "		int wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,",
        "							  cpu_online_mask);",
        "		if (wake_cpu < nr_cpu_ids) {",
        "			p->wake_cpu = wake_cpu;",
        "			get_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;",
        "		}",
        "	}",
        "#endif",
        "	wake_up_process(p);",
        "	return true;",
        "}",
        "",
        "#ifdef CONFIG_WQ_CPU_INTENSIVE_REPORT",
        "",
        "/*",
        " * Concurrency-managed per-cpu work items that hog CPU for longer than",
        " * wq_cpu_intensive_thresh_us trigger the automatic CPU_INTENSIVE mechanism,",
        " * which prevents them from stalling other concurrency-managed work items. If a",
        " * work function keeps triggering this mechanism, it's likely that the work item",
        " * should be using an unbound workqueue instead.",
        " *",
        " * wq_cpu_intensive_report() tracks work functions which trigger such conditions",
        " * and report them so that they can be examined and converted to use unbound",
        " * workqueues as appropriate. To avoid flooding the console, each violating work",
        " * function is tracked and reported with exponential backoff.",
        " */",
        "#define WCI_MAX_ENTS 128",
        "",
        "struct wci_ent {",
        "	work_func_t		func;",
        "	atomic64_t		cnt;",
        "	struct hlist_node	hash_node;",
        "};",
        "",
        "static struct wci_ent wci_ents[WCI_MAX_ENTS];",
        "static int wci_nr_ents;",
        "static DEFINE_RAW_SPINLOCK(wci_lock);",
        "static DEFINE_HASHTABLE(wci_hash, ilog2(WCI_MAX_ENTS));",
        "",
        "static struct wci_ent *wci_find_ent(work_func_t func)",
        "{",
        "	struct wci_ent *ent;",
        "",
        "	hash_for_each_possible_rcu(wci_hash, ent, hash_node,",
        "				   (unsigned long)func) {",
        "		if (ent->func == func)",
        "			return ent;",
        "	}",
        "	return NULL;",
        "}",
        "",
        "static void wq_cpu_intensive_report(work_func_t func)",
        "{",
        "	struct wci_ent *ent;",
        "",
        "restart:",
        "	ent = wci_find_ent(func);",
        "	if (ent) {",
        "		u64 cnt;",
        "",
        "		/*",
        "		 * Start reporting from the warning_thresh and back off",
        "		 * exponentially.",
        "		 */",
        "		cnt = atomic64_inc_return_relaxed(&ent->cnt);",
        "		if (wq_cpu_intensive_warning_thresh &&",
        "		    cnt >= wq_cpu_intensive_warning_thresh &&",
        "		    is_power_of_2(cnt + 1 - wq_cpu_intensive_warning_thresh))",
        "			printk_deferred(KERN_WARNING \"workqueue: %ps hogged CPU for >%luus %llu times, consider switching to WQ_UNBOUND\\n\",",
        "					ent->func, wq_cpu_intensive_thresh_us,",
        "					atomic64_read(&ent->cnt));",
        "		return;",
        "	}",
        "",
        "	/*",
        "	 * @func is a new violation. Allocate a new entry for it. If wcn_ents[]",
        "	 * is exhausted, something went really wrong and we probably made enough",
        "	 * noise already.",
        "	 */",
        "	if (wci_nr_ents >= WCI_MAX_ENTS)",
        "		return;",
        "",
        "	raw_spin_lock(&wci_lock);",
        "",
        "	if (wci_nr_ents >= WCI_MAX_ENTS) {",
        "		raw_spin_unlock(&wci_lock);",
        "		return;",
        "	}",
        "",
        "	if (wci_find_ent(func)) {",
        "		raw_spin_unlock(&wci_lock);",
        "		goto restart;",
        "	}",
        "",
        "	ent = &wci_ents[wci_nr_ents++];",
        "	ent->func = func;",
        "	atomic64_set(&ent->cnt, 0);",
        "	hash_add_rcu(wci_hash, &ent->hash_node, (unsigned long)func);",
        "",
        "	raw_spin_unlock(&wci_lock);",
        "",
        "	goto restart;",
        "}",
        "",
        "#else	/* CONFIG_WQ_CPU_INTENSIVE_REPORT */",
        "static void wq_cpu_intensive_report(work_func_t func) {}",
        "#endif	/* CONFIG_WQ_CPU_INTENSIVE_REPORT */",
        "",
        "/**",
        " * wq_worker_running - a worker is running again",
        " * @task: task waking up",
        " *",
        " * This function is called when a worker returns from schedule()",
        " */",
        "void wq_worker_running(struct task_struct *task)",
        "{",
        "	struct worker *worker = kthread_data(task);",
        "",
        "	if (!READ_ONCE(worker->sleeping))",
        "		return;",
        "",
        "	/*",
        "	 * If preempted by unbind_workers() between the WORKER_NOT_RUNNING check",
        "	 * and the nr_running increment below, we may ruin the nr_running reset",
        "	 * and leave with an unexpected pool->nr_running == 1 on the newly unbound",
        "	 * pool. Protect against such race.",
        "	 */",
        "	preempt_disable();",
        "	if (!(worker->flags & WORKER_NOT_RUNNING))",
        "		worker->pool->nr_running++;",
        "	preempt_enable();",
        "",
        "	/*",
        "	 * CPU intensive auto-detection cares about how long a work item hogged",
        "	 * CPU without sleeping. Reset the starting timestamp on wakeup.",
        "	 */",
        "	worker->current_at = worker->task->se.sum_exec_runtime;",
        "",
        "	WRITE_ONCE(worker->sleeping, 0);",
        "}",
        "",
        "/**",
        " * wq_worker_sleeping - a worker is going to sleep",
        " * @task: task going to sleep",
        " *",
        " * This function is called from schedule() when a busy worker is",
        " * going to sleep.",
        " */",
        "void wq_worker_sleeping(struct task_struct *task)",
        "{",
        "	struct worker *worker = kthread_data(task);",
        "	struct worker_pool *pool;",
        "",
        "	/*",
        "	 * Rescuers, which may not have all the fields set up like normal",
        "	 * workers, also reach here, let's not access anything before",
        "	 * checking NOT_RUNNING.",
        "	 */",
        "	if (worker->flags & WORKER_NOT_RUNNING)",
        "		return;",
        "",
        "	pool = worker->pool;",
        "",
        "	/* Return if preempted before wq_worker_running() was reached */",
        "	if (READ_ONCE(worker->sleeping))",
        "		return;",
        "",
        "	WRITE_ONCE(worker->sleeping, 1);",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	/*",
        "	 * Recheck in case unbind_workers() preempted us. We don't",
        "	 * want to decrement nr_running after the worker is unbound",
        "	 * and nr_running has been reset.",
        "	 */",
        "	if (worker->flags & WORKER_NOT_RUNNING) {",
        "		raw_spin_unlock_irq(&pool->lock);",
        "		return;",
        "	}",
        "",
        "	pool->nr_running--;",
        "	if (kick_pool(pool))",
        "		worker->current_pwq->stats[PWQ_STAT_CM_WAKEUP]++;",
        "",
        "	raw_spin_unlock_irq(&pool->lock);",
        "}",
        "",
        "/**",
        " * wq_worker_tick - a scheduler tick occurred while a kworker is running",
        " * @task: task currently running",
        " *",
        " * Called from sched_tick(). We're in the IRQ context and the current",
        " * worker's fields which follow the 'K' locking rule can be accessed safely.",
        " */",
        "void wq_worker_tick(struct task_struct *task)",
        "{",
        "	struct worker *worker = kthread_data(task);",
        "	struct pool_workqueue *pwq = worker->current_pwq;",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	if (!pwq)",
        "		return;",
        "",
        "	pwq->stats[PWQ_STAT_CPU_TIME] += TICK_USEC;",
        "",
        "	if (!wq_cpu_intensive_thresh_us)",
        "		return;",
        "",
        "	/*",
        "	 * If the current worker is concurrency managed and hogged the CPU for",
        "	 * longer than wq_cpu_intensive_thresh_us, it's automatically marked",
        "	 * CPU_INTENSIVE to avoid stalling other concurrency-managed work items.",
        "	 *",
        "	 * Set @worker->sleeping means that @worker is in the process of",
        "	 * switching out voluntarily and won't be contributing to",
        "	 * @pool->nr_running until it wakes up. As wq_worker_sleeping() also",
        "	 * decrements ->nr_running, setting CPU_INTENSIVE here can lead to",
        "	 * double decrements. The task is releasing the CPU anyway. Let's skip.",
        "	 * We probably want to make this prettier in the future.",
        "	 */",
        "	if ((worker->flags & WORKER_NOT_RUNNING) || READ_ONCE(worker->sleeping) ||",
        "	    worker->task->se.sum_exec_runtime - worker->current_at <",
        "	    wq_cpu_intensive_thresh_us * NSEC_PER_USEC)",
        "		return;",
        "",
        "	raw_spin_lock(&pool->lock);",
        "",
        "	worker_set_flags(worker, WORKER_CPU_INTENSIVE);",
        "	wq_cpu_intensive_report(worker->current_func);",
        "	pwq->stats[PWQ_STAT_CPU_INTENSIVE]++;",
        "",
        "	if (kick_pool(pool))",
        "		pwq->stats[PWQ_STAT_CM_WAKEUP]++;",
        "",
        "	raw_spin_unlock(&pool->lock);",
        "}",
        "",
        "/**",
        " * wq_worker_last_func - retrieve worker's last work function",
        " * @task: Task to retrieve last work function of.",
        " *",
        " * Determine the last function a worker executed. This is called from",
        " * the scheduler to get a worker's last known identity.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(rq->lock)",
        " *",
        " * This function is called during schedule() when a kworker is going",
        " * to sleep. It's used by psi to identify aggregation workers during",
        " * dequeuing, to allow periodic aggregation to shut-off when that",
        " * worker is the last task in the system or cgroup to go to sleep.",
        " *",
        " * As this function doesn't involve any workqueue-related locking, it",
        " * only returns stable values when called from inside the scheduler's",
        " * queuing and dequeuing paths, when @task, which must be a kworker,",
        " * is guaranteed to not be processing any works.",
        " *",
        " * Return:",
        " * The last work function %current executed as a worker, NULL if it",
        " * hasn't executed any work yet.",
        " */",
        "work_func_t wq_worker_last_func(struct task_struct *task)",
        "{",
        "	struct worker *worker = kthread_data(task);",
        "",
        "	return worker->last_func;",
        "}",
        "",
        "/**",
        " * wq_node_nr_active - Determine wq_node_nr_active to use",
        " * @wq: workqueue of interest",
        " * @node: NUMA node, can be %NUMA_NO_NODE",
        " *",
        " * Determine wq_node_nr_active to use for @wq on @node. Returns:",
        " *",
        " * - %NULL for per-cpu workqueues as they don't need to use shared nr_active.",
        " *",
        " * - node_nr_active[nr_node_ids] if @node is %NUMA_NO_NODE.",
        " *",
        " * - Otherwise, node_nr_active[@node].",
        " */",
        "static struct wq_node_nr_active *wq_node_nr_active(struct workqueue_struct *wq,",
        "						   int node)",
        "{",
        "	if (!(wq->flags & WQ_UNBOUND))",
        "		return NULL;",
        "",
        "	if (node == NUMA_NO_NODE)",
        "		node = nr_node_ids;",
        "",
        "	return wq->node_nr_active[node];",
        "}",
        "",
        "/**",
        " * wq_update_node_max_active - Update per-node max_actives to use",
        " * @wq: workqueue to update",
        " * @off_cpu: CPU that's going down, -1 if a CPU is not going down",
        " *",
        " * Update @wq->node_nr_active[]->max. @wq must be unbound. max_active is",
        " * distributed among nodes according to the proportions of numbers of online",
        " * cpus. The result is always between @wq->min_active and max_active.",
        " */",
        "static void wq_update_node_max_active(struct workqueue_struct *wq, int off_cpu)",
        "{",
        "	struct cpumask *effective = unbound_effective_cpumask(wq);",
        "	int min_active = READ_ONCE(wq->min_active);",
        "	int max_active = READ_ONCE(wq->max_active);",
        "	int total_cpus, node;",
        "",
        "	lockdep_assert_held(&wq->mutex);",
        "",
        "	if (!wq_topo_initialized)",
        "		return;",
        "",
        "	if (off_cpu >= 0 && !cpumask_test_cpu(off_cpu, effective))",
        "		off_cpu = -1;",
        "",
        "	total_cpus = cpumask_weight_and(effective, cpu_online_mask);",
        "	if (off_cpu >= 0)",
        "		total_cpus--;",
        "",
        "	/* If all CPUs of the wq get offline, use the default values */",
        "	if (unlikely(!total_cpus)) {",
        "		for_each_node(node)",
        "			wq_node_nr_active(wq, node)->max = min_active;",
        "",
        "		wq_node_nr_active(wq, NUMA_NO_NODE)->max = max_active;",
        "		return;",
        "	}",
        "",
        "	for_each_node(node) {",
        "		int node_cpus;",
        "",
        "		node_cpus = cpumask_weight_and(effective, cpumask_of_node(node));",
        "		if (off_cpu >= 0 && cpu_to_node(off_cpu) == node)",
        "			node_cpus--;",
        "",
        "		wq_node_nr_active(wq, node)->max =",
        "			clamp(DIV_ROUND_UP(max_active * node_cpus, total_cpus),",
        "			      min_active, max_active);",
        "	}",
        "",
        "	wq_node_nr_active(wq, NUMA_NO_NODE)->max = max_active;",
        "}",
        "",
        "/**",
        " * get_pwq - get an extra reference on the specified pool_workqueue",
        " * @pwq: pool_workqueue to get",
        " *",
        " * Obtain an extra reference on @pwq.  The caller should guarantee that",
        " * @pwq has positive refcnt and be holding the matching pool->lock.",
        " */",
        "static void get_pwq(struct pool_workqueue *pwq)",
        "{",
        "	lockdep_assert_held(&pwq->pool->lock);",
        "	WARN_ON_ONCE(pwq->refcnt <= 0);",
        "	pwq->refcnt++;",
        "}",
        "",
        "/**",
        " * put_pwq - put a pool_workqueue reference",
        " * @pwq: pool_workqueue to put",
        " *",
        " * Drop a reference of @pwq.  If its refcnt reaches zero, schedule its",
        " * destruction.  The caller should be holding the matching pool->lock.",
        " */",
        "static void put_pwq(struct pool_workqueue *pwq)",
        "{",
        "	lockdep_assert_held(&pwq->pool->lock);",
        "	if (likely(--pwq->refcnt))",
        "		return;",
        "	/*",
        "	 * @pwq can't be released under pool->lock, bounce to a dedicated",
        "	 * kthread_worker to avoid A-A deadlocks.",
        "	 */",
        "	kthread_queue_work(pwq_release_worker, &pwq->release_work);",
        "}",
        "",
        "/**",
        " * put_pwq_unlocked - put_pwq() with surrounding pool lock/unlock",
        " * @pwq: pool_workqueue to put (can be %NULL)",
        " *",
        " * put_pwq() with locking.  This function also allows %NULL @pwq.",
        " */",
        "static void put_pwq_unlocked(struct pool_workqueue *pwq)",
        "{",
        "	if (pwq) {",
        "		/*",
        "		 * As both pwqs and pools are RCU protected, the",
        "		 * following lock operations are safe.",
        "		 */",
        "		raw_spin_lock_irq(&pwq->pool->lock);",
        "		put_pwq(pwq);",
        "		raw_spin_unlock_irq(&pwq->pool->lock);",
        "	}",
        "}",
        "",
        "static bool pwq_is_empty(struct pool_workqueue *pwq)",
        "{",
        "	return !pwq->nr_active && list_empty(&pwq->inactive_works);",
        "}",
        "",
        "static void __pwq_activate_work(struct pool_workqueue *pwq,",
        "				struct work_struct *work)",
        "{",
        "	unsigned long *wdb = work_data_bits(work);",
        "",
        "	WARN_ON_ONCE(!(*wdb & WORK_STRUCT_INACTIVE));",
        "	trace_workqueue_activate_work(work);",
        "	if (list_empty(&pwq->pool->worklist))",
        "		pwq->pool->watchdog_ts = jiffies;",
        "	move_linked_works(work, &pwq->pool->worklist, NULL);",
        "	__clear_bit(WORK_STRUCT_INACTIVE_BIT, wdb);",
        "}",
        "",
        "static bool tryinc_node_nr_active(struct wq_node_nr_active *nna)",
        "{",
        "	int max = READ_ONCE(nna->max);",
        "",
        "	while (true) {",
        "		int old, tmp;",
        "",
        "		old = atomic_read(&nna->nr);",
        "		if (old >= max)",
        "			return false;",
        "		tmp = atomic_cmpxchg_relaxed(&nna->nr, old, old + 1);",
        "		if (tmp == old)",
        "			return true;",
        "	}",
        "}",
        "",
        "/**",
        " * pwq_tryinc_nr_active - Try to increment nr_active for a pwq",
        " * @pwq: pool_workqueue of interest",
        " * @fill: max_active may have increased, try to increase concurrency level",
        " *",
        " * Try to increment nr_active for @pwq. Returns %true if an nr_active count is",
        " * successfully obtained. %false otherwise.",
        " */",
        "static bool pwq_tryinc_nr_active(struct pool_workqueue *pwq, bool fill)",
        "{",
        "	struct workqueue_struct *wq = pwq->wq;",
        "	struct worker_pool *pool = pwq->pool;",
        "	struct wq_node_nr_active *nna = wq_node_nr_active(wq, pool->node);",
        "	bool obtained = false;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	if (!nna) {",
        "		/* BH or per-cpu workqueue, pwq->nr_active is sufficient */",
        "		obtained = pwq->nr_active < READ_ONCE(wq->max_active);",
        "		goto out;",
        "	}",
        "",
        "	if (unlikely(pwq->plugged))",
        "		return false;",
        "",
        "	/*",
        "	 * Unbound workqueue uses per-node shared nr_active $nna. If @pwq is",
        "	 * already waiting on $nna, pwq_dec_nr_active() will maintain the",
        "	 * concurrency level. Don't jump the line.",
        "	 *",
        "	 * We need to ignore the pending test after max_active has increased as",
        "	 * pwq_dec_nr_active() can only maintain the concurrency level but not",
        "	 * increase it. This is indicated by @fill.",
        "	 */",
        "	if (!list_empty(&pwq->pending_node) && likely(!fill))",
        "		goto out;",
        "",
        "	obtained = tryinc_node_nr_active(nna);",
        "	if (obtained)",
        "		goto out;",
        "",
        "	/*",
        "	 * Lockless acquisition failed. Lock, add ourself to $nna->pending_pwqs",
        "	 * and try again. The smp_mb() is paired with the implied memory barrier",
        "	 * of atomic_dec_return() in pwq_dec_nr_active() to ensure that either",
        "	 * we see the decremented $nna->nr or they see non-empty",
        "	 * $nna->pending_pwqs.",
        "	 */",
        "	raw_spin_lock(&nna->lock);",
        "",
        "	if (list_empty(&pwq->pending_node))",
        "		list_add_tail(&pwq->pending_node, &nna->pending_pwqs);",
        "	else if (likely(!fill))",
        "		goto out_unlock;",
        "",
        "	smp_mb();",
        "",
        "	obtained = tryinc_node_nr_active(nna);",
        "",
        "	/*",
        "	 * If @fill, @pwq might have already been pending. Being spuriously",
        "	 * pending in cold paths doesn't affect anything. Let's leave it be.",
        "	 */",
        "	if (obtained && likely(!fill))",
        "		list_del_init(&pwq->pending_node);",
        "",
        "out_unlock:",
        "	raw_spin_unlock(&nna->lock);",
        "out:",
        "	if (obtained)",
        "		pwq->nr_active++;",
        "	return obtained;",
        "}",
        "",
        "/**",
        " * pwq_activate_first_inactive - Activate the first inactive work item on a pwq",
        " * @pwq: pool_workqueue of interest",
        " * @fill: max_active may have increased, try to increase concurrency level",
        " *",
        " * Activate the first inactive work item of @pwq if available and allowed by",
        " * max_active limit.",
        " *",
        " * Returns %true if an inactive work item has been activated. %false if no",
        " * inactive work item is found or max_active limit is reached.",
        " */",
        "static bool pwq_activate_first_inactive(struct pool_workqueue *pwq, bool fill)",
        "{",
        "	struct work_struct *work =",
        "		list_first_entry_or_null(&pwq->inactive_works,",
        "					 struct work_struct, entry);",
        "",
        "	if (work && pwq_tryinc_nr_active(pwq, fill)) {",
        "		__pwq_activate_work(pwq, work);",
        "		return true;",
        "	} else {",
        "		return false;",
        "	}",
        "}",
        "",
        "/**",
        " * unplug_oldest_pwq - unplug the oldest pool_workqueue",
        " * @wq: workqueue_struct where its oldest pwq is to be unplugged",
        " *",
        " * This function should only be called for ordered workqueues where only the",
        " * oldest pwq is unplugged, the others are plugged to suspend execution to",
        " * ensure proper work item ordering::",
        " *",
        " *    dfl_pwq --------------+     [P] - plugged",
        " *                          |",
        " *                          v",
        " *    pwqs -> A -> B [P] -> C [P] (newest)",
        " *            |    |        |",
        " *            1    3        5",
        " *            |    |        |",
        " *            2    4        6",
        " *",
        " * When the oldest pwq is drained and removed, this function should be called",
        " * to unplug the next oldest one to start its work item execution. Note that",
        " * pwq's are linked into wq->pwqs with the oldest first, so the first one in",
        " * the list is the oldest.",
        " */",
        "static void unplug_oldest_pwq(struct workqueue_struct *wq)",
        "{",
        "	struct pool_workqueue *pwq;",
        "",
        "	lockdep_assert_held(&wq->mutex);",
        "",
        "	/* Caller should make sure that pwqs isn't empty before calling */",
        "	pwq = list_first_entry_or_null(&wq->pwqs, struct pool_workqueue,",
        "				       pwqs_node);",
        "	raw_spin_lock_irq(&pwq->pool->lock);",
        "	if (pwq->plugged) {",
        "		pwq->plugged = false;",
        "		if (pwq_activate_first_inactive(pwq, true))",
        "			kick_pool(pwq->pool);",
        "	}",
        "	raw_spin_unlock_irq(&pwq->pool->lock);",
        "}",
        "",
        "/**",
        " * node_activate_pending_pwq - Activate a pending pwq on a wq_node_nr_active",
        " * @nna: wq_node_nr_active to activate a pending pwq for",
        " * @caller_pool: worker_pool the caller is locking",
        " *",
        " * Activate a pwq in @nna->pending_pwqs. Called with @caller_pool locked.",
        " * @caller_pool may be unlocked and relocked to lock other worker_pools.",
        " */",
        "static void node_activate_pending_pwq(struct wq_node_nr_active *nna,",
        "				      struct worker_pool *caller_pool)",
        "{",
        "	struct worker_pool *locked_pool = caller_pool;",
        "	struct pool_workqueue *pwq;",
        "	struct work_struct *work;",
        "",
        "	lockdep_assert_held(&caller_pool->lock);",
        "",
        "	raw_spin_lock(&nna->lock);",
        "retry:",
        "	pwq = list_first_entry_or_null(&nna->pending_pwqs,",
        "				       struct pool_workqueue, pending_node);",
        "	if (!pwq)",
        "		goto out_unlock;",
        "",
        "	/*",
        "	 * If @pwq is for a different pool than @locked_pool, we need to lock",
        "	 * @pwq->pool->lock. Let's trylock first. If unsuccessful, do the unlock",
        "	 * / lock dance. For that, we also need to release @nna->lock as it's",
        "	 * nested inside pool locks.",
        "	 */",
        "	if (pwq->pool != locked_pool) {",
        "		raw_spin_unlock(&locked_pool->lock);",
        "		locked_pool = pwq->pool;",
        "		if (!raw_spin_trylock(&locked_pool->lock)) {",
        "			raw_spin_unlock(&nna->lock);",
        "			raw_spin_lock(&locked_pool->lock);",
        "			raw_spin_lock(&nna->lock);",
        "			goto retry;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * $pwq may not have any inactive work items due to e.g. cancellations.",
        "	 * Drop it from pending_pwqs and see if there's another one.",
        "	 */",
        "	work = list_first_entry_or_null(&pwq->inactive_works,",
        "					struct work_struct, entry);",
        "	if (!work) {",
        "		list_del_init(&pwq->pending_node);",
        "		goto retry;",
        "	}",
        "",
        "	/*",
        "	 * Acquire an nr_active count and activate the inactive work item. If",
        "	 * $pwq still has inactive work items, rotate it to the end of the",
        "	 * pending_pwqs so that we round-robin through them. This means that",
        "	 * inactive work items are not activated in queueing order which is fine",
        "	 * given that there has never been any ordering across different pwqs.",
        "	 */",
        "	if (likely(tryinc_node_nr_active(nna))) {",
        "		pwq->nr_active++;",
        "		__pwq_activate_work(pwq, work);",
        "",
        "		if (list_empty(&pwq->inactive_works))",
        "			list_del_init(&pwq->pending_node);",
        "		else",
        "			list_move_tail(&pwq->pending_node, &nna->pending_pwqs);",
        "",
        "		/* if activating a foreign pool, make sure it's running */",
        "		if (pwq->pool != caller_pool)",
        "			kick_pool(pwq->pool);",
        "	}",
        "",
        "out_unlock:",
        "	raw_spin_unlock(&nna->lock);",
        "	if (locked_pool != caller_pool) {",
        "		raw_spin_unlock(&locked_pool->lock);",
        "		raw_spin_lock(&caller_pool->lock);",
        "	}",
        "}",
        "",
        "/**",
        " * pwq_dec_nr_active - Retire an active count",
        " * @pwq: pool_workqueue of interest",
        " *",
        " * Decrement @pwq's nr_active and try to activate the first inactive work item.",
        " * For unbound workqueues, this function may temporarily drop @pwq->pool->lock.",
        " */",
        "static void pwq_dec_nr_active(struct pool_workqueue *pwq)",
        "{",
        "	struct worker_pool *pool = pwq->pool;",
        "	struct wq_node_nr_active *nna = wq_node_nr_active(pwq->wq, pool->node);",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "",
        "	/*",
        "	 * @pwq->nr_active should be decremented for both percpu and unbound",
        "	 * workqueues.",
        "	 */",
        "	pwq->nr_active--;",
        "",
        "	/*",
        "	 * For a percpu workqueue, it's simple. Just need to kick the first",
        "	 * inactive work item on @pwq itself.",
        "	 */",
        "	if (!nna) {",
        "		pwq_activate_first_inactive(pwq, false);",
        "		return;",
        "	}",
        "",
        "	/*",
        "	 * If @pwq is for an unbound workqueue, it's more complicated because",
        "	 * multiple pwqs and pools may be sharing the nr_active count. When a",
        "	 * pwq needs to wait for an nr_active count, it puts itself on",
        "	 * $nna->pending_pwqs. The following atomic_dec_return()'s implied",
        "	 * memory barrier is paired with smp_mb() in pwq_tryinc_nr_active() to",
        "	 * guarantee that either we see non-empty pending_pwqs or they see",
        "	 * decremented $nna->nr.",
        "	 *",
        "	 * $nna->max may change as CPUs come online/offline and @pwq->wq's",
        "	 * max_active gets updated. However, it is guaranteed to be equal to or",
        "	 * larger than @pwq->wq->min_active which is above zero unless freezing.",
        "	 * This maintains the forward progress guarantee.",
        "	 */",
        "	if (atomic_dec_return(&nna->nr) >= READ_ONCE(nna->max))",
        "		return;",
        "",
        "	if (!list_empty(&nna->pending_pwqs))",
        "		node_activate_pending_pwq(nna, pool);",
        "}",
        "",
        "/**",
        " * pwq_dec_nr_in_flight - decrement pwq's nr_in_flight",
        " * @pwq: pwq of interest",
        " * @work_data: work_data of work which left the queue",
        " *",
        " * A work either has completed or is removed from pending queue,",
        " * decrement nr_in_flight of its pwq and handle workqueue flushing.",
        " *",
        " * NOTE:",
        " * For unbound workqueues, this function may temporarily drop @pwq->pool->lock",
        " * and thus should be called after all other state updates for the in-flight",
        " * work item is complete.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, unsigned long work_data)",
        "{",
        "	int color = get_work_color(work_data);",
        "",
        "	if (!(work_data & WORK_STRUCT_INACTIVE))",
        "		pwq_dec_nr_active(pwq);",
        "",
        "	pwq->nr_in_flight[color]--;",
        "",
        "	/* is flush in progress and are we at the flushing tip? */",
        "	if (likely(pwq->flush_color != color))",
        "		goto out_put;",
        "",
        "	/* are there still in-flight works? */",
        "	if (pwq->nr_in_flight[color])",
        "		goto out_put;",
        "",
        "	/* this pwq is done, clear flush_color */",
        "	pwq->flush_color = -1;",
        "",
        "	/*",
        "	 * If this was the last pwq, wake up the first flusher.  It",
        "	 * will handle the rest.",
        "	 */",
        "	if (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))",
        "		complete(&pwq->wq->first_flusher->done);",
        "out_put:",
        "	put_pwq(pwq);",
        "}",
        "",
        "/**",
        " * try_to_grab_pending - steal work item from worklist and disable irq",
        " * @work: work item to steal",
        " * @cflags: %WORK_CANCEL_ flags",
        " * @irq_flags: place to store irq state",
        " *",
        " * Try to grab PENDING bit of @work.  This function can handle @work in any",
        " * stable state - idle, on timer or on worklist.",
        " *",
        " * Return:",
        " *",
        " *  ========	================================================================",
        " *  1		if @work was pending and we successfully stole PENDING",
        " *  0		if @work was idle and we claimed PENDING",
        " *  -EAGAIN	if PENDING couldn't be grabbed at the moment, safe to busy-retry",
        " *  ========	================================================================",
        " *",
        " * Note:",
        " * On >= 0 return, the caller owns @work's PENDING bit.  To avoid getting",
        " * interrupted while holding PENDING and @work off queue, irq must be",
        " * disabled on entry.  This, combined with delayed_work->timer being",
        " * irqsafe, ensures that we return -EAGAIN for finite short period of time.",
        " *",
        " * On successful return, >= 0, irq is disabled and the caller is",
        " * responsible for releasing it using local_irq_restore(*@irq_flags).",
        " *",
        " * This function is safe to call from any context including IRQ handler.",
        " */",
        "static int try_to_grab_pending(struct work_struct *work, u32 cflags,",
        "			       unsigned long *irq_flags)",
        "{",
        "	struct worker_pool *pool;",
        "	struct pool_workqueue *pwq;",
        "",
        "	local_irq_save(*irq_flags);",
        "",
        "	/* try to steal the timer if it exists */",
        "	if (cflags & WORK_CANCEL_DELAYED) {",
        "		struct delayed_work *dwork = to_delayed_work(work);",
        "",
        "		/*",
        "		 * dwork->timer is irqsafe.  If del_timer() fails, it's",
        "		 * guaranteed that the timer is not queued anywhere and not",
        "		 * running on the local CPU.",
        "		 */",
        "		if (likely(del_timer(&dwork->timer)))",
        "			return 1;",
        "	}",
        "",
        "	/* try to claim PENDING the normal way */",
        "	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))",
        "		return 0;",
        "",
        "	rcu_read_lock();",
        "	/*",
        "	 * The queueing is in progress, or it is already queued. Try to",
        "	 * steal it from ->worklist without clearing WORK_STRUCT_PENDING.",
        "	 */",
        "	pool = get_work_pool(work);",
        "	if (!pool)",
        "		goto fail;",
        "",
        "	raw_spin_lock(&pool->lock);",
        "	/*",
        "	 * work->data is guaranteed to point to pwq only while the work",
        "	 * item is queued on pwq->wq, and both updating work->data to point",
        "	 * to pwq on queueing and to pool on dequeueing are done under",
        "	 * pwq->pool->lock.  This in turn guarantees that, if work->data",
        "	 * points to pwq which is associated with a locked pool, the work",
        "	 * item is currently queued on that pool.",
        "	 */",
        "	pwq = get_work_pwq(work);",
        "	if (pwq && pwq->pool == pool) {",
        "		unsigned long work_data = *work_data_bits(work);",
        "",
        "		debug_work_deactivate(work);",
        "",
        "		/*",
        "		 * A cancelable inactive work item must be in the",
        "		 * pwq->inactive_works since a queued barrier can't be",
        "		 * canceled (see the comments in insert_wq_barrier()).",
        "		 *",
        "		 * An inactive work item cannot be deleted directly because",
        "		 * it might have linked barrier work items which, if left",
        "		 * on the inactive_works list, will confuse pwq->nr_active",
        "		 * management later on and cause stall.  Move the linked",
        "		 * barrier work items to the worklist when deleting the grabbed",
        "		 * item. Also keep WORK_STRUCT_INACTIVE in work_data, so that",
        "		 * it doesn't participate in nr_active management in later",
        "		 * pwq_dec_nr_in_flight().",
        "		 */",
        "		if (work_data & WORK_STRUCT_INACTIVE)",
        "			move_linked_works(work, &pwq->pool->worklist, NULL);",
        "",
        "		list_del_init(&work->entry);",
        "",
        "		/*",
        "		 * work->data points to pwq iff queued. Let's point to pool. As",
        "		 * this destroys work->data needed by the next step, stash it.",
        "		 */",
        "		set_work_pool_and_keep_pending(work, pool->id,",
        "					       pool_offq_flags(pool));",
        "",
        "		/* must be the last step, see the function comment */",
        "		pwq_dec_nr_in_flight(pwq, work_data);",
        "",
        "		raw_spin_unlock(&pool->lock);",
        "		rcu_read_unlock();",
        "		return 1;",
        "	}",
        "	raw_spin_unlock(&pool->lock);",
        "fail:",
        "	rcu_read_unlock();",
        "	local_irq_restore(*irq_flags);",
        "	return -EAGAIN;",
        "}",
        "",
        "/**",
        " * work_grab_pending - steal work item from worklist and disable irq",
        " * @work: work item to steal",
        " * @cflags: %WORK_CANCEL_ flags",
        " * @irq_flags: place to store IRQ state",
        " *",
        " * Grab PENDING bit of @work. @work can be in any stable state - idle, on timer",
        " * or on worklist.",
        " *",
        " * Can be called from any context. IRQ is disabled on return with IRQ state",
        " * stored in *@irq_flags. The caller is responsible for re-enabling it using",
        " * local_irq_restore().",
        " *",
        " * Returns %true if @work was pending. %false if idle.",
        " */",
        "static bool work_grab_pending(struct work_struct *work, u32 cflags,",
        "			      unsigned long *irq_flags)",
        "{",
        "	int ret;",
        "",
        "	while (true) {",
        "		ret = try_to_grab_pending(work, cflags, irq_flags);",
        "		if (ret >= 0)",
        "			return ret;",
        "		cpu_relax();",
        "	}",
        "}",
        "",
        "/**",
        " * insert_work - insert a work into a pool",
        " * @pwq: pwq @work belongs to",
        " * @work: work to insert",
        " * @head: insertion point",
        " * @extra_flags: extra WORK_STRUCT_* flags to set",
        " *",
        " * Insert @work which belongs to @pwq after @head.  @extra_flags is or'd to",
        " * work_struct flags.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void insert_work(struct pool_workqueue *pwq, struct work_struct *work,",
        "			struct list_head *head, unsigned int extra_flags)",
        "{",
        "	debug_work_activate(work);",
        "",
        "	/* record the work call stack in order to print it in KASAN reports */",
        "	kasan_record_aux_stack_noalloc(work);",
        "",
        "	/* we own @work, set data and link */",
        "	set_work_pwq(work, pwq, extra_flags);",
        "	list_add_tail(&work->entry, head);",
        "	get_pwq(pwq);",
        "}",
        "",
        "/*",
        " * Test whether @work is being queued from another work executing on the",
        " * same workqueue.",
        " */",
        "static bool is_chained_work(struct workqueue_struct *wq)",
        "{",
        "	struct worker *worker;",
        "",
        "	worker = current_wq_worker();",
        "	/*",
        "	 * Return %true iff I'm a worker executing a work item on @wq.  If",
        "	 * I'm @worker, it's safe to dereference it without locking.",
        "	 */",
        "	return worker && worker->current_pwq->wq == wq;",
        "}",
        "",
        "/*",
        " * When queueing an unbound work item to a wq, prefer local CPU if allowed",
        " * by wq_unbound_cpumask.  Otherwise, round robin among the allowed ones to",
        " * avoid perturbing sensitive tasks.",
        " */",
        "static int wq_select_unbound_cpu(int cpu)",
        "{",
        "	int new_cpu;",
        "",
        "	if (likely(!wq_debug_force_rr_cpu)) {",
        "		if (cpumask_test_cpu(cpu, wq_unbound_cpumask))",
        "			return cpu;",
        "	} else {",
        "		pr_warn_once(\"workqueue: round-robin CPU selection forced, expect performance impact\\n\");",
        "	}",
        "",
        "	new_cpu = __this_cpu_read(wq_rr_cpu_last);",
        "	new_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);",
        "	if (unlikely(new_cpu >= nr_cpu_ids)) {",
        "		new_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);",
        "		if (unlikely(new_cpu >= nr_cpu_ids))",
        "			return cpu;",
        "	}",
        "	__this_cpu_write(wq_rr_cpu_last, new_cpu);",
        "",
        "	return new_cpu;",
        "}",
        "",
        "static void __queue_work(int cpu, struct workqueue_struct *wq,",
        "			 struct work_struct *work)",
        "{",
        "	struct pool_workqueue *pwq;",
        "	struct worker_pool *last_pool, *pool;",
        "	unsigned int work_flags;",
        "	unsigned int req_cpu = cpu;",
        "",
        "	/*",
        "	 * While a work item is PENDING && off queue, a task trying to",
        "	 * steal the PENDING will busy-loop waiting for it to either get",
        "	 * queued or lose PENDING.  Grabbing PENDING and queueing should",
        "	 * happen with IRQ disabled.",
        "	 */",
        "	lockdep_assert_irqs_disabled();",
        "",
        "	/*",
        "	 * For a draining wq, only works from the same workqueue are",
        "	 * allowed. The __WQ_DESTROYING helps to spot the issue that",
        "	 * queues a new work item to a wq after destroy_workqueue(wq).",
        "	 */",
        "	if (unlikely(wq->flags & (__WQ_DESTROYING | __WQ_DRAINING) &&",
        "		     WARN_ON_ONCE(!is_chained_work(wq))))",
        "		return;",
        "	rcu_read_lock();",
        "retry:",
        "	/* pwq which will be used unless @work is executing elsewhere */",
        "	if (req_cpu == WORK_CPU_UNBOUND) {",
        "		if (wq->flags & WQ_UNBOUND)",
        "			cpu = wq_select_unbound_cpu(raw_smp_processor_id());",
        "		else",
        "			cpu = raw_smp_processor_id();",
        "	}",
        "",
        "	pwq = rcu_dereference(*per_cpu_ptr(wq->cpu_pwq, cpu));",
        "	pool = pwq->pool;",
        "",
        "	/*",
        "	 * If @work was previously on a different pool, it might still be",
        "	 * running there, in which case the work needs to be queued on that",
        "	 * pool to guarantee non-reentrancy.",
        "	 *",
        "	 * For ordered workqueue, work items must be queued on the newest pwq",
        "	 * for accurate order management.  Guaranteed order also guarantees",
        "	 * non-reentrancy.  See the comments above unplug_oldest_pwq().",
        "	 */",
        "	last_pool = get_work_pool(work);",
        "	if (last_pool && last_pool != pool && !(wq->flags & __WQ_ORDERED)) {",
        "		struct worker *worker;",
        "",
        "		raw_spin_lock(&last_pool->lock);",
        "",
        "		worker = find_worker_executing_work(last_pool, work);",
        "",
        "		if (worker && worker->current_pwq->wq == wq) {",
        "			pwq = worker->current_pwq;",
        "			pool = pwq->pool;",
        "			WARN_ON_ONCE(pool != last_pool);",
        "		} else {",
        "			/* meh... not running there, queue here */",
        "			raw_spin_unlock(&last_pool->lock);",
        "			raw_spin_lock(&pool->lock);",
        "		}",
        "	} else {",
        "		raw_spin_lock(&pool->lock);",
        "	}",
        "",
        "	/*",
        "	 * pwq is determined and locked. For unbound pools, we could have raced",
        "	 * with pwq release and it could already be dead. If its refcnt is zero,",
        "	 * repeat pwq selection. Note that unbound pwqs never die without",
        "	 * another pwq replacing it in cpu_pwq or while work items are executing",
        "	 * on it, so the retrying is guaranteed to make forward-progress.",
        "	 */",
        "	if (unlikely(!pwq->refcnt)) {",
        "		if (wq->flags & WQ_UNBOUND) {",
        "			raw_spin_unlock(&pool->lock);",
        "			cpu_relax();",
        "			goto retry;",
        "		}",
        "		/* oops */",
        "		WARN_ONCE(true, \"workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt\",",
        "			  wq->name, cpu);",
        "	}",
        "",
        "	/* pwq determined, queue */",
        "	trace_workqueue_queue_work(req_cpu, pwq, work);",
        "",
        "	if (WARN_ON(!list_empty(&work->entry)))",
        "		goto out;",
        "",
        "	pwq->nr_in_flight[pwq->work_color]++;",
        "	work_flags = work_color_to_flags(pwq->work_color);",
        "",
        "	/*",
        "	 * Limit the number of concurrently active work items to max_active.",
        "	 * @work must also queue behind existing inactive work items to maintain",
        "	 * ordering when max_active changes. See wq_adjust_max_active().",
        "	 */",
        "	if (list_empty(&pwq->inactive_works) && pwq_tryinc_nr_active(pwq, false)) {",
        "		if (list_empty(&pool->worklist))",
        "			pool->watchdog_ts = jiffies;",
        "",
        "		trace_workqueue_activate_work(work);",
        "		insert_work(pwq, work, &pool->worklist, work_flags);",
        "		kick_pool(pool);",
        "	} else {",
        "		work_flags |= WORK_STRUCT_INACTIVE;",
        "		insert_work(pwq, work, &pwq->inactive_works, work_flags);",
        "	}",
        "",
        "out:",
        "	raw_spin_unlock(&pool->lock);",
        "	rcu_read_unlock();",
        "}",
        "",
        "static bool clear_pending_if_disabled(struct work_struct *work)",
        "{",
        "	unsigned long data = *work_data_bits(work);",
        "	struct work_offq_data offqd;",
        "",
        "	if (likely((data & WORK_STRUCT_PWQ) ||",
        "		   !(data & WORK_OFFQ_DISABLE_MASK)))",
        "		return false;",
        "",
        "	work_offqd_unpack(&offqd, data);",
        "	set_work_pool_and_clear_pending(work, offqd.pool_id,",
        "					work_offqd_pack_flags(&offqd));",
        "	return true;",
        "}",
        "",
        "/**",
        " * queue_work_on - queue work on specific cpu",
        " * @cpu: CPU number to execute work on",
        " * @wq: workqueue to use",
        " * @work: work to queue",
        " *",
        " * We queue the work to a specific CPU, the caller must ensure it",
        " * can't go away.  Callers that fail to ensure that the specified",
        " * CPU cannot go away will execute on a randomly chosen CPU.",
        " * But note well that callers specifying a CPU that never has been",
        " * online will get a splat.",
        " *",
        " * Return: %false if @work was already on a queue, %true otherwise.",
        " */",
        "bool queue_work_on(int cpu, struct workqueue_struct *wq,",
        "		   struct work_struct *work)",
        "{",
        "	bool ret = false;",
        "	unsigned long irq_flags;",
        "",
        "	local_irq_save(irq_flags);",
        "",
        "	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&",
        "	    !clear_pending_if_disabled(work)) {",
        "		__queue_work(cpu, wq, work);",
        "		ret = true;",
        "	}",
        "",
        "	local_irq_restore(irq_flags);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(queue_work_on);",
        "",
        "/**",
        " * select_numa_node_cpu - Select a CPU based on NUMA node",
        " * @node: NUMA node ID that we want to select a CPU from",
        " *",
        " * This function will attempt to find a \"random\" cpu available on a given",
        " * node. If there are no CPUs available on the given node it will return",
        " * WORK_CPU_UNBOUND indicating that we should just schedule to any",
        " * available CPU if we need to schedule this work.",
        " */",
        "static int select_numa_node_cpu(int node)",
        "{",
        "	int cpu;",
        "",
        "	/* Delay binding to CPU if node is not valid or online */",
        "	if (node < 0 || node >= MAX_NUMNODES || !node_online(node))",
        "		return WORK_CPU_UNBOUND;",
        "",
        "	/* Use local node/cpu if we are already there */",
        "	cpu = raw_smp_processor_id();",
        "	if (node == cpu_to_node(cpu))",
        "		return cpu;",
        "",
        "	/* Use \"random\" otherwise know as \"first\" online CPU of node */",
        "	cpu = cpumask_any_and(cpumask_of_node(node), cpu_online_mask);",
        "",
        "	/* If CPU is valid return that, otherwise just defer */",
        "	return cpu < nr_cpu_ids ? cpu : WORK_CPU_UNBOUND;",
        "}",
        "",
        "/**",
        " * queue_work_node - queue work on a \"random\" cpu for a given NUMA node",
        " * @node: NUMA node that we are targeting the work for",
        " * @wq: workqueue to use",
        " * @work: work to queue",
        " *",
        " * We queue the work to a \"random\" CPU within a given NUMA node. The basic",
        " * idea here is to provide a way to somehow associate work with a given",
        " * NUMA node.",
        " *",
        " * This function will only make a best effort attempt at getting this onto",
        " * the right NUMA node. If no node is requested or the requested node is",
        " * offline then we just fall back to standard queue_work behavior.",
        " *",
        " * Currently the \"random\" CPU ends up being the first available CPU in the",
        " * intersection of cpu_online_mask and the cpumask of the node, unless we",
        " * are running on the node. In that case we just use the current CPU.",
        " *",
        " * Return: %false if @work was already on a queue, %true otherwise.",
        " */",
        "bool queue_work_node(int node, struct workqueue_struct *wq,",
        "		     struct work_struct *work)",
        "{",
        "	unsigned long irq_flags;",
        "	bool ret = false;",
        "",
        "	/*",
        "	 * This current implementation is specific to unbound workqueues.",
        "	 * Specifically we only return the first available CPU for a given",
        "	 * node instead of cycling through individual CPUs within the node.",
        "	 *",
        "	 * If this is used with a per-cpu workqueue then the logic in",
        "	 * workqueue_select_cpu_near would need to be updated to allow for",
        "	 * some round robin type logic.",
        "	 */",
        "	WARN_ON_ONCE(!(wq->flags & WQ_UNBOUND));",
        "",
        "	local_irq_save(irq_flags);",
        "",
        "	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&",
        "	    !clear_pending_if_disabled(work)) {",
        "		int cpu = select_numa_node_cpu(node);",
        "",
        "		__queue_work(cpu, wq, work);",
        "		ret = true;",
        "	}",
        "",
        "	local_irq_restore(irq_flags);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(queue_work_node);",
        "",
        "void delayed_work_timer_fn(struct timer_list *t)",
        "{",
        "	struct delayed_work *dwork = from_timer(dwork, t, timer);",
        "",
        "	/* should have been called from irqsafe timer with irq already off */",
        "	__queue_work(dwork->cpu, dwork->wq, &dwork->work);",
        "}",
        "EXPORT_SYMBOL(delayed_work_timer_fn);",
        "",
        "static void __queue_delayed_work(int cpu, struct workqueue_struct *wq,",
        "				struct delayed_work *dwork, unsigned long delay)",
        "{",
        "	struct timer_list *timer = &dwork->timer;",
        "	struct work_struct *work = &dwork->work;",
        "",
        "	WARN_ON_ONCE(!wq);",
        "	WARN_ON_ONCE(timer->function != delayed_work_timer_fn);",
        "	WARN_ON_ONCE(timer_pending(timer));",
        "	WARN_ON_ONCE(!list_empty(&work->entry));",
        "",
        "	/*",
        "	 * If @delay is 0, queue @dwork->work immediately.  This is for",
        "	 * both optimization and correctness.  The earliest @timer can",
        "	 * expire is on the closest next tick and delayed_work users depend",
        "	 * on that there's no such delay when @delay is 0.",
        "	 */",
        "	if (!delay) {",
        "		__queue_work(cpu, wq, &dwork->work);",
        "		return;",
        "	}",
        "",
        "	WARN_ON_ONCE(cpu != WORK_CPU_UNBOUND && !cpu_online(cpu));",
        "	dwork->wq = wq;",
        "	dwork->cpu = cpu;",
        "	timer->expires = jiffies + delay;",
        "",
        "	if (housekeeping_enabled(HK_TYPE_TIMER)) {",
        "		/* If the current cpu is a housekeeping cpu, use it. */",
        "		cpu = smp_processor_id();",
        "		if (!housekeeping_test_cpu(cpu, HK_TYPE_TIMER))",
        "			cpu = housekeeping_any_cpu(HK_TYPE_TIMER);",
        "		add_timer_on(timer, cpu);",
        "	} else {",
        "		if (likely(cpu == WORK_CPU_UNBOUND))",
        "			add_timer_global(timer);",
        "		else",
        "			add_timer_on(timer, cpu);",
        "	}",
        "}",
        "",
        "/**",
        " * queue_delayed_work_on - queue work on specific CPU after delay",
        " * @cpu: CPU number to execute work on",
        " * @wq: workqueue to use",
        " * @dwork: work to queue",
        " * @delay: number of jiffies to wait before queueing",
        " *",
        " * We queue the delayed_work to a specific CPU, for non-zero delays the",
        " * caller must ensure it is online and can't go away. Callers that fail",
        " * to ensure this, may get @dwork->timer queued to an offlined CPU and",
        " * this will prevent queueing of @dwork->work unless the offlined CPU",
        " * becomes online again.",
        " *",
        " * Return: %false if @work was already on a queue, %true otherwise.  If",
        " * @delay is zero and @dwork is idle, it will be scheduled for immediate",
        " * execution.",
        " */",
        "bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,",
        "			   struct delayed_work *dwork, unsigned long delay)",
        "{",
        "	struct work_struct *work = &dwork->work;",
        "	bool ret = false;",
        "	unsigned long irq_flags;",
        "",
        "	/* read the comment in __queue_work() */",
        "	local_irq_save(irq_flags);",
        "",
        "	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&",
        "	    !clear_pending_if_disabled(work)) {",
        "		__queue_delayed_work(cpu, wq, dwork, delay);",
        "		ret = true;",
        "	}",
        "",
        "	local_irq_restore(irq_flags);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(queue_delayed_work_on);",
        "",
        "/**",
        " * mod_delayed_work_on - modify delay of or queue a delayed work on specific CPU",
        " * @cpu: CPU number to execute work on",
        " * @wq: workqueue to use",
        " * @dwork: work to queue",
        " * @delay: number of jiffies to wait before queueing",
        " *",
        " * If @dwork is idle, equivalent to queue_delayed_work_on(); otherwise,",
        " * modify @dwork's timer so that it expires after @delay.  If @delay is",
        " * zero, @work is guaranteed to be scheduled immediately regardless of its",
        " * current state.",
        " *",
        " * Return: %false if @dwork was idle and queued, %true if @dwork was",
        " * pending and its timer was modified.",
        " *",
        " * This function is safe to call from any context including IRQ handler.",
        " * See try_to_grab_pending() for details.",
        " */",
        "bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,",
        "			 struct delayed_work *dwork, unsigned long delay)",
        "{",
        "	unsigned long irq_flags;",
        "	bool ret;",
        "",
        "	ret = work_grab_pending(&dwork->work, WORK_CANCEL_DELAYED, &irq_flags);",
        "",
        "	if (!clear_pending_if_disabled(&dwork->work))",
        "		__queue_delayed_work(cpu, wq, dwork, delay);",
        "",
        "	local_irq_restore(irq_flags);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(mod_delayed_work_on);",
        "",
        "static void rcu_work_rcufn(struct rcu_head *rcu)",
        "{",
        "	struct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);",
        "",
        "	/* read the comment in __queue_work() */",
        "	local_irq_disable();",
        "	__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);",
        "	local_irq_enable();",
        "}",
        "",
        "/**",
        " * queue_rcu_work - queue work after a RCU grace period",
        " * @wq: workqueue to use",
        " * @rwork: work to queue",
        " *",
        " * Return: %false if @rwork was already pending, %true otherwise.  Note",
        " * that a full RCU grace period is guaranteed only after a %true return.",
        " * While @rwork is guaranteed to be executed after a %false return, the",
        " * execution may happen before a full RCU grace period has passed.",
        " */",
        "bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)",
        "{",
        "	struct work_struct *work = &rwork->work;",
        "",
        "	/*",
        "	 * rcu_work can't be canceled or disabled. Warn if the user reached",
        "	 * inside @rwork and disabled the inner work.",
        "	 */",
        "	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&",
        "	    !WARN_ON_ONCE(clear_pending_if_disabled(work))) {",
        "		rwork->wq = wq;",
        "		call_rcu_hurry(&rwork->rcu, rcu_work_rcufn);",
        "		return true;",
        "	}",
        "",
        "	return false;",
        "}",
        "EXPORT_SYMBOL(queue_rcu_work);",
        "",
        "static struct worker *alloc_worker(int node)",
        "{",
        "	struct worker *worker;",
        "",
        "	worker = kzalloc_node(sizeof(*worker), GFP_KERNEL, node);",
        "	if (worker) {",
        "		INIT_LIST_HEAD(&worker->entry);",
        "		INIT_LIST_HEAD(&worker->scheduled);",
        "		INIT_LIST_HEAD(&worker->node);",
        "		/* on creation a worker is in !idle && prep state */",
        "		worker->flags = WORKER_PREP;",
        "	}",
        "	return worker;",
        "}",
        "",
        "static cpumask_t *pool_allowed_cpus(struct worker_pool *pool)",
        "{",
        "	if (pool->cpu < 0 && pool->attrs->affn_strict)",
        "		return pool->attrs->__pod_cpumask;",
        "	else",
        "		return pool->attrs->cpumask;",
        "}",
        "",
        "/**",
        " * worker_attach_to_pool() - attach a worker to a pool",
        " * @worker: worker to be attached",
        " * @pool: the target pool",
        " *",
        " * Attach @worker to @pool.  Once attached, the %WORKER_UNBOUND flag and",
        " * cpu-binding of @worker are kept coordinated with the pool across",
        " * cpu-[un]hotplugs.",
        " */",
        "static void worker_attach_to_pool(struct worker *worker,",
        "				  struct worker_pool *pool)",
        "{",
        "	mutex_lock(&wq_pool_attach_mutex);",
        "",
        "	/*",
        "	 * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains stable",
        "	 * across this function. See the comments above the flag definition for",
        "	 * details. BH workers are, while per-CPU, always DISASSOCIATED.",
        "	 */",
        "	if (pool->flags & POOL_DISASSOCIATED) {",
        "		worker->flags |= WORKER_UNBOUND;",
        "	} else {",
        "		WARN_ON_ONCE(pool->flags & POOL_BH);",
        "		kthread_set_per_cpu(worker->task, pool->cpu);",
        "	}",
        "",
        "	if (worker->rescue_wq)",
        "		set_cpus_allowed_ptr(worker->task, pool_allowed_cpus(pool));",
        "",
        "	list_add_tail(&worker->node, &pool->workers);",
        "	worker->pool = pool;",
        "",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "}",
        "",
        "static void unbind_worker(struct worker *worker)",
        "{",
        "	lockdep_assert_held(&wq_pool_attach_mutex);",
        "",
        "	kthread_set_per_cpu(worker->task, -1);",
        "	if (cpumask_intersects(wq_unbound_cpumask, cpu_active_mask))",
        "		WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, wq_unbound_cpumask) < 0);",
        "	else",
        "		WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_possible_mask) < 0);",
        "}",
        "",
        "",
        "static void detach_worker(struct worker *worker)",
        "{",
        "	lockdep_assert_held(&wq_pool_attach_mutex);",
        "",
        "	unbind_worker(worker);",
        "	list_del(&worker->node);",
        "}",
        "",
        "/**",
        " * worker_detach_from_pool() - detach a worker from its pool",
        " * @worker: worker which is attached to its pool",
        " *",
        " * Undo the attaching which had been done in worker_attach_to_pool().  The",
        " * caller worker shouldn't access to the pool after detached except it has",
        " * other reference to the pool.",
        " */",
        "static void worker_detach_from_pool(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	/* there is one permanent BH worker per CPU which should never detach */",
        "	WARN_ON_ONCE(pool->flags & POOL_BH);",
        "",
        "	mutex_lock(&wq_pool_attach_mutex);",
        "	detach_worker(worker);",
        "	worker->pool = NULL;",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "",
        "	/* clear leftover flags without pool->lock after it is detached */",
        "	worker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);",
        "}",
        "",
        "static int format_worker_id(char *buf, size_t size, struct worker *worker,",
        "			    struct worker_pool *pool)",
        "{",
        "	if (worker->rescue_wq)",
        "		return scnprintf(buf, size, \"kworker/R-%s\",",
        "				 worker->rescue_wq->name);",
        "",
        "	if (pool) {",
        "		if (pool->cpu >= 0)",
        "			return scnprintf(buf, size, \"kworker/%d:%d%s\",",
        "					 pool->cpu, worker->id,",
        "					 pool->attrs->nice < 0  ? \"H\" : \"\");",
        "		else",
        "			return scnprintf(buf, size, \"kworker/u%d:%d\",",
        "					 pool->id, worker->id);",
        "	} else {",
        "		return scnprintf(buf, size, \"kworker/dying\");",
        "	}",
        "}",
        "",
        "/**",
        " * create_worker - create a new workqueue worker",
        " * @pool: pool the new worker will belong to",
        " *",
        " * Create and start a new worker which is attached to @pool.",
        " *",
        " * CONTEXT:",
        " * Might sleep.  Does GFP_KERNEL allocations.",
        " *",
        " * Return:",
        " * Pointer to the newly created worker.",
        " */",
        "static struct worker *create_worker(struct worker_pool *pool)",
        "{",
        "	struct worker *worker;",
        "	int id;",
        "",
        "	/* ID is needed to determine kthread name */",
        "	id = ida_alloc(&pool->worker_ida, GFP_KERNEL);",
        "	if (id < 0) {",
        "		pr_err_once(\"workqueue: Failed to allocate a worker ID: %pe\\n\",",
        "			    ERR_PTR(id));",
        "		return NULL;",
        "	}",
        "",
        "	worker = alloc_worker(pool->node);",
        "	if (!worker) {",
        "		pr_err_once(\"workqueue: Failed to allocate a worker\\n\");",
        "		goto fail;",
        "	}",
        "",
        "	worker->id = id;",
        "",
        "	if (!(pool->flags & POOL_BH)) {",
        "		char id_buf[WORKER_ID_LEN];",
        "",
        "		format_worker_id(id_buf, sizeof(id_buf), worker, pool);",
        "		worker->task = kthread_create_on_node(worker_thread, worker,",
        "						      pool->node, \"%s\", id_buf);",
        "		if (IS_ERR(worker->task)) {",
        "			if (PTR_ERR(worker->task) == -EINTR) {",
        "				pr_err(\"workqueue: Interrupted when creating a worker thread \\\"%s\\\"\\n\",",
        "				       id_buf);",
        "			} else {",
        "				pr_err_once(\"workqueue: Failed to create a worker thread: %pe\",",
        "					    worker->task);",
        "			}",
        "			goto fail;",
        "		}",
        "",
        "		set_user_nice(worker->task, pool->attrs->nice);",
        "		kthread_bind_mask(worker->task, pool_allowed_cpus(pool));",
        "	}",
        "",
        "	/* successful, attach the worker to the pool */",
        "	worker_attach_to_pool(worker, pool);",
        "",
        "	/* start the newly created worker */",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	worker->pool->nr_workers++;",
        "	worker_enter_idle(worker);",
        "",
        "	/*",
        "	 * @worker is waiting on a completion in kthread() and will trigger hung",
        "	 * check if not woken up soon. As kick_pool() is noop if @pool is empty,",
        "	 * wake it up explicitly.",
        "	 */",
        "	if (worker->task)",
        "		wake_up_process(worker->task);",
        "",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	return worker;",
        "",
        "fail:",
        "	ida_free(&pool->worker_ida, id);",
        "	kfree(worker);",
        "	return NULL;",
        "}",
        "",
        "static void detach_dying_workers(struct list_head *cull_list)",
        "{",
        "	struct worker *worker;",
        "",
        "	list_for_each_entry(worker, cull_list, entry)",
        "		detach_worker(worker);",
        "}",
        "",
        "static void reap_dying_workers(struct list_head *cull_list)",
        "{",
        "	struct worker *worker, *tmp;",
        "",
        "	list_for_each_entry_safe(worker, tmp, cull_list, entry) {",
        "		list_del_init(&worker->entry);",
        "		kthread_stop_put(worker->task);",
        "		kfree(worker);",
        "	}",
        "}",
        "",
        "/**",
        " * set_worker_dying - Tag a worker for destruction",
        " * @worker: worker to be destroyed",
        " * @list: transfer worker away from its pool->idle_list and into list",
        " *",
        " * Tag @worker for destruction and adjust @pool stats accordingly.  The worker",
        " * should be idle.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void set_worker_dying(struct worker *worker, struct list_head *list)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	lockdep_assert_held(&pool->lock);",
        "	lockdep_assert_held(&wq_pool_attach_mutex);",
        "",
        "	/* sanity check frenzy */",
        "	if (WARN_ON(worker->current_work) ||",
        "	    WARN_ON(!list_empty(&worker->scheduled)) ||",
        "	    WARN_ON(!(worker->flags & WORKER_IDLE)))",
        "		return;",
        "",
        "	pool->nr_workers--;",
        "	pool->nr_idle--;",
        "",
        "	worker->flags |= WORKER_DIE;",
        "",
        "	list_move(&worker->entry, list);",
        "",
        "	/* get an extra task struct reference for later kthread_stop_put() */",
        "	get_task_struct(worker->task);",
        "}",
        "",
        "/**",
        " * idle_worker_timeout - check if some idle workers can now be deleted.",
        " * @t: The pool's idle_timer that just expired",
        " *",
        " * The timer is armed in worker_enter_idle(). Note that it isn't disarmed in",
        " * worker_leave_idle(), as a worker flicking between idle and active while its",
        " * pool is at the too_many_workers() tipping point would cause too much timer",
        " * housekeeping overhead. Since IDLE_WORKER_TIMEOUT is long enough, we just let",
        " * it expire and re-evaluate things from there.",
        " */",
        "static void idle_worker_timeout(struct timer_list *t)",
        "{",
        "	struct worker_pool *pool = from_timer(pool, t, idle_timer);",
        "	bool do_cull = false;",
        "",
        "	if (work_pending(&pool->idle_cull_work))",
        "		return;",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	if (too_many_workers(pool)) {",
        "		struct worker *worker;",
        "		unsigned long expires;",
        "",
        "		/* idle_list is kept in LIFO order, check the last one */",
        "		worker = list_last_entry(&pool->idle_list, struct worker, entry);",
        "		expires = worker->last_active + IDLE_WORKER_TIMEOUT;",
        "		do_cull = !time_before(jiffies, expires);",
        "",
        "		if (!do_cull)",
        "			mod_timer(&pool->idle_timer, expires);",
        "	}",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	if (do_cull)",
        "		queue_work(system_unbound_wq, &pool->idle_cull_work);",
        "}",
        "",
        "/**",
        " * idle_cull_fn - cull workers that have been idle for too long.",
        " * @work: the pool's work for handling these idle workers",
        " *",
        " * This goes through a pool's idle workers and gets rid of those that have been",
        " * idle for at least IDLE_WORKER_TIMEOUT seconds.",
        " *",
        " * We don't want to disturb isolated CPUs because of a pcpu kworker being",
        " * culled, so this also resets worker affinity. This requires a sleepable",
        " * context, hence the split between timer callback and work item.",
        " */",
        "static void idle_cull_fn(struct work_struct *work)",
        "{",
        "	struct worker_pool *pool = container_of(work, struct worker_pool, idle_cull_work);",
        "	LIST_HEAD(cull_list);",
        "",
        "	/*",
        "	 * Grabbing wq_pool_attach_mutex here ensures an already-running worker",
        "	 * cannot proceed beyong set_pf_worker() in its self-destruct path.",
        "	 * This is required as a previously-preempted worker could run after",
        "	 * set_worker_dying() has happened but before detach_dying_workers() did.",
        "	 */",
        "	mutex_lock(&wq_pool_attach_mutex);",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	while (too_many_workers(pool)) {",
        "		struct worker *worker;",
        "		unsigned long expires;",
        "",
        "		worker = list_last_entry(&pool->idle_list, struct worker, entry);",
        "		expires = worker->last_active + IDLE_WORKER_TIMEOUT;",
        "",
        "		if (time_before(jiffies, expires)) {",
        "			mod_timer(&pool->idle_timer, expires);",
        "			break;",
        "		}",
        "",
        "		set_worker_dying(worker, &cull_list);",
        "	}",
        "",
        "	raw_spin_unlock_irq(&pool->lock);",
        "	detach_dying_workers(&cull_list);",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "",
        "	reap_dying_workers(&cull_list);",
        "}",
        "",
        "static void send_mayday(struct work_struct *work)",
        "{",
        "	struct pool_workqueue *pwq = get_work_pwq(work);",
        "	struct workqueue_struct *wq = pwq->wq;",
        "",
        "	lockdep_assert_held(&wq_mayday_lock);",
        "",
        "	if (!wq->rescuer)",
        "		return;",
        "",
        "	/* mayday mayday mayday */",
        "	if (list_empty(&pwq->mayday_node)) {",
        "		/*",
        "		 * If @pwq is for an unbound wq, its base ref may be put at",
        "		 * any time due to an attribute change.  Pin @pwq until the",
        "		 * rescuer is done with it.",
        "		 */",
        "		get_pwq(pwq);",
        "		list_add_tail(&pwq->mayday_node, &wq->maydays);",
        "		wake_up_process(wq->rescuer->task);",
        "		pwq->stats[PWQ_STAT_MAYDAY]++;",
        "	}",
        "}",
        "",
        "static void pool_mayday_timeout(struct timer_list *t)",
        "{",
        "	struct worker_pool *pool = from_timer(pool, t, mayday_timer);",
        "	struct work_struct *work;",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "	raw_spin_lock(&wq_mayday_lock);		/* for wq->maydays */",
        "",
        "	if (need_to_create_worker(pool)) {",
        "		/*",
        "		 * We've been trying to create a new worker but",
        "		 * haven't been successful.  We might be hitting an",
        "		 * allocation deadlock.  Send distress signals to",
        "		 * rescuers.",
        "		 */",
        "		list_for_each_entry(work, &pool->worklist, entry)",
        "			send_mayday(work);",
        "	}",
        "",
        "	raw_spin_unlock(&wq_mayday_lock);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);",
        "}",
        "",
        "/**",
        " * maybe_create_worker - create a new worker if necessary",
        " * @pool: pool to create a new worker for",
        " *",
        " * Create a new worker for @pool if necessary.  @pool is guaranteed to",
        " * have at least one idle worker on return from this function.  If",
        " * creating a new worker takes longer than MAYDAY_INTERVAL, mayday is",
        " * sent to all rescuers with works scheduled on @pool to resolve",
        " * possible allocation deadlock.",
        " *",
        " * On return, need_to_create_worker() is guaranteed to be %false and",
        " * may_start_working() %true.",
        " *",
        " * LOCKING:",
        " * raw_spin_lock_irq(pool->lock) which may be released and regrabbed",
        " * multiple times.  Does GFP_KERNEL allocations.  Called only from",
        " * manager.",
        " */",
        "static void maybe_create_worker(struct worker_pool *pool)",
        "__releases(&pool->lock)",
        "__acquires(&pool->lock)",
        "{",
        "restart:",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	/* if we don't make progress in MAYDAY_INITIAL_TIMEOUT, call for help */",
        "	mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);",
        "",
        "	while (true) {",
        "		if (create_worker(pool) || !need_to_create_worker(pool))",
        "			break;",
        "",
        "		schedule_timeout_interruptible(CREATE_COOLDOWN);",
        "",
        "		if (!need_to_create_worker(pool))",
        "			break;",
        "	}",
        "",
        "	del_timer_sync(&pool->mayday_timer);",
        "	raw_spin_lock_irq(&pool->lock);",
        "	/*",
        "	 * This is necessary even after a new worker was just successfully",
        "	 * created as @pool->lock was dropped and the new worker might have",
        "	 * already become busy.",
        "	 */",
        "	if (need_to_create_worker(pool))",
        "		goto restart;",
        "}",
        "",
        "/**",
        " * manage_workers - manage worker pool",
        " * @worker: self",
        " *",
        " * Assume the manager role and manage the worker pool @worker belongs",
        " * to.  At any given time, there can be only zero or one manager per",
        " * pool.  The exclusion is handled automatically by this function.",
        " *",
        " * The caller can safely start processing works on false return.  On",
        " * true return, it's guaranteed that need_to_create_worker() is false",
        " * and may_start_working() is true.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock) which may be released and regrabbed",
        " * multiple times.  Does GFP_KERNEL allocations.",
        " *",
        " * Return:",
        " * %false if the pool doesn't need management and the caller can safely",
        " * start processing works, %true if management function was performed and",
        " * the conditions that the caller verified before calling the function may",
        " * no longer be true.",
        " */",
        "static bool manage_workers(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	if (pool->flags & POOL_MANAGER_ACTIVE)",
        "		return false;",
        "",
        "	pool->flags |= POOL_MANAGER_ACTIVE;",
        "	pool->manager = worker;",
        "",
        "	maybe_create_worker(pool);",
        "",
        "	pool->manager = NULL;",
        "	pool->flags &= ~POOL_MANAGER_ACTIVE;",
        "	rcuwait_wake_up(&manager_wait);",
        "	return true;",
        "}",
        "",
        "/**",
        " * process_one_work - process single work",
        " * @worker: self",
        " * @work: work to process",
        " *",
        " * Process @work.  This function contains all the logics necessary to",
        " * process a single work including synchronization against and",
        " * interaction with other workers on the same cpu, queueing and",
        " * flushing.  As long as context requirement is met, any worker can",
        " * call this function to process a work.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock) which is released and regrabbed.",
        " */",
        "static void process_one_work(struct worker *worker, struct work_struct *work)",
        "__releases(&pool->lock)",
        "__acquires(&pool->lock)",
        "{",
        "	struct pool_workqueue *pwq = get_work_pwq(work);",
        "	struct worker_pool *pool = worker->pool;",
        "	unsigned long work_data;",
        "	int lockdep_start_depth, rcu_start_depth;",
        "	bool bh_draining = pool->flags & POOL_BH_DRAINING;",
        "#ifdef CONFIG_LOCKDEP",
        "	/*",
        "	 * It is permissible to free the struct work_struct from",
        "	 * inside the function that is called from it, this we need to",
        "	 * take into account for lockdep too.  To avoid bogus \"held",
        "	 * lock freed\" warnings as well as problems when looking into",
        "	 * work->lockdep_map, make a copy and use that here.",
        "	 */",
        "	struct lockdep_map lockdep_map;",
        "",
        "	lockdep_copy_map(&lockdep_map, &work->lockdep_map);",
        "#endif",
        "	/* ensure we're on the correct CPU */",
        "	WARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&",
        "		     raw_smp_processor_id() != pool->cpu);",
        "",
        "	/* claim and dequeue */",
        "	debug_work_deactivate(work);",
        "	hash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);",
        "	worker->current_work = work;",
        "	worker->current_func = work->func;",
        "	worker->current_pwq = pwq;",
        "	if (worker->task)",
        "		worker->current_at = worker->task->se.sum_exec_runtime;",
        "	work_data = *work_data_bits(work);",
        "	worker->current_color = get_work_color(work_data);",
        "",
        "	/*",
        "	 * Record wq name for cmdline and debug reporting, may get",
        "	 * overridden through set_worker_desc().",
        "	 */",
        "	strscpy(worker->desc, pwq->wq->name, WORKER_DESC_LEN);",
        "",
        "	list_del_init(&work->entry);",
        "",
        "	/*",
        "	 * CPU intensive works don't participate in concurrency management.",
        "	 * They're the scheduler's responsibility.  This takes @worker out",
        "	 * of concurrency management and the next code block will chain",
        "	 * execution of the pending work items.",
        "	 */",
        "	if (unlikely(pwq->wq->flags & WQ_CPU_INTENSIVE))",
        "		worker_set_flags(worker, WORKER_CPU_INTENSIVE);",
        "",
        "	/*",
        "	 * Kick @pool if necessary. It's always noop for per-cpu worker pools",
        "	 * since nr_running would always be >= 1 at this point. This is used to",
        "	 * chain execution of the pending work items for WORKER_NOT_RUNNING",
        "	 * workers such as the UNBOUND and CPU_INTENSIVE ones.",
        "	 */",
        "	kick_pool(pool);",
        "",
        "	/*",
        "	 * Record the last pool and clear PENDING which should be the last",
        "	 * update to @work.  Also, do this inside @pool->lock so that",
        "	 * PENDING and queued state changes happen together while IRQ is",
        "	 * disabled.",
        "	 */",
        "	set_work_pool_and_clear_pending(work, pool->id, pool_offq_flags(pool));",
        "",
        "	pwq->stats[PWQ_STAT_STARTED]++;",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	rcu_start_depth = rcu_preempt_depth();",
        "	lockdep_start_depth = lockdep_depth(current);",
        "	/* see drain_dead_softirq_workfn() */",
        "	if (!bh_draining)",
        "		lock_map_acquire(pwq->wq->lockdep_map);",
        "	lock_map_acquire(&lockdep_map);",
        "	/*",
        "	 * Strictly speaking we should mark the invariant state without holding",
        "	 * any locks, that is, before these two lock_map_acquire()'s.",
        "	 *",
        "	 * However, that would result in:",
        "	 *",
        "	 *   A(W1)",
        "	 *   WFC(C)",
        "	 *		A(W1)",
        "	 *		C(C)",
        "	 *",
        "	 * Which would create W1->C->W1 dependencies, even though there is no",
        "	 * actual deadlock possible. There are two solutions, using a",
        "	 * read-recursive acquire on the work(queue) 'locks', but this will then",
        "	 * hit the lockdep limitation on recursive locks, or simply discard",
        "	 * these locks.",
        "	 *",
        "	 * AFAICT there is no possible deadlock scenario between the",
        "	 * flush_work() and complete() primitives (except for single-threaded",
        "	 * workqueues), so hiding them isn't a problem.",
        "	 */",
        "	lockdep_invariant_state(true);",
        "	trace_workqueue_execute_start(work);",
        "	worker->current_func(work);",
        "	/*",
        "	 * While we must be careful to not use \"work\" after this, the trace",
        "	 * point will only record its address.",
        "	 */",
        "	trace_workqueue_execute_end(work, worker->current_func);",
        "	pwq->stats[PWQ_STAT_COMPLETED]++;",
        "	lock_map_release(&lockdep_map);",
        "	if (!bh_draining)",
        "		lock_map_release(pwq->wq->lockdep_map);",
        "",
        "	if (unlikely((worker->task && in_atomic()) ||",
        "		     lockdep_depth(current) != lockdep_start_depth ||",
        "		     rcu_preempt_depth() != rcu_start_depth)) {",
        "		pr_err(\"BUG: workqueue leaked atomic, lock or RCU: %s[%d]\\n\"",
        "		       \"     preempt=0x%08x lock=%d->%d RCU=%d->%d workfn=%ps\\n\",",
        "		       current->comm, task_pid_nr(current), preempt_count(),",
        "		       lockdep_start_depth, lockdep_depth(current),",
        "		       rcu_start_depth, rcu_preempt_depth(),",
        "		       worker->current_func);",
        "		debug_show_held_locks(current);",
        "		dump_stack();",
        "	}",
        "",
        "	/*",
        "	 * The following prevents a kworker from hogging CPU on !PREEMPTION",
        "	 * kernels, where a requeueing work item waiting for something to",
        "	 * happen could deadlock with stop_machine as such work item could",
        "	 * indefinitely requeue itself while all other CPUs are trapped in",
        "	 * stop_machine. At the same time, report a quiescent RCU state so",
        "	 * the same condition doesn't freeze RCU.",
        "	 */",
        "	if (worker->task)",
        "		cond_resched();",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	/*",
        "	 * In addition to %WQ_CPU_INTENSIVE, @worker may also have been marked",
        "	 * CPU intensive by wq_worker_tick() if @work hogged CPU longer than",
        "	 * wq_cpu_intensive_thresh_us. Clear it.",
        "	 */",
        "	worker_clr_flags(worker, WORKER_CPU_INTENSIVE);",
        "",
        "	/* tag the worker for identification in schedule() */",
        "	worker->last_func = worker->current_func;",
        "",
        "	/* we're done with it, release */",
        "	hash_del(&worker->hentry);",
        "	worker->current_work = NULL;",
        "	worker->current_func = NULL;",
        "	worker->current_pwq = NULL;",
        "	worker->current_color = INT_MAX;",
        "",
        "	/* must be the last step, see the function comment */",
        "	pwq_dec_nr_in_flight(pwq, work_data);",
        "}",
        "",
        "/**",
        " * process_scheduled_works - process scheduled works",
        " * @worker: self",
        " *",
        " * Process all scheduled works.  Please note that the scheduled list",
        " * may change while processing a work, so this function repeatedly",
        " * fetches a work from the top and executes it.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock) which may be released and regrabbed",
        " * multiple times.",
        " */",
        "static void process_scheduled_works(struct worker *worker)",
        "{",
        "	struct work_struct *work;",
        "	bool first = true;",
        "",
        "	while ((work = list_first_entry_or_null(&worker->scheduled,",
        "						struct work_struct, entry))) {",
        "		if (first) {",
        "			worker->pool->watchdog_ts = jiffies;",
        "			first = false;",
        "		}",
        "		process_one_work(worker, work);",
        "	}",
        "}",
        "",
        "static void set_pf_worker(bool val)",
        "{",
        "	mutex_lock(&wq_pool_attach_mutex);",
        "	if (val)",
        "		current->flags |= PF_WQ_WORKER;",
        "	else",
        "		current->flags &= ~PF_WQ_WORKER;",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "}",
        "",
        "/**",
        " * worker_thread - the worker thread function",
        " * @__worker: self",
        " *",
        " * The worker thread function.  All workers belong to a worker_pool -",
        " * either a per-cpu one or dynamic unbound one.  These workers process all",
        " * work items regardless of their specific target workqueue.  The only",
        " * exception is work items which belong to workqueues with a rescuer which",
        " * will be explained in rescuer_thread().",
        " *",
        " * Return: 0",
        " */",
        "static int worker_thread(void *__worker)",
        "{",
        "	struct worker *worker = __worker;",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	/* tell the scheduler that this is a workqueue worker */",
        "	set_pf_worker(true);",
        "woke_up:",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	/* am I supposed to die? */",
        "	if (unlikely(worker->flags & WORKER_DIE)) {",
        "		raw_spin_unlock_irq(&pool->lock);",
        "		set_pf_worker(false);",
        "		/*",
        "		 * The worker is dead and PF_WQ_WORKER is cleared, worker->pool",
        "		 * shouldn't be accessed, reset it to NULL in case otherwise.",
        "		 */",
        "		worker->pool = NULL;",
        "		ida_free(&pool->worker_ida, worker->id);",
        "		return 0;",
        "	}",
        "",
        "	worker_leave_idle(worker);",
        "recheck:",
        "	/* no more worker necessary? */",
        "	if (!need_more_worker(pool))",
        "		goto sleep;",
        "",
        "	/* do we need to manage? */",
        "	if (unlikely(!may_start_working(pool)) && manage_workers(worker))",
        "		goto recheck;",
        "",
        "	/*",
        "	 * ->scheduled list can only be filled while a worker is",
        "	 * preparing to process a work or actually processing it.",
        "	 * Make sure nobody diddled with it while I was sleeping.",
        "	 */",
        "	WARN_ON_ONCE(!list_empty(&worker->scheduled));",
        "",
        "	/*",
        "	 * Finish PREP stage.  We're guaranteed to have at least one idle",
        "	 * worker or that someone else has already assumed the manager",
        "	 * role.  This is where @worker starts participating in concurrency",
        "	 * management if applicable and concurrency management is restored",
        "	 * after being rebound.  See rebind_workers() for details.",
        "	 */",
        "	worker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);",
        "",
        "	do {",
        "		struct work_struct *work =",
        "			list_first_entry(&pool->worklist,",
        "					 struct work_struct, entry);",
        "",
        "		if (assign_work(work, worker, NULL))",
        "			process_scheduled_works(worker);",
        "	} while (keep_working(pool));",
        "",
        "	worker_set_flags(worker, WORKER_PREP);",
        "sleep:",
        "	/*",
        "	 * pool->lock is held and there's no work to process and no need to",
        "	 * manage, sleep.  Workers are woken up only while holding",
        "	 * pool->lock or from local cpu, so setting the current state",
        "	 * before releasing pool->lock is enough to prevent losing any",
        "	 * event.",
        "	 */",
        "	worker_enter_idle(worker);",
        "	__set_current_state(TASK_IDLE);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "	schedule();",
        "	goto woke_up;",
        "}",
        "",
        "/**",
        " * rescuer_thread - the rescuer thread function",
        " * @__rescuer: self",
        " *",
        " * Workqueue rescuer thread function.  There's one rescuer for each",
        " * workqueue which has WQ_MEM_RECLAIM set.",
        " *",
        " * Regular work processing on a pool may block trying to create a new",
        " * worker which uses GFP_KERNEL allocation which has slight chance of",
        " * developing into deadlock if some works currently on the same queue",
        " * need to be processed to satisfy the GFP_KERNEL allocation.  This is",
        " * the problem rescuer solves.",
        " *",
        " * When such condition is possible, the pool summons rescuers of all",
        " * workqueues which have works queued on the pool and let them process",
        " * those works so that forward progress can be guaranteed.",
        " *",
        " * This should happen rarely.",
        " *",
        " * Return: 0",
        " */",
        "static int rescuer_thread(void *__rescuer)",
        "{",
        "	struct worker *rescuer = __rescuer;",
        "	struct workqueue_struct *wq = rescuer->rescue_wq;",
        "	bool should_stop;",
        "",
        "	set_user_nice(current, RESCUER_NICE_LEVEL);",
        "",
        "	/*",
        "	 * Mark rescuer as worker too.  As WORKER_PREP is never cleared, it",
        "	 * doesn't participate in concurrency management.",
        "	 */",
        "	set_pf_worker(true);",
        "repeat:",
        "	set_current_state(TASK_IDLE);",
        "",
        "	/*",
        "	 * By the time the rescuer is requested to stop, the workqueue",
        "	 * shouldn't have any work pending, but @wq->maydays may still have",
        "	 * pwq(s) queued.  This can happen by non-rescuer workers consuming",
        "	 * all the work items before the rescuer got to them.  Go through",
        "	 * @wq->maydays processing before acting on should_stop so that the",
        "	 * list is always empty on exit.",
        "	 */",
        "	should_stop = kthread_should_stop();",
        "",
        "	/* see whether any pwq is asking for help */",
        "	raw_spin_lock_irq(&wq_mayday_lock);",
        "",
        "	while (!list_empty(&wq->maydays)) {",
        "		struct pool_workqueue *pwq = list_first_entry(&wq->maydays,",
        "					struct pool_workqueue, mayday_node);",
        "		struct worker_pool *pool = pwq->pool;",
        "		struct work_struct *work, *n;",
        "",
        "		__set_current_state(TASK_RUNNING);",
        "		list_del_init(&pwq->mayday_node);",
        "",
        "		raw_spin_unlock_irq(&wq_mayday_lock);",
        "",
        "		worker_attach_to_pool(rescuer, pool);",
        "",
        "		raw_spin_lock_irq(&pool->lock);",
        "",
        "		/*",
        "		 * Slurp in all works issued via this workqueue and",
        "		 * process'em.",
        "		 */",
        "		WARN_ON_ONCE(!list_empty(&rescuer->scheduled));",
        "		list_for_each_entry_safe(work, n, &pool->worklist, entry) {",
        "			if (get_work_pwq(work) == pwq &&",
        "			    assign_work(work, rescuer, &n))",
        "				pwq->stats[PWQ_STAT_RESCUED]++;",
        "		}",
        "",
        "		if (!list_empty(&rescuer->scheduled)) {",
        "			process_scheduled_works(rescuer);",
        "",
        "			/*",
        "			 * The above execution of rescued work items could",
        "			 * have created more to rescue through",
        "			 * pwq_activate_first_inactive() or chained",
        "			 * queueing.  Let's put @pwq back on mayday list so",
        "			 * that such back-to-back work items, which may be",
        "			 * being used to relieve memory pressure, don't",
        "			 * incur MAYDAY_INTERVAL delay inbetween.",
        "			 */",
        "			if (pwq->nr_active && need_to_create_worker(pool)) {",
        "				raw_spin_lock(&wq_mayday_lock);",
        "				/*",
        "				 * Queue iff we aren't racing destruction",
        "				 * and somebody else hasn't queued it already.",
        "				 */",
        "				if (wq->rescuer && list_empty(&pwq->mayday_node)) {",
        "					get_pwq(pwq);",
        "					list_add_tail(&pwq->mayday_node, &wq->maydays);",
        "				}",
        "				raw_spin_unlock(&wq_mayday_lock);",
        "			}",
        "		}",
        "",
        "		/*",
        "		 * Leave this pool. Notify regular workers; otherwise, we end up",
        "		 * with 0 concurrency and stalling the execution.",
        "		 */",
        "		kick_pool(pool);",
        "",
        "		raw_spin_unlock_irq(&pool->lock);",
        "",
        "		worker_detach_from_pool(rescuer);",
        "",
        "		/*",
        "		 * Put the reference grabbed by send_mayday().  @pool might",
        "		 * go away any time after it.",
        "		 */",
        "		put_pwq_unlocked(pwq);",
        "",
        "		raw_spin_lock_irq(&wq_mayday_lock);",
        "	}",
        "",
        "	raw_spin_unlock_irq(&wq_mayday_lock);",
        "",
        "	if (should_stop) {",
        "		__set_current_state(TASK_RUNNING);",
        "		set_pf_worker(false);",
        "		return 0;",
        "	}",
        "",
        "	/* rescuers should never participate in concurrency management */",
        "	WARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));",
        "	schedule();",
        "	goto repeat;",
        "}",
        "",
        "static void bh_worker(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "	int nr_restarts = BH_WORKER_RESTARTS;",
        "	unsigned long end = jiffies + BH_WORKER_JIFFIES;",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "	worker_leave_idle(worker);",
        "",
        "	/*",
        "	 * This function follows the structure of worker_thread(). See there for",
        "	 * explanations on each step.",
        "	 */",
        "	if (!need_more_worker(pool))",
        "		goto done;",
        "",
        "	WARN_ON_ONCE(!list_empty(&worker->scheduled));",
        "	worker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);",
        "",
        "	do {",
        "		struct work_struct *work =",
        "			list_first_entry(&pool->worklist,",
        "					 struct work_struct, entry);",
        "",
        "		if (assign_work(work, worker, NULL))",
        "			process_scheduled_works(worker);",
        "	} while (keep_working(pool) &&",
        "		 --nr_restarts && time_before(jiffies, end));",
        "",
        "	worker_set_flags(worker, WORKER_PREP);",
        "done:",
        "	worker_enter_idle(worker);",
        "	kick_pool(pool);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "}",
        "",
        "/*",
        " * TODO: Convert all tasklet users to workqueue and use softirq directly.",
        " *",
        " * This is currently called from tasklet[_hi]action() and thus is also called",
        " * whenever there are tasklets to run. Let's do an early exit if there's nothing",
        " * queued. Once conversion from tasklet is complete, the need_more_worker() test",
        " * can be dropped.",
        " *",
        " * After full conversion, we'll add worker->softirq_action, directly use the",
        " * softirq action and obtain the worker pointer from the softirq_action pointer.",
        " */",
        "void workqueue_softirq_action(bool highpri)",
        "{",
        "	struct worker_pool *pool =",
        "		&per_cpu(bh_worker_pools, smp_processor_id())[highpri];",
        "	if (need_more_worker(pool))",
        "		bh_worker(list_first_entry(&pool->workers, struct worker, node));",
        "}",
        "",
        "struct wq_drain_dead_softirq_work {",
        "	struct work_struct	work;",
        "	struct worker_pool	*pool;",
        "	struct completion	done;",
        "};",
        "",
        "static void drain_dead_softirq_workfn(struct work_struct *work)",
        "{",
        "	struct wq_drain_dead_softirq_work *dead_work =",
        "		container_of(work, struct wq_drain_dead_softirq_work, work);",
        "	struct worker_pool *pool = dead_work->pool;",
        "	bool repeat;",
        "",
        "	/*",
        "	 * @pool's CPU is dead and we want to execute its still pending work",
        "	 * items from this BH work item which is running on a different CPU. As",
        "	 * its CPU is dead, @pool can't be kicked and, as work execution path",
        "	 * will be nested, a lockdep annotation needs to be suppressed. Mark",
        "	 * @pool with %POOL_BH_DRAINING for the special treatments.",
        "	 */",
        "	raw_spin_lock_irq(&pool->lock);",
        "	pool->flags |= POOL_BH_DRAINING;",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	bh_worker(list_first_entry(&pool->workers, struct worker, node));",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "	pool->flags &= ~POOL_BH_DRAINING;",
        "	repeat = need_more_worker(pool);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	/*",
        "	 * bh_worker() might hit consecutive execution limit and bail. If there",
        "	 * still are pending work items, reschedule self and return so that we",
        "	 * don't hog this CPU's BH.",
        "	 */",
        "	if (repeat) {",
        "		if (pool->attrs->nice == HIGHPRI_NICE_LEVEL)",
        "			queue_work(system_bh_highpri_wq, work);",
        "		else",
        "			queue_work(system_bh_wq, work);",
        "	} else {",
        "		complete(&dead_work->done);",
        "	}",
        "}",
        "",
        "/*",
        " * @cpu is dead. Drain the remaining BH work items on the current CPU. It's",
        " * possible to allocate dead_work per CPU and avoid flushing. However, then we",
        " * have to worry about draining overlapping with CPU coming back online or",
        " * nesting (one CPU's dead_work queued on another CPU which is also dead and so",
        " * on). Let's keep it simple and drain them synchronously. These are BH work",
        " * items which shouldn't be requeued on the same pool. Shouldn't take long.",
        " */",
        "void workqueue_softirq_dead(unsigned int cpu)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < NR_STD_WORKER_POOLS; i++) {",
        "		struct worker_pool *pool = &per_cpu(bh_worker_pools, cpu)[i];",
        "		struct wq_drain_dead_softirq_work dead_work;",
        "",
        "		if (!need_more_worker(pool))",
        "			continue;",
        "",
        "		INIT_WORK_ONSTACK(&dead_work.work, drain_dead_softirq_workfn);",
        "		dead_work.pool = pool;",
        "		init_completion(&dead_work.done);",
        "",
        "		if (pool->attrs->nice == HIGHPRI_NICE_LEVEL)",
        "			queue_work(system_bh_highpri_wq, &dead_work.work);",
        "		else",
        "			queue_work(system_bh_wq, &dead_work.work);",
        "",
        "		wait_for_completion(&dead_work.done);",
        "		destroy_work_on_stack(&dead_work.work);",
        "	}",
        "}",
        "",
        "/**",
        " * check_flush_dependency - check for flush dependency sanity",
        " * @target_wq: workqueue being flushed",
        " * @target_work: work item being flushed (NULL for workqueue flushes)",
        " * @from_cancel: are we called from the work cancel path",
        " *",
        " * %current is trying to flush the whole @target_wq or @target_work on it.",
        " * If this is not the cancel path (which implies work being flushed is either",
        " * already running, or will not be at all), check if @target_wq doesn't have",
        " * %WQ_MEM_RECLAIM and verify that %current is not reclaiming memory or running",
        " * on a workqueue which doesn't have %WQ_MEM_RECLAIM as that can break forward-",
        " * progress guarantee leading to a deadlock.",
        " */",
        "static void check_flush_dependency(struct workqueue_struct *target_wq,",
        "				   struct work_struct *target_work,",
        "				   bool from_cancel)",
        "{",
        "	work_func_t target_func;",
        "	struct worker *worker;",
        "",
        "	if (from_cancel || target_wq->flags & WQ_MEM_RECLAIM)",
        "		return;",
        "",
        "	worker = current_wq_worker();",
        "	target_func = target_work ? target_work->func : NULL;",
        "",
        "	WARN_ONCE(current->flags & PF_MEMALLOC,",
        "		  \"workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%ps\",",
        "		  current->pid, current->comm, target_wq->name, target_func);",
        "	WARN_ONCE(worker && ((worker->current_pwq->wq->flags &",
        "			      (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),",
        "		  \"workqueue: WQ_MEM_RECLAIM %s:%ps is flushing !WQ_MEM_RECLAIM %s:%ps\",",
        "		  worker->current_pwq->wq->name, worker->current_func,",
        "		  target_wq->name, target_func);",
        "}",
        "",
        "struct wq_barrier {",
        "	struct work_struct	work;",
        "	struct completion	done;",
        "	struct task_struct	*task;	/* purely informational */",
        "};",
        "",
        "static void wq_barrier_func(struct work_struct *work)",
        "{",
        "	struct wq_barrier *barr = container_of(work, struct wq_barrier, work);",
        "	complete(&barr->done);",
        "}",
        "",
        "/**",
        " * insert_wq_barrier - insert a barrier work",
        " * @pwq: pwq to insert barrier into",
        " * @barr: wq_barrier to insert",
        " * @target: target work to attach @barr to",
        " * @worker: worker currently executing @target, NULL if @target is not executing",
        " *",
        " * @barr is linked to @target such that @barr is completed only after",
        " * @target finishes execution.  Please note that the ordering",
        " * guarantee is observed only with respect to @target and on the local",
        " * cpu.",
        " *",
        " * Currently, a queued barrier can't be canceled.  This is because",
        " * try_to_grab_pending() can't determine whether the work to be",
        " * grabbed is at the head of the queue and thus can't clear LINKED",
        " * flag of the previous work while there must be a valid next work",
        " * after a work with LINKED flag set.",
        " *",
        " * Note that when @worker is non-NULL, @target may be modified",
        " * underneath us, so we can't reliably determine pwq from @target.",
        " *",
        " * CONTEXT:",
        " * raw_spin_lock_irq(pool->lock).",
        " */",
        "static void insert_wq_barrier(struct pool_workqueue *pwq,",
        "			      struct wq_barrier *barr,",
        "			      struct work_struct *target, struct worker *worker)",
        "{",
        "	static __maybe_unused struct lock_class_key bh_key, thr_key;",
        "	unsigned int work_flags = 0;",
        "	unsigned int work_color;",
        "	struct list_head *head;",
        "",
        "	/*",
        "	 * debugobject calls are safe here even with pool->lock locked",
        "	 * as we know for sure that this will not trigger any of the",
        "	 * checks and call back into the fixup functions where we",
        "	 * might deadlock.",
        "	 *",
        "	 * BH and threaded workqueues need separate lockdep keys to avoid",
        "	 * spuriously triggering \"inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W}",
        "	 * usage\".",
        "	 */",
        "	INIT_WORK_ONSTACK_KEY(&barr->work, wq_barrier_func,",
        "			      (pwq->wq->flags & WQ_BH) ? &bh_key : &thr_key);",
        "	__set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));",
        "",
        "	init_completion_map(&barr->done, &target->lockdep_map);",
        "",
        "	barr->task = current;",
        "",
        "	/* The barrier work item does not participate in nr_active. */",
        "	work_flags |= WORK_STRUCT_INACTIVE;",
        "",
        "	/*",
        "	 * If @target is currently being executed, schedule the",
        "	 * barrier to the worker; otherwise, put it after @target.",
        "	 */",
        "	if (worker) {",
        "		head = worker->scheduled.next;",
        "		work_color = worker->current_color;",
        "	} else {",
        "		unsigned long *bits = work_data_bits(target);",
        "",
        "		head = target->entry.next;",
        "		/* there can already be other linked works, inherit and set */",
        "		work_flags |= *bits & WORK_STRUCT_LINKED;",
        "		work_color = get_work_color(*bits);",
        "		__set_bit(WORK_STRUCT_LINKED_BIT, bits);",
        "	}",
        "",
        "	pwq->nr_in_flight[work_color]++;",
        "	work_flags |= work_color_to_flags(work_color);",
        "",
        "	insert_work(pwq, &barr->work, head, work_flags);",
        "}",
        "",
        "/**",
        " * flush_workqueue_prep_pwqs - prepare pwqs for workqueue flushing",
        " * @wq: workqueue being flushed",
        " * @flush_color: new flush color, < 0 for no-op",
        " * @work_color: new work color, < 0 for no-op",
        " *",
        " * Prepare pwqs for workqueue flushing.",
        " *",
        " * If @flush_color is non-negative, flush_color on all pwqs should be",
        " * -1.  If no pwq has in-flight commands at the specified color, all",
        " * pwq->flush_color's stay at -1 and %false is returned.  If any pwq",
        " * has in flight commands, its pwq->flush_color is set to",
        " * @flush_color, @wq->nr_pwqs_to_flush is updated accordingly, pwq",
        " * wakeup logic is armed and %true is returned.",
        " *",
        " * The caller should have initialized @wq->first_flusher prior to",
        " * calling this function with non-negative @flush_color.  If",
        " * @flush_color is negative, no flush color update is done and %false",
        " * is returned.",
        " *",
        " * If @work_color is non-negative, all pwqs should have the same",
        " * work_color which is previous to @work_color and all will be",
        " * advanced to @work_color.",
        " *",
        " * CONTEXT:",
        " * mutex_lock(wq->mutex).",
        " *",
        " * Return:",
        " * %true if @flush_color >= 0 and there's something to flush.  %false",
        " * otherwise.",
        " */",
        "static bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,",
        "				      int flush_color, int work_color)",
        "{",
        "	bool wait = false;",
        "	struct pool_workqueue *pwq;",
        "	struct worker_pool *current_pool = NULL;",
        "",
        "	if (flush_color >= 0) {",
        "		WARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));",
        "		atomic_set(&wq->nr_pwqs_to_flush, 1);",
        "	}",
        "",
        "	/*",
        "	 * For unbound workqueue, pwqs will map to only a few pools.",
        "	 * Most of the time, pwqs within the same pool will be linked",
        "	 * sequentially to wq->pwqs by cpu index. So in the majority",
        "	 * of pwq iters, the pool is the same, only doing lock/unlock",
        "	 * if the pool has changed. This can largely reduce expensive",
        "	 * lock operations.",
        "	 */",
        "	for_each_pwq(pwq, wq) {",
        "		if (current_pool != pwq->pool) {",
        "			if (likely(current_pool))",
        "				raw_spin_unlock_irq(&current_pool->lock);",
        "			current_pool = pwq->pool;",
        "			raw_spin_lock_irq(&current_pool->lock);",
        "		}",
        "",
        "		if (flush_color >= 0) {",
        "			WARN_ON_ONCE(pwq->flush_color != -1);",
        "",
        "			if (pwq->nr_in_flight[flush_color]) {",
        "				pwq->flush_color = flush_color;",
        "				atomic_inc(&wq->nr_pwqs_to_flush);",
        "				wait = true;",
        "			}",
        "		}",
        "",
        "		if (work_color >= 0) {",
        "			WARN_ON_ONCE(work_color != work_next_color(pwq->work_color));",
        "			pwq->work_color = work_color;",
        "		}",
        "",
        "	}",
        "",
        "	if (current_pool)",
        "		raw_spin_unlock_irq(&current_pool->lock);",
        "",
        "	if (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))",
        "		complete(&wq->first_flusher->done);",
        "",
        "	return wait;",
        "}",
        "",
        "static void touch_wq_lockdep_map(struct workqueue_struct *wq)",
        "{",
        "#ifdef CONFIG_LOCKDEP",
        "	if (unlikely(!wq->lockdep_map))",
        "		return;",
        "",
        "	if (wq->flags & WQ_BH)",
        "		local_bh_disable();",
        "",
        "	lock_map_acquire(wq->lockdep_map);",
        "	lock_map_release(wq->lockdep_map);",
        "",
        "	if (wq->flags & WQ_BH)",
        "		local_bh_enable();",
        "#endif",
        "}",
        "",
        "static void touch_work_lockdep_map(struct work_struct *work,",
        "				   struct workqueue_struct *wq)",
        "{",
        "#ifdef CONFIG_LOCKDEP",
        "	if (wq->flags & WQ_BH)",
        "		local_bh_disable();",
        "",
        "	lock_map_acquire(&work->lockdep_map);",
        "	lock_map_release(&work->lockdep_map);",
        "",
        "	if (wq->flags & WQ_BH)",
        "		local_bh_enable();",
        "#endif",
        "}",
        "",
        "/**",
        " * __flush_workqueue - ensure that any scheduled work has run to completion.",
        " * @wq: workqueue to flush",
        " *",
        " * This function sleeps until all work items which were queued on entry",
        " * have finished execution, but it is not livelocked by new incoming ones.",
        " */",
        "void __flush_workqueue(struct workqueue_struct *wq)",
        "{",
        "	struct wq_flusher this_flusher = {",
        "		.list = LIST_HEAD_INIT(this_flusher.list),",
        "		.flush_color = -1,",
        "		.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, (*wq->lockdep_map)),",
        "	};",
        "	int next_color;",
        "",
        "	if (WARN_ON(!wq_online))",
        "		return;",
        "",
        "	touch_wq_lockdep_map(wq);",
        "",
        "	mutex_lock(&wq->mutex);",
        "",
        "	/*",
        "	 * Start-to-wait phase",
        "	 */",
        "	next_color = work_next_color(wq->work_color);",
        "",
        "	if (next_color != wq->flush_color) {",
        "		/*",
        "		 * Color space is not full.  The current work_color",
        "		 * becomes our flush_color and work_color is advanced",
        "		 * by one.",
        "		 */",
        "		WARN_ON_ONCE(!list_empty(&wq->flusher_overflow));",
        "		this_flusher.flush_color = wq->work_color;",
        "		wq->work_color = next_color;",
        "",
        "		if (!wq->first_flusher) {",
        "			/* no flush in progress, become the first flusher */",
        "			WARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);",
        "",
        "			wq->first_flusher = &this_flusher;",
        "",
        "			if (!flush_workqueue_prep_pwqs(wq, wq->flush_color,",
        "						       wq->work_color)) {",
        "				/* nothing to flush, done */",
        "				wq->flush_color = next_color;",
        "				wq->first_flusher = NULL;",
        "				goto out_unlock;",
        "			}",
        "		} else {",
        "			/* wait in queue */",
        "			WARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);",
        "			list_add_tail(&this_flusher.list, &wq->flusher_queue);",
        "			flush_workqueue_prep_pwqs(wq, -1, wq->work_color);",
        "		}",
        "	} else {",
        "		/*",
        "		 * Oops, color space is full, wait on overflow queue.",
        "		 * The next flush completion will assign us",
        "		 * flush_color and transfer to flusher_queue.",
        "		 */",
        "		list_add_tail(&this_flusher.list, &wq->flusher_overflow);",
        "	}",
        "",
        "	check_flush_dependency(wq, NULL, false);",
        "",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	wait_for_completion(&this_flusher.done);",
        "",
        "	/*",
        "	 * Wake-up-and-cascade phase",
        "	 *",
        "	 * First flushers are responsible for cascading flushes and",
        "	 * handling overflow.  Non-first flushers can simply return.",
        "	 */",
        "	if (READ_ONCE(wq->first_flusher) != &this_flusher)",
        "		return;",
        "",
        "	mutex_lock(&wq->mutex);",
        "",
        "	/* we might have raced, check again with mutex held */",
        "	if (wq->first_flusher != &this_flusher)",
        "		goto out_unlock;",
        "",
        "	WRITE_ONCE(wq->first_flusher, NULL);",
        "",
        "	WARN_ON_ONCE(!list_empty(&this_flusher.list));",
        "	WARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);",
        "",
        "	while (true) {",
        "		struct wq_flusher *next, *tmp;",
        "",
        "		/* complete all the flushers sharing the current flush color */",
        "		list_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {",
        "			if (next->flush_color != wq->flush_color)",
        "				break;",
        "			list_del_init(&next->list);",
        "			complete(&next->done);",
        "		}",
        "",
        "		WARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&",
        "			     wq->flush_color != work_next_color(wq->work_color));",
        "",
        "		/* this flush_color is finished, advance by one */",
        "		wq->flush_color = work_next_color(wq->flush_color);",
        "",
        "		/* one color has been freed, handle overflow queue */",
        "		if (!list_empty(&wq->flusher_overflow)) {",
        "			/*",
        "			 * Assign the same color to all overflowed",
        "			 * flushers, advance work_color and append to",
        "			 * flusher_queue.  This is the start-to-wait",
        "			 * phase for these overflowed flushers.",
        "			 */",
        "			list_for_each_entry(tmp, &wq->flusher_overflow, list)",
        "				tmp->flush_color = wq->work_color;",
        "",
        "			wq->work_color = work_next_color(wq->work_color);",
        "",
        "			list_splice_tail_init(&wq->flusher_overflow,",
        "					      &wq->flusher_queue);",
        "			flush_workqueue_prep_pwqs(wq, -1, wq->work_color);",
        "		}",
        "",
        "		if (list_empty(&wq->flusher_queue)) {",
        "			WARN_ON_ONCE(wq->flush_color != wq->work_color);",
        "			break;",
        "		}",
        "",
        "		/*",
        "		 * Need to flush more colors.  Make the next flusher",
        "		 * the new first flusher and arm pwqs.",
        "		 */",
        "		WARN_ON_ONCE(wq->flush_color == wq->work_color);",
        "		WARN_ON_ONCE(wq->flush_color != next->flush_color);",
        "",
        "		list_del_init(&next->list);",
        "		wq->first_flusher = next;",
        "",
        "		if (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))",
        "			break;",
        "",
        "		/*",
        "		 * Meh... this color is already done, clear first",
        "		 * flusher and repeat cascading.",
        "		 */",
        "		wq->first_flusher = NULL;",
        "	}",
        "",
        "out_unlock:",
        "	mutex_unlock(&wq->mutex);",
        "}",
        "EXPORT_SYMBOL(__flush_workqueue);",
        "",
        "/**",
        " * drain_workqueue - drain a workqueue",
        " * @wq: workqueue to drain",
        " *",
        " * Wait until the workqueue becomes empty.  While draining is in progress,",
        " * only chain queueing is allowed.  IOW, only currently pending or running",
        " * work items on @wq can queue further work items on it.  @wq is flushed",
        " * repeatedly until it becomes empty.  The number of flushing is determined",
        " * by the depth of chaining and should be relatively short.  Whine if it",
        " * takes too long.",
        " */",
        "void drain_workqueue(struct workqueue_struct *wq)",
        "{",
        "	unsigned int flush_cnt = 0;",
        "	struct pool_workqueue *pwq;",
        "",
        "	/*",
        "	 * __queue_work() needs to test whether there are drainers, is much",
        "	 * hotter than drain_workqueue() and already looks at @wq->flags.",
        "	 * Use __WQ_DRAINING so that queue doesn't have to check nr_drainers.",
        "	 */",
        "	mutex_lock(&wq->mutex);",
        "	if (!wq->nr_drainers++)",
        "		wq->flags |= __WQ_DRAINING;",
        "	mutex_unlock(&wq->mutex);",
        "reflush:",
        "	__flush_workqueue(wq);",
        "",
        "	mutex_lock(&wq->mutex);",
        "",
        "	for_each_pwq(pwq, wq) {",
        "		bool drained;",
        "",
        "		raw_spin_lock_irq(&pwq->pool->lock);",
        "		drained = pwq_is_empty(pwq);",
        "		raw_spin_unlock_irq(&pwq->pool->lock);",
        "",
        "		if (drained)",
        "			continue;",
        "",
        "		if (++flush_cnt == 10 ||",
        "		    (flush_cnt % 100 == 0 && flush_cnt <= 1000))",
        "			pr_warn(\"workqueue %s: %s() isn't complete after %u tries\\n\",",
        "				wq->name, __func__, flush_cnt);",
        "",
        "		mutex_unlock(&wq->mutex);",
        "		goto reflush;",
        "	}",
        "",
        "	if (!--wq->nr_drainers)",
        "		wq->flags &= ~__WQ_DRAINING;",
        "	mutex_unlock(&wq->mutex);",
        "}",
        "EXPORT_SYMBOL_GPL(drain_workqueue);",
        "",
        "static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr,",
        "			     bool from_cancel)",
        "{",
        "	struct worker *worker = NULL;",
        "	struct worker_pool *pool;",
        "	struct pool_workqueue *pwq;",
        "	struct workqueue_struct *wq;",
        "",
        "	rcu_read_lock();",
        "	pool = get_work_pool(work);",
        "	if (!pool) {",
        "		rcu_read_unlock();",
        "		return false;",
        "	}",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "	/* see the comment in try_to_grab_pending() with the same code */",
        "	pwq = get_work_pwq(work);",
        "	if (pwq) {",
        "		if (unlikely(pwq->pool != pool))",
        "			goto already_gone;",
        "	} else {",
        "		worker = find_worker_executing_work(pool, work);",
        "		if (!worker)",
        "			goto already_gone;",
        "		pwq = worker->current_pwq;",
        "	}",
        "",
        "	wq = pwq->wq;",
        "	check_flush_dependency(wq, work, from_cancel);",
        "",
        "	insert_wq_barrier(pwq, barr, work, worker);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	touch_work_lockdep_map(work, wq);",
        "",
        "	/*",
        "	 * Force a lock recursion deadlock when using flush_work() inside a",
        "	 * single-threaded or rescuer equipped workqueue.",
        "	 *",
        "	 * For single threaded workqueues the deadlock happens when the work",
        "	 * is after the work issuing the flush_work(). For rescuer equipped",
        "	 * workqueues the deadlock happens when the rescuer stalls, blocking",
        "	 * forward progress.",
        "	 */",
        "	if (!from_cancel && (wq->saved_max_active == 1 || wq->rescuer))",
        "		touch_wq_lockdep_map(wq);",
        "",
        "	rcu_read_unlock();",
        "	return true;",
        "already_gone:",
        "	raw_spin_unlock_irq(&pool->lock);",
        "	rcu_read_unlock();",
        "	return false;",
        "}",
        "",
        "static bool __flush_work(struct work_struct *work, bool from_cancel)",
        "{",
        "	struct wq_barrier barr;",
        "",
        "	if (WARN_ON(!wq_online))",
        "		return false;",
        "",
        "	if (WARN_ON(!work->func))",
        "		return false;",
        "",
        "	if (!start_flush_work(work, &barr, from_cancel))",
        "		return false;",
        "",
        "	/*",
        "	 * start_flush_work() returned %true. If @from_cancel is set, we know",
        "	 * that @work must have been executing during start_flush_work() and",
        "	 * can't currently be queued. Its data must contain OFFQ bits. If @work",
        "	 * was queued on a BH workqueue, we also know that it was running in the",
        "	 * BH context and thus can be busy-waited.",
        "	 */",
        "	if (from_cancel) {",
        "		unsigned long data = *work_data_bits(work);",
        "",
        "		if (!WARN_ON_ONCE(data & WORK_STRUCT_PWQ) &&",
        "		    (data & WORK_OFFQ_BH)) {",
        "			/*",
        "			 * On RT, prevent a live lock when %current preempted",
        "			 * soft interrupt processing or prevents ksoftirqd from",
        "			 * running by keeping flipping BH. If the BH work item",
        "			 * runs on a different CPU then this has no effect other",
        "			 * than doing the BH disable/enable dance for nothing.",
        "			 * This is copied from",
        "			 * kernel/softirq.c::tasklet_unlock_spin_wait().",
        "			 */",
        "			while (!try_wait_for_completion(&barr.done)) {",
        "				if (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
        "					local_bh_disable();",
        "					local_bh_enable();",
        "				} else {",
        "					cpu_relax();",
        "				}",
        "			}",
        "			goto out_destroy;",
        "		}",
        "	}",
        "",
        "	wait_for_completion(&barr.done);",
        "",
        "out_destroy:",
        "	destroy_work_on_stack(&barr.work);",
        "	return true;",
        "}",
        "",
        "/**",
        " * flush_work - wait for a work to finish executing the last queueing instance",
        " * @work: the work to flush",
        " *",
        " * Wait until @work has finished execution.  @work is guaranteed to be idle",
        " * on return if it hasn't been requeued since flush started.",
        " *",
        " * Return:",
        " * %true if flush_work() waited for the work to finish execution,",
        " * %false if it was already idle.",
        " */",
        "bool flush_work(struct work_struct *work)",
        "{",
        "	might_sleep();",
        "	return __flush_work(work, false);",
        "}",
        "EXPORT_SYMBOL_GPL(flush_work);",
        "",
        "/**",
        " * flush_delayed_work - wait for a dwork to finish executing the last queueing",
        " * @dwork: the delayed work to flush",
        " *",
        " * Delayed timer is cancelled and the pending work is queued for",
        " * immediate execution.  Like flush_work(), this function only",
        " * considers the last queueing instance of @dwork.",
        " *",
        " * Return:",
        " * %true if flush_work() waited for the work to finish execution,",
        " * %false if it was already idle.",
        " */",
        "bool flush_delayed_work(struct delayed_work *dwork)",
        "{",
        "	local_irq_disable();",
        "	if (del_timer_sync(&dwork->timer))",
        "		__queue_work(dwork->cpu, dwork->wq, &dwork->work);",
        "	local_irq_enable();",
        "	return flush_work(&dwork->work);",
        "}",
        "EXPORT_SYMBOL(flush_delayed_work);",
        "",
        "/**",
        " * flush_rcu_work - wait for a rwork to finish executing the last queueing",
        " * @rwork: the rcu work to flush",
        " *",
        " * Return:",
        " * %true if flush_rcu_work() waited for the work to finish execution,",
        " * %false if it was already idle.",
        " */",
        "bool flush_rcu_work(struct rcu_work *rwork)",
        "{",
        "	if (test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&rwork->work))) {",
        "		rcu_barrier();",
        "		flush_work(&rwork->work);",
        "		return true;",
        "	} else {",
        "		return flush_work(&rwork->work);",
        "	}",
        "}",
        "EXPORT_SYMBOL(flush_rcu_work);",
        "",
        "static void work_offqd_disable(struct work_offq_data *offqd)",
        "{",
        "	const unsigned long max = (1lu << WORK_OFFQ_DISABLE_BITS) - 1;",
        "",
        "	if (likely(offqd->disable < max))",
        "		offqd->disable++;",
        "	else",
        "		WARN_ONCE(true, \"workqueue: work disable count overflowed\\n\");",
        "}",
        "",
        "static void work_offqd_enable(struct work_offq_data *offqd)",
        "{",
        "	if (likely(offqd->disable > 0))",
        "		offqd->disable--;",
        "	else",
        "		WARN_ONCE(true, \"workqueue: work disable count underflowed\\n\");",
        "}",
        "",
        "static bool __cancel_work(struct work_struct *work, u32 cflags)",
        "{",
        "	struct work_offq_data offqd;",
        "	unsigned long irq_flags;",
        "	int ret;",
        "",
        "	ret = work_grab_pending(work, cflags, &irq_flags);",
        "",
        "	work_offqd_unpack(&offqd, *work_data_bits(work));",
        "",
        "	if (cflags & WORK_CANCEL_DISABLE)",
        "		work_offqd_disable(&offqd);",
        "",
        "	set_work_pool_and_clear_pending(work, offqd.pool_id,",
        "					work_offqd_pack_flags(&offqd));",
        "	local_irq_restore(irq_flags);",
        "	return ret;",
        "}",
        "",
        "static bool __cancel_work_sync(struct work_struct *work, u32 cflags)",
        "{",
        "	bool ret;",
        "",
        "	ret = __cancel_work(work, cflags | WORK_CANCEL_DISABLE);",
        "",
        "	if (*work_data_bits(work) & WORK_OFFQ_BH)",
        "		WARN_ON_ONCE(in_hardirq());",
        "	else",
        "		might_sleep();",
        "",
        "	/*",
        "	 * Skip __flush_work() during early boot when we know that @work isn't",
        "	 * executing. This allows canceling during early boot.",
        "	 */",
        "	if (wq_online)",
        "		__flush_work(work, true);",
        "",
        "	if (!(cflags & WORK_CANCEL_DISABLE))",
        "		enable_work(work);",
        "",
        "	return ret;",
        "}",
        "",
        "/*",
        " * See cancel_delayed_work()",
        " */",
        "bool cancel_work(struct work_struct *work)",
        "{",
        "	return __cancel_work(work, 0);",
        "}",
        "EXPORT_SYMBOL(cancel_work);",
        "",
        "/**",
        " * cancel_work_sync - cancel a work and wait for it to finish",
        " * @work: the work to cancel",
        " *",
        " * Cancel @work and wait for its execution to finish. This function can be used",
        " * even if the work re-queues itself or migrates to another workqueue. On return",
        " * from this function, @work is guaranteed to be not pending or executing on any",
        " * CPU as long as there aren't racing enqueues.",
        " *",
        " * cancel_work_sync(&delayed_work->work) must not be used for delayed_work's.",
        " * Use cancel_delayed_work_sync() instead.",
        " *",
        " * Must be called from a sleepable context if @work was last queued on a non-BH",
        " * workqueue. Can also be called from non-hardirq atomic contexts including BH",
        " * if @work was last queued on a BH workqueue.",
        " *",
        " * Returns %true if @work was pending, %false otherwise.",
        " */",
        "bool cancel_work_sync(struct work_struct *work)",
        "{",
        "	return __cancel_work_sync(work, 0);",
        "}",
        "EXPORT_SYMBOL_GPL(cancel_work_sync);",
        "",
        "/**",
        " * cancel_delayed_work - cancel a delayed work",
        " * @dwork: delayed_work to cancel",
        " *",
        " * Kill off a pending delayed_work.",
        " *",
        " * Return: %true if @dwork was pending and canceled; %false if it wasn't",
        " * pending.",
        " *",
        " * Note:",
        " * The work callback function may still be running on return, unless",
        " * it returns %true and the work doesn't re-arm itself.  Explicitly flush or",
        " * use cancel_delayed_work_sync() to wait on it.",
        " *",
        " * This function is safe to call from any context including IRQ handler.",
        " */",
        "bool cancel_delayed_work(struct delayed_work *dwork)",
        "{",
        "	return __cancel_work(&dwork->work, WORK_CANCEL_DELAYED);",
        "}",
        "EXPORT_SYMBOL(cancel_delayed_work);",
        "",
        "/**",
        " * cancel_delayed_work_sync - cancel a delayed work and wait for it to finish",
        " * @dwork: the delayed work cancel",
        " *",
        " * This is cancel_work_sync() for delayed works.",
        " *",
        " * Return:",
        " * %true if @dwork was pending, %false otherwise.",
        " */",
        "bool cancel_delayed_work_sync(struct delayed_work *dwork)",
        "{",
        "	return __cancel_work_sync(&dwork->work, WORK_CANCEL_DELAYED);",
        "}",
        "EXPORT_SYMBOL(cancel_delayed_work_sync);",
        "",
        "/**",
        " * disable_work - Disable and cancel a work item",
        " * @work: work item to disable",
        " *",
        " * Disable @work by incrementing its disable count and cancel it if currently",
        " * pending. As long as the disable count is non-zero, any attempt to queue @work",
        " * will fail and return %false. The maximum supported disable depth is 2 to the",
        " * power of %WORK_OFFQ_DISABLE_BITS, currently 65536.",
        " *",
        " * Can be called from any context. Returns %true if @work was pending, %false",
        " * otherwise.",
        " */",
        "bool disable_work(struct work_struct *work)",
        "{",
        "	return __cancel_work(work, WORK_CANCEL_DISABLE);",
        "}",
        "EXPORT_SYMBOL_GPL(disable_work);",
        "",
        "/**",
        " * disable_work_sync - Disable, cancel and drain a work item",
        " * @work: work item to disable",
        " *",
        " * Similar to disable_work() but also wait for @work to finish if currently",
        " * executing.",
        " *",
        " * Must be called from a sleepable context if @work was last queued on a non-BH",
        " * workqueue. Can also be called from non-hardirq atomic contexts including BH",
        " * if @work was last queued on a BH workqueue.",
        " *",
        " * Returns %true if @work was pending, %false otherwise.",
        " */",
        "bool disable_work_sync(struct work_struct *work)",
        "{",
        "	return __cancel_work_sync(work, WORK_CANCEL_DISABLE);",
        "}",
        "EXPORT_SYMBOL_GPL(disable_work_sync);",
        "",
        "/**",
        " * enable_work - Enable a work item",
        " * @work: work item to enable",
        " *",
        " * Undo disable_work[_sync]() by decrementing @work's disable count. @work can",
        " * only be queued if its disable count is 0.",
        " *",
        " * Can be called from any context. Returns %true if the disable count reached 0.",
        " * Otherwise, %false.",
        " */",
        "bool enable_work(struct work_struct *work)",
        "{",
        "	struct work_offq_data offqd;",
        "	unsigned long irq_flags;",
        "",
        "	work_grab_pending(work, 0, &irq_flags);",
        "",
        "	work_offqd_unpack(&offqd, *work_data_bits(work));",
        "	work_offqd_enable(&offqd);",
        "	set_work_pool_and_clear_pending(work, offqd.pool_id,",
        "					work_offqd_pack_flags(&offqd));",
        "	local_irq_restore(irq_flags);",
        "",
        "	return !offqd.disable;",
        "}",
        "EXPORT_SYMBOL_GPL(enable_work);",
        "",
        "/**",
        " * disable_delayed_work - Disable and cancel a delayed work item",
        " * @dwork: delayed work item to disable",
        " *",
        " * disable_work() for delayed work items.",
        " */",
        "bool disable_delayed_work(struct delayed_work *dwork)",
        "{",
        "	return __cancel_work(&dwork->work,",
        "			     WORK_CANCEL_DELAYED | WORK_CANCEL_DISABLE);",
        "}",
        "EXPORT_SYMBOL_GPL(disable_delayed_work);",
        "",
        "/**",
        " * disable_delayed_work_sync - Disable, cancel and drain a delayed work item",
        " * @dwork: delayed work item to disable",
        " *",
        " * disable_work_sync() for delayed work items.",
        " */",
        "bool disable_delayed_work_sync(struct delayed_work *dwork)",
        "{",
        "	return __cancel_work_sync(&dwork->work,",
        "				  WORK_CANCEL_DELAYED | WORK_CANCEL_DISABLE);",
        "}",
        "EXPORT_SYMBOL_GPL(disable_delayed_work_sync);",
        "",
        "/**",
        " * enable_delayed_work - Enable a delayed work item",
        " * @dwork: delayed work item to enable",
        " *",
        " * enable_work() for delayed work items.",
        " */",
        "bool enable_delayed_work(struct delayed_work *dwork)",
        "{",
        "	return enable_work(&dwork->work);",
        "}",
        "EXPORT_SYMBOL_GPL(enable_delayed_work);",
        "",
        "/**",
        " * schedule_on_each_cpu - execute a function synchronously on each online CPU",
        " * @func: the function to call",
        " *",
        " * schedule_on_each_cpu() executes @func on each online CPU using the",
        " * system workqueue and blocks until all CPUs have completed.",
        " * schedule_on_each_cpu() is very slow.",
        " *",
        " * Return:",
        " * 0 on success, -errno on failure.",
        " */",
        "int schedule_on_each_cpu(work_func_t func)",
        "{",
        "	int cpu;",
        "	struct work_struct __percpu *works;",
        "",
        "	works = alloc_percpu(struct work_struct);",
        "	if (!works)",
        "		return -ENOMEM;",
        "",
        "	cpus_read_lock();",
        "",
        "	for_each_online_cpu(cpu) {",
        "		struct work_struct *work = per_cpu_ptr(works, cpu);",
        "",
        "		INIT_WORK(work, func);",
        "		schedule_work_on(cpu, work);",
        "	}",
        "",
        "	for_each_online_cpu(cpu)",
        "		flush_work(per_cpu_ptr(works, cpu));",
        "",
        "	cpus_read_unlock();",
        "	free_percpu(works);",
        "	return 0;",
        "}",
        "",
        "/**",
        " * execute_in_process_context - reliably execute the routine with user context",
        " * @fn:		the function to execute",
        " * @ew:		guaranteed storage for the execute work structure (must",
        " *		be available when the work executes)",
        " *",
        " * Executes the function immediately if process context is available,",
        " * otherwise schedules the function for delayed execution.",
        " *",
        " * Return:	0 - function was executed",
        " *		1 - function was scheduled for execution",
        " */",
        "int execute_in_process_context(work_func_t fn, struct execute_work *ew)",
        "{",
        "	if (!in_interrupt()) {",
        "		fn(&ew->work);",
        "		return 0;",
        "	}",
        "",
        "	INIT_WORK(&ew->work, fn);",
        "	schedule_work(&ew->work);",
        "",
        "	return 1;",
        "}",
        "EXPORT_SYMBOL_GPL(execute_in_process_context);",
        "",
        "/**",
        " * free_workqueue_attrs - free a workqueue_attrs",
        " * @attrs: workqueue_attrs to free",
        " *",
        " * Undo alloc_workqueue_attrs().",
        " */",
        "void free_workqueue_attrs(struct workqueue_attrs *attrs)",
        "{",
        "	if (attrs) {",
        "		free_cpumask_var(attrs->cpumask);",
        "		free_cpumask_var(attrs->__pod_cpumask);",
        "		kfree(attrs);",
        "	}",
        "}",
        "",
        "/**",
        " * alloc_workqueue_attrs - allocate a workqueue_attrs",
        " *",
        " * Allocate a new workqueue_attrs, initialize with default settings and",
        " * return it.",
        " *",
        " * Return: The allocated new workqueue_attr on success. %NULL on failure.",
        " */",
        "struct workqueue_attrs *alloc_workqueue_attrs(void)",
        "{",
        "	struct workqueue_attrs *attrs;",
        "",
        "	attrs = kzalloc(sizeof(*attrs), GFP_KERNEL);",
        "	if (!attrs)",
        "		goto fail;",
        "	if (!alloc_cpumask_var(&attrs->cpumask, GFP_KERNEL))",
        "		goto fail;",
        "	if (!alloc_cpumask_var(&attrs->__pod_cpumask, GFP_KERNEL))",
        "		goto fail;",
        "",
        "	cpumask_copy(attrs->cpumask, cpu_possible_mask);",
        "	attrs->affn_scope = WQ_AFFN_DFL;",
        "	return attrs;",
        "fail:",
        "	free_workqueue_attrs(attrs);",
        "	return NULL;",
        "}",
        "",
        "static void copy_workqueue_attrs(struct workqueue_attrs *to,",
        "				 const struct workqueue_attrs *from)",
        "{",
        "	to->nice = from->nice;",
        "	cpumask_copy(to->cpumask, from->cpumask);",
        "	cpumask_copy(to->__pod_cpumask, from->__pod_cpumask);",
        "	to->affn_strict = from->affn_strict;",
        "",
        "	/*",
        "	 * Unlike hash and equality test, copying shouldn't ignore wq-only",
        "	 * fields as copying is used for both pool and wq attrs. Instead,",
        "	 * get_unbound_pool() explicitly clears the fields.",
        "	 */",
        "	to->affn_scope = from->affn_scope;",
        "	to->ordered = from->ordered;",
        "}",
        "",
        "/*",
        " * Some attrs fields are workqueue-only. Clear them for worker_pool's. See the",
        " * comments in 'struct workqueue_attrs' definition.",
        " */",
        "static void wqattrs_clear_for_pool(struct workqueue_attrs *attrs)",
        "{",
        "	attrs->affn_scope = WQ_AFFN_NR_TYPES;",
        "	attrs->ordered = false;",
        "	if (attrs->affn_strict)",
        "		cpumask_copy(attrs->cpumask, cpu_possible_mask);",
        "}",
        "",
        "/* hash value of the content of @attr */",
        "static u32 wqattrs_hash(const struct workqueue_attrs *attrs)",
        "{",
        "	u32 hash = 0;",
        "",
        "	hash = jhash_1word(attrs->nice, hash);",
        "	hash = jhash_1word(attrs->affn_strict, hash);",
        "	hash = jhash(cpumask_bits(attrs->__pod_cpumask),",
        "		     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);",
        "	if (!attrs->affn_strict)",
        "		hash = jhash(cpumask_bits(attrs->cpumask),",
        "			     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);",
        "	return hash;",
        "}",
        "",
        "/* content equality test */",
        "static bool wqattrs_equal(const struct workqueue_attrs *a,",
        "			  const struct workqueue_attrs *b)",
        "{",
        "	if (a->nice != b->nice)",
        "		return false;",
        "	if (a->affn_strict != b->affn_strict)",
        "		return false;",
        "	if (!cpumask_equal(a->__pod_cpumask, b->__pod_cpumask))",
        "		return false;",
        "	if (!a->affn_strict && !cpumask_equal(a->cpumask, b->cpumask))",
        "		return false;",
        "	return true;",
        "}",
        "",
        "/* Update @attrs with actually available CPUs */",
        "static void wqattrs_actualize_cpumask(struct workqueue_attrs *attrs,",
        "				      const cpumask_t *unbound_cpumask)",
        "{",
        "	/*",
        "	 * Calculate the effective CPU mask of @attrs given @unbound_cpumask. If",
        "	 * @attrs->cpumask doesn't overlap with @unbound_cpumask, we fallback to",
        "	 * @unbound_cpumask.",
        "	 */",
        "	cpumask_and(attrs->cpumask, attrs->cpumask, unbound_cpumask);",
        "	if (unlikely(cpumask_empty(attrs->cpumask)))",
        "		cpumask_copy(attrs->cpumask, unbound_cpumask);",
        "}",
        "",
        "/* find wq_pod_type to use for @attrs */",
        "static const struct wq_pod_type *",
        "wqattrs_pod_type(const struct workqueue_attrs *attrs)",
        "{",
        "	enum wq_affn_scope scope;",
        "	struct wq_pod_type *pt;",
        "",
        "	/* to synchronize access to wq_affn_dfl */",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	if (attrs->affn_scope == WQ_AFFN_DFL)",
        "		scope = wq_affn_dfl;",
        "	else",
        "		scope = attrs->affn_scope;",
        "",
        "	pt = &wq_pod_types[scope];",
        "",
        "	if (!WARN_ON_ONCE(attrs->affn_scope == WQ_AFFN_NR_TYPES) &&",
        "	    likely(pt->nr_pods))",
        "		return pt;",
        "",
        "	/*",
        "	 * Before workqueue_init_topology(), only SYSTEM is available which is",
        "	 * initialized in workqueue_init_early().",
        "	 */",
        "	pt = &wq_pod_types[WQ_AFFN_SYSTEM];",
        "	BUG_ON(!pt->nr_pods);",
        "	return pt;",
        "}",
        "",
        "/**",
        " * init_worker_pool - initialize a newly zalloc'd worker_pool",
        " * @pool: worker_pool to initialize",
        " *",
        " * Initialize a newly zalloc'd @pool.  It also allocates @pool->attrs.",
        " *",
        " * Return: 0 on success, -errno on failure.  Even on failure, all fields",
        " * inside @pool proper are initialized and put_unbound_pool() can be called",
        " * on @pool safely to release it.",
        " */",
        "static int init_worker_pool(struct worker_pool *pool)",
        "{",
        "	raw_spin_lock_init(&pool->lock);",
        "	pool->id = -1;",
        "	pool->cpu = -1;",
        "	pool->node = NUMA_NO_NODE;",
        "	pool->flags |= POOL_DISASSOCIATED;",
        "	pool->watchdog_ts = jiffies;",
        "	INIT_LIST_HEAD(&pool->worklist);",
        "	INIT_LIST_HEAD(&pool->idle_list);",
        "	hash_init(pool->busy_hash);",
        "",
        "	timer_setup(&pool->idle_timer, idle_worker_timeout, TIMER_DEFERRABLE);",
        "	INIT_WORK(&pool->idle_cull_work, idle_cull_fn);",
        "",
        "	timer_setup(&pool->mayday_timer, pool_mayday_timeout, 0);",
        "",
        "	INIT_LIST_HEAD(&pool->workers);",
        "",
        "	ida_init(&pool->worker_ida);",
        "	INIT_HLIST_NODE(&pool->hash_node);",
        "	pool->refcnt = 1;",
        "",
        "	/* shouldn't fail above this point */",
        "	pool->attrs = alloc_workqueue_attrs();",
        "	if (!pool->attrs)",
        "		return -ENOMEM;",
        "",
        "	wqattrs_clear_for_pool(pool->attrs);",
        "",
        "	return 0;",
        "}",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "static void wq_init_lockdep(struct workqueue_struct *wq)",
        "{",
        "	char *lock_name;",
        "",
        "	lockdep_register_key(&wq->key);",
        "	lock_name = kasprintf(GFP_KERNEL, \"%s%s\", \"(wq_completion)\", wq->name);",
        "	if (!lock_name)",
        "		lock_name = wq->name;",
        "",
        "	wq->lock_name = lock_name;",
        "	wq->lockdep_map = &wq->__lockdep_map;",
        "	lockdep_init_map(wq->lockdep_map, lock_name, &wq->key, 0);",
        "}",
        "",
        "static void wq_unregister_lockdep(struct workqueue_struct *wq)",
        "{",
        "	if (wq->lockdep_map != &wq->__lockdep_map)",
        "		return;",
        "",
        "	lockdep_unregister_key(&wq->key);",
        "}",
        "",
        "static void wq_free_lockdep(struct workqueue_struct *wq)",
        "{",
        "	if (wq->lockdep_map != &wq->__lockdep_map)",
        "		return;",
        "",
        "	if (wq->lock_name != wq->name)",
        "		kfree(wq->lock_name);",
        "}",
        "#else",
        "static void wq_init_lockdep(struct workqueue_struct *wq)",
        "{",
        "}",
        "",
        "static void wq_unregister_lockdep(struct workqueue_struct *wq)",
        "{",
        "}",
        "",
        "static void wq_free_lockdep(struct workqueue_struct *wq)",
        "{",
        "}",
        "#endif",
        "",
        "static void free_node_nr_active(struct wq_node_nr_active **nna_ar)",
        "{",
        "	int node;",
        "",
        "	for_each_node(node) {",
        "		kfree(nna_ar[node]);",
        "		nna_ar[node] = NULL;",
        "	}",
        "",
        "	kfree(nna_ar[nr_node_ids]);",
        "	nna_ar[nr_node_ids] = NULL;",
        "}",
        "",
        "static void init_node_nr_active(struct wq_node_nr_active *nna)",
        "{",
        "	nna->max = WQ_DFL_MIN_ACTIVE;",
        "	atomic_set(&nna->nr, 0);",
        "	raw_spin_lock_init(&nna->lock);",
        "	INIT_LIST_HEAD(&nna->pending_pwqs);",
        "}",
        "",
        "/*",
        " * Each node's nr_active counter will be accessed mostly from its own node and",
        " * should be allocated in the node.",
        " */",
        "static int alloc_node_nr_active(struct wq_node_nr_active **nna_ar)",
        "{",
        "	struct wq_node_nr_active *nna;",
        "	int node;",
        "",
        "	for_each_node(node) {",
        "		nna = kzalloc_node(sizeof(*nna), GFP_KERNEL, node);",
        "		if (!nna)",
        "			goto err_free;",
        "		init_node_nr_active(nna);",
        "		nna_ar[node] = nna;",
        "	}",
        "",
        "	/* [nr_node_ids] is used as the fallback */",
        "	nna = kzalloc_node(sizeof(*nna), GFP_KERNEL, NUMA_NO_NODE);",
        "	if (!nna)",
        "		goto err_free;",
        "	init_node_nr_active(nna);",
        "	nna_ar[nr_node_ids] = nna;",
        "",
        "	return 0;",
        "",
        "err_free:",
        "	free_node_nr_active(nna_ar);",
        "	return -ENOMEM;",
        "}",
        "",
        "static void rcu_free_wq(struct rcu_head *rcu)",
        "{",
        "	struct workqueue_struct *wq =",
        "		container_of(rcu, struct workqueue_struct, rcu);",
        "",
        "	if (wq->flags & WQ_UNBOUND)",
        "		free_node_nr_active(wq->node_nr_active);",
        "",
        "	wq_free_lockdep(wq);",
        "	free_percpu(wq->cpu_pwq);",
        "	free_workqueue_attrs(wq->unbound_attrs);",
        "	kfree(wq);",
        "}",
        "",
        "static void rcu_free_pool(struct rcu_head *rcu)",
        "{",
        "	struct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);",
        "",
        "	ida_destroy(&pool->worker_ida);",
        "	free_workqueue_attrs(pool->attrs);",
        "	kfree(pool);",
        "}",
        "",
        "/**",
        " * put_unbound_pool - put a worker_pool",
        " * @pool: worker_pool to put",
        " *",
        " * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU",
        " * safe manner.  get_unbound_pool() calls this function on its failure path",
        " * and this function should be able to release pools which went through,",
        " * successfully or not, init_worker_pool().",
        " *",
        " * Should be called with wq_pool_mutex held.",
        " */",
        "static void put_unbound_pool(struct worker_pool *pool)",
        "{",
        "	struct worker *worker;",
        "	LIST_HEAD(cull_list);",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	if (--pool->refcnt)",
        "		return;",
        "",
        "	/* sanity checks */",
        "	if (WARN_ON(!(pool->cpu < 0)) ||",
        "	    WARN_ON(!list_empty(&pool->worklist)))",
        "		return;",
        "",
        "	/* release id and unhash */",
        "	if (pool->id >= 0)",
        "		idr_remove(&worker_pool_idr, pool->id);",
        "	hash_del(&pool->hash_node);",
        "",
        "	/*",
        "	 * Become the manager and destroy all workers.  This prevents",
        "	 * @pool's workers from blocking on attach_mutex.  We're the last",
        "	 * manager and @pool gets freed with the flag set.",
        "	 *",
        "	 * Having a concurrent manager is quite unlikely to happen as we can",
        "	 * only get here with",
        "	 *   pwq->refcnt == pool->refcnt == 0",
        "	 * which implies no work queued to the pool, which implies no worker can",
        "	 * become the manager. However a worker could have taken the role of",
        "	 * manager before the refcnts dropped to 0, since maybe_create_worker()",
        "	 * drops pool->lock",
        "	 */",
        "	while (true) {",
        "		rcuwait_wait_event(&manager_wait,",
        "				   !(pool->flags & POOL_MANAGER_ACTIVE),",
        "				   TASK_UNINTERRUPTIBLE);",
        "",
        "		mutex_lock(&wq_pool_attach_mutex);",
        "		raw_spin_lock_irq(&pool->lock);",
        "		if (!(pool->flags & POOL_MANAGER_ACTIVE)) {",
        "			pool->flags |= POOL_MANAGER_ACTIVE;",
        "			break;",
        "		}",
        "		raw_spin_unlock_irq(&pool->lock);",
        "		mutex_unlock(&wq_pool_attach_mutex);",
        "	}",
        "",
        "	while ((worker = first_idle_worker(pool)))",
        "		set_worker_dying(worker, &cull_list);",
        "	WARN_ON(pool->nr_workers || pool->nr_idle);",
        "	raw_spin_unlock_irq(&pool->lock);",
        "",
        "	detach_dying_workers(&cull_list);",
        "",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "",
        "	reap_dying_workers(&cull_list);",
        "",
        "	/* shut down the timers */",
        "	del_timer_sync(&pool->idle_timer);",
        "	cancel_work_sync(&pool->idle_cull_work);",
        "	del_timer_sync(&pool->mayday_timer);",
        "",
        "	/* RCU protected to allow dereferences from get_work_pool() */",
        "	call_rcu(&pool->rcu, rcu_free_pool);",
        "}",
        "",
        "/**",
        " * get_unbound_pool - get a worker_pool with the specified attributes",
        " * @attrs: the attributes of the worker_pool to get",
        " *",
        " * Obtain a worker_pool which has the same attributes as @attrs, bump the",
        " * reference count and return it.  If there already is a matching",
        " * worker_pool, it will be used; otherwise, this function attempts to",
        " * create a new one.",
        " *",
        " * Should be called with wq_pool_mutex held.",
        " *",
        " * Return: On success, a worker_pool with the same attributes as @attrs.",
        " * On failure, %NULL.",
        " */",
        "static struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)",
        "{",
        "	struct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_NUMA];",
        "	u32 hash = wqattrs_hash(attrs);",
        "	struct worker_pool *pool;",
        "	int pod, node = NUMA_NO_NODE;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	/* do we already have a matching pool? */",
        "	hash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {",
        "		if (wqattrs_equal(pool->attrs, attrs)) {",
        "			pool->refcnt++;",
        "			return pool;",
        "		}",
        "	}",
        "",
        "	/* If __pod_cpumask is contained inside a NUMA pod, that's our node */",
        "	for (pod = 0; pod < pt->nr_pods; pod++) {",
        "		if (cpumask_subset(attrs->__pod_cpumask, pt->pod_cpus[pod])) {",
        "			node = pt->pod_node[pod];",
        "			break;",
        "		}",
        "	}",
        "",
        "	/* nope, create a new one */",
        "	pool = kzalloc_node(sizeof(*pool), GFP_KERNEL, node);",
        "	if (!pool || init_worker_pool(pool) < 0)",
        "		goto fail;",
        "",
        "	pool->node = node;",
        "	copy_workqueue_attrs(pool->attrs, attrs);",
        "	wqattrs_clear_for_pool(pool->attrs);",
        "",
        "	if (worker_pool_assign_id(pool) < 0)",
        "		goto fail;",
        "",
        "	/* create and start the initial worker */",
        "	if (wq_online && !create_worker(pool))",
        "		goto fail;",
        "",
        "	/* install */",
        "	hash_add(unbound_pool_hash, &pool->hash_node, hash);",
        "",
        "	return pool;",
        "fail:",
        "	if (pool)",
        "		put_unbound_pool(pool);",
        "	return NULL;",
        "}",
        "",
        "/*",
        " * Scheduled on pwq_release_worker by put_pwq() when an unbound pwq hits zero",
        " * refcnt and needs to be destroyed.",
        " */",
        "static void pwq_release_workfn(struct kthread_work *work)",
        "{",
        "	struct pool_workqueue *pwq = container_of(work, struct pool_workqueue,",
        "						  release_work);",
        "	struct workqueue_struct *wq = pwq->wq;",
        "	struct worker_pool *pool = pwq->pool;",
        "	bool is_last = false;",
        "",
        "	/*",
        "	 * When @pwq is not linked, it doesn't hold any reference to the",
        "	 * @wq, and @wq is invalid to access.",
        "	 */",
        "	if (!list_empty(&pwq->pwqs_node)) {",
        "		mutex_lock(&wq->mutex);",
        "		list_del_rcu(&pwq->pwqs_node);",
        "		is_last = list_empty(&wq->pwqs);",
        "",
        "		/*",
        "		 * For ordered workqueue with a plugged dfl_pwq, restart it now.",
        "		 */",
        "		if (!is_last && (wq->flags & __WQ_ORDERED))",
        "			unplug_oldest_pwq(wq);",
        "",
        "		mutex_unlock(&wq->mutex);",
        "	}",
        "",
        "	if (wq->flags & WQ_UNBOUND) {",
        "		mutex_lock(&wq_pool_mutex);",
        "		put_unbound_pool(pool);",
        "		mutex_unlock(&wq_pool_mutex);",
        "	}",
        "",
        "	if (!list_empty(&pwq->pending_node)) {",
        "		struct wq_node_nr_active *nna =",
        "			wq_node_nr_active(pwq->wq, pwq->pool->node);",
        "",
        "		raw_spin_lock_irq(&nna->lock);",
        "		list_del_init(&pwq->pending_node);",
        "		raw_spin_unlock_irq(&nna->lock);",
        "	}",
        "",
        "	kfree_rcu(pwq, rcu);",
        "",
        "	/*",
        "	 * If we're the last pwq going away, @wq is already dead and no one",
        "	 * is gonna access it anymore.  Schedule RCU free.",
        "	 */",
        "	if (is_last) {",
        "		wq_unregister_lockdep(wq);",
        "		call_rcu(&wq->rcu, rcu_free_wq);",
        "	}",
        "}",
        "",
        "/* initialize newly allocated @pwq which is associated with @wq and @pool */",
        "static void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,",
        "		     struct worker_pool *pool)",
        "{",
        "	BUG_ON((unsigned long)pwq & ~WORK_STRUCT_PWQ_MASK);",
        "",
        "	memset(pwq, 0, sizeof(*pwq));",
        "",
        "	pwq->pool = pool;",
        "	pwq->wq = wq;",
        "	pwq->flush_color = -1;",
        "	pwq->refcnt = 1;",
        "	INIT_LIST_HEAD(&pwq->inactive_works);",
        "	INIT_LIST_HEAD(&pwq->pending_node);",
        "	INIT_LIST_HEAD(&pwq->pwqs_node);",
        "	INIT_LIST_HEAD(&pwq->mayday_node);",
        "	kthread_init_work(&pwq->release_work, pwq_release_workfn);",
        "}",
        "",
        "/* sync @pwq with the current state of its associated wq and link it */",
        "static void link_pwq(struct pool_workqueue *pwq)",
        "{",
        "	struct workqueue_struct *wq = pwq->wq;",
        "",
        "	lockdep_assert_held(&wq->mutex);",
        "",
        "	/* may be called multiple times, ignore if already linked */",
        "	if (!list_empty(&pwq->pwqs_node))",
        "		return;",
        "",
        "	/* set the matching work_color */",
        "	pwq->work_color = wq->work_color;",
        "",
        "	/* link in @pwq */",
        "	list_add_tail_rcu(&pwq->pwqs_node, &wq->pwqs);",
        "}",
        "",
        "/* obtain a pool matching @attr and create a pwq associating the pool and @wq */",
        "static struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,",
        "					const struct workqueue_attrs *attrs)",
        "{",
        "	struct worker_pool *pool;",
        "	struct pool_workqueue *pwq;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	pool = get_unbound_pool(attrs);",
        "	if (!pool)",
        "		return NULL;",
        "",
        "	pwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);",
        "	if (!pwq) {",
        "		put_unbound_pool(pool);",
        "		return NULL;",
        "	}",
        "",
        "	init_pwq(pwq, wq, pool);",
        "	return pwq;",
        "}",
        "",
        "static void apply_wqattrs_lock(void)",
        "{",
        "	mutex_lock(&wq_pool_mutex);",
        "}",
        "",
        "static void apply_wqattrs_unlock(void)",
        "{",
        "	mutex_unlock(&wq_pool_mutex);",
        "}",
        "",
        "/**",
        " * wq_calc_pod_cpumask - calculate a wq_attrs' cpumask for a pod",
        " * @attrs: the wq_attrs of the default pwq of the target workqueue",
        " * @cpu: the target CPU",
        " *",
        " * Calculate the cpumask a workqueue with @attrs should use on @pod.",
        " * The result is stored in @attrs->__pod_cpumask.",
        " *",
        " * If pod affinity is not enabled, @attrs->cpumask is always used. If enabled",
        " * and @pod has online CPUs requested by @attrs, the returned cpumask is the",
        " * intersection of the possible CPUs of @pod and @attrs->cpumask.",
        " *",
        " * The caller is responsible for ensuring that the cpumask of @pod stays stable.",
        " */",
        "static void wq_calc_pod_cpumask(struct workqueue_attrs *attrs, int cpu)",
        "{",
        "	const struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
        "	int pod = pt->cpu_pod[cpu];",
        "",
        "	/* calculate possible CPUs in @pod that @attrs wants */",
        "	cpumask_and(attrs->__pod_cpumask, pt->pod_cpus[pod], attrs->cpumask);",
        "	/* does @pod have any online CPUs @attrs wants? */",
        "	if (!cpumask_intersects(attrs->__pod_cpumask, wq_online_cpumask)) {",
        "		cpumask_copy(attrs->__pod_cpumask, attrs->cpumask);",
        "		return;",
        "	}",
        "}",
        "",
        "/* install @pwq into @wq and return the old pwq, @cpu < 0 for dfl_pwq */",
        "static struct pool_workqueue *install_unbound_pwq(struct workqueue_struct *wq,",
        "					int cpu, struct pool_workqueue *pwq)",
        "{",
        "	struct pool_workqueue __rcu **slot = unbound_pwq_slot(wq, cpu);",
        "	struct pool_workqueue *old_pwq;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "	lockdep_assert_held(&wq->mutex);",
        "",
        "	/* link_pwq() can handle duplicate calls */",
        "	link_pwq(pwq);",
        "",
        "	old_pwq = rcu_access_pointer(*slot);",
        "	rcu_assign_pointer(*slot, pwq);",
        "	return old_pwq;",
        "}",
        "",
        "/* context to store the prepared attrs & pwqs before applying */",
        "struct apply_wqattrs_ctx {",
        "	struct workqueue_struct	*wq;		/* target workqueue */",
        "	struct workqueue_attrs	*attrs;		/* attrs to apply */",
        "	struct list_head	list;		/* queued for batching commit */",
        "	struct pool_workqueue	*dfl_pwq;",
        "	struct pool_workqueue	*pwq_tbl[];",
        "};",
        "",
        "/* free the resources after success or abort */",
        "static void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)",
        "{",
        "	if (ctx) {",
        "		int cpu;",
        "",
        "		for_each_possible_cpu(cpu)",
        "			put_pwq_unlocked(ctx->pwq_tbl[cpu]);",
        "		put_pwq_unlocked(ctx->dfl_pwq);",
        "",
        "		free_workqueue_attrs(ctx->attrs);",
        "",
        "		kfree(ctx);",
        "	}",
        "}",
        "",
        "/* allocate the attrs and pwqs for later installation */",
        "static struct apply_wqattrs_ctx *",
        "apply_wqattrs_prepare(struct workqueue_struct *wq,",
        "		      const struct workqueue_attrs *attrs,",
        "		      const cpumask_var_t unbound_cpumask)",
        "{",
        "	struct apply_wqattrs_ctx *ctx;",
        "	struct workqueue_attrs *new_attrs;",
        "	int cpu;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	if (WARN_ON(attrs->affn_scope < 0 ||",
        "		    attrs->affn_scope >= WQ_AFFN_NR_TYPES))",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	ctx = kzalloc(struct_size(ctx, pwq_tbl, nr_cpu_ids), GFP_KERNEL);",
        "",
        "	new_attrs = alloc_workqueue_attrs();",
        "	if (!ctx || !new_attrs)",
        "		goto out_free;",
        "",
        "	/*",
        "	 * If something goes wrong during CPU up/down, we'll fall back to",
        "	 * the default pwq covering whole @attrs->cpumask.  Always create",
        "	 * it even if we don't use it immediately.",
        "	 */",
        "	copy_workqueue_attrs(new_attrs, attrs);",
        "	wqattrs_actualize_cpumask(new_attrs, unbound_cpumask);",
        "	cpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);",
        "	ctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);",
        "	if (!ctx->dfl_pwq)",
        "		goto out_free;",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		if (new_attrs->ordered) {",
        "			ctx->dfl_pwq->refcnt++;",
        "			ctx->pwq_tbl[cpu] = ctx->dfl_pwq;",
        "		} else {",
        "			wq_calc_pod_cpumask(new_attrs, cpu);",
        "			ctx->pwq_tbl[cpu] = alloc_unbound_pwq(wq, new_attrs);",
        "			if (!ctx->pwq_tbl[cpu])",
        "				goto out_free;",
        "		}",
        "	}",
        "",
        "	/* save the user configured attrs and sanitize it. */",
        "	copy_workqueue_attrs(new_attrs, attrs);",
        "	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);",
        "	cpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);",
        "	ctx->attrs = new_attrs;",
        "",
        "	/*",
        "	 * For initialized ordered workqueues, there should only be one pwq",
        "	 * (dfl_pwq). Set the plugged flag of ctx->dfl_pwq to suspend execution",
        "	 * of newly queued work items until execution of older work items in",
        "	 * the old pwq's have completed.",
        "	 */",
        "	if ((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs))",
        "		ctx->dfl_pwq->plugged = true;",
        "",
        "	ctx->wq = wq;",
        "	return ctx;",
        "",
        "out_free:",
        "	free_workqueue_attrs(new_attrs);",
        "	apply_wqattrs_cleanup(ctx);",
        "	return ERR_PTR(-ENOMEM);",
        "}",
        "",
        "/* set attrs and install prepared pwqs, @ctx points to old pwqs on return */",
        "static void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)",
        "{",
        "	int cpu;",
        "",
        "	/* all pwqs have been created successfully, let's install'em */",
        "	mutex_lock(&ctx->wq->mutex);",
        "",
        "	copy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);",
        "",
        "	/* save the previous pwqs and install the new ones */",
        "	for_each_possible_cpu(cpu)",
        "		ctx->pwq_tbl[cpu] = install_unbound_pwq(ctx->wq, cpu,",
        "							ctx->pwq_tbl[cpu]);",
        "	ctx->dfl_pwq = install_unbound_pwq(ctx->wq, -1, ctx->dfl_pwq);",
        "",
        "	/* update node_nr_active->max */",
        "	wq_update_node_max_active(ctx->wq, -1);",
        "",
        "	/* rescuer needs to respect wq cpumask changes */",
        "	if (ctx->wq->rescuer)",
        "		set_cpus_allowed_ptr(ctx->wq->rescuer->task,",
        "				     unbound_effective_cpumask(ctx->wq));",
        "",
        "	mutex_unlock(&ctx->wq->mutex);",
        "}",
        "",
        "static int apply_workqueue_attrs_locked(struct workqueue_struct *wq,",
        "					const struct workqueue_attrs *attrs)",
        "{",
        "	struct apply_wqattrs_ctx *ctx;",
        "",
        "	/* only unbound workqueues can change attributes */",
        "	if (WARN_ON(!(wq->flags & WQ_UNBOUND)))",
        "		return -EINVAL;",
        "",
        "	ctx = apply_wqattrs_prepare(wq, attrs, wq_unbound_cpumask);",
        "	if (IS_ERR(ctx))",
        "		return PTR_ERR(ctx);",
        "",
        "	/* the ctx has been prepared successfully, let's commit it */",
        "	apply_wqattrs_commit(ctx);",
        "	apply_wqattrs_cleanup(ctx);",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue",
        " * @wq: the target workqueue",
        " * @attrs: the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()",
        " *",
        " * Apply @attrs to an unbound workqueue @wq. Unless disabled, this function maps",
        " * a separate pwq to each CPU pod with possibles CPUs in @attrs->cpumask so that",
        " * work items are affine to the pod it was issued on. Older pwqs are released as",
        " * in-flight work items finish. Note that a work item which repeatedly requeues",
        " * itself back-to-back will stay on its current pwq.",
        " *",
        " * Performs GFP_KERNEL allocations.",
        " *",
        " * Return: 0 on success and -errno on failure.",
        " */",
        "int apply_workqueue_attrs(struct workqueue_struct *wq,",
        "			  const struct workqueue_attrs *attrs)",
        "{",
        "	int ret;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "	ret = apply_workqueue_attrs_locked(wq, attrs);",
        "	mutex_unlock(&wq_pool_mutex);",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " * unbound_wq_update_pwq - update a pwq slot for CPU hot[un]plug",
        " * @wq: the target workqueue",
        " * @cpu: the CPU to update the pwq slot for",
        " *",
        " * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and",
        " * %CPU_DOWN_FAILED.  @cpu is in the same pod of the CPU being hot[un]plugged.",
        " *",
        " *",
        " * If pod affinity can't be adjusted due to memory allocation failure, it falls",
        " * back to @wq->dfl_pwq which may not be optimal but is always correct.",
        " *",
        " * Note that when the last allowed CPU of a pod goes offline for a workqueue",
        " * with a cpumask spanning multiple pods, the workers which were already",
        " * executing the work items for the workqueue will lose their CPU affinity and",
        " * may execute on any CPU. This is similar to how per-cpu workqueues behave on",
        " * CPU_DOWN. If a workqueue user wants strict affinity, it's the user's",
        " * responsibility to flush the work item from CPU_DOWN_PREPARE.",
        " */",
        "static void unbound_wq_update_pwq(struct workqueue_struct *wq, int cpu)",
        "{",
        "	struct pool_workqueue *old_pwq = NULL, *pwq;",
        "	struct workqueue_attrs *target_attrs;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	if (!(wq->flags & WQ_UNBOUND) || wq->unbound_attrs->ordered)",
        "		return;",
        "",
        "	/*",
        "	 * We don't wanna alloc/free wq_attrs for each wq for each CPU.",
        "	 * Let's use a preallocated one.  The following buf is protected by",
        "	 * CPU hotplug exclusion.",
        "	 */",
        "	target_attrs = unbound_wq_update_pwq_attrs_buf;",
        "",
        "	copy_workqueue_attrs(target_attrs, wq->unbound_attrs);",
        "	wqattrs_actualize_cpumask(target_attrs, wq_unbound_cpumask);",
        "",
        "	/* nothing to do if the target cpumask matches the current pwq */",
        "	wq_calc_pod_cpumask(target_attrs, cpu);",
        "	if (wqattrs_equal(target_attrs, unbound_pwq(wq, cpu)->pool->attrs))",
        "		return;",
        "",
        "	/* create a new pwq */",
        "	pwq = alloc_unbound_pwq(wq, target_attrs);",
        "	if (!pwq) {",
        "		pr_warn(\"workqueue: allocation failed while updating CPU pod affinity of \\\"%s\\\"\\n\",",
        "			wq->name);",
        "		goto use_dfl_pwq;",
        "	}",
        "",
        "	/* Install the new pwq. */",
        "	mutex_lock(&wq->mutex);",
        "	old_pwq = install_unbound_pwq(wq, cpu, pwq);",
        "	goto out_unlock;",
        "",
        "use_dfl_pwq:",
        "	mutex_lock(&wq->mutex);",
        "	pwq = unbound_pwq(wq, -1);",
        "	raw_spin_lock_irq(&pwq->pool->lock);",
        "	get_pwq(pwq);",
        "	raw_spin_unlock_irq(&pwq->pool->lock);",
        "	old_pwq = install_unbound_pwq(wq, cpu, pwq);",
        "out_unlock:",
        "	mutex_unlock(&wq->mutex);",
        "	put_pwq_unlocked(old_pwq);",
        "}",
        "",
        "static int alloc_and_link_pwqs(struct workqueue_struct *wq)",
        "{",
        "	bool highpri = wq->flags & WQ_HIGHPRI;",
        "	int cpu, ret;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	wq->cpu_pwq = alloc_percpu(struct pool_workqueue *);",
        "	if (!wq->cpu_pwq)",
        "		goto enomem;",
        "",
        "	if (!(wq->flags & WQ_UNBOUND)) {",
        "		struct worker_pool __percpu *pools;",
        "",
        "		if (wq->flags & WQ_BH)",
        "			pools = bh_worker_pools;",
        "		else",
        "			pools = cpu_worker_pools;",
        "",
        "		for_each_possible_cpu(cpu) {",
        "			struct pool_workqueue **pwq_p;",
        "			struct worker_pool *pool;",
        "",
        "			pool = &(per_cpu_ptr(pools, cpu)[highpri]);",
        "			pwq_p = per_cpu_ptr(wq->cpu_pwq, cpu);",
        "",
        "			*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,",
        "						       pool->node);",
        "			if (!*pwq_p)",
        "				goto enomem;",
        "",
        "			init_pwq(*pwq_p, wq, pool);",
        "",
        "			mutex_lock(&wq->mutex);",
        "			link_pwq(*pwq_p);",
        "			mutex_unlock(&wq->mutex);",
        "		}",
        "		return 0;",
        "	}",
        "",
        "	if (wq->flags & __WQ_ORDERED) {",
        "		struct pool_workqueue *dfl_pwq;",
        "",
        "		ret = apply_workqueue_attrs_locked(wq, ordered_wq_attrs[highpri]);",
        "		/* there should only be single pwq for ordering guarantee */",
        "		dfl_pwq = rcu_access_pointer(wq->dfl_pwq);",
        "		WARN(!ret && (wq->pwqs.next != &dfl_pwq->pwqs_node ||",
        "			      wq->pwqs.prev != &dfl_pwq->pwqs_node),",
        "		     \"ordering guarantee broken for workqueue %s\\n\", wq->name);",
        "	} else {",
        "		ret = apply_workqueue_attrs_locked(wq, unbound_std_wq_attrs[highpri]);",
        "	}",
        "",
        "	return ret;",
        "",
        "enomem:",
        "	if (wq->cpu_pwq) {",
        "		for_each_possible_cpu(cpu) {",
        "			struct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);",
        "",
        "			if (pwq)",
        "				kmem_cache_free(pwq_cache, pwq);",
        "		}",
        "		free_percpu(wq->cpu_pwq);",
        "		wq->cpu_pwq = NULL;",
        "	}",
        "	return -ENOMEM;",
        "}",
        "",
        "static int wq_clamp_max_active(int max_active, unsigned int flags,",
        "			       const char *name)",
        "{",
        "	if (max_active < 1 || max_active > WQ_MAX_ACTIVE)",
        "		pr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",",
        "			max_active, name, 1, WQ_MAX_ACTIVE);",
        "",
        "	return clamp_val(max_active, 1, WQ_MAX_ACTIVE);",
        "}",
        "",
        "/*",
        " * Workqueues which may be used during memory reclaim should have a rescuer",
        " * to guarantee forward progress.",
        " */",
        "static int init_rescuer(struct workqueue_struct *wq)",
        "{",
        "	struct worker *rescuer;",
        "	char id_buf[WORKER_ID_LEN];",
        "	int ret;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	if (!(wq->flags & WQ_MEM_RECLAIM))",
        "		return 0;",
        "",
        "	rescuer = alloc_worker(NUMA_NO_NODE);",
        "	if (!rescuer) {",
        "		pr_err(\"workqueue: Failed to allocate a rescuer for wq \\\"%s\\\"\\n\",",
        "		       wq->name);",
        "		return -ENOMEM;",
        "	}",
        "",
        "	rescuer->rescue_wq = wq;",
        "	format_worker_id(id_buf, sizeof(id_buf), rescuer, NULL);",
        "",
        "	rescuer->task = kthread_create(rescuer_thread, rescuer, \"%s\", id_buf);",
        "	if (IS_ERR(rescuer->task)) {",
        "		ret = PTR_ERR(rescuer->task);",
        "		pr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\"%s\\\": %pe\",",
        "		       wq->name, ERR_PTR(ret));",
        "		kfree(rescuer);",
        "		return ret;",
        "	}",
        "",
        "	wq->rescuer = rescuer;",
        "	if (wq->flags & WQ_UNBOUND)",
        "		kthread_bind_mask(rescuer->task, unbound_effective_cpumask(wq));",
        "	else",
        "		kthread_bind_mask(rescuer->task, cpu_possible_mask);",
        "	wake_up_process(rescuer->task);",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * wq_adjust_max_active - update a wq's max_active to the current setting",
        " * @wq: target workqueue",
        " *",
        " * If @wq isn't freezing, set @wq->max_active to the saved_max_active and",
        " * activate inactive work items accordingly. If @wq is freezing, clear",
        " * @wq->max_active to zero.",
        " */",
        "static void wq_adjust_max_active(struct workqueue_struct *wq)",
        "{",
        "	bool activated;",
        "	int new_max, new_min;",
        "",
        "	lockdep_assert_held(&wq->mutex);",
        "",
        "	if ((wq->flags & WQ_FREEZABLE) && workqueue_freezing) {",
        "		new_max = 0;",
        "		new_min = 0;",
        "	} else {",
        "		new_max = wq->saved_max_active;",
        "		new_min = wq->saved_min_active;",
        "	}",
        "",
        "	if (wq->max_active == new_max && wq->min_active == new_min)",
        "		return;",
        "",
        "	/*",
        "	 * Update @wq->max/min_active and then kick inactive work items if more",
        "	 * active work items are allowed. This doesn't break work item ordering",
        "	 * because new work items are always queued behind existing inactive",
        "	 * work items if there are any.",
        "	 */",
        "	WRITE_ONCE(wq->max_active, new_max);",
        "	WRITE_ONCE(wq->min_active, new_min);",
        "",
        "	if (wq->flags & WQ_UNBOUND)",
        "		wq_update_node_max_active(wq, -1);",
        "",
        "	if (new_max == 0)",
        "		return;",
        "",
        "	/*",
        "	 * Round-robin through pwq's activating the first inactive work item",
        "	 * until max_active is filled.",
        "	 */",
        "	do {",
        "		struct pool_workqueue *pwq;",
        "",
        "		activated = false;",
        "		for_each_pwq(pwq, wq) {",
        "			unsigned long irq_flags;",
        "",
        "			/* can be called during early boot w/ irq disabled */",
        "			raw_spin_lock_irqsave(&pwq->pool->lock, irq_flags);",
        "			if (pwq_activate_first_inactive(pwq, true)) {",
        "				activated = true;",
        "				kick_pool(pwq->pool);",
        "			}",
        "			raw_spin_unlock_irqrestore(&pwq->pool->lock, irq_flags);",
        "		}",
        "	} while (activated);",
        "}",
        "",
        "__printf(1, 0)",
        "static struct workqueue_struct *__alloc_workqueue(const char *fmt,",
        "						  unsigned int flags,",
        "						  int max_active, va_list args)",
        "{",
        "	struct workqueue_struct *wq;",
        "	size_t wq_size;",
        "	int name_len;",
        "",
        "	if (flags & WQ_BH) {",
        "		if (WARN_ON_ONCE(flags & ~__WQ_BH_ALLOWS))",
        "			return NULL;",
        "		if (WARN_ON_ONCE(max_active))",
        "			return NULL;",
        "	}",
        "",
        "	/* see the comment above the definition of WQ_POWER_EFFICIENT */",
        "	if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)",
        "		flags |= WQ_UNBOUND;",
        "",
        "	/* allocate wq and format name */",
        "	if (flags & WQ_UNBOUND)",
        "		wq_size = struct_size(wq, node_nr_active, nr_node_ids + 1);",
        "	else",
        "		wq_size = sizeof(*wq);",
        "",
        "	wq = kzalloc(wq_size, GFP_KERNEL);",
        "	if (!wq)",
        "		return NULL;",
        "",
        "	if (flags & WQ_UNBOUND) {",
        "		wq->unbound_attrs = alloc_workqueue_attrs();",
        "		if (!wq->unbound_attrs)",
        "			goto err_free_wq;",
        "	}",
        "",
        "	name_len = vsnprintf(wq->name, sizeof(wq->name), fmt, args);",
        "",
        "	if (name_len >= WQ_NAME_LEN)",
        "		pr_warn_once(\"workqueue: name exceeds WQ_NAME_LEN. Truncating to: %s\\n\",",
        "			     wq->name);",
        "",
        "	if (flags & WQ_BH) {",
        "		/*",
        "		 * BH workqueues always share a single execution context per CPU",
        "		 * and don't impose any max_active limit.",
        "		 */",
        "		max_active = INT_MAX;",
        "	} else {",
        "		max_active = max_active ?: WQ_DFL_ACTIVE;",
        "		max_active = wq_clamp_max_active(max_active, flags, wq->name);",
        "	}",
        "",
        "	/* init wq */",
        "	wq->flags = flags;",
        "	wq->max_active = max_active;",
        "	wq->min_active = min(max_active, WQ_DFL_MIN_ACTIVE);",
        "	wq->saved_max_active = wq->max_active;",
        "	wq->saved_min_active = wq->min_active;",
        "	mutex_init(&wq->mutex);",
        "	atomic_set(&wq->nr_pwqs_to_flush, 0);",
        "	INIT_LIST_HEAD(&wq->pwqs);",
        "	INIT_LIST_HEAD(&wq->flusher_queue);",
        "	INIT_LIST_HEAD(&wq->flusher_overflow);",
        "	INIT_LIST_HEAD(&wq->maydays);",
        "",
        "	INIT_LIST_HEAD(&wq->list);",
        "",
        "	if (flags & WQ_UNBOUND) {",
        "		if (alloc_node_nr_active(wq->node_nr_active) < 0)",
        "			goto err_free_wq;",
        "	}",
        "",
        "	/*",
        "	 * wq_pool_mutex protects the workqueues list, allocations of PWQs,",
        "	 * and the global freeze state.",
        "	 */",
        "	apply_wqattrs_lock();",
        "",
        "	if (alloc_and_link_pwqs(wq) < 0)",
        "		goto err_unlock_free_node_nr_active;",
        "",
        "	mutex_lock(&wq->mutex);",
        "	wq_adjust_max_active(wq);",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	list_add_tail_rcu(&wq->list, &workqueues);",
        "",
        "	if (wq_online && init_rescuer(wq) < 0)",
        "		goto err_unlock_destroy;",
        "",
        "	apply_wqattrs_unlock();",
        "",
        "	if ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))",
        "		goto err_destroy;",
        "",
        "	return wq;",
        "",
        "err_unlock_free_node_nr_active:",
        "	apply_wqattrs_unlock();",
        "	/*",
        "	 * Failed alloc_and_link_pwqs() may leave pending pwq->release_work,",
        "	 * flushing the pwq_release_worker ensures that the pwq_release_workfn()",
        "	 * completes before calling kfree(wq).",
        "	 */",
        "	if (wq->flags & WQ_UNBOUND) {",
        "		kthread_flush_worker(pwq_release_worker);",
        "		free_node_nr_active(wq->node_nr_active);",
        "	}",
        "err_free_wq:",
        "	free_workqueue_attrs(wq->unbound_attrs);",
        "	kfree(wq);",
        "	return NULL;",
        "err_unlock_destroy:",
        "	apply_wqattrs_unlock();",
        "err_destroy:",
        "	destroy_workqueue(wq);",
        "	return NULL;",
        "}",
        "",
        "__printf(1, 4)",
        "struct workqueue_struct *alloc_workqueue(const char *fmt,",
        "					 unsigned int flags,",
        "					 int max_active, ...)",
        "{",
        "	struct workqueue_struct *wq;",
        "	va_list args;",
        "",
        "	va_start(args, max_active);",
        "	wq = __alloc_workqueue(fmt, flags, max_active, args);",
        "	va_end(args);",
        "	if (!wq)",
        "		return NULL;",
        "",
        "	wq_init_lockdep(wq);",
        "",
        "	return wq;",
        "}",
        "EXPORT_SYMBOL_GPL(alloc_workqueue);",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "__printf(1, 5)",
        "struct workqueue_struct *",
        "alloc_workqueue_lockdep_map(const char *fmt, unsigned int flags,",
        "			    int max_active, struct lockdep_map *lockdep_map, ...)",
        "{",
        "	struct workqueue_struct *wq;",
        "	va_list args;",
        "",
        "	va_start(args, lockdep_map);",
        "	wq = __alloc_workqueue(fmt, flags, max_active, args);",
        "	va_end(args);",
        "	if (!wq)",
        "		return NULL;",
        "",
        "	wq->lockdep_map = lockdep_map;",
        "",
        "	return wq;",
        "}",
        "EXPORT_SYMBOL_GPL(alloc_workqueue_lockdep_map);",
        "#endif",
        "",
        "static bool pwq_busy(struct pool_workqueue *pwq)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < WORK_NR_COLORS; i++)",
        "		if (pwq->nr_in_flight[i])",
        "			return true;",
        "",
        "	if ((pwq != rcu_access_pointer(pwq->wq->dfl_pwq)) && (pwq->refcnt > 1))",
        "		return true;",
        "	if (!pwq_is_empty(pwq))",
        "		return true;",
        "",
        "	return false;",
        "}",
        "",
        "/**",
        " * destroy_workqueue - safely terminate a workqueue",
        " * @wq: target workqueue",
        " *",
        " * Safely destroy a workqueue. All work currently pending will be done first.",
        " */",
        "void destroy_workqueue(struct workqueue_struct *wq)",
        "{",
        "	struct pool_workqueue *pwq;",
        "	int cpu;",
        "",
        "	/*",
        "	 * Remove it from sysfs first so that sanity check failure doesn't",
        "	 * lead to sysfs name conflicts.",
        "	 */",
        "	workqueue_sysfs_unregister(wq);",
        "",
        "	/* mark the workqueue destruction is in progress */",
        "	mutex_lock(&wq->mutex);",
        "	wq->flags |= __WQ_DESTROYING;",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	/* drain it before proceeding with destruction */",
        "	drain_workqueue(wq);",
        "",
        "	/* kill rescuer, if sanity checks fail, leave it w/o rescuer */",
        "	if (wq->rescuer) {",
        "		struct worker *rescuer = wq->rescuer;",
        "",
        "		/* this prevents new queueing */",
        "		raw_spin_lock_irq(&wq_mayday_lock);",
        "		wq->rescuer = NULL;",
        "		raw_spin_unlock_irq(&wq_mayday_lock);",
        "",
        "		/* rescuer will empty maydays list before exiting */",
        "		kthread_stop(rescuer->task);",
        "		kfree(rescuer);",
        "	}",
        "",
        "	/*",
        "	 * Sanity checks - grab all the locks so that we wait for all",
        "	 * in-flight operations which may do put_pwq().",
        "	 */",
        "	mutex_lock(&wq_pool_mutex);",
        "	mutex_lock(&wq->mutex);",
        "	for_each_pwq(pwq, wq) {",
        "		raw_spin_lock_irq(&pwq->pool->lock);",
        "		if (WARN_ON(pwq_busy(pwq))) {",
        "			pr_warn(\"%s: %s has the following busy pwq\\n\",",
        "				__func__, wq->name);",
        "			show_pwq(pwq);",
        "			raw_spin_unlock_irq(&pwq->pool->lock);",
        "			mutex_unlock(&wq->mutex);",
        "			mutex_unlock(&wq_pool_mutex);",
        "			show_one_workqueue(wq);",
        "			return;",
        "		}",
        "		raw_spin_unlock_irq(&pwq->pool->lock);",
        "	}",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	/*",
        "	 * wq list is used to freeze wq, remove from list after",
        "	 * flushing is complete in case freeze races us.",
        "	 */",
        "	list_del_rcu(&wq->list);",
        "	mutex_unlock(&wq_pool_mutex);",
        "",
        "	/*",
        "	 * We're the sole accessor of @wq. Directly access cpu_pwq and dfl_pwq",
        "	 * to put the base refs. @wq will be auto-destroyed from the last",
        "	 * pwq_put. RCU read lock prevents @wq from going away from under us.",
        "	 */",
        "	rcu_read_lock();",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		put_pwq_unlocked(unbound_pwq(wq, cpu));",
        "		RCU_INIT_POINTER(*unbound_pwq_slot(wq, cpu), NULL);",
        "	}",
        "",
        "	put_pwq_unlocked(unbound_pwq(wq, -1));",
        "	RCU_INIT_POINTER(*unbound_pwq_slot(wq, -1), NULL);",
        "",
        "	rcu_read_unlock();",
        "}",
        "EXPORT_SYMBOL_GPL(destroy_workqueue);",
        "",
        "/**",
        " * workqueue_set_max_active - adjust max_active of a workqueue",
        " * @wq: target workqueue",
        " * @max_active: new max_active value.",
        " *",
        " * Set max_active of @wq to @max_active. See the alloc_workqueue() function",
        " * comment.",
        " *",
        " * CONTEXT:",
        " * Don't call from IRQ context.",
        " */",
        "void workqueue_set_max_active(struct workqueue_struct *wq, int max_active)",
        "{",
        "	/* max_active doesn't mean anything for BH workqueues */",
        "	if (WARN_ON(wq->flags & WQ_BH))",
        "		return;",
        "	/* disallow meddling with max_active for ordered workqueues */",
        "	if (WARN_ON(wq->flags & __WQ_ORDERED))",
        "		return;",
        "",
        "	max_active = wq_clamp_max_active(max_active, wq->flags, wq->name);",
        "",
        "	mutex_lock(&wq->mutex);",
        "",
        "	wq->saved_max_active = max_active;",
        "	if (wq->flags & WQ_UNBOUND)",
        "		wq->saved_min_active = min(wq->saved_min_active, max_active);",
        "",
        "	wq_adjust_max_active(wq);",
        "",
        "	mutex_unlock(&wq->mutex);",
        "}",
        "EXPORT_SYMBOL_GPL(workqueue_set_max_active);",
        "",
        "/**",
        " * workqueue_set_min_active - adjust min_active of an unbound workqueue",
        " * @wq: target unbound workqueue",
        " * @min_active: new min_active value",
        " *",
        " * Set min_active of an unbound workqueue. Unlike other types of workqueues, an",
        " * unbound workqueue is not guaranteed to be able to process max_active",
        " * interdependent work items. Instead, an unbound workqueue is guaranteed to be",
        " * able to process min_active number of interdependent work items which is",
        " * %WQ_DFL_MIN_ACTIVE by default.",
        " *",
        " * Use this function to adjust the min_active value between 0 and the current",
        " * max_active.",
        " */",
        "void workqueue_set_min_active(struct workqueue_struct *wq, int min_active)",
        "{",
        "	/* min_active is only meaningful for non-ordered unbound workqueues */",
        "	if (WARN_ON((wq->flags & (WQ_BH | WQ_UNBOUND | __WQ_ORDERED)) !=",
        "		    WQ_UNBOUND))",
        "		return;",
        "",
        "	mutex_lock(&wq->mutex);",
        "	wq->saved_min_active = clamp(min_active, 0, wq->saved_max_active);",
        "	wq_adjust_max_active(wq);",
        "	mutex_unlock(&wq->mutex);",
        "}",
        "",
        "/**",
        " * current_work - retrieve %current task's work struct",
        " *",
        " * Determine if %current task is a workqueue worker and what it's working on.",
        " * Useful to find out the context that the %current task is running in.",
        " *",
        " * Return: work struct if %current task is a workqueue worker, %NULL otherwise.",
        " */",
        "struct work_struct *current_work(void)",
        "{",
        "	struct worker *worker = current_wq_worker();",
        "",
        "	return worker ? worker->current_work : NULL;",
        "}",
        "EXPORT_SYMBOL(current_work);",
        "",
        "/**",
        " * current_is_workqueue_rescuer - is %current workqueue rescuer?",
        " *",
        " * Determine whether %current is a workqueue rescuer.  Can be used from",
        " * work functions to determine whether it's being run off the rescuer task.",
        " *",
        " * Return: %true if %current is a workqueue rescuer. %false otherwise.",
        " */",
        "bool current_is_workqueue_rescuer(void)",
        "{",
        "	struct worker *worker = current_wq_worker();",
        "",
        "	return worker && worker->rescue_wq;",
        "}",
        "",
        "/**",
        " * workqueue_congested - test whether a workqueue is congested",
        " * @cpu: CPU in question",
        " * @wq: target workqueue",
        " *",
        " * Test whether @wq's cpu workqueue for @cpu is congested.  There is",
        " * no synchronization around this function and the test result is",
        " * unreliable and only useful as advisory hints or for debugging.",
        " *",
        " * If @cpu is WORK_CPU_UNBOUND, the test is performed on the local CPU.",
        " *",
        " * With the exception of ordered workqueues, all workqueues have per-cpu",
        " * pool_workqueues, each with its own congested state. A workqueue being",
        " * congested on one CPU doesn't mean that the workqueue is contested on any",
        " * other CPUs.",
        " *",
        " * Return:",
        " * %true if congested, %false otherwise.",
        " */",
        "bool workqueue_congested(int cpu, struct workqueue_struct *wq)",
        "{",
        "	struct pool_workqueue *pwq;",
        "	bool ret;",
        "",
        "	rcu_read_lock();",
        "	preempt_disable();",
        "",
        "	if (cpu == WORK_CPU_UNBOUND)",
        "		cpu = smp_processor_id();",
        "",
        "	pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);",
        "	ret = !list_empty(&pwq->inactive_works);",
        "",
        "	preempt_enable();",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(workqueue_congested);",
        "",
        "/**",
        " * work_busy - test whether a work is currently pending or running",
        " * @work: the work to be tested",
        " *",
        " * Test whether @work is currently pending or running.  There is no",
        " * synchronization around this function and the test result is",
        " * unreliable and only useful as advisory hints or for debugging.",
        " *",
        " * Return:",
        " * OR'd bitmask of WORK_BUSY_* bits.",
        " */",
        "unsigned int work_busy(struct work_struct *work)",
        "{",
        "	struct worker_pool *pool;",
        "	unsigned long irq_flags;",
        "	unsigned int ret = 0;",
        "",
        "	if (work_pending(work))",
        "		ret |= WORK_BUSY_PENDING;",
        "",
        "	rcu_read_lock();",
        "	pool = get_work_pool(work);",
        "	if (pool) {",
        "		raw_spin_lock_irqsave(&pool->lock, irq_flags);",
        "		if (find_worker_executing_work(pool, work))",
        "			ret |= WORK_BUSY_RUNNING;",
        "		raw_spin_unlock_irqrestore(&pool->lock, irq_flags);",
        "	}",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(work_busy);",
        "",
        "/**",
        " * set_worker_desc - set description for the current work item",
        " * @fmt: printf-style format string",
        " * @...: arguments for the format string",
        " *",
        " * This function can be called by a running work function to describe what",
        " * the work item is about.  If the worker task gets dumped, this",
        " * information will be printed out together to help debugging.  The",
        " * description can be at most WORKER_DESC_LEN including the trailing '\\0'.",
        " */",
        "void set_worker_desc(const char *fmt, ...)",
        "{",
        "	struct worker *worker = current_wq_worker();",
        "	va_list args;",
        "",
        "	if (worker) {",
        "		va_start(args, fmt);",
        "		vsnprintf(worker->desc, sizeof(worker->desc), fmt, args);",
        "		va_end(args);",
        "	}",
        "}",
        "EXPORT_SYMBOL_GPL(set_worker_desc);",
        "",
        "/**",
        " * print_worker_info - print out worker information and description",
        " * @log_lvl: the log level to use when printing",
        " * @task: target task",
        " *",
        " * If @task is a worker and currently executing a work item, print out the",
        " * name of the workqueue being serviced and worker description set with",
        " * set_worker_desc() by the currently executing work item.",
        " *",
        " * This function can be safely called on any task as long as the",
        " * task_struct itself is accessible.  While safe, this function isn't",
        " * synchronized and may print out mixups or garbages of limited length.",
        " */",
        "void print_worker_info(const char *log_lvl, struct task_struct *task)",
        "{",
        "	work_func_t *fn = NULL;",
        "	char name[WQ_NAME_LEN] = { };",
        "	char desc[WORKER_DESC_LEN] = { };",
        "	struct pool_workqueue *pwq = NULL;",
        "	struct workqueue_struct *wq = NULL;",
        "	struct worker *worker;",
        "",
        "	if (!(task->flags & PF_WQ_WORKER))",
        "		return;",
        "",
        "	/*",
        "	 * This function is called without any synchronization and @task",
        "	 * could be in any state.  Be careful with dereferences.",
        "	 */",
        "	worker = kthread_probe_data(task);",
        "",
        "	/*",
        "	 * Carefully copy the associated workqueue's workfn, name and desc.",
        "	 * Keep the original last '\\0' in case the original is garbage.",
        "	 */",
        "	copy_from_kernel_nofault(&fn, &worker->current_func, sizeof(fn));",
        "	copy_from_kernel_nofault(&pwq, &worker->current_pwq, sizeof(pwq));",
        "	copy_from_kernel_nofault(&wq, &pwq->wq, sizeof(wq));",
        "	copy_from_kernel_nofault(name, wq->name, sizeof(name) - 1);",
        "	copy_from_kernel_nofault(desc, worker->desc, sizeof(desc) - 1);",
        "",
        "	if (fn || name[0] || desc[0]) {",
        "		printk(\"%sWorkqueue: %s %ps\", log_lvl, name, fn);",
        "		if (strcmp(name, desc))",
        "			pr_cont(\" (%s)\", desc);",
        "		pr_cont(\"\\n\");",
        "	}",
        "}",
        "",
        "static void pr_cont_pool_info(struct worker_pool *pool)",
        "{",
        "	pr_cont(\" cpus=%*pbl\", nr_cpumask_bits, pool->attrs->cpumask);",
        "	if (pool->node != NUMA_NO_NODE)",
        "		pr_cont(\" node=%d\", pool->node);",
        "	pr_cont(\" flags=0x%x\", pool->flags);",
        "	if (pool->flags & POOL_BH)",
        "		pr_cont(\" bh%s\",",
        "			pool->attrs->nice == HIGHPRI_NICE_LEVEL ? \"-hi\" : \"\");",
        "	else",
        "		pr_cont(\" nice=%d\", pool->attrs->nice);",
        "}",
        "",
        "static void pr_cont_worker_id(struct worker *worker)",
        "{",
        "	struct worker_pool *pool = worker->pool;",
        "",
        "	if (pool->flags & WQ_BH)",
        "		pr_cont(\"bh%s\",",
        "			pool->attrs->nice == HIGHPRI_NICE_LEVEL ? \"-hi\" : \"\");",
        "	else",
        "		pr_cont(\"%d%s\", task_pid_nr(worker->task),",
        "			worker->rescue_wq ? \"(RESCUER)\" : \"\");",
        "}",
        "",
        "struct pr_cont_work_struct {",
        "	bool comma;",
        "	work_func_t func;",
        "	long ctr;",
        "};",
        "",
        "static void pr_cont_work_flush(bool comma, work_func_t func, struct pr_cont_work_struct *pcwsp)",
        "{",
        "	if (!pcwsp->ctr)",
        "		goto out_record;",
        "	if (func == pcwsp->func) {",
        "		pcwsp->ctr++;",
        "		return;",
        "	}",
        "	if (pcwsp->ctr == 1)",
        "		pr_cont(\"%s %ps\", pcwsp->comma ? \",\" : \"\", pcwsp->func);",
        "	else",
        "		pr_cont(\"%s %ld*%ps\", pcwsp->comma ? \",\" : \"\", pcwsp->ctr, pcwsp->func);",
        "	pcwsp->ctr = 0;",
        "out_record:",
        "	if ((long)func == -1L)",
        "		return;",
        "	pcwsp->comma = comma;",
        "	pcwsp->func = func;",
        "	pcwsp->ctr = 1;",
        "}",
        "",
        "static void pr_cont_work(bool comma, struct work_struct *work, struct pr_cont_work_struct *pcwsp)",
        "{",
        "	if (work->func == wq_barrier_func) {",
        "		struct wq_barrier *barr;",
        "",
        "		barr = container_of(work, struct wq_barrier, work);",
        "",
        "		pr_cont_work_flush(comma, (work_func_t)-1, pcwsp);",
        "		pr_cont(\"%s BAR(%d)\", comma ? \",\" : \"\",",
        "			task_pid_nr(barr->task));",
        "	} else {",
        "		if (!comma)",
        "			pr_cont_work_flush(comma, (work_func_t)-1, pcwsp);",
        "		pr_cont_work_flush(comma, work->func, pcwsp);",
        "	}",
        "}",
        "",
        "static void show_pwq(struct pool_workqueue *pwq)",
        "{",
        "	struct pr_cont_work_struct pcws = { .ctr = 0, };",
        "	struct worker_pool *pool = pwq->pool;",
        "	struct work_struct *work;",
        "	struct worker *worker;",
        "	bool has_in_flight = false, has_pending = false;",
        "	int bkt;",
        "",
        "	pr_info(\"  pwq %d:\", pool->id);",
        "	pr_cont_pool_info(pool);",
        "",
        "	pr_cont(\" active=%d refcnt=%d%s\\n\",",
        "		pwq->nr_active, pwq->refcnt,",
        "		!list_empty(&pwq->mayday_node) ? \" MAYDAY\" : \"\");",
        "",
        "	hash_for_each(pool->busy_hash, bkt, worker, hentry) {",
        "		if (worker->current_pwq == pwq) {",
        "			has_in_flight = true;",
        "			break;",
        "		}",
        "	}",
        "	if (has_in_flight) {",
        "		bool comma = false;",
        "",
        "		pr_info(\"    in-flight:\");",
        "		hash_for_each(pool->busy_hash, bkt, worker, hentry) {",
        "			if (worker->current_pwq != pwq)",
        "				continue;",
        "",
        "			pr_cont(\" %s\", comma ? \",\" : \"\");",
        "			pr_cont_worker_id(worker);",
        "			pr_cont(\":%ps\", worker->current_func);",
        "			list_for_each_entry(work, &worker->scheduled, entry)",
        "				pr_cont_work(false, work, &pcws);",
        "			pr_cont_work_flush(comma, (work_func_t)-1L, &pcws);",
        "			comma = true;",
        "		}",
        "		pr_cont(\"\\n\");",
        "	}",
        "",
        "	list_for_each_entry(work, &pool->worklist, entry) {",
        "		if (get_work_pwq(work) == pwq) {",
        "			has_pending = true;",
        "			break;",
        "		}",
        "	}",
        "	if (has_pending) {",
        "		bool comma = false;",
        "",
        "		pr_info(\"    pending:\");",
        "		list_for_each_entry(work, &pool->worklist, entry) {",
        "			if (get_work_pwq(work) != pwq)",
        "				continue;",
        "",
        "			pr_cont_work(comma, work, &pcws);",
        "			comma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);",
        "		}",
        "		pr_cont_work_flush(comma, (work_func_t)-1L, &pcws);",
        "		pr_cont(\"\\n\");",
        "	}",
        "",
        "	if (!list_empty(&pwq->inactive_works)) {",
        "		bool comma = false;",
        "",
        "		pr_info(\"    inactive:\");",
        "		list_for_each_entry(work, &pwq->inactive_works, entry) {",
        "			pr_cont_work(comma, work, &pcws);",
        "			comma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);",
        "		}",
        "		pr_cont_work_flush(comma, (work_func_t)-1L, &pcws);",
        "		pr_cont(\"\\n\");",
        "	}",
        "}",
        "",
        "/**",
        " * show_one_workqueue - dump state of specified workqueue",
        " * @wq: workqueue whose state will be printed",
        " */",
        "void show_one_workqueue(struct workqueue_struct *wq)",
        "{",
        "	struct pool_workqueue *pwq;",
        "	bool idle = true;",
        "	unsigned long irq_flags;",
        "",
        "	for_each_pwq(pwq, wq) {",
        "		if (!pwq_is_empty(pwq)) {",
        "			idle = false;",
        "			break;",
        "		}",
        "	}",
        "	if (idle) /* Nothing to print for idle workqueue */",
        "		return;",
        "",
        "	pr_info(\"workqueue %s: flags=0x%x\\n\", wq->name, wq->flags);",
        "",
        "	for_each_pwq(pwq, wq) {",
        "		raw_spin_lock_irqsave(&pwq->pool->lock, irq_flags);",
        "		if (!pwq_is_empty(pwq)) {",
        "			/*",
        "			 * Defer printing to avoid deadlocks in console",
        "			 * drivers that queue work while holding locks",
        "			 * also taken in their write paths.",
        "			 */",
        "			printk_deferred_enter();",
        "			show_pwq(pwq);",
        "			printk_deferred_exit();",
        "		}",
        "		raw_spin_unlock_irqrestore(&pwq->pool->lock, irq_flags);",
        "		/*",
        "		 * We could be printing a lot from atomic context, e.g.",
        "		 * sysrq-t -> show_all_workqueues(). Avoid triggering",
        "		 * hard lockup.",
        "		 */",
        "		touch_nmi_watchdog();",
        "	}",
        "",
        "}",
        "",
        "/**",
        " * show_one_worker_pool - dump state of specified worker pool",
        " * @pool: worker pool whose state will be printed",
        " */",
        "static void show_one_worker_pool(struct worker_pool *pool)",
        "{",
        "	struct worker *worker;",
        "	bool first = true;",
        "	unsigned long irq_flags;",
        "	unsigned long hung = 0;",
        "",
        "	raw_spin_lock_irqsave(&pool->lock, irq_flags);",
        "	if (pool->nr_workers == pool->nr_idle)",
        "		goto next_pool;",
        "",
        "	/* How long the first pending work is waiting for a worker. */",
        "	if (!list_empty(&pool->worklist))",
        "		hung = jiffies_to_msecs(jiffies - pool->watchdog_ts) / 1000;",
        "",
        "	/*",
        "	 * Defer printing to avoid deadlocks in console drivers that",
        "	 * queue work while holding locks also taken in their write",
        "	 * paths.",
        "	 */",
        "	printk_deferred_enter();",
        "	pr_info(\"pool %d:\", pool->id);",
        "	pr_cont_pool_info(pool);",
        "	pr_cont(\" hung=%lus workers=%d\", hung, pool->nr_workers);",
        "	if (pool->manager)",
        "		pr_cont(\" manager: %d\",",
        "			task_pid_nr(pool->manager->task));",
        "	list_for_each_entry(worker, &pool->idle_list, entry) {",
        "		pr_cont(\" %s\", first ? \"idle: \" : \"\");",
        "		pr_cont_worker_id(worker);",
        "		first = false;",
        "	}",
        "	pr_cont(\"\\n\");",
        "	printk_deferred_exit();",
        "next_pool:",
        "	raw_spin_unlock_irqrestore(&pool->lock, irq_flags);",
        "	/*",
        "	 * We could be printing a lot from atomic context, e.g.",
        "	 * sysrq-t -> show_all_workqueues(). Avoid triggering",
        "	 * hard lockup.",
        "	 */",
        "	touch_nmi_watchdog();",
        "",
        "}",
        "",
        "/**",
        " * show_all_workqueues - dump workqueue state",
        " *",
        " * Called from a sysrq handler and prints out all busy workqueues and pools.",
        " */",
        "void show_all_workqueues(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "	struct worker_pool *pool;",
        "	int pi;",
        "",
        "	rcu_read_lock();",
        "",
        "	pr_info(\"Showing busy workqueues and worker pools:\\n\");",
        "",
        "	list_for_each_entry_rcu(wq, &workqueues, list)",
        "		show_one_workqueue(wq);",
        "",
        "	for_each_pool(pool, pi)",
        "		show_one_worker_pool(pool);",
        "",
        "	rcu_read_unlock();",
        "}",
        "",
        "/**",
        " * show_freezable_workqueues - dump freezable workqueue state",
        " *",
        " * Called from try_to_freeze_tasks() and prints out all freezable workqueues",
        " * still busy.",
        " */",
        "void show_freezable_workqueues(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "",
        "	rcu_read_lock();",
        "",
        "	pr_info(\"Showing freezable workqueues that are still busy:\\n\");",
        "",
        "	list_for_each_entry_rcu(wq, &workqueues, list) {",
        "		if (!(wq->flags & WQ_FREEZABLE))",
        "			continue;",
        "		show_one_workqueue(wq);",
        "	}",
        "",
        "	rcu_read_unlock();",
        "}",
        "",
        "/* used to show worker information through /proc/PID/{comm,stat,status} */",
        "void wq_worker_comm(char *buf, size_t size, struct task_struct *task)",
        "{",
        "	/* stabilize PF_WQ_WORKER and worker pool association */",
        "	mutex_lock(&wq_pool_attach_mutex);",
        "",
        "	if (task->flags & PF_WQ_WORKER) {",
        "		struct worker *worker = kthread_data(task);",
        "		struct worker_pool *pool = worker->pool;",
        "		int off;",
        "",
        "		off = format_worker_id(buf, size, worker, pool);",
        "",
        "		if (pool) {",
        "			raw_spin_lock_irq(&pool->lock);",
        "			/*",
        "			 * ->desc tracks information (wq name or",
        "			 * set_worker_desc()) for the latest execution.  If",
        "			 * current, prepend '+', otherwise '-'.",
        "			 */",
        "			if (worker->desc[0] != '\\0') {",
        "				if (worker->current_work)",
        "					scnprintf(buf + off, size - off, \"+%s\",",
        "						  worker->desc);",
        "				else",
        "					scnprintf(buf + off, size - off, \"-%s\",",
        "						  worker->desc);",
        "			}",
        "			raw_spin_unlock_irq(&pool->lock);",
        "		}",
        "	} else {",
        "		strscpy(buf, task->comm, size);",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_attach_mutex);",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "",
        "/*",
        " * CPU hotplug.",
        " *",
        " * There are two challenges in supporting CPU hotplug.  Firstly, there",
        " * are a lot of assumptions on strong associations among work, pwq and",
        " * pool which make migrating pending and scheduled works very",
        " * difficult to implement without impacting hot paths.  Secondly,",
        " * worker pools serve mix of short, long and very long running works making",
        " * blocked draining impractical.",
        " *",
        " * This is solved by allowing the pools to be disassociated from the CPU",
        " * running as an unbound one and allowing it to be reattached later if the",
        " * cpu comes back online.",
        " */",
        "",
        "static void unbind_workers(int cpu)",
        "{",
        "	struct worker_pool *pool;",
        "	struct worker *worker;",
        "",
        "	for_each_cpu_worker_pool(pool, cpu) {",
        "		mutex_lock(&wq_pool_attach_mutex);",
        "		raw_spin_lock_irq(&pool->lock);",
        "",
        "		/*",
        "		 * We've blocked all attach/detach operations. Make all workers",
        "		 * unbound and set DISASSOCIATED.  Before this, all workers",
        "		 * must be on the cpu.  After this, they may become diasporas.",
        "		 * And the preemption disabled section in their sched callbacks",
        "		 * are guaranteed to see WORKER_UNBOUND since the code here",
        "		 * is on the same cpu.",
        "		 */",
        "		for_each_pool_worker(worker, pool)",
        "			worker->flags |= WORKER_UNBOUND;",
        "",
        "		pool->flags |= POOL_DISASSOCIATED;",
        "",
        "		/*",
        "		 * The handling of nr_running in sched callbacks are disabled",
        "		 * now.  Zap nr_running.  After this, nr_running stays zero and",
        "		 * need_more_worker() and keep_working() are always true as",
        "		 * long as the worklist is not empty.  This pool now behaves as",
        "		 * an unbound (in terms of concurrency management) pool which",
        "		 * are served by workers tied to the pool.",
        "		 */",
        "		pool->nr_running = 0;",
        "",
        "		/*",
        "		 * With concurrency management just turned off, a busy",
        "		 * worker blocking could lead to lengthy stalls.  Kick off",
        "		 * unbound chain execution of currently pending work items.",
        "		 */",
        "		kick_pool(pool);",
        "",
        "		raw_spin_unlock_irq(&pool->lock);",
        "",
        "		for_each_pool_worker(worker, pool)",
        "			unbind_worker(worker);",
        "",
        "		mutex_unlock(&wq_pool_attach_mutex);",
        "	}",
        "}",
        "",
        "/**",
        " * rebind_workers - rebind all workers of a pool to the associated CPU",
        " * @pool: pool of interest",
        " *",
        " * @pool->cpu is coming online.  Rebind all workers to the CPU.",
        " */",
        "static void rebind_workers(struct worker_pool *pool)",
        "{",
        "	struct worker *worker;",
        "",
        "	lockdep_assert_held(&wq_pool_attach_mutex);",
        "",
        "	/*",
        "	 * Restore CPU affinity of all workers.  As all idle workers should",
        "	 * be on the run-queue of the associated CPU before any local",
        "	 * wake-ups for concurrency management happen, restore CPU affinity",
        "	 * of all workers first and then clear UNBOUND.  As we're called",
        "	 * from CPU_ONLINE, the following shouldn't fail.",
        "	 */",
        "	for_each_pool_worker(worker, pool) {",
        "		kthread_set_per_cpu(worker->task, pool->cpu);",
        "		WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,",
        "						  pool_allowed_cpus(pool)) < 0);",
        "	}",
        "",
        "	raw_spin_lock_irq(&pool->lock);",
        "",
        "	pool->flags &= ~POOL_DISASSOCIATED;",
        "",
        "	for_each_pool_worker(worker, pool) {",
        "		unsigned int worker_flags = worker->flags;",
        "",
        "		/*",
        "		 * We want to clear UNBOUND but can't directly call",
        "		 * worker_clr_flags() or adjust nr_running.  Atomically",
        "		 * replace UNBOUND with another NOT_RUNNING flag REBOUND.",
        "		 * @worker will clear REBOUND using worker_clr_flags() when",
        "		 * it initiates the next execution cycle thus restoring",
        "		 * concurrency management.  Note that when or whether",
        "		 * @worker clears REBOUND doesn't affect correctness.",
        "		 *",
        "		 * WRITE_ONCE() is necessary because @worker->flags may be",
        "		 * tested without holding any lock in",
        "		 * wq_worker_running().  Without it, NOT_RUNNING test may",
        "		 * fail incorrectly leading to premature concurrency",
        "		 * management operations.",
        "		 */",
        "		WARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));",
        "		worker_flags |= WORKER_REBOUND;",
        "		worker_flags &= ~WORKER_UNBOUND;",
        "		WRITE_ONCE(worker->flags, worker_flags);",
        "	}",
        "",
        "	raw_spin_unlock_irq(&pool->lock);",
        "}",
        "",
        "/**",
        " * restore_unbound_workers_cpumask - restore cpumask of unbound workers",
        " * @pool: unbound pool of interest",
        " * @cpu: the CPU which is coming up",
        " *",
        " * An unbound pool may end up with a cpumask which doesn't have any online",
        " * CPUs.  When a worker of such pool get scheduled, the scheduler resets",
        " * its cpus_allowed.  If @cpu is in @pool's cpumask which didn't have any",
        " * online CPU before, cpus_allowed of all its workers should be restored.",
        " */",
        "static void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)",
        "{",
        "	static cpumask_t cpumask;",
        "	struct worker *worker;",
        "",
        "	lockdep_assert_held(&wq_pool_attach_mutex);",
        "",
        "	/* is @cpu allowed for @pool? */",
        "	if (!cpumask_test_cpu(cpu, pool->attrs->cpumask))",
        "		return;",
        "",
        "	cpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);",
        "",
        "	/* as we're called from CPU_ONLINE, the following shouldn't fail */",
        "	for_each_pool_worker(worker, pool)",
        "		WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);",
        "}",
        "",
        "int workqueue_prepare_cpu(unsigned int cpu)",
        "{",
        "	struct worker_pool *pool;",
        "",
        "	for_each_cpu_worker_pool(pool, cpu) {",
        "		if (pool->nr_workers)",
        "			continue;",
        "		if (!create_worker(pool))",
        "			return -ENOMEM;",
        "	}",
        "	return 0;",
        "}",
        "",
        "int workqueue_online_cpu(unsigned int cpu)",
        "{",
        "	struct worker_pool *pool;",
        "	struct workqueue_struct *wq;",
        "	int pi;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	cpumask_set_cpu(cpu, wq_online_cpumask);",
        "",
        "	for_each_pool(pool, pi) {",
        "		/* BH pools aren't affected by hotplug */",
        "		if (pool->flags & POOL_BH)",
        "			continue;",
        "",
        "		mutex_lock(&wq_pool_attach_mutex);",
        "		if (pool->cpu == cpu)",
        "			rebind_workers(pool);",
        "		else if (pool->cpu < 0)",
        "			restore_unbound_workers_cpumask(pool, cpu);",
        "		mutex_unlock(&wq_pool_attach_mutex);",
        "	}",
        "",
        "	/* update pod affinity of unbound workqueues */",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		struct workqueue_attrs *attrs = wq->unbound_attrs;",
        "",
        "		if (attrs) {",
        "			const struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
        "			int tcpu;",
        "",
        "			for_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
        "				unbound_wq_update_pwq(wq, tcpu);",
        "",
        "			mutex_lock(&wq->mutex);",
        "			wq_update_node_max_active(wq, -1);",
        "			mutex_unlock(&wq->mutex);",
        "		}",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "	return 0;",
        "}",
        "",
        "int workqueue_offline_cpu(unsigned int cpu)",
        "{",
        "	struct workqueue_struct *wq;",
        "",
        "	/* unbinding per-cpu workers should happen on the local CPU */",
        "	if (WARN_ON(cpu != smp_processor_id()))",
        "		return -1;",
        "",
        "	unbind_workers(cpu);",
        "",
        "	/* update pod affinity of unbound workqueues */",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	cpumask_clear_cpu(cpu, wq_online_cpumask);",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		struct workqueue_attrs *attrs = wq->unbound_attrs;",
        "",
        "		if (attrs) {",
        "			const struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
        "			int tcpu;",
        "",
        "			for_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
        "				unbound_wq_update_pwq(wq, tcpu);",
        "",
        "			mutex_lock(&wq->mutex);",
        "			wq_update_node_max_active(wq, cpu);",
        "			mutex_unlock(&wq->mutex);",
        "		}",
        "	}",
        "	mutex_unlock(&wq_pool_mutex);",
        "",
        "	return 0;",
        "}",
        "",
        "struct work_for_cpu {",
        "	struct work_struct work;",
        "	long (*fn)(void *);",
        "	void *arg;",
        "	long ret;",
        "};",
        "",
        "static void work_for_cpu_fn(struct work_struct *work)",
        "{",
        "	struct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);",
        "",
        "	wfc->ret = wfc->fn(wfc->arg);",
        "}",
        "",
        "/**",
        " * work_on_cpu_key - run a function in thread context on a particular cpu",
        " * @cpu: the cpu to run on",
        " * @fn: the function to run",
        " * @arg: the function arg",
        " * @key: The lock class key for lock debugging purposes",
        " *",
        " * It is up to the caller to ensure that the cpu doesn't go offline.",
        " * The caller must not hold any locks which would prevent @fn from completing.",
        " *",
        " * Return: The value @fn returns.",
        " */",
        "long work_on_cpu_key(int cpu, long (*fn)(void *),",
        "		     void *arg, struct lock_class_key *key)",
        "{",
        "	struct work_for_cpu wfc = { .fn = fn, .arg = arg };",
        "",
        "	INIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);",
        "	schedule_work_on(cpu, &wfc.work);",
        "	flush_work(&wfc.work);",
        "	destroy_work_on_stack(&wfc.work);",
        "	return wfc.ret;",
        "}",
        "EXPORT_SYMBOL_GPL(work_on_cpu_key);",
        "",
        "/**",
        " * work_on_cpu_safe_key - run a function in thread context on a particular cpu",
        " * @cpu: the cpu to run on",
        " * @fn:  the function to run",
        " * @arg: the function argument",
        " * @key: The lock class key for lock debugging purposes",
        " *",
        " * Disables CPU hotplug and calls work_on_cpu(). The caller must not hold",
        " * any locks which would prevent @fn from completing.",
        " *",
        " * Return: The value @fn returns.",
        " */",
        "long work_on_cpu_safe_key(int cpu, long (*fn)(void *),",
        "			  void *arg, struct lock_class_key *key)",
        "{",
        "	long ret = -ENODEV;",
        "",
        "	cpus_read_lock();",
        "	if (cpu_online(cpu))",
        "		ret = work_on_cpu_key(cpu, fn, arg, key);",
        "	cpus_read_unlock();",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(work_on_cpu_safe_key);",
        "#endif /* CONFIG_SMP */",
        "",
        "#ifdef CONFIG_FREEZER",
        "",
        "/**",
        " * freeze_workqueues_begin - begin freezing workqueues",
        " *",
        " * Start freezing workqueues.  After this function returns, all freezable",
        " * workqueues will queue new works to their inactive_works list instead of",
        " * pool->worklist.",
        " *",
        " * CONTEXT:",
        " * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.",
        " */",
        "void freeze_workqueues_begin(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	WARN_ON_ONCE(workqueue_freezing);",
        "	workqueue_freezing = true;",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		mutex_lock(&wq->mutex);",
        "		wq_adjust_max_active(wq);",
        "		mutex_unlock(&wq->mutex);",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "}",
        "",
        "/**",
        " * freeze_workqueues_busy - are freezable workqueues still busy?",
        " *",
        " * Check whether freezing is complete.  This function must be called",
        " * between freeze_workqueues_begin() and thaw_workqueues().",
        " *",
        " * CONTEXT:",
        " * Grabs and releases wq_pool_mutex.",
        " *",
        " * Return:",
        " * %true if some freezable workqueues are still busy.  %false if freezing",
        " * is complete.",
        " */",
        "bool freeze_workqueues_busy(void)",
        "{",
        "	bool busy = false;",
        "	struct workqueue_struct *wq;",
        "	struct pool_workqueue *pwq;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	WARN_ON_ONCE(!workqueue_freezing);",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		if (!(wq->flags & WQ_FREEZABLE))",
        "			continue;",
        "		/*",
        "		 * nr_active is monotonically decreasing.  It's safe",
        "		 * to peek without lock.",
        "		 */",
        "		rcu_read_lock();",
        "		for_each_pwq(pwq, wq) {",
        "			WARN_ON_ONCE(pwq->nr_active < 0);",
        "			if (pwq->nr_active) {",
        "				busy = true;",
        "				rcu_read_unlock();",
        "				goto out_unlock;",
        "			}",
        "		}",
        "		rcu_read_unlock();",
        "	}",
        "out_unlock:",
        "	mutex_unlock(&wq_pool_mutex);",
        "	return busy;",
        "}",
        "",
        "/**",
        " * thaw_workqueues - thaw workqueues",
        " *",
        " * Thaw workqueues.  Normal queueing is restored and all collected",
        " * frozen works are transferred to their respective pool worklists.",
        " *",
        " * CONTEXT:",
        " * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.",
        " */",
        "void thaw_workqueues(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	if (!workqueue_freezing)",
        "		goto out_unlock;",
        "",
        "	workqueue_freezing = false;",
        "",
        "	/* restore max_active and repopulate worklist */",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		mutex_lock(&wq->mutex);",
        "		wq_adjust_max_active(wq);",
        "		mutex_unlock(&wq->mutex);",
        "	}",
        "",
        "out_unlock:",
        "	mutex_unlock(&wq_pool_mutex);",
        "}",
        "#endif /* CONFIG_FREEZER */",
        "",
        "static int workqueue_apply_unbound_cpumask(const cpumask_var_t unbound_cpumask)",
        "{",
        "	LIST_HEAD(ctxs);",
        "	int ret = 0;",
        "	struct workqueue_struct *wq;",
        "	struct apply_wqattrs_ctx *ctx, *n;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		if (!(wq->flags & WQ_UNBOUND) || (wq->flags & __WQ_DESTROYING))",
        "			continue;",
        "",
        "		ctx = apply_wqattrs_prepare(wq, wq->unbound_attrs, unbound_cpumask);",
        "		if (IS_ERR(ctx)) {",
        "			ret = PTR_ERR(ctx);",
        "			break;",
        "		}",
        "",
        "		list_add_tail(&ctx->list, &ctxs);",
        "	}",
        "",
        "	list_for_each_entry_safe(ctx, n, &ctxs, list) {",
        "		if (!ret)",
        "			apply_wqattrs_commit(ctx);",
        "		apply_wqattrs_cleanup(ctx);",
        "	}",
        "",
        "	if (!ret) {",
        "		mutex_lock(&wq_pool_attach_mutex);",
        "		cpumask_copy(wq_unbound_cpumask, unbound_cpumask);",
        "		mutex_unlock(&wq_pool_attach_mutex);",
        "	}",
        "	return ret;",
        "}",
        "",
        "/**",
        " * workqueue_unbound_exclude_cpumask - Exclude given CPUs from unbound cpumask",
        " * @exclude_cpumask: the cpumask to be excluded from wq_unbound_cpumask",
        " *",
        " * This function can be called from cpuset code to provide a set of isolated",
        " * CPUs that should be excluded from wq_unbound_cpumask.",
        " */",
        "int workqueue_unbound_exclude_cpumask(cpumask_var_t exclude_cpumask)",
        "{",
        "	cpumask_var_t cpumask;",
        "	int ret = 0;",
        "",
        "	if (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))",
        "		return -ENOMEM;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	/*",
        "	 * If the operation fails, it will fall back to",
        "	 * wq_requested_unbound_cpumask which is initially set to",
        "	 * (HK_TYPE_WQ âˆ© HK_TYPE_DOMAIN) house keeping mask and rewritten",
        "	 * by any subsequent write to workqueue/cpumask sysfs file.",
        "	 */",
        "	if (!cpumask_andnot(cpumask, wq_requested_unbound_cpumask, exclude_cpumask))",
        "		cpumask_copy(cpumask, wq_requested_unbound_cpumask);",
        "	if (!cpumask_equal(cpumask, wq_unbound_cpumask))",
        "		ret = workqueue_apply_unbound_cpumask(cpumask);",
        "",
        "	/* Save the current isolated cpumask & export it via sysfs */",
        "	if (!ret)",
        "		cpumask_copy(wq_isolated_cpumask, exclude_cpumask);",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "	free_cpumask_var(cpumask);",
        "	return ret;",
        "}",
        "",
        "static int parse_affn_scope(const char *val)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {",
        "		if (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))",
        "			return i;",
        "	}",
        "	return -EINVAL;",
        "}",
        "",
        "static int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)",
        "{",
        "	struct workqueue_struct *wq;",
        "	int affn, cpu;",
        "",
        "	affn = parse_affn_scope(val);",
        "	if (affn < 0)",
        "		return affn;",
        "	if (affn == WQ_AFFN_DFL)",
        "		return -EINVAL;",
        "",
        "	cpus_read_lock();",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	wq_affn_dfl = affn;",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		for_each_online_cpu(cpu)",
        "			unbound_wq_update_pwq(wq, cpu);",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "	cpus_read_unlock();",
        "",
        "	return 0;",
        "}",
        "",
        "static int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)",
        "{",
        "	return scnprintf(buffer, PAGE_SIZE, \"%s\\n\", wq_affn_names[wq_affn_dfl]);",
        "}",
        "",
        "static const struct kernel_param_ops wq_affn_dfl_ops = {",
        "	.set	= wq_affn_dfl_set,",
        "	.get	= wq_affn_dfl_get,",
        "};",
        "",
        "module_param_cb(default_affinity_scope, &wq_affn_dfl_ops, NULL, 0644);",
        "",
        "#ifdef CONFIG_SYSFS",
        "/*",
        " * Workqueues with WQ_SYSFS flag set is visible to userland via",
        " * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the",
        " * following attributes.",
        " *",
        " *  per_cpu		RO bool	: whether the workqueue is per-cpu or unbound",
        " *  max_active		RW int	: maximum number of in-flight work items",
        " *",
        " * Unbound workqueues have the following extra attributes.",
        " *",
        " *  nice		RW int	: nice value of the workers",
        " *  cpumask		RW mask	: bitmask of allowed CPUs for the workers",
        " *  affinity_scope	RW str  : worker CPU affinity scope (cache, numa, none)",
        " *  affinity_strict	RW bool : worker CPU affinity is strict",
        " */",
        "struct wq_device {",
        "	struct workqueue_struct		*wq;",
        "	struct device			dev;",
        "};",
        "",
        "static struct workqueue_struct *dev_to_wq(struct device *dev)",
        "{",
        "	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);",
        "",
        "	return wq_dev->wq;",
        "}",
        "",
        "static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,",
        "			    char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "",
        "	return scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));",
        "}",
        "static DEVICE_ATTR_RO(per_cpu);",
        "",
        "static ssize_t max_active_show(struct device *dev,",
        "			       struct device_attribute *attr, char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "",
        "	return scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);",
        "}",
        "",
        "static ssize_t max_active_store(struct device *dev,",
        "				struct device_attribute *attr, const char *buf,",
        "				size_t count)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	int val;",
        "",
        "	if (sscanf(buf, \"%d\", &val) != 1 || val <= 0)",
        "		return -EINVAL;",
        "",
        "	workqueue_set_max_active(wq, val);",
        "	return count;",
        "}",
        "static DEVICE_ATTR_RW(max_active);",
        "",
        "static struct attribute *wq_sysfs_attrs[] = {",
        "	&dev_attr_per_cpu.attr,",
        "	&dev_attr_max_active.attr,",
        "	NULL,",
        "};",
        "ATTRIBUTE_GROUPS(wq_sysfs);",
        "",
        "static ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,",
        "			    char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	int written;",
        "",
        "	mutex_lock(&wq->mutex);",
        "	written = scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->unbound_attrs->nice);",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	return written;",
        "}",
        "",
        "/* prepare workqueue_attrs for sysfs store operations */",
        "static struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)",
        "{",
        "	struct workqueue_attrs *attrs;",
        "",
        "	lockdep_assert_held(&wq_pool_mutex);",
        "",
        "	attrs = alloc_workqueue_attrs();",
        "	if (!attrs)",
        "		return NULL;",
        "",
        "	copy_workqueue_attrs(attrs, wq->unbound_attrs);",
        "	return attrs;",
        "}",
        "",
        "static ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,",
        "			     const char *buf, size_t count)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	struct workqueue_attrs *attrs;",
        "	int ret = -ENOMEM;",
        "",
        "	apply_wqattrs_lock();",
        "",
        "	attrs = wq_sysfs_prep_attrs(wq);",
        "	if (!attrs)",
        "		goto out_unlock;",
        "",
        "	if (sscanf(buf, \"%d\", &attrs->nice) == 1 &&",
        "	    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)",
        "		ret = apply_workqueue_attrs_locked(wq, attrs);",
        "	else",
        "		ret = -EINVAL;",
        "",
        "out_unlock:",
        "	apply_wqattrs_unlock();",
        "	free_workqueue_attrs(attrs);",
        "	return ret ?: count;",
        "}",
        "",
        "static ssize_t wq_cpumask_show(struct device *dev,",
        "			       struct device_attribute *attr, char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	int written;",
        "",
        "	mutex_lock(&wq->mutex);",
        "	written = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",",
        "			    cpumask_pr_args(wq->unbound_attrs->cpumask));",
        "	mutex_unlock(&wq->mutex);",
        "	return written;",
        "}",
        "",
        "static ssize_t wq_cpumask_store(struct device *dev,",
        "				struct device_attribute *attr,",
        "				const char *buf, size_t count)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	struct workqueue_attrs *attrs;",
        "	int ret = -ENOMEM;",
        "",
        "	apply_wqattrs_lock();",
        "",
        "	attrs = wq_sysfs_prep_attrs(wq);",
        "	if (!attrs)",
        "		goto out_unlock;",
        "",
        "	ret = cpumask_parse(buf, attrs->cpumask);",
        "	if (!ret)",
        "		ret = apply_workqueue_attrs_locked(wq, attrs);",
        "",
        "out_unlock:",
        "	apply_wqattrs_unlock();",
        "	free_workqueue_attrs(attrs);",
        "	return ret ?: count;",
        "}",
        "",
        "static ssize_t wq_affn_scope_show(struct device *dev,",
        "				  struct device_attribute *attr, char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	int written;",
        "",
        "	mutex_lock(&wq->mutex);",
        "	if (wq->unbound_attrs->affn_scope == WQ_AFFN_DFL)",
        "		written = scnprintf(buf, PAGE_SIZE, \"%s (%s)\\n\",",
        "				    wq_affn_names[WQ_AFFN_DFL],",
        "				    wq_affn_names[wq_affn_dfl]);",
        "	else",
        "		written = scnprintf(buf, PAGE_SIZE, \"%s\\n\",",
        "				    wq_affn_names[wq->unbound_attrs->affn_scope]);",
        "	mutex_unlock(&wq->mutex);",
        "",
        "	return written;",
        "}",
        "",
        "static ssize_t wq_affn_scope_store(struct device *dev,",
        "				   struct device_attribute *attr,",
        "				   const char *buf, size_t count)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	struct workqueue_attrs *attrs;",
        "	int affn, ret = -ENOMEM;",
        "",
        "	affn = parse_affn_scope(buf);",
        "	if (affn < 0)",
        "		return affn;",
        "",
        "	apply_wqattrs_lock();",
        "	attrs = wq_sysfs_prep_attrs(wq);",
        "	if (attrs) {",
        "		attrs->affn_scope = affn;",
        "		ret = apply_workqueue_attrs_locked(wq, attrs);",
        "	}",
        "	apply_wqattrs_unlock();",
        "	free_workqueue_attrs(attrs);",
        "	return ret ?: count;",
        "}",
        "",
        "static ssize_t wq_affinity_strict_show(struct device *dev,",
        "				       struct device_attribute *attr, char *buf)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "",
        "	return scnprintf(buf, PAGE_SIZE, \"%d\\n\",",
        "			 wq->unbound_attrs->affn_strict);",
        "}",
        "",
        "static ssize_t wq_affinity_strict_store(struct device *dev,",
        "					struct device_attribute *attr,",
        "					const char *buf, size_t count)",
        "{",
        "	struct workqueue_struct *wq = dev_to_wq(dev);",
        "	struct workqueue_attrs *attrs;",
        "	int v, ret = -ENOMEM;",
        "",
        "	if (sscanf(buf, \"%d\", &v) != 1)",
        "		return -EINVAL;",
        "",
        "	apply_wqattrs_lock();",
        "	attrs = wq_sysfs_prep_attrs(wq);",
        "	if (attrs) {",
        "		attrs->affn_strict = (bool)v;",
        "		ret = apply_workqueue_attrs_locked(wq, attrs);",
        "	}",
        "	apply_wqattrs_unlock();",
        "	free_workqueue_attrs(attrs);",
        "	return ret ?: count;",
        "}",
        "",
        "static struct device_attribute wq_sysfs_unbound_attrs[] = {",
        "	__ATTR(nice, 0644, wq_nice_show, wq_nice_store),",
        "	__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),",
        "	__ATTR(affinity_scope, 0644, wq_affn_scope_show, wq_affn_scope_store),",
        "	__ATTR(affinity_strict, 0644, wq_affinity_strict_show, wq_affinity_strict_store),",
        "	__ATTR_NULL,",
        "};",
        "",
        "static const struct bus_type wq_subsys = {",
        "	.name				= \"workqueue\",",
        "	.dev_groups			= wq_sysfs_groups,",
        "};",
        "",
        "/**",
        " *  workqueue_set_unbound_cpumask - Set the low-level unbound cpumask",
        " *  @cpumask: the cpumask to set",
        " *",
        " *  The low-level workqueues cpumask is a global cpumask that limits",
        " *  the affinity of all unbound workqueues.  This function check the @cpumask",
        " *  and apply it to all unbound workqueues and updates all pwqs of them.",
        " *",
        " *  Return:	0	- Success",
        " *		-EINVAL	- Invalid @cpumask",
        " *		-ENOMEM	- Failed to allocate memory for attrs or pwqs.",
        " */",
        "static int workqueue_set_unbound_cpumask(cpumask_var_t cpumask)",
        "{",
        "	int ret = -EINVAL;",
        "",
        "	/*",
        "	 * Not excluding isolated cpus on purpose.",
        "	 * If the user wishes to include them, we allow that.",
        "	 */",
        "	cpumask_and(cpumask, cpumask, cpu_possible_mask);",
        "	if (!cpumask_empty(cpumask)) {",
        "		ret = 0;",
        "		apply_wqattrs_lock();",
        "		if (!cpumask_equal(cpumask, wq_unbound_cpumask))",
        "			ret = workqueue_apply_unbound_cpumask(cpumask);",
        "		if (!ret)",
        "			cpumask_copy(wq_requested_unbound_cpumask, cpumask);",
        "		apply_wqattrs_unlock();",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "static ssize_t __wq_cpumask_show(struct device *dev,",
        "		struct device_attribute *attr, char *buf, cpumask_var_t mask)",
        "{",
        "	int written;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "	written = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\", cpumask_pr_args(mask));",
        "	mutex_unlock(&wq_pool_mutex);",
        "",
        "	return written;",
        "}",
        "",
        "static ssize_t cpumask_requested_show(struct device *dev,",
        "		struct device_attribute *attr, char *buf)",
        "{",
        "	return __wq_cpumask_show(dev, attr, buf, wq_requested_unbound_cpumask);",
        "}",
        "static DEVICE_ATTR_RO(cpumask_requested);",
        "",
        "static ssize_t cpumask_isolated_show(struct device *dev,",
        "		struct device_attribute *attr, char *buf)",
        "{",
        "	return __wq_cpumask_show(dev, attr, buf, wq_isolated_cpumask);",
        "}",
        "static DEVICE_ATTR_RO(cpumask_isolated);",
        "",
        "static ssize_t cpumask_show(struct device *dev,",
        "		struct device_attribute *attr, char *buf)",
        "{",
        "	return __wq_cpumask_show(dev, attr, buf, wq_unbound_cpumask);",
        "}",
        "",
        "static ssize_t cpumask_store(struct device *dev,",
        "		struct device_attribute *attr, const char *buf, size_t count)",
        "{",
        "	cpumask_var_t cpumask;",
        "	int ret;",
        "",
        "	if (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))",
        "		return -ENOMEM;",
        "",
        "	ret = cpumask_parse(buf, cpumask);",
        "	if (!ret)",
        "		ret = workqueue_set_unbound_cpumask(cpumask);",
        "",
        "	free_cpumask_var(cpumask);",
        "	return ret ? ret : count;",
        "}",
        "static DEVICE_ATTR_RW(cpumask);",
        "",
        "static struct attribute *wq_sysfs_cpumask_attrs[] = {",
        "	&dev_attr_cpumask.attr,",
        "	&dev_attr_cpumask_requested.attr,",
        "	&dev_attr_cpumask_isolated.attr,",
        "	NULL,",
        "};",
        "ATTRIBUTE_GROUPS(wq_sysfs_cpumask);",
        "",
        "static int __init wq_sysfs_init(void)",
        "{",
        "	return subsys_virtual_register(&wq_subsys, wq_sysfs_cpumask_groups);",
        "}",
        "core_initcall(wq_sysfs_init);",
        "",
        "static void wq_device_release(struct device *dev)",
        "{",
        "	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);",
        "",
        "	kfree(wq_dev);",
        "}",
        "",
        "/**",
        " * workqueue_sysfs_register - make a workqueue visible in sysfs",
        " * @wq: the workqueue to register",
        " *",
        " * Expose @wq in sysfs under /sys/bus/workqueue/devices.",
        " * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set",
        " * which is the preferred method.",
        " *",
        " * Workqueue user should use this function directly iff it wants to apply",
        " * workqueue_attrs before making the workqueue visible in sysfs; otherwise,",
        " * apply_workqueue_attrs() may race against userland updating the",
        " * attributes.",
        " *",
        " * Return: 0 on success, -errno on failure.",
        " */",
        "int workqueue_sysfs_register(struct workqueue_struct *wq)",
        "{",
        "	struct wq_device *wq_dev;",
        "	int ret;",
        "",
        "	/*",
        "	 * Adjusting max_active breaks ordering guarantee.  Disallow exposing",
        "	 * ordered workqueues.",
        "	 */",
        "	if (WARN_ON(wq->flags & __WQ_ORDERED))",
        "		return -EINVAL;",
        "",
        "	wq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);",
        "	if (!wq_dev)",
        "		return -ENOMEM;",
        "",
        "	wq_dev->wq = wq;",
        "	wq_dev->dev.bus = &wq_subsys;",
        "	wq_dev->dev.release = wq_device_release;",
        "	dev_set_name(&wq_dev->dev, \"%s\", wq->name);",
        "",
        "	/*",
        "	 * unbound_attrs are created separately.  Suppress uevent until",
        "	 * everything is ready.",
        "	 */",
        "	dev_set_uevent_suppress(&wq_dev->dev, true);",
        "",
        "	ret = device_register(&wq_dev->dev);",
        "	if (ret) {",
        "		put_device(&wq_dev->dev);",
        "		wq->wq_dev = NULL;",
        "		return ret;",
        "	}",
        "",
        "	if (wq->flags & WQ_UNBOUND) {",
        "		struct device_attribute *attr;",
        "",
        "		for (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {",
        "			ret = device_create_file(&wq_dev->dev, attr);",
        "			if (ret) {",
        "				device_unregister(&wq_dev->dev);",
        "				wq->wq_dev = NULL;",
        "				return ret;",
        "			}",
        "		}",
        "	}",
        "",
        "	dev_set_uevent_suppress(&wq_dev->dev, false);",
        "	kobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);",
        "	return 0;",
        "}",
        "",
        "/**",
        " * workqueue_sysfs_unregister - undo workqueue_sysfs_register()",
        " * @wq: the workqueue to unregister",
        " *",
        " * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.",
        " */",
        "static void workqueue_sysfs_unregister(struct workqueue_struct *wq)",
        "{",
        "	struct wq_device *wq_dev = wq->wq_dev;",
        "",
        "	if (!wq->wq_dev)",
        "		return;",
        "",
        "	wq->wq_dev = NULL;",
        "	device_unregister(&wq_dev->dev);",
        "}",
        "#else	/* CONFIG_SYSFS */",
        "static void workqueue_sysfs_unregister(struct workqueue_struct *wq)	{ }",
        "#endif	/* CONFIG_SYSFS */",
        "",
        "/*",
        " * Workqueue watchdog.",
        " *",
        " * Stall may be caused by various bugs - missing WQ_MEM_RECLAIM, illegal",
        " * flush dependency, a concurrency managed work item which stays RUNNING",
        " * indefinitely.  Workqueue stalls can be very difficult to debug as the",
        " * usual warning mechanisms don't trigger and internal workqueue state is",
        " * largely opaque.",
        " *",
        " * Workqueue watchdog monitors all worker pools periodically and dumps",
        " * state if some pools failed to make forward progress for a while where",
        " * forward progress is defined as the first item on ->worklist changing.",
        " *",
        " * This mechanism is controlled through the kernel parameter",
        " * \"workqueue.watchdog_thresh\" which can be updated at runtime through the",
        " * corresponding sysfs parameter file.",
        " */",
        "#ifdef CONFIG_WQ_WATCHDOG",
        "",
        "static unsigned long wq_watchdog_thresh = 30;",
        "static struct timer_list wq_watchdog_timer;",
        "",
        "static unsigned long wq_watchdog_touched = INITIAL_JIFFIES;",
        "static DEFINE_PER_CPU(unsigned long, wq_watchdog_touched_cpu) = INITIAL_JIFFIES;",
        "",
        "static unsigned int wq_panic_on_stall;",
        "module_param_named(panic_on_stall, wq_panic_on_stall, uint, 0644);",
        "",
        "/*",
        " * Show workers that might prevent the processing of pending work items.",
        " * The only candidates are CPU-bound workers in the running state.",
        " * Pending work items should be handled by another idle worker",
        " * in all other situations.",
        " */",
        "static void show_cpu_pool_hog(struct worker_pool *pool)",
        "{",
        "	struct worker *worker;",
        "	unsigned long irq_flags;",
        "	int bkt;",
        "",
        "	raw_spin_lock_irqsave(&pool->lock, irq_flags);",
        "",
        "	hash_for_each(pool->busy_hash, bkt, worker, hentry) {",
        "		if (task_is_running(worker->task)) {",
        "			/*",
        "			 * Defer printing to avoid deadlocks in console",
        "			 * drivers that queue work while holding locks",
        "			 * also taken in their write paths.",
        "			 */",
        "			printk_deferred_enter();",
        "",
        "			pr_info(\"pool %d:\\n\", pool->id);",
        "			sched_show_task(worker->task);",
        "",
        "			printk_deferred_exit();",
        "		}",
        "	}",
        "",
        "	raw_spin_unlock_irqrestore(&pool->lock, irq_flags);",
        "}",
        "",
        "static void show_cpu_pools_hogs(void)",
        "{",
        "	struct worker_pool *pool;",
        "	int pi;",
        "",
        "	pr_info(\"Showing backtraces of running workers in stalled CPU-bound worker pools:\\n\");",
        "",
        "	rcu_read_lock();",
        "",
        "	for_each_pool(pool, pi) {",
        "		if (pool->cpu_stall)",
        "			show_cpu_pool_hog(pool);",
        "",
        "	}",
        "",
        "	rcu_read_unlock();",
        "}",
        "",
        "static void panic_on_wq_watchdog(void)",
        "{",
        "	static unsigned int wq_stall;",
        "",
        "	if (wq_panic_on_stall) {",
        "		wq_stall++;",
        "		BUG_ON(wq_stall >= wq_panic_on_stall);",
        "	}",
        "}",
        "",
        "static void wq_watchdog_reset_touched(void)",
        "{",
        "	int cpu;",
        "",
        "	wq_watchdog_touched = jiffies;",
        "	for_each_possible_cpu(cpu)",
        "		per_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;",
        "}",
        "",
        "static void wq_watchdog_timer_fn(struct timer_list *unused)",
        "{",
        "	unsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
        "	bool lockup_detected = false;",
        "	bool cpu_pool_stall = false;",
        "	unsigned long now = jiffies;",
        "	struct worker_pool *pool;",
        "	int pi;",
        "",
        "	if (!thresh)",
        "		return;",
        "",
        "	rcu_read_lock();",
        "",
        "	for_each_pool(pool, pi) {",
        "		unsigned long pool_ts, touched, ts;",
        "",
        "		pool->cpu_stall = false;",
        "		if (list_empty(&pool->worklist))",
        "			continue;",
        "",
        "		/*",
        "		 * If a virtual machine is stopped by the host it can look to",
        "		 * the watchdog like a stall.",
        "		 */",
        "		kvm_check_and_clear_guest_paused();",
        "",
        "		/* get the latest of pool and touched timestamps */",
        "		if (pool->cpu >= 0)",
        "			touched = READ_ONCE(per_cpu(wq_watchdog_touched_cpu, pool->cpu));",
        "		else",
        "			touched = READ_ONCE(wq_watchdog_touched);",
        "		pool_ts = READ_ONCE(pool->watchdog_ts);",
        "",
        "		if (time_after(pool_ts, touched))",
        "			ts = pool_ts;",
        "		else",
        "			ts = touched;",
        "",
        "		/* did we stall? */",
        "		if (time_after(now, ts + thresh)) {",
        "			lockup_detected = true;",
        "			if (pool->cpu >= 0 && !(pool->flags & POOL_BH)) {",
        "				pool->cpu_stall = true;",
        "				cpu_pool_stall = true;",
        "			}",
        "			pr_emerg(\"BUG: workqueue lockup - pool\");",
        "			pr_cont_pool_info(pool);",
        "			pr_cont(\" stuck for %us!\\n\",",
        "				jiffies_to_msecs(now - pool_ts) / 1000);",
        "		}",
        "",
        "",
        "	}",
        "",
        "	rcu_read_unlock();",
        "",
        "	if (lockup_detected)",
        "		show_all_workqueues();",
        "",
        "	if (cpu_pool_stall)",
        "		show_cpu_pools_hogs();",
        "",
        "	if (lockup_detected)",
        "		panic_on_wq_watchdog();",
        "",
        "	wq_watchdog_reset_touched();",
        "	mod_timer(&wq_watchdog_timer, jiffies + thresh);",
        "}",
        "",
        "notrace void wq_watchdog_touch(int cpu)",
        "{",
        "	unsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
        "	unsigned long touch_ts = READ_ONCE(wq_watchdog_touched);",
        "	unsigned long now = jiffies;",
        "",
        "	if (cpu >= 0)",
        "		per_cpu(wq_watchdog_touched_cpu, cpu) = now;",
        "	else",
        "		WARN_ONCE(1, \"%s should be called with valid CPU\", __func__);",
        "",
        "	/* Don't unnecessarily store to global cacheline */",
        "	if (time_after(now, touch_ts + thresh / 4))",
        "		WRITE_ONCE(wq_watchdog_touched, jiffies);",
        "}",
        "",
        "static void wq_watchdog_set_thresh(unsigned long thresh)",
        "{",
        "	wq_watchdog_thresh = 0;",
        "	del_timer_sync(&wq_watchdog_timer);",
        "",
        "	if (thresh) {",
        "		wq_watchdog_thresh = thresh;",
        "		wq_watchdog_reset_touched();",
        "		mod_timer(&wq_watchdog_timer, jiffies + thresh * HZ);",
        "	}",
        "}",
        "",
        "static int wq_watchdog_param_set_thresh(const char *val,",
        "					const struct kernel_param *kp)",
        "{",
        "	unsigned long thresh;",
        "	int ret;",
        "",
        "	ret = kstrtoul(val, 0, &thresh);",
        "	if (ret)",
        "		return ret;",
        "",
        "	if (system_wq)",
        "		wq_watchdog_set_thresh(thresh);",
        "	else",
        "		wq_watchdog_thresh = thresh;",
        "",
        "	return 0;",
        "}",
        "",
        "static const struct kernel_param_ops wq_watchdog_thresh_ops = {",
        "	.set	= wq_watchdog_param_set_thresh,",
        "	.get	= param_get_ulong,",
        "};",
        "",
        "module_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,",
        "		0644);",
        "",
        "static void wq_watchdog_init(void)",
        "{",
        "	timer_setup(&wq_watchdog_timer, wq_watchdog_timer_fn, TIMER_DEFERRABLE);",
        "	wq_watchdog_set_thresh(wq_watchdog_thresh);",
        "}",
        "",
        "#else	/* CONFIG_WQ_WATCHDOG */",
        "",
        "static inline void wq_watchdog_init(void) { }",
        "",
        "#endif	/* CONFIG_WQ_WATCHDOG */",
        "",
        "static void bh_pool_kick_normal(struct irq_work *irq_work)",
        "{",
        "	raise_softirq_irqoff(TASKLET_SOFTIRQ);",
        "}",
        "",
        "static void bh_pool_kick_highpri(struct irq_work *irq_work)",
        "{",
        "	raise_softirq_irqoff(HI_SOFTIRQ);",
        "}",
        "",
        "static void __init restrict_unbound_cpumask(const char *name, const struct cpumask *mask)",
        "{",
        "	if (!cpumask_intersects(wq_unbound_cpumask, mask)) {",
        "		pr_warn(\"workqueue: Restricting unbound_cpumask (%*pb) with %s (%*pb) leaves no CPU, ignoring\\n\",",
        "			cpumask_pr_args(wq_unbound_cpumask), name, cpumask_pr_args(mask));",
        "		return;",
        "	}",
        "",
        "	cpumask_and(wq_unbound_cpumask, wq_unbound_cpumask, mask);",
        "}",
        "",
        "static void __init init_cpu_worker_pool(struct worker_pool *pool, int cpu, int nice)",
        "{",
        "	BUG_ON(init_worker_pool(pool));",
        "	pool->cpu = cpu;",
        "	cpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));",
        "	cpumask_copy(pool->attrs->__pod_cpumask, cpumask_of(cpu));",
        "	pool->attrs->nice = nice;",
        "	pool->attrs->affn_strict = true;",
        "	pool->node = cpu_to_node(cpu);",
        "",
        "	/* alloc pool ID */",
        "	mutex_lock(&wq_pool_mutex);",
        "	BUG_ON(worker_pool_assign_id(pool));",
        "	mutex_unlock(&wq_pool_mutex);",
        "}",
        "",
        "/**",
        " * workqueue_init_early - early init for workqueue subsystem",
        " *",
        " * This is the first step of three-staged workqueue subsystem initialization and",
        " * invoked as soon as the bare basics - memory allocation, cpumasks and idr are",
        " * up. It sets up all the data structures and system workqueues and allows early",
        " * boot code to create workqueues and queue/cancel work items. Actual work item",
        " * execution starts only after kthreads can be created and scheduled right",
        " * before early initcalls.",
        " */",
        "void __init workqueue_init_early(void)",
        "{",
        "	struct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_SYSTEM];",
        "	int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };",
        "	void (*irq_work_fns[2])(struct irq_work *) = { bh_pool_kick_normal,",
        "						       bh_pool_kick_highpri };",
        "	int i, cpu;",
        "",
        "	BUILD_BUG_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));",
        "",
        "	BUG_ON(!alloc_cpumask_var(&wq_online_cpumask, GFP_KERNEL));",
        "	BUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));",
        "	BUG_ON(!alloc_cpumask_var(&wq_requested_unbound_cpumask, GFP_KERNEL));",
        "	BUG_ON(!zalloc_cpumask_var(&wq_isolated_cpumask, GFP_KERNEL));",
        "",
        "	cpumask_copy(wq_online_cpumask, cpu_online_mask);",
        "	cpumask_copy(wq_unbound_cpumask, cpu_possible_mask);",
        "	restrict_unbound_cpumask(\"HK_TYPE_WQ\", housekeeping_cpumask(HK_TYPE_WQ));",
        "	restrict_unbound_cpumask(\"HK_TYPE_DOMAIN\", housekeeping_cpumask(HK_TYPE_DOMAIN));",
        "	if (!cpumask_empty(&wq_cmdline_cpumask))",
        "		restrict_unbound_cpumask(\"workqueue.unbound_cpus\", &wq_cmdline_cpumask);",
        "",
        "	cpumask_copy(wq_requested_unbound_cpumask, wq_unbound_cpumask);",
        "",
        "	pwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);",
        "",
        "	unbound_wq_update_pwq_attrs_buf = alloc_workqueue_attrs();",
        "	BUG_ON(!unbound_wq_update_pwq_attrs_buf);",
        "",
        "	/*",
        "	 * If nohz_full is enabled, set power efficient workqueue as unbound.",
        "	 * This allows workqueue items to be moved to HK CPUs.",
        "	 */",
        "	if (housekeeping_enabled(HK_TYPE_TICK))",
        "		wq_power_efficient = true;",
        "",
        "	/* initialize WQ_AFFN_SYSTEM pods */",
        "	pt->pod_cpus = kcalloc(1, sizeof(pt->pod_cpus[0]), GFP_KERNEL);",
        "	pt->pod_node = kcalloc(1, sizeof(pt->pod_node[0]), GFP_KERNEL);",
        "	pt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);",
        "	BUG_ON(!pt->pod_cpus || !pt->pod_node || !pt->cpu_pod);",
        "",
        "	BUG_ON(!zalloc_cpumask_var_node(&pt->pod_cpus[0], GFP_KERNEL, NUMA_NO_NODE));",
        "",
        "	pt->nr_pods = 1;",
        "	cpumask_copy(pt->pod_cpus[0], cpu_possible_mask);",
        "	pt->pod_node[0] = NUMA_NO_NODE;",
        "	pt->cpu_pod[0] = 0;",
        "",
        "	/* initialize BH and CPU pools */",
        "	for_each_possible_cpu(cpu) {",
        "		struct worker_pool *pool;",
        "",
        "		i = 0;",
        "		for_each_bh_worker_pool(pool, cpu) {",
        "			init_cpu_worker_pool(pool, cpu, std_nice[i]);",
        "			pool->flags |= POOL_BH;",
        "			init_irq_work(bh_pool_irq_work(pool), irq_work_fns[i]);",
        "			i++;",
        "		}",
        "",
        "		i = 0;",
        "		for_each_cpu_worker_pool(pool, cpu)",
        "			init_cpu_worker_pool(pool, cpu, std_nice[i++]);",
        "	}",
        "",
        "	/* create default unbound and ordered wq attrs */",
        "	for (i = 0; i < NR_STD_WORKER_POOLS; i++) {",
        "		struct workqueue_attrs *attrs;",
        "",
        "		BUG_ON(!(attrs = alloc_workqueue_attrs()));",
        "		attrs->nice = std_nice[i];",
        "		unbound_std_wq_attrs[i] = attrs;",
        "",
        "		/*",
        "		 * An ordered wq should have only one pwq as ordering is",
        "		 * guaranteed by max_active which is enforced by pwqs.",
        "		 */",
        "		BUG_ON(!(attrs = alloc_workqueue_attrs()));",
        "		attrs->nice = std_nice[i];",
        "		attrs->ordered = true;",
        "		ordered_wq_attrs[i] = attrs;",
        "	}",
        "",
        "	system_wq = alloc_workqueue(\"events\", 0, 0);",
        "	system_highpri_wq = alloc_workqueue(\"events_highpri\", WQ_HIGHPRI, 0);",
        "	system_long_wq = alloc_workqueue(\"events_long\", 0, 0);",
        "	system_unbound_wq = alloc_workqueue(\"events_unbound\", WQ_UNBOUND,",
        "					    WQ_MAX_ACTIVE);",
        "	system_freezable_wq = alloc_workqueue(\"events_freezable\",",
        "					      WQ_FREEZABLE, 0);",
        "	system_power_efficient_wq = alloc_workqueue(\"events_power_efficient\",",
        "					      WQ_POWER_EFFICIENT, 0);",
        "	system_freezable_power_efficient_wq = alloc_workqueue(\"events_freezable_pwr_efficient\",",
        "					      WQ_FREEZABLE | WQ_POWER_EFFICIENT,",
        "					      0);",
        "	system_bh_wq = alloc_workqueue(\"events_bh\", WQ_BH, 0);",
        "	system_bh_highpri_wq = alloc_workqueue(\"events_bh_highpri\",",
        "					       WQ_BH | WQ_HIGHPRI, 0);",
        "	BUG_ON(!system_wq || !system_highpri_wq || !system_long_wq ||",
        "	       !system_unbound_wq || !system_freezable_wq ||",
        "	       !system_power_efficient_wq ||",
        "	       !system_freezable_power_efficient_wq ||",
        "	       !system_bh_wq || !system_bh_highpri_wq);",
        "}",
        "",
        "static void __init wq_cpu_intensive_thresh_init(void)",
        "{",
        "	unsigned long thresh;",
        "	unsigned long bogo;",
        "",
        "	pwq_release_worker = kthread_create_worker(0, \"pool_workqueue_release\");",
        "	BUG_ON(IS_ERR(pwq_release_worker));",
        "",
        "	/* if the user set it to a specific value, keep it */",
        "	if (wq_cpu_intensive_thresh_us != ULONG_MAX)",
        "		return;",
        "",
        "	/*",
        "	 * The default of 10ms is derived from the fact that most modern (as of",
        "	 * 2023) processors can do a lot in 10ms and that it's just below what",
        "	 * most consider human-perceivable. However, the kernel also runs on a",
        "	 * lot slower CPUs including microcontrollers where the threshold is way",
        "	 * too low.",
        "	 *",
        "	 * Let's scale up the threshold upto 1 second if BogoMips is below 4000.",
        "	 * This is by no means accurate but it doesn't have to be. The mechanism",
        "	 * is still useful even when the threshold is fully scaled up. Also, as",
        "	 * the reports would usually be applicable to everyone, some machines",
        "	 * operating on longer thresholds won't significantly diminish their",
        "	 * usefulness.",
        "	 */",
        "	thresh = 10 * USEC_PER_MSEC;",
        "",
        "	/* see init/calibrate.c for lpj -> BogoMIPS calculation */",
        "	bogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);",
        "	if (bogo < 4000)",
        "		thresh = min_t(unsigned long, thresh * 4000 / bogo, USEC_PER_SEC);",
        "",
        "	pr_debug(\"wq_cpu_intensive_thresh: lpj=%lu BogoMIPS=%lu thresh_us=%lu\\n\",",
        "		 loops_per_jiffy, bogo, thresh);",
        "",
        "	wq_cpu_intensive_thresh_us = thresh;",
        "}",
        "",
        "/**",
        " * workqueue_init - bring workqueue subsystem fully online",
        " *",
        " * This is the second step of three-staged workqueue subsystem initialization",
        " * and invoked as soon as kthreads can be created and scheduled. Workqueues have",
        " * been created and work items queued on them, but there are no kworkers",
        " * executing the work items yet. Populate the worker pools with the initial",
        " * workers and enable future kworker creations.",
        " */",
        "void __init workqueue_init(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "	struct worker_pool *pool;",
        "	int cpu, bkt;",
        "",
        "	wq_cpu_intensive_thresh_init();",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	/*",
        "	 * Per-cpu pools created earlier could be missing node hint. Fix them",
        "	 * up. Also, create a rescuer for workqueues that requested it.",
        "	 */",
        "	for_each_possible_cpu(cpu) {",
        "		for_each_bh_worker_pool(pool, cpu)",
        "			pool->node = cpu_to_node(cpu);",
        "		for_each_cpu_worker_pool(pool, cpu)",
        "			pool->node = cpu_to_node(cpu);",
        "	}",
        "",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		WARN(init_rescuer(wq),",
        "		     \"workqueue: failed to create early rescuer for %s\",",
        "		     wq->name);",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "",
        "	/*",
        "	 * Create the initial workers. A BH pool has one pseudo worker that",
        "	 * represents the shared BH execution context and thus doesn't get",
        "	 * affected by hotplug events. Create the BH pseudo workers for all",
        "	 * possible CPUs here.",
        "	 */",
        "	for_each_possible_cpu(cpu)",
        "		for_each_bh_worker_pool(pool, cpu)",
        "			BUG_ON(!create_worker(pool));",
        "",
        "	for_each_online_cpu(cpu) {",
        "		for_each_cpu_worker_pool(pool, cpu) {",
        "			pool->flags &= ~POOL_DISASSOCIATED;",
        "			BUG_ON(!create_worker(pool));",
        "		}",
        "	}",
        "",
        "	hash_for_each(unbound_pool_hash, bkt, pool, hash_node)",
        "		BUG_ON(!create_worker(pool));",
        "",
        "	wq_online = true;",
        "	wq_watchdog_init();",
        "}",
        "",
        "/*",
        " * Initialize @pt by first initializing @pt->cpu_pod[] with pod IDs according to",
        " * @cpu_shares_pod(). Each subset of CPUs that share a pod is assigned a unique",
        " * and consecutive pod ID. The rest of @pt is initialized accordingly.",
        " */",
        "static void __init init_pod_type(struct wq_pod_type *pt,",
        "				 bool (*cpus_share_pod)(int, int))",
        "{",
        "	int cur, pre, cpu, pod;",
        "",
        "	pt->nr_pods = 0;",
        "",
        "	/* init @pt->cpu_pod[] according to @cpus_share_pod() */",
        "	pt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);",
        "	BUG_ON(!pt->cpu_pod);",
        "",
        "	for_each_possible_cpu(cur) {",
        "		for_each_possible_cpu(pre) {",
        "			if (pre >= cur) {",
        "				pt->cpu_pod[cur] = pt->nr_pods++;",
        "				break;",
        "			}",
        "			if (cpus_share_pod(cur, pre)) {",
        "				pt->cpu_pod[cur] = pt->cpu_pod[pre];",
        "				break;",
        "			}",
        "		}",
        "	}",
        "",
        "	/* init the rest to match @pt->cpu_pod[] */",
        "	pt->pod_cpus = kcalloc(pt->nr_pods, sizeof(pt->pod_cpus[0]), GFP_KERNEL);",
        "	pt->pod_node = kcalloc(pt->nr_pods, sizeof(pt->pod_node[0]), GFP_KERNEL);",
        "	BUG_ON(!pt->pod_cpus || !pt->pod_node);",
        "",
        "	for (pod = 0; pod < pt->nr_pods; pod++)",
        "		BUG_ON(!zalloc_cpumask_var(&pt->pod_cpus[pod], GFP_KERNEL));",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		cpumask_set_cpu(cpu, pt->pod_cpus[pt->cpu_pod[cpu]]);",
        "		pt->pod_node[pt->cpu_pod[cpu]] = cpu_to_node(cpu);",
        "	}",
        "}",
        "",
        "static bool __init cpus_dont_share(int cpu0, int cpu1)",
        "{",
        "	return false;",
        "}",
        "",
        "static bool __init cpus_share_smt(int cpu0, int cpu1)",
        "{",
        "#ifdef CONFIG_SCHED_SMT",
        "	return cpumask_test_cpu(cpu0, cpu_smt_mask(cpu1));",
        "#else",
        "	return false;",
        "#endif",
        "}",
        "",
        "static bool __init cpus_share_numa(int cpu0, int cpu1)",
        "{",
        "	return cpu_to_node(cpu0) == cpu_to_node(cpu1);",
        "}",
        "",
        "/**",
        " * workqueue_init_topology - initialize CPU pods for unbound workqueues",
        " *",
        " * This is the third step of three-staged workqueue subsystem initialization and",
        " * invoked after SMP and topology information are fully initialized. It",
        " * initializes the unbound CPU pods accordingly.",
        " */",
        "void __init workqueue_init_topology(void)",
        "{",
        "	struct workqueue_struct *wq;",
        "	int cpu;",
        "",
        "	init_pod_type(&wq_pod_types[WQ_AFFN_CPU], cpus_dont_share);",
        "	init_pod_type(&wq_pod_types[WQ_AFFN_SMT], cpus_share_smt);",
        "	init_pod_type(&wq_pod_types[WQ_AFFN_CACHE], cpus_share_cache);",
        "	init_pod_type(&wq_pod_types[WQ_AFFN_NUMA], cpus_share_numa);",
        "",
        "	wq_topo_initialized = true;",
        "",
        "	mutex_lock(&wq_pool_mutex);",
        "",
        "	/*",
        "	 * Workqueues allocated earlier would have all CPUs sharing the default",
        "	 * worker pool. Explicitly call unbound_wq_update_pwq() on all workqueue",
        "	 * and CPU combinations to apply per-pod sharing.",
        "	 */",
        "	list_for_each_entry(wq, &workqueues, list) {",
        "		for_each_online_cpu(cpu)",
        "			unbound_wq_update_pwq(wq, cpu);",
        "		if (wq->flags & WQ_UNBOUND) {",
        "			mutex_lock(&wq->mutex);",
        "			wq_update_node_max_active(wq, -1);",
        "			mutex_unlock(&wq->mutex);",
        "		}",
        "	}",
        "",
        "	mutex_unlock(&wq_pool_mutex);",
        "}",
        "",
        "void __warn_flushing_systemwide_wq(void)",
        "{",
        "	pr_warn(\"WARNING: Flushing system-wide workqueues will be prohibited in near future.\\n\");",
        "	dump_stack();",
        "}",
        "EXPORT_SYMBOL(__warn_flushing_systemwide_wq);",
        "",
        "static int __init workqueue_unbound_cpus_setup(char *str)",
        "{",
        "	if (cpulist_parse(str, &wq_cmdline_cpumask) < 0) {",
        "		cpumask_clear(&wq_cmdline_cpumask);",
        "		pr_warn(\"workqueue.unbound_cpus: incorrect CPU range, using default\\n\");",
        "	}",
        "",
        "	return 1;",
        "}",
        "__setup(\"workqueue.unbound_cpus=\", workqueue_unbound_cpus_setup);"
    ]
  },
  "kernel_kprobes_c": {
    path: "kernel/kprobes.c",
    covered: [298, 293],
    totalLines: 3049,
    coveredCount: 2,
    coveragePct: 0.1,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-or-later",
        "/*",
        " *  Kernel Probes (KProbes)",
        " *",
        " * Copyright (C) IBM Corporation, 2002, 2004",
        " *",
        " * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel",
        " *		Probes initial implementation (includes suggestions from",
        " *		Rusty Russell).",
        " * 2004-Aug	Updated by Prasanna S Panchamukhi <prasanna@in.ibm.com> with",
        " *		hlists and exceptions notifier as suggested by Andi Kleen.",
        " * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes",
        " *		interface to access function arguments.",
        " * 2004-Sep	Prasanna S Panchamukhi <prasanna@in.ibm.com> Changed Kprobes",
        " *		exceptions notifier to be first on the priority list.",
        " * 2005-May	Hien Nguyen <hien@us.ibm.com>, Jim Keniston",
        " *		<jkenisto@us.ibm.com> and Prasanna S Panchamukhi",
        " *		<prasanna@in.ibm.com> added function-return probes.",
        " */",
        "",
        "#define pr_fmt(fmt) \"kprobes: \" fmt",
        "",
        "#include <linux/kprobes.h>",
        "#include <linux/hash.h>",
        "#include <linux/init.h>",
        "#include <linux/slab.h>",
        "#include <linux/stddef.h>",
        "#include <linux/export.h>",
        "#include <linux/kallsyms.h>",
        "#include <linux/freezer.h>",
        "#include <linux/seq_file.h>",
        "#include <linux/debugfs.h>",
        "#include <linux/sysctl.h>",
        "#include <linux/kdebug.h>",
        "#include <linux/memory.h>",
        "#include <linux/ftrace.h>",
        "#include <linux/cpu.h>",
        "#include <linux/jump_label.h>",
        "#include <linux/static_call.h>",
        "#include <linux/perf_event.h>",
        "#include <linux/execmem.h>",
        "",
        "#include <asm/sections.h>",
        "#include <asm/cacheflush.h>",
        "#include <asm/errno.h>",
        "#include <linux/uaccess.h>",
        "",
        "#define KPROBE_HASH_BITS 6",
        "#define KPROBE_TABLE_SIZE (1 << KPROBE_HASH_BITS)",
        "",
        "#if !defined(CONFIG_OPTPROBES) || !defined(CONFIG_SYSCTL)",
        "#define kprobe_sysctls_init() do { } while (0)",
        "#endif",
        "",
        "static int kprobes_initialized;",
        "/* kprobe_table can be accessed by",
        " * - Normal hlist traversal and RCU add/del under 'kprobe_mutex' is held.",
        " * Or",
        " * - RCU hlist traversal under disabling preempt (breakpoint handlers)",
        " */",
        "static struct hlist_head kprobe_table[KPROBE_TABLE_SIZE];",
        "",
        "/* NOTE: change this value only with 'kprobe_mutex' held */",
        "static bool kprobes_all_disarmed;",
        "",
        "/* This protects 'kprobe_table' and 'optimizing_list' */",
        "static DEFINE_MUTEX(kprobe_mutex);",
        "static DEFINE_PER_CPU(struct kprobe *, kprobe_instance);",
        "",
        "kprobe_opcode_t * __weak kprobe_lookup_name(const char *name,",
        "					unsigned int __unused)",
        "{",
        "	return ((kprobe_opcode_t *)(kallsyms_lookup_name(name)));",
        "}",
        "",
        "/*",
        " * Blacklist -- list of 'struct kprobe_blacklist_entry' to store info where",
        " * kprobes can not probe.",
        " */",
        "static LIST_HEAD(kprobe_blacklist);",
        "",
        "#ifdef __ARCH_WANT_KPROBES_INSN_SLOT",
        "/*",
        " * 'kprobe::ainsn.insn' points to the copy of the instruction to be",
        " * single-stepped. x86_64, POWER4 and above have no-exec support and",
        " * stepping on the instruction on a vmalloced/kmalloced/data page",
        " * is a recipe for disaster",
        " */",
        "struct kprobe_insn_page {",
        "	struct list_head list;",
        "	kprobe_opcode_t *insns;		/* Page of instruction slots */",
        "	struct kprobe_insn_cache *cache;",
        "	int nused;",
        "	int ngarbage;",
        "	char slot_used[];",
        "};",
        "",
        "static int slots_per_page(struct kprobe_insn_cache *c)",
        "{",
        "	return PAGE_SIZE/(c->insn_size * sizeof(kprobe_opcode_t));",
        "}",
        "",
        "enum kprobe_slot_state {",
        "	SLOT_CLEAN = 0,",
        "	SLOT_DIRTY = 1,",
        "	SLOT_USED = 2,",
        "};",
        "",
        "void __weak *alloc_insn_page(void)",
        "{",
        "	/*",
        "	 * Use execmem_alloc() so this page is within +/- 2GB of where the",
        "	 * kernel image and loaded module images reside. This is required",
        "	 * for most of the architectures.",
        "	 * (e.g. x86-64 needs this to handle the %rip-relative fixups.)",
        "	 */",
        "	return execmem_alloc(EXECMEM_KPROBES, PAGE_SIZE);",
        "}",
        "",
        "static void free_insn_page(void *page)",
        "{",
        "	execmem_free(page);",
        "}",
        "",
        "struct kprobe_insn_cache kprobe_insn_slots = {",
        "	.mutex = __MUTEX_INITIALIZER(kprobe_insn_slots.mutex),",
        "	.alloc = alloc_insn_page,",
        "	.free = free_insn_page,",
        "	.sym = KPROBE_INSN_PAGE_SYM,",
        "	.pages = LIST_HEAD_INIT(kprobe_insn_slots.pages),",
        "	.insn_size = MAX_INSN_SIZE,",
        "	.nr_garbage = 0,",
        "};",
        "static int collect_garbage_slots(struct kprobe_insn_cache *c);",
        "",
        "/**",
        " * __get_insn_slot() - Find a slot on an executable page for an instruction.",
        " * We allocate an executable page if there's no room on existing ones.",
        " */",
        "kprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c)",
        "{",
        "	struct kprobe_insn_page *kip;",
        "	kprobe_opcode_t *slot = NULL;",
        "",
        "	/* Since the slot array is not protected by rcu, we need a mutex */",
        "	mutex_lock(&c->mutex);",
        " retry:",
        "	rcu_read_lock();",
        "	list_for_each_entry_rcu(kip, &c->pages, list) {",
        "		if (kip->nused < slots_per_page(c)) {",
        "			int i;",
        "",
        "			for (i = 0; i < slots_per_page(c); i++) {",
        "				if (kip->slot_used[i] == SLOT_CLEAN) {",
        "					kip->slot_used[i] = SLOT_USED;",
        "					kip->nused++;",
        "					slot = kip->insns + (i * c->insn_size);",
        "					rcu_read_unlock();",
        "					goto out;",
        "				}",
        "			}",
        "			/* kip->nused is broken. Fix it. */",
        "			kip->nused = slots_per_page(c);",
        "			WARN_ON(1);",
        "		}",
        "	}",
        "	rcu_read_unlock();",
        "",
        "	/* If there are any garbage slots, collect it and try again. */",
        "	if (c->nr_garbage && collect_garbage_slots(c) == 0)",
        "		goto retry;",
        "",
        "	/* All out of space.  Need to allocate a new page. */",
        "	kip = kmalloc(struct_size(kip, slot_used, slots_per_page(c)), GFP_KERNEL);",
        "	if (!kip)",
        "		goto out;",
        "",
        "	kip->insns = c->alloc();",
        "	if (!kip->insns) {",
        "		kfree(kip);",
        "		goto out;",
        "	}",
        "	INIT_LIST_HEAD(&kip->list);",
        "	memset(kip->slot_used, SLOT_CLEAN, slots_per_page(c));",
        "	kip->slot_used[0] = SLOT_USED;",
        "	kip->nused = 1;",
        "	kip->ngarbage = 0;",
        "	kip->cache = c;",
        "	list_add_rcu(&kip->list, &c->pages);",
        "	slot = kip->insns;",
        "",
        "	/* Record the perf ksymbol register event after adding the page */",
        "	perf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL, (unsigned long)kip->insns,",
        "			   PAGE_SIZE, false, c->sym);",
        "out:",
        "	mutex_unlock(&c->mutex);",
        "	return slot;",
        "}",
        "",
        "/* Return true if all garbages are collected, otherwise false. */",
        "static bool collect_one_slot(struct kprobe_insn_page *kip, int idx)",
        "{",
        "	kip->slot_used[idx] = SLOT_CLEAN;",
        "	kip->nused--;",
        "	if (kip->nused != 0)",
        "		return false;",
        "",
        "	/*",
        "	 * Page is no longer in use.  Free it unless",
        "	 * it's the last one.  We keep the last one",
        "	 * so as not to have to set it up again the",
        "	 * next time somebody inserts a probe.",
        "	 */",
        "	if (!list_is_singular(&kip->list)) {",
        "		/*",
        "		 * Record perf ksymbol unregister event before removing",
        "		 * the page.",
        "		 */",
        "		perf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL,",
        "				   (unsigned long)kip->insns, PAGE_SIZE, true,",
        "				   kip->cache->sym);",
        "		list_del_rcu(&kip->list);",
        "		synchronize_rcu();",
        "		kip->cache->free(kip->insns);",
        "		kfree(kip);",
        "	}",
        "	return true;",
        "}",
        "",
        "static int collect_garbage_slots(struct kprobe_insn_cache *c)",
        "{",
        "	struct kprobe_insn_page *kip, *next;",
        "",
        "	/* Ensure no-one is interrupted on the garbages */",
        "	synchronize_rcu();",
        "",
        "	list_for_each_entry_safe(kip, next, &c->pages, list) {",
        "		int i;",
        "",
        "		if (kip->ngarbage == 0)",
        "			continue;",
        "		kip->ngarbage = 0;	/* we will collect all garbages */",
        "		for (i = 0; i < slots_per_page(c); i++) {",
        "			if (kip->slot_used[i] == SLOT_DIRTY && collect_one_slot(kip, i))",
        "				break;",
        "		}",
        "	}",
        "	c->nr_garbage = 0;",
        "	return 0;",
        "}",
        "",
        "void __free_insn_slot(struct kprobe_insn_cache *c,",
        "		      kprobe_opcode_t *slot, int dirty)",
        "{",
        "	struct kprobe_insn_page *kip;",
        "	long idx;",
        "",
        "	mutex_lock(&c->mutex);",
        "	rcu_read_lock();",
        "	list_for_each_entry_rcu(kip, &c->pages, list) {",
        "		idx = ((long)slot - (long)kip->insns) /",
        "			(c->insn_size * sizeof(kprobe_opcode_t));",
        "		if (idx >= 0 && idx < slots_per_page(c))",
        "			goto out;",
        "	}",
        "	/* Could not find this slot. */",
        "	WARN_ON(1);",
        "	kip = NULL;",
        "out:",
        "	rcu_read_unlock();",
        "	/* Mark and sweep: this may sleep */",
        "	if (kip) {",
        "		/* Check double free */",
        "		WARN_ON(kip->slot_used[idx] != SLOT_USED);",
        "		if (dirty) {",
        "			kip->slot_used[idx] = SLOT_DIRTY;",
        "			kip->ngarbage++;",
        "			if (++c->nr_garbage > slots_per_page(c))",
        "				collect_garbage_slots(c);",
        "		} else {",
        "			collect_one_slot(kip, idx);",
        "		}",
        "	}",
        "	mutex_unlock(&c->mutex);",
        "}",
        "",
        "/*",
        " * Check given address is on the page of kprobe instruction slots.",
        " * This will be used for checking whether the address on a stack",
        " * is on a text area or not.",
        " */",
        "bool __is_insn_slot_addr(struct kprobe_insn_cache *c, unsigned long addr)",
        "{",
        "	struct kprobe_insn_page *kip;",
        "	bool ret = false;",
        "",
        "	rcu_read_lock();",
        "	list_for_each_entry_rcu(kip, &c->pages, list) {",
        "		if (addr >= (unsigned long)kip->insns &&",
        "		    addr < (unsigned long)kip->insns + PAGE_SIZE) {",
        "			ret = true;",
        "			break;",
        "		}",
        "	}",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "int kprobe_cache_get_kallsym(struct kprobe_insn_cache *c, unsigned int *symnum,",
        "			     unsigned long *value, char *type, char *sym)",
        "{",
        "	struct kprobe_insn_page *kip;",
        "	int ret = -ERANGE;",
        "",
        "	rcu_read_lock();",
        "	list_for_each_entry_rcu(kip, &c->pages, list) {",
        "		if ((*symnum)--)",
        "			continue;",
        "		strscpy(sym, c->sym, KSYM_NAME_LEN);",
        "		*type = 't';",
        "		*value = (unsigned long)kip->insns;",
        "		ret = 0;",
        "		break;",
        "	}",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "#ifdef CONFIG_OPTPROBES",
        "void __weak *alloc_optinsn_page(void)",
        "{",
        "	return alloc_insn_page();",
        "}",
        "",
        "void __weak free_optinsn_page(void *page)",
        "{",
        "	free_insn_page(page);",
        "}",
        "",
        "/* For optimized_kprobe buffer */",
        "struct kprobe_insn_cache kprobe_optinsn_slots = {",
        "	.mutex = __MUTEX_INITIALIZER(kprobe_optinsn_slots.mutex),",
        "	.alloc = alloc_optinsn_page,",
        "	.free = free_optinsn_page,",
        "	.sym = KPROBE_OPTINSN_PAGE_SYM,",
        "	.pages = LIST_HEAD_INIT(kprobe_optinsn_slots.pages),",
        "	/* .insn_size is initialized later */",
        "	.nr_garbage = 0,",
        "};",
        "#endif /* CONFIG_OPTPROBES */",
        "#endif /* __ARCH_WANT_KPROBES_INSN_SLOT */",
        "",
        "/* We have preemption disabled.. so it is safe to use __ versions */",
        "static inline void set_kprobe_instance(struct kprobe *kp)",
        "{",
        "	__this_cpu_write(kprobe_instance, kp);",
        "}",
        "",
        "static inline void reset_kprobe_instance(void)",
        "{",
        "	__this_cpu_write(kprobe_instance, NULL);",
        "}",
        "",
        "/*",
        " * This routine is called either:",
        " *	- under the 'kprobe_mutex' - during kprobe_[un]register().",
        " *				OR",
        " *	- with preemption disabled - from architecture specific code.",
        " */",
        "struct kprobe *get_kprobe(void *addr)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "",
        "	head = &kprobe_table[hash_ptr(addr, KPROBE_HASH_BITS)];",
        "	hlist_for_each_entry_rcu(p, head, hlist,",
        "				 lockdep_is_held(&kprobe_mutex)) {",
        "		if (p->addr == addr)",
        "			return p;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "NOKPROBE_SYMBOL(get_kprobe);",
        "",
        "static int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs);",
        "",
        "/* Return true if 'p' is an aggregator */",
        "static inline bool kprobe_aggrprobe(struct kprobe *p)",
        "{",
        "	return p->pre_handler == aggr_pre_handler;",
        "}",
        "",
        "/* Return true if 'p' is unused */",
        "static inline bool kprobe_unused(struct kprobe *p)",
        "{",
        "	return kprobe_aggrprobe(p) && kprobe_disabled(p) &&",
        "	       list_empty(&p->list);",
        "}",
        "",
        "/* Keep all fields in the kprobe consistent. */",
        "static inline void copy_kprobe(struct kprobe *ap, struct kprobe *p)",
        "{",
        "	memcpy(&p->opcode, &ap->opcode, sizeof(kprobe_opcode_t));",
        "	memcpy(&p->ainsn, &ap->ainsn, sizeof(struct arch_specific_insn));",
        "}",
        "",
        "#ifdef CONFIG_OPTPROBES",
        "/* NOTE: This is protected by 'kprobe_mutex'. */",
        "static bool kprobes_allow_optimization;",
        "",
        "/*",
        " * Call all 'kprobe::pre_handler' on the list, but ignores its return value.",
        " * This must be called from arch-dep optimized caller.",
        " */",
        "void opt_pre_handler(struct kprobe *p, struct pt_regs *regs)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	list_for_each_entry_rcu(kp, &p->list, list) {",
        "		if (kp->pre_handler && likely(!kprobe_disabled(kp))) {",
        "			set_kprobe_instance(kp);",
        "			kp->pre_handler(kp, regs);",
        "		}",
        "		reset_kprobe_instance();",
        "	}",
        "}",
        "NOKPROBE_SYMBOL(opt_pre_handler);",
        "",
        "/* Free optimized instructions and optimized_kprobe */",
        "static void free_aggr_kprobe(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "	arch_remove_optimized_kprobe(op);",
        "	arch_remove_kprobe(p);",
        "	kfree(op);",
        "}",
        "",
        "/* Return true if the kprobe is ready for optimization. */",
        "static inline int kprobe_optready(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	if (kprobe_aggrprobe(p)) {",
        "		op = container_of(p, struct optimized_kprobe, kp);",
        "		return arch_prepared_optinsn(&op->optinsn);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/* Return true if the kprobe is disarmed. Note: p must be on hash list */",
        "bool kprobe_disarmed(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	/* If kprobe is not aggr/opt probe, just return kprobe is disabled */",
        "	if (!kprobe_aggrprobe(p))",
        "		return kprobe_disabled(p);",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "",
        "	return kprobe_disabled(p) && list_empty(&op->list);",
        "}",
        "",
        "/* Return true if the probe is queued on (un)optimizing lists */",
        "static bool kprobe_queued(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	if (kprobe_aggrprobe(p)) {",
        "		op = container_of(p, struct optimized_kprobe, kp);",
        "		if (!list_empty(&op->list))",
        "			return true;",
        "	}",
        "	return false;",
        "}",
        "",
        "/*",
        " * Return an optimized kprobe whose optimizing code replaces",
        " * instructions including 'addr' (exclude breakpoint).",
        " */",
        "static struct kprobe *get_optimized_kprobe(kprobe_opcode_t *addr)",
        "{",
        "	int i;",
        "	struct kprobe *p = NULL;",
        "	struct optimized_kprobe *op;",
        "",
        "	/* Don't check i == 0, since that is a breakpoint case. */",
        "	for (i = 1; !p && i < MAX_OPTIMIZED_LENGTH / sizeof(kprobe_opcode_t); i++)",
        "		p = get_kprobe(addr - i);",
        "",
        "	if (p && kprobe_optready(p)) {",
        "		op = container_of(p, struct optimized_kprobe, kp);",
        "		if (arch_within_optimized_kprobe(op, addr))",
        "			return p;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/* Optimization staging list, protected by 'kprobe_mutex' */",
        "static LIST_HEAD(optimizing_list);",
        "static LIST_HEAD(unoptimizing_list);",
        "static LIST_HEAD(freeing_list);",
        "",
        "static void kprobe_optimizer(struct work_struct *work);",
        "static DECLARE_DELAYED_WORK(optimizing_work, kprobe_optimizer);",
        "#define OPTIMIZE_DELAY 5",
        "",
        "/*",
        " * Optimize (replace a breakpoint with a jump) kprobes listed on",
        " * 'optimizing_list'.",
        " */",
        "static void do_optimize_kprobes(void)",
        "{",
        "	lockdep_assert_held(&text_mutex);",
        "	/*",
        "	 * The optimization/unoptimization refers 'online_cpus' via",
        "	 * stop_machine() and cpu-hotplug modifies the 'online_cpus'.",
        "	 * And same time, 'text_mutex' will be held in cpu-hotplug and here.",
        "	 * This combination can cause a deadlock (cpu-hotplug tries to lock",
        "	 * 'text_mutex' but stop_machine() can not be done because",
        "	 * the 'online_cpus' has been changed)",
        "	 * To avoid this deadlock, caller must have locked cpu-hotplug",
        "	 * for preventing cpu-hotplug outside of 'text_mutex' locking.",
        "	 */",
        "	lockdep_assert_cpus_held();",
        "",
        "	/* Optimization never be done when disarmed */",
        "	if (kprobes_all_disarmed || !kprobes_allow_optimization ||",
        "	    list_empty(&optimizing_list))",
        "		return;",
        "",
        "	arch_optimize_kprobes(&optimizing_list);",
        "}",
        "",
        "/*",
        " * Unoptimize (replace a jump with a breakpoint and remove the breakpoint",
        " * if need) kprobes listed on 'unoptimizing_list'.",
        " */",
        "static void do_unoptimize_kprobes(void)",
        "{",
        "	struct optimized_kprobe *op, *tmp;",
        "",
        "	lockdep_assert_held(&text_mutex);",
        "	/* See comment in do_optimize_kprobes() */",
        "	lockdep_assert_cpus_held();",
        "",
        "	if (!list_empty(&unoptimizing_list))",
        "		arch_unoptimize_kprobes(&unoptimizing_list, &freeing_list);",
        "",
        "	/* Loop on 'freeing_list' for disarming and removing from kprobe hash list */",
        "	list_for_each_entry_safe(op, tmp, &freeing_list, list) {",
        "		/* Switching from detour code to origin */",
        "		op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;",
        "		/* Disarm probes if marked disabled and not gone */",
        "		if (kprobe_disabled(&op->kp) && !kprobe_gone(&op->kp))",
        "			arch_disarm_kprobe(&op->kp);",
        "		if (kprobe_unused(&op->kp)) {",
        "			/*",
        "			 * Remove unused probes from hash list. After waiting",
        "			 * for synchronization, these probes are reclaimed.",
        "			 * (reclaiming is done by do_free_cleaned_kprobes().)",
        "			 */",
        "			hlist_del_rcu(&op->kp.hlist);",
        "		} else",
        "			list_del_init(&op->list);",
        "	}",
        "}",
        "",
        "/* Reclaim all kprobes on the 'freeing_list' */",
        "static void do_free_cleaned_kprobes(void)",
        "{",
        "	struct optimized_kprobe *op, *tmp;",
        "",
        "	list_for_each_entry_safe(op, tmp, &freeing_list, list) {",
        "		list_del_init(&op->list);",
        "		if (WARN_ON_ONCE(!kprobe_unused(&op->kp))) {",
        "			/*",
        "			 * This must not happen, but if there is a kprobe",
        "			 * still in use, keep it on kprobes hash list.",
        "			 */",
        "			continue;",
        "		}",
        "		free_aggr_kprobe(&op->kp);",
        "	}",
        "}",
        "",
        "/* Start optimizer after OPTIMIZE_DELAY passed */",
        "static void kick_kprobe_optimizer(void)",
        "{",
        "	schedule_delayed_work(&optimizing_work, OPTIMIZE_DELAY);",
        "}",
        "",
        "/* Kprobe jump optimizer */",
        "static void kprobe_optimizer(struct work_struct *work)",
        "{",
        "	mutex_lock(&kprobe_mutex);",
        "	cpus_read_lock();",
        "	mutex_lock(&text_mutex);",
        "",
        "	/*",
        "	 * Step 1: Unoptimize kprobes and collect cleaned (unused and disarmed)",
        "	 * kprobes before waiting for quiesence period.",
        "	 */",
        "	do_unoptimize_kprobes();",
        "",
        "	/*",
        "	 * Step 2: Wait for quiesence period to ensure all potentially",
        "	 * preempted tasks to have normally scheduled. Because optprobe",
        "	 * may modify multiple instructions, there is a chance that Nth",
        "	 * instruction is preempted. In that case, such tasks can return",
        "	 * to 2nd-Nth byte of jump instruction. This wait is for avoiding it.",
        "	 * Note that on non-preemptive kernel, this is transparently converted",
        "	 * to synchronoze_sched() to wait for all interrupts to have completed.",
        "	 */",
        "	synchronize_rcu_tasks();",
        "",
        "	/* Step 3: Optimize kprobes after quiesence period */",
        "	do_optimize_kprobes();",
        "",
        "	/* Step 4: Free cleaned kprobes after quiesence period */",
        "	do_free_cleaned_kprobes();",
        "",
        "	mutex_unlock(&text_mutex);",
        "	cpus_read_unlock();",
        "",
        "	/* Step 5: Kick optimizer again if needed */",
        "	if (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list))",
        "		kick_kprobe_optimizer();",
        "",
        "	mutex_unlock(&kprobe_mutex);",
        "}",
        "",
        "/* Wait for completing optimization and unoptimization */",
        "void wait_for_kprobe_optimizer(void)",
        "{",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	while (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list)) {",
        "		mutex_unlock(&kprobe_mutex);",
        "",
        "		/* This will also make 'optimizing_work' execute immmediately */",
        "		flush_delayed_work(&optimizing_work);",
        "		/* 'optimizing_work' might not have been queued yet, relax */",
        "		cpu_relax();",
        "",
        "		mutex_lock(&kprobe_mutex);",
        "	}",
        "",
        "	mutex_unlock(&kprobe_mutex);",
        "}",
        "",
        "bool optprobe_queued_unopt(struct optimized_kprobe *op)",
        "{",
        "	struct optimized_kprobe *_op;",
        "",
        "	list_for_each_entry(_op, &unoptimizing_list, list) {",
        "		if (op == _op)",
        "			return true;",
        "	}",
        "",
        "	return false;",
        "}",
        "",
        "/* Optimize kprobe if p is ready to be optimized */",
        "static void optimize_kprobe(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	/* Check if the kprobe is disabled or not ready for optimization. */",
        "	if (!kprobe_optready(p) || !kprobes_allow_optimization ||",
        "	    (kprobe_disabled(p) || kprobes_all_disarmed))",
        "		return;",
        "",
        "	/* kprobes with 'post_handler' can not be optimized */",
        "	if (p->post_handler)",
        "		return;",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "",
        "	/* Check there is no other kprobes at the optimized instructions */",
        "	if (arch_check_optimized_kprobe(op) < 0)",
        "		return;",
        "",
        "	/* Check if it is already optimized. */",
        "	if (op->kp.flags & KPROBE_FLAG_OPTIMIZED) {",
        "		if (optprobe_queued_unopt(op)) {",
        "			/* This is under unoptimizing. Just dequeue the probe */",
        "			list_del_init(&op->list);",
        "		}",
        "		return;",
        "	}",
        "	op->kp.flags |= KPROBE_FLAG_OPTIMIZED;",
        "",
        "	/*",
        "	 * On the 'unoptimizing_list' and 'optimizing_list',",
        "	 * 'op' must have OPTIMIZED flag",
        "	 */",
        "	if (WARN_ON_ONCE(!list_empty(&op->list)))",
        "		return;",
        "",
        "	list_add(&op->list, &optimizing_list);",
        "	kick_kprobe_optimizer();",
        "}",
        "",
        "/* Short cut to direct unoptimizing */",
        "static void force_unoptimize_kprobe(struct optimized_kprobe *op)",
        "{",
        "	lockdep_assert_cpus_held();",
        "	arch_unoptimize_kprobe(op);",
        "	op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;",
        "}",
        "",
        "/* Unoptimize a kprobe if p is optimized */",
        "static void unoptimize_kprobe(struct kprobe *p, bool force)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	if (!kprobe_aggrprobe(p) || kprobe_disarmed(p))",
        "		return; /* This is not an optprobe nor optimized */",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "	if (!kprobe_optimized(p))",
        "		return;",
        "",
        "	if (!list_empty(&op->list)) {",
        "		if (optprobe_queued_unopt(op)) {",
        "			/* Queued in unoptimizing queue */",
        "			if (force) {",
        "				/*",
        "				 * Forcibly unoptimize the kprobe here, and queue it",
        "				 * in the freeing list for release afterwards.",
        "				 */",
        "				force_unoptimize_kprobe(op);",
        "				list_move(&op->list, &freeing_list);",
        "			}",
        "		} else {",
        "			/* Dequeue from the optimizing queue */",
        "			list_del_init(&op->list);",
        "			op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;",
        "		}",
        "		return;",
        "	}",
        "",
        "	/* Optimized kprobe case */",
        "	if (force) {",
        "		/* Forcibly update the code: this is a special case */",
        "		force_unoptimize_kprobe(op);",
        "	} else {",
        "		list_add(&op->list, &unoptimizing_list);",
        "		kick_kprobe_optimizer();",
        "	}",
        "}",
        "",
        "/* Cancel unoptimizing for reusing */",
        "static int reuse_unused_kprobe(struct kprobe *ap)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	/*",
        "	 * Unused kprobe MUST be on the way of delayed unoptimizing (means",
        "	 * there is still a relative jump) and disabled.",
        "	 */",
        "	op = container_of(ap, struct optimized_kprobe, kp);",
        "	WARN_ON_ONCE(list_empty(&op->list));",
        "	/* Enable the probe again */",
        "	ap->flags &= ~KPROBE_FLAG_DISABLED;",
        "	/* Optimize it again. (remove from 'op->list') */",
        "	if (!kprobe_optready(ap))",
        "		return -EINVAL;",
        "",
        "	optimize_kprobe(ap);",
        "	return 0;",
        "}",
        "",
        "/* Remove optimized instructions */",
        "static void kill_optimized_kprobe(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "	if (!list_empty(&op->list))",
        "		/* Dequeue from the (un)optimization queue */",
        "		list_del_init(&op->list);",
        "	op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;",
        "",
        "	if (kprobe_unused(p)) {",
        "		/*",
        "		 * Unused kprobe is on unoptimizing or freeing list. We move it",
        "		 * to freeing_list and let the kprobe_optimizer() remove it from",
        "		 * the kprobe hash list and free it.",
        "		 */",
        "		if (optprobe_queued_unopt(op))",
        "			list_move(&op->list, &freeing_list);",
        "	}",
        "",
        "	/* Don't touch the code, because it is already freed. */",
        "	arch_remove_optimized_kprobe(op);",
        "}",
        "",
        "static inline",
        "void __prepare_optimized_kprobe(struct optimized_kprobe *op, struct kprobe *p)",
        "{",
        "	if (!kprobe_ftrace(p))",
        "		arch_prepare_optimized_kprobe(op, p);",
        "}",
        "",
        "/* Try to prepare optimized instructions */",
        "static void prepare_optimized_kprobe(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	op = container_of(p, struct optimized_kprobe, kp);",
        "	__prepare_optimized_kprobe(op, p);",
        "}",
        "",
        "/* Allocate new optimized_kprobe and try to prepare optimized instructions. */",
        "static struct kprobe *alloc_aggr_kprobe(struct kprobe *p)",
        "{",
        "	struct optimized_kprobe *op;",
        "",
        "	op = kzalloc(sizeof(struct optimized_kprobe), GFP_KERNEL);",
        "	if (!op)",
        "		return NULL;",
        "",
        "	INIT_LIST_HEAD(&op->list);",
        "	op->kp.addr = p->addr;",
        "	__prepare_optimized_kprobe(op, p);",
        "",
        "	return &op->kp;",
        "}",
        "",
        "static void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p);",
        "",
        "/*",
        " * Prepare an optimized_kprobe and optimize it.",
        " * NOTE: 'p' must be a normal registered kprobe.",
        " */",
        "static void try_to_optimize_kprobe(struct kprobe *p)",
        "{",
        "	struct kprobe *ap;",
        "	struct optimized_kprobe *op;",
        "",
        "	/* Impossible to optimize ftrace-based kprobe. */",
        "	if (kprobe_ftrace(p))",
        "		return;",
        "",
        "	/* For preparing optimization, jump_label_text_reserved() is called. */",
        "	cpus_read_lock();",
        "	jump_label_lock();",
        "	mutex_lock(&text_mutex);",
        "",
        "	ap = alloc_aggr_kprobe(p);",
        "	if (!ap)",
        "		goto out;",
        "",
        "	op = container_of(ap, struct optimized_kprobe, kp);",
        "	if (!arch_prepared_optinsn(&op->optinsn)) {",
        "		/* If failed to setup optimizing, fallback to kprobe. */",
        "		arch_remove_optimized_kprobe(op);",
        "		kfree(op);",
        "		goto out;",
        "	}",
        "",
        "	init_aggr_kprobe(ap, p);",
        "	optimize_kprobe(ap);	/* This just kicks optimizer thread. */",
        "",
        "out:",
        "	mutex_unlock(&text_mutex);",
        "	jump_label_unlock();",
        "	cpus_read_unlock();",
        "}",
        "",
        "static void optimize_all_kprobes(void)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	unsigned int i;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "	/* If optimization is already allowed, just return. */",
        "	if (kprobes_allow_optimization)",
        "		goto out;",
        "",
        "	cpus_read_lock();",
        "	kprobes_allow_optimization = true;",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		hlist_for_each_entry(p, head, hlist)",
        "			if (!kprobe_disabled(p))",
        "				optimize_kprobe(p);",
        "	}",
        "	cpus_read_unlock();",
        "	pr_info(\"kprobe jump-optimization is enabled. All kprobes are optimized if possible.\\n\");",
        "out:",
        "	mutex_unlock(&kprobe_mutex);",
        "}",
        "",
        "#ifdef CONFIG_SYSCTL",
        "static void unoptimize_all_kprobes(void)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	unsigned int i;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "	/* If optimization is already prohibited, just return. */",
        "	if (!kprobes_allow_optimization) {",
        "		mutex_unlock(&kprobe_mutex);",
        "		return;",
        "	}",
        "",
        "	cpus_read_lock();",
        "	kprobes_allow_optimization = false;",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		hlist_for_each_entry(p, head, hlist) {",
        "			if (!kprobe_disabled(p))",
        "				unoptimize_kprobe(p, false);",
        "		}",
        "	}",
        "	cpus_read_unlock();",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	/* Wait for unoptimizing completion. */",
        "	wait_for_kprobe_optimizer();",
        "	pr_info(\"kprobe jump-optimization is disabled. All kprobes are based on software breakpoint.\\n\");",
        "}",
        "",
        "static DEFINE_MUTEX(kprobe_sysctl_mutex);",
        "static int sysctl_kprobes_optimization;",
        "static int proc_kprobes_optimization_handler(const struct ctl_table *table,",
        "					     int write, void *buffer,",
        "					     size_t *length, loff_t *ppos)",
        "{",
        "	int ret;",
        "",
        "	mutex_lock(&kprobe_sysctl_mutex);",
        "	sysctl_kprobes_optimization = kprobes_allow_optimization ? 1 : 0;",
        "	ret = proc_dointvec_minmax(table, write, buffer, length, ppos);",
        "",
        "	if (sysctl_kprobes_optimization)",
        "		optimize_all_kprobes();",
        "	else",
        "		unoptimize_all_kprobes();",
        "	mutex_unlock(&kprobe_sysctl_mutex);",
        "",
        "	return ret;",
        "}",
        "",
        "static struct ctl_table kprobe_sysctls[] = {",
        "	{",
        "		.procname	= \"kprobes-optimization\",",
        "		.data		= &sysctl_kprobes_optimization,",
        "		.maxlen		= sizeof(int),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_kprobes_optimization_handler,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_ONE,",
        "	},",
        "};",
        "",
        "static void __init kprobe_sysctls_init(void)",
        "{",
        "	register_sysctl_init(\"debug\", kprobe_sysctls);",
        "}",
        "#endif /* CONFIG_SYSCTL */",
        "",
        "/* Put a breakpoint for a probe. */",
        "static void __arm_kprobe(struct kprobe *p)",
        "{",
        "	struct kprobe *_p;",
        "",
        "	lockdep_assert_held(&text_mutex);",
        "",
        "	/* Find the overlapping optimized kprobes. */",
        "	_p = get_optimized_kprobe(p->addr);",
        "	if (unlikely(_p))",
        "		/* Fallback to unoptimized kprobe */",
        "		unoptimize_kprobe(_p, true);",
        "",
        "	arch_arm_kprobe(p);",
        "	optimize_kprobe(p);	/* Try to optimize (add kprobe to a list) */",
        "}",
        "",
        "/* Remove the breakpoint of a probe. */",
        "static void __disarm_kprobe(struct kprobe *p, bool reopt)",
        "{",
        "	struct kprobe *_p;",
        "",
        "	lockdep_assert_held(&text_mutex);",
        "",
        "	/* Try to unoptimize */",
        "	unoptimize_kprobe(p, kprobes_all_disarmed);",
        "",
        "	if (!kprobe_queued(p)) {",
        "		arch_disarm_kprobe(p);",
        "		/* If another kprobe was blocked, re-optimize it. */",
        "		_p = get_optimized_kprobe(p->addr);",
        "		if (unlikely(_p) && reopt)",
        "			optimize_kprobe(_p);",
        "	}",
        "	/*",
        "	 * TODO: Since unoptimization and real disarming will be done by",
        "	 * the worker thread, we can not check whether another probe are",
        "	 * unoptimized because of this probe here. It should be re-optimized",
        "	 * by the worker thread.",
        "	 */",
        "}",
        "",
        "#else /* !CONFIG_OPTPROBES */",
        "",
        "#define optimize_kprobe(p)			do {} while (0)",
        "#define unoptimize_kprobe(p, f)			do {} while (0)",
        "#define kill_optimized_kprobe(p)		do {} while (0)",
        "#define prepare_optimized_kprobe(p)		do {} while (0)",
        "#define try_to_optimize_kprobe(p)		do {} while (0)",
        "#define __arm_kprobe(p)				arch_arm_kprobe(p)",
        "#define __disarm_kprobe(p, o)			arch_disarm_kprobe(p)",
        "#define kprobe_disarmed(p)			kprobe_disabled(p)",
        "#define wait_for_kprobe_optimizer()		do {} while (0)",
        "",
        "static int reuse_unused_kprobe(struct kprobe *ap)",
        "{",
        "	/*",
        "	 * If the optimized kprobe is NOT supported, the aggr kprobe is",
        "	 * released at the same time that the last aggregated kprobe is",
        "	 * unregistered.",
        "	 * Thus there should be no chance to reuse unused kprobe.",
        "	 */",
        "	WARN_ON_ONCE(1);",
        "	return -EINVAL;",
        "}",
        "",
        "static void free_aggr_kprobe(struct kprobe *p)",
        "{",
        "	arch_remove_kprobe(p);",
        "	kfree(p);",
        "}",
        "",
        "static struct kprobe *alloc_aggr_kprobe(struct kprobe *p)",
        "{",
        "	return kzalloc(sizeof(struct kprobe), GFP_KERNEL);",
        "}",
        "#endif /* CONFIG_OPTPROBES */",
        "",
        "#ifdef CONFIG_KPROBES_ON_FTRACE",
        "static struct ftrace_ops kprobe_ftrace_ops __read_mostly = {",
        "	.func = kprobe_ftrace_handler,",
        "	.flags = FTRACE_OPS_FL_SAVE_REGS,",
        "};",
        "",
        "static struct ftrace_ops kprobe_ipmodify_ops __read_mostly = {",
        "	.func = kprobe_ftrace_handler,",
        "	.flags = FTRACE_OPS_FL_SAVE_REGS | FTRACE_OPS_FL_IPMODIFY,",
        "};",
        "",
        "static int kprobe_ipmodify_enabled;",
        "static int kprobe_ftrace_enabled;",
        "bool kprobe_ftrace_disabled;",
        "",
        "static int __arm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,",
        "			       int *cnt)",
        "{",
        "	int ret;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	ret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 0, 0);",
        "	if (WARN_ONCE(ret < 0, \"Failed to arm kprobe-ftrace at %pS (error %d)\\n\", p->addr, ret))",
        "		return ret;",
        "",
        "	if (*cnt == 0) {",
        "		ret = register_ftrace_function(ops);",
        "		if (WARN(ret < 0, \"Failed to register kprobe-ftrace (error %d)\\n\", ret))",
        "			goto err_ftrace;",
        "	}",
        "",
        "	(*cnt)++;",
        "	return ret;",
        "",
        "err_ftrace:",
        "	/*",
        "	 * At this point, sinec ops is not registered, we should be sefe from",
        "	 * registering empty filter.",
        "	 */",
        "	ftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);",
        "	return ret;",
        "}",
        "",
        "static int arm_kprobe_ftrace(struct kprobe *p)",
        "{",
        "	bool ipmodify = (p->post_handler != NULL);",
        "",
        "	return __arm_kprobe_ftrace(p,",
        "		ipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,",
        "		ipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);",
        "}",
        "",
        "static int __disarm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,",
        "				  int *cnt)",
        "{",
        "	int ret;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	if (*cnt == 1) {",
        "		ret = unregister_ftrace_function(ops);",
        "		if (WARN(ret < 0, \"Failed to unregister kprobe-ftrace (error %d)\\n\", ret))",
        "			return ret;",
        "	}",
        "",
        "	(*cnt)--;",
        "",
        "	ret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);",
        "	WARN_ONCE(ret < 0, \"Failed to disarm kprobe-ftrace at %pS (error %d)\\n\",",
        "		  p->addr, ret);",
        "	return ret;",
        "}",
        "",
        "static int disarm_kprobe_ftrace(struct kprobe *p)",
        "{",
        "	bool ipmodify = (p->post_handler != NULL);",
        "",
        "	return __disarm_kprobe_ftrace(p,",
        "		ipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,",
        "		ipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);",
        "}",
        "",
        "void kprobe_ftrace_kill(void)",
        "{",
        "	kprobe_ftrace_disabled = true;",
        "}",
        "#else	/* !CONFIG_KPROBES_ON_FTRACE */",
        "static inline int arm_kprobe_ftrace(struct kprobe *p)",
        "{",
        "	return -ENODEV;",
        "}",
        "",
        "static inline int disarm_kprobe_ftrace(struct kprobe *p)",
        "{",
        "	return -ENODEV;",
        "}",
        "#endif",
        "",
        "static int prepare_kprobe(struct kprobe *p)",
        "{",
        "	/* Must ensure p->addr is really on ftrace */",
        "	if (kprobe_ftrace(p))",
        "		return arch_prepare_kprobe_ftrace(p);",
        "",
        "	return arch_prepare_kprobe(p);",
        "}",
        "",
        "static int arm_kprobe(struct kprobe *kp)",
        "{",
        "	if (unlikely(kprobe_ftrace(kp)))",
        "		return arm_kprobe_ftrace(kp);",
        "",
        "	cpus_read_lock();",
        "	mutex_lock(&text_mutex);",
        "	__arm_kprobe(kp);",
        "	mutex_unlock(&text_mutex);",
        "	cpus_read_unlock();",
        "",
        "	return 0;",
        "}",
        "",
        "static int disarm_kprobe(struct kprobe *kp, bool reopt)",
        "{",
        "	if (unlikely(kprobe_ftrace(kp)))",
        "		return disarm_kprobe_ftrace(kp);",
        "",
        "	cpus_read_lock();",
        "	mutex_lock(&text_mutex);",
        "	__disarm_kprobe(kp, reopt);",
        "	mutex_unlock(&text_mutex);",
        "	cpus_read_unlock();",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Aggregate handlers for multiple kprobes support - these handlers",
        " * take care of invoking the individual kprobe handlers on p->list",
        " */",
        "static int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	list_for_each_entry_rcu(kp, &p->list, list) {",
        "		if (kp->pre_handler && likely(!kprobe_disabled(kp))) {",
        "			set_kprobe_instance(kp);",
        "			if (kp->pre_handler(kp, regs))",
        "				return 1;",
        "		}",
        "		reset_kprobe_instance();",
        "	}",
        "	return 0;",
        "}",
        "NOKPROBE_SYMBOL(aggr_pre_handler);",
        "",
        "static void aggr_post_handler(struct kprobe *p, struct pt_regs *regs,",
        "			      unsigned long flags)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	list_for_each_entry_rcu(kp, &p->list, list) {",
        "		if (kp->post_handler && likely(!kprobe_disabled(kp))) {",
        "			set_kprobe_instance(kp);",
        "			kp->post_handler(kp, regs, flags);",
        "			reset_kprobe_instance();",
        "		}",
        "	}",
        "}",
        "NOKPROBE_SYMBOL(aggr_post_handler);",
        "",
        "/* Walks the list and increments 'nmissed' if 'p' has child probes. */",
        "void kprobes_inc_nmissed_count(struct kprobe *p)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	if (!kprobe_aggrprobe(p)) {",
        "		p->nmissed++;",
        "	} else {",
        "		list_for_each_entry_rcu(kp, &p->list, list)",
        "			kp->nmissed++;",
        "	}",
        "}",
        "NOKPROBE_SYMBOL(kprobes_inc_nmissed_count);",
        "",
        "static struct kprobe kprobe_busy = {",
        "	.addr = (void *) get_kprobe,",
        "};",
        "",
        "void kprobe_busy_begin(void)",
        "{",
        "	struct kprobe_ctlblk *kcb;",
        "",
        "	preempt_disable();",
        "	__this_cpu_write(current_kprobe, &kprobe_busy);",
        "	kcb = get_kprobe_ctlblk();",
        "	kcb->kprobe_status = KPROBE_HIT_ACTIVE;",
        "}",
        "",
        "void kprobe_busy_end(void)",
        "{",
        "	__this_cpu_write(current_kprobe, NULL);",
        "	preempt_enable();",
        "}",
        "",
        "/* Add the new probe to 'ap->list'. */",
        "static int add_new_kprobe(struct kprobe *ap, struct kprobe *p)",
        "{",
        "	if (p->post_handler)",
        "		unoptimize_kprobe(ap, true);	/* Fall back to normal kprobe */",
        "",
        "	list_add_rcu(&p->list, &ap->list);",
        "	if (p->post_handler && !ap->post_handler)",
        "		ap->post_handler = aggr_post_handler;",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Fill in the required fields of the aggregator kprobe. Replace the",
        " * earlier kprobe in the hlist with the aggregator kprobe.",
        " */",
        "static void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p)",
        "{",
        "	/* Copy the insn slot of 'p' to 'ap'. */",
        "	copy_kprobe(p, ap);",
        "	flush_insn_slot(ap);",
        "	ap->addr = p->addr;",
        "	ap->flags = p->flags & ~KPROBE_FLAG_OPTIMIZED;",
        "	ap->pre_handler = aggr_pre_handler;",
        "	/* We don't care the kprobe which has gone. */",
        "	if (p->post_handler && !kprobe_gone(p))",
        "		ap->post_handler = aggr_post_handler;",
        "",
        "	INIT_LIST_HEAD(&ap->list);",
        "	INIT_HLIST_NODE(&ap->hlist);",
        "",
        "	list_add_rcu(&p->list, &ap->list);",
        "	hlist_replace_rcu(&p->hlist, &ap->hlist);",
        "}",
        "",
        "/*",
        " * This registers the second or subsequent kprobe at the same address.",
        " */",
        "static int register_aggr_kprobe(struct kprobe *orig_p, struct kprobe *p)",
        "{",
        "	int ret = 0;",
        "	struct kprobe *ap = orig_p;",
        "",
        "	cpus_read_lock();",
        "",
        "	/* For preparing optimization, jump_label_text_reserved() is called */",
        "	jump_label_lock();",
        "	mutex_lock(&text_mutex);",
        "",
        "	if (!kprobe_aggrprobe(orig_p)) {",
        "		/* If 'orig_p' is not an 'aggr_kprobe', create new one. */",
        "		ap = alloc_aggr_kprobe(orig_p);",
        "		if (!ap) {",
        "			ret = -ENOMEM;",
        "			goto out;",
        "		}",
        "		init_aggr_kprobe(ap, orig_p);",
        "	} else if (kprobe_unused(ap)) {",
        "		/* This probe is going to die. Rescue it */",
        "		ret = reuse_unused_kprobe(ap);",
        "		if (ret)",
        "			goto out;",
        "	}",
        "",
        "	if (kprobe_gone(ap)) {",
        "		/*",
        "		 * Attempting to insert new probe at the same location that",
        "		 * had a probe in the module vaddr area which already",
        "		 * freed. So, the instruction slot has already been",
        "		 * released. We need a new slot for the new probe.",
        "		 */",
        "		ret = arch_prepare_kprobe(ap);",
        "		if (ret)",
        "			/*",
        "			 * Even if fail to allocate new slot, don't need to",
        "			 * free the 'ap'. It will be used next time, or",
        "			 * freed by unregister_kprobe().",
        "			 */",
        "			goto out;",
        "",
        "		/* Prepare optimized instructions if possible. */",
        "		prepare_optimized_kprobe(ap);",
        "",
        "		/*",
        "		 * Clear gone flag to prevent allocating new slot again, and",
        "		 * set disabled flag because it is not armed yet.",
        "		 */",
        "		ap->flags = (ap->flags & ~KPROBE_FLAG_GONE)",
        "			    | KPROBE_FLAG_DISABLED;",
        "	}",
        "",
        "	/* Copy the insn slot of 'p' to 'ap'. */",
        "	copy_kprobe(ap, p);",
        "	ret = add_new_kprobe(ap, p);",
        "",
        "out:",
        "	mutex_unlock(&text_mutex);",
        "	jump_label_unlock();",
        "	cpus_read_unlock();",
        "",
        "	if (ret == 0 && kprobe_disabled(ap) && !kprobe_disabled(p)) {",
        "		ap->flags &= ~KPROBE_FLAG_DISABLED;",
        "		if (!kprobes_all_disarmed) {",
        "			/* Arm the breakpoint again. */",
        "			ret = arm_kprobe(ap);",
        "			if (ret) {",
        "				ap->flags |= KPROBE_FLAG_DISABLED;",
        "				list_del_rcu(&p->list);",
        "				synchronize_rcu();",
        "			}",
        "		}",
        "	}",
        "	return ret;",
        "}",
        "",
        "bool __weak arch_within_kprobe_blacklist(unsigned long addr)",
        "{",
        "	/* The '__kprobes' functions and entry code must not be probed. */",
        "	return addr >= (unsigned long)__kprobes_text_start &&",
        "	       addr < (unsigned long)__kprobes_text_end;",
        "}",
        "",
        "static bool __within_kprobe_blacklist(unsigned long addr)",
        "{",
        "	struct kprobe_blacklist_entry *ent;",
        "",
        "	if (arch_within_kprobe_blacklist(addr))",
        "		return true;",
        "	/*",
        "	 * If 'kprobe_blacklist' is defined, check the address and",
        "	 * reject any probe registration in the prohibited area.",
        "	 */",
        "	list_for_each_entry(ent, &kprobe_blacklist, list) {",
        "		if (addr >= ent->start_addr && addr < ent->end_addr)",
        "			return true;",
        "	}",
        "	return false;",
        "}",
        "",
        "bool within_kprobe_blacklist(unsigned long addr)",
        "{",
        "	char symname[KSYM_NAME_LEN], *p;",
        "",
        "	if (__within_kprobe_blacklist(addr))",
        "		return true;",
        "",
        "	/* Check if the address is on a suffixed-symbol */",
        "	if (!lookup_symbol_name(addr, symname)) {",
        "		p = strchr(symname, '.');",
        "		if (!p)",
        "			return false;",
        "		*p = '\\0';",
        "		addr = (unsigned long)kprobe_lookup_name(symname, 0);",
        "		if (addr)",
        "			return __within_kprobe_blacklist(addr);",
        "	}",
        "	return false;",
        "}",
        "",
        "/*",
        " * arch_adjust_kprobe_addr - adjust the address",
        " * @addr: symbol base address",
        " * @offset: offset within the symbol",
        " * @on_func_entry: was this @addr+@offset on the function entry",
        " *",
        " * Typically returns @addr + @offset, except for special cases where the",
        " * function might be prefixed by a CFI landing pad, in that case any offset",
        " * inside the landing pad is mapped to the first 'real' instruction of the",
        " * symbol.",
        " *",
        " * Specifically, for things like IBT/BTI, skip the resp. ENDBR/BTI.C",
        " * instruction at +0.",
        " */",
        "kprobe_opcode_t *__weak arch_adjust_kprobe_addr(unsigned long addr,",
        "						unsigned long offset,",
        "						bool *on_func_entry)",
        "{",
        "	*on_func_entry = !offset;",
        "	return (kprobe_opcode_t *)(addr + offset);",
        "}",
        "",
        "/*",
        " * If 'symbol_name' is specified, look it up and add the 'offset'",
        " * to it. This way, we can specify a relative address to a symbol.",
        " * This returns encoded errors if it fails to look up symbol or invalid",
        " * combination of parameters.",
        " */",
        "static kprobe_opcode_t *",
        "_kprobe_addr(kprobe_opcode_t *addr, const char *symbol_name,",
        "	     unsigned long offset, bool *on_func_entry)",
        "{",
        "	if ((symbol_name && addr) || (!symbol_name && !addr))",
        "		goto invalid;",
        "",
        "	if (symbol_name) {",
        "		/*",
        "		 * Input: @sym + @offset",
        "		 * Output: @addr + @offset",
        "		 *",
        "		 * NOTE: kprobe_lookup_name() does *NOT* fold the offset",
        "		 *       argument into it's output!",
        "		 */",
        "		addr = kprobe_lookup_name(symbol_name, offset);",
        "		if (!addr)",
        "			return ERR_PTR(-ENOENT);",
        "	}",
        "",
        "	/*",
        "	 * So here we have @addr + @offset, displace it into a new",
        "	 * @addr' + @offset' where @addr' is the symbol start address.",
        "	 */",
        "	addr = (void *)addr + offset;",
        "	if (!kallsyms_lookup_size_offset((unsigned long)addr, NULL, &offset))",
        "		return ERR_PTR(-ENOENT);",
        "	addr = (void *)addr - offset;",
        "",
        "	/*",
        "	 * Then ask the architecture to re-combine them, taking care of",
        "	 * magical function entry details while telling us if this was indeed",
        "	 * at the start of the function.",
        "	 */",
        "	addr = arch_adjust_kprobe_addr((unsigned long)addr, offset, on_func_entry);",
        "	if (addr)",
        "		return addr;",
        "",
        "invalid:",
        "	return ERR_PTR(-EINVAL);",
        "}",
        "",
        "static kprobe_opcode_t *kprobe_addr(struct kprobe *p)",
        "{",
        "	bool on_func_entry;",
        "	return _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);",
        "}",
        "",
        "/*",
        " * Check the 'p' is valid and return the aggregator kprobe",
        " * at the same address.",
        " */",
        "static struct kprobe *__get_valid_kprobe(struct kprobe *p)",
        "{",
        "	struct kprobe *ap, *list_p;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	ap = get_kprobe(p->addr);",
        "	if (unlikely(!ap))",
        "		return NULL;",
        "",
        "	if (p != ap) {",
        "		list_for_each_entry(list_p, &ap->list, list)",
        "			if (list_p == p)",
        "			/* kprobe p is a valid probe */",
        "				goto valid;",
        "		return NULL;",
        "	}",
        "valid:",
        "	return ap;",
        "}",
        "",
        "/*",
        " * Warn and return error if the kprobe is being re-registered since",
        " * there must be a software bug.",
        " */",
        "static inline int warn_kprobe_rereg(struct kprobe *p)",
        "{",
        "	int ret = 0;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "	if (WARN_ON_ONCE(__get_valid_kprobe(p)))",
        "		ret = -EINVAL;",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	return ret;",
        "}",
        "",
        "static int check_ftrace_location(struct kprobe *p)",
        "{",
        "	unsigned long addr = (unsigned long)p->addr;",
        "",
        "	if (ftrace_location(addr) == addr) {",
        "#ifdef CONFIG_KPROBES_ON_FTRACE",
        "		p->flags |= KPROBE_FLAG_FTRACE;",
        "#else",
        "		return -EINVAL;",
        "#endif",
        "	}",
        "	return 0;",
        "}",
        "",
        "static bool is_cfi_preamble_symbol(unsigned long addr)",
        "{",
        "	char symbuf[KSYM_NAME_LEN];",
        "",
        "	if (lookup_symbol_name(addr, symbuf))",
        "		return false;",
        "",
        "	return str_has_prefix(symbuf, \"__cfi_\") ||",
        "		str_has_prefix(symbuf, \"__pfx_\");",
        "}",
        "",
        "static int check_kprobe_address_safe(struct kprobe *p,",
        "				     struct module **probed_mod)",
        "{",
        "	int ret;",
        "",
        "	ret = check_ftrace_location(p);",
        "	if (ret)",
        "		return ret;",
        "	jump_label_lock();",
        "	preempt_disable();",
        "",
        "	/* Ensure the address is in a text area, and find a module if exists. */",
        "	*probed_mod = NULL;",
        "	if (!core_kernel_text((unsigned long) p->addr)) {",
        "		*probed_mod = __module_text_address((unsigned long) p->addr);",
        "		if (!(*probed_mod)) {",
        "			ret = -EINVAL;",
        "			goto out;",
        "		}",
        "	}",
        "	/* Ensure it is not in reserved area. */",
        "	if (in_gate_area_no_mm((unsigned long) p->addr) ||",
        "	    within_kprobe_blacklist((unsigned long) p->addr) ||",
        "	    jump_label_text_reserved(p->addr, p->addr) ||",
        "	    static_call_text_reserved(p->addr, p->addr) ||",
        "	    find_bug((unsigned long)p->addr) ||",
        "	    is_cfi_preamble_symbol((unsigned long)p->addr)) {",
        "		ret = -EINVAL;",
        "		goto out;",
        "	}",
        "",
        "	/* Get module refcount and reject __init functions for loaded modules. */",
        "	if (IS_ENABLED(CONFIG_MODULES) && *probed_mod) {",
        "		/*",
        "		 * We must hold a refcount of the probed module while updating",
        "		 * its code to prohibit unexpected unloading.",
        "		 */",
        "		if (unlikely(!try_module_get(*probed_mod))) {",
        "			ret = -ENOENT;",
        "			goto out;",
        "		}",
        "",
        "		/*",
        "		 * If the module freed '.init.text', we couldn't insert",
        "		 * kprobes in there.",
        "		 */",
        "		if (within_module_init((unsigned long)p->addr, *probed_mod) &&",
        "		    !module_is_coming(*probed_mod)) {",
        "			module_put(*probed_mod);",
        "			*probed_mod = NULL;",
        "			ret = -ENOENT;",
        "		}",
        "	}",
        "",
        "out:",
        "	preempt_enable();",
        "	jump_label_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "int register_kprobe(struct kprobe *p)",
        "{",
        "	int ret;",
        "	struct kprobe *old_p;",
        "	struct module *probed_mod;",
        "	kprobe_opcode_t *addr;",
        "	bool on_func_entry;",
        "",
        "	/* Adjust probe address from symbol */",
        "	addr = _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);",
        "	if (IS_ERR(addr))",
        "		return PTR_ERR(addr);",
        "	p->addr = addr;",
        "",
        "	ret = warn_kprobe_rereg(p);",
        "	if (ret)",
        "		return ret;",
        "",
        "	/* User can pass only KPROBE_FLAG_DISABLED to register_kprobe */",
        "	p->flags &= KPROBE_FLAG_DISABLED;",
        "	p->nmissed = 0;",
        "	INIT_LIST_HEAD(&p->list);",
        "",
        "	ret = check_kprobe_address_safe(p, &probed_mod);",
        "	if (ret)",
        "		return ret;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	if (on_func_entry)",
        "		p->flags |= KPROBE_FLAG_ON_FUNC_ENTRY;",
        "",
        "	old_p = get_kprobe(p->addr);",
        "	if (old_p) {",
        "		/* Since this may unoptimize 'old_p', locking 'text_mutex'. */",
        "		ret = register_aggr_kprobe(old_p, p);",
        "		goto out;",
        "	}",
        "",
        "	cpus_read_lock();",
        "	/* Prevent text modification */",
        "	mutex_lock(&text_mutex);",
        "	ret = prepare_kprobe(p);",
        "	mutex_unlock(&text_mutex);",
        "	cpus_read_unlock();",
        "	if (ret)",
        "		goto out;",
        "",
        "	INIT_HLIST_NODE(&p->hlist);",
        "	hlist_add_head_rcu(&p->hlist,",
        "		       &kprobe_table[hash_ptr(p->addr, KPROBE_HASH_BITS)]);",
        "",
        "	if (!kprobes_all_disarmed && !kprobe_disabled(p)) {",
        "		ret = arm_kprobe(p);",
        "		if (ret) {",
        "			hlist_del_rcu(&p->hlist);",
        "			synchronize_rcu();",
        "			goto out;",
        "		}",
        "	}",
        "",
        "	/* Try to optimize kprobe */",
        "	try_to_optimize_kprobe(p);",
        "out:",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	if (probed_mod)",
        "		module_put(probed_mod);",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kprobe);",
        "",
        "/* Check if all probes on the 'ap' are disabled. */",
        "static bool aggr_kprobe_disabled(struct kprobe *ap)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	list_for_each_entry(kp, &ap->list, list)",
        "		if (!kprobe_disabled(kp))",
        "			/*",
        "			 * Since there is an active probe on the list,",
        "			 * we can't disable this 'ap'.",
        "			 */",
        "			return false;",
        "",
        "	return true;",
        "}",
        "",
        "static struct kprobe *__disable_kprobe(struct kprobe *p)",
        "{",
        "	struct kprobe *orig_p;",
        "	int ret;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	/* Get an original kprobe for return */",
        "	orig_p = __get_valid_kprobe(p);",
        "	if (unlikely(orig_p == NULL))",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	if (kprobe_disabled(p))",
        "		return orig_p;",
        "",
        "	/* Disable probe if it is a child probe */",
        "	if (p != orig_p)",
        "		p->flags |= KPROBE_FLAG_DISABLED;",
        "",
        "	/* Try to disarm and disable this/parent probe */",
        "	if (p == orig_p || aggr_kprobe_disabled(orig_p)) {",
        "		/*",
        "		 * Don't be lazy here.  Even if 'kprobes_all_disarmed'",
        "		 * is false, 'orig_p' might not have been armed yet.",
        "		 * Note arm_all_kprobes() __tries__ to arm all kprobes",
        "		 * on the best effort basis.",
        "		 */",
        "		if (!kprobes_all_disarmed && !kprobe_disabled(orig_p)) {",
        "			ret = disarm_kprobe(orig_p, true);",
        "			if (ret) {",
        "				p->flags &= ~KPROBE_FLAG_DISABLED;",
        "				return ERR_PTR(ret);",
        "			}",
        "		}",
        "		orig_p->flags |= KPROBE_FLAG_DISABLED;",
        "	}",
        "",
        "	return orig_p;",
        "}",
        "",
        "/*",
        " * Unregister a kprobe without a scheduler synchronization.",
        " */",
        "static int __unregister_kprobe_top(struct kprobe *p)",
        "{",
        "	struct kprobe *ap, *list_p;",
        "",
        "	/* Disable kprobe. This will disarm it if needed. */",
        "	ap = __disable_kprobe(p);",
        "	if (IS_ERR(ap))",
        "		return PTR_ERR(ap);",
        "",
        "	if (ap == p)",
        "		/*",
        "		 * This probe is an independent(and non-optimized) kprobe",
        "		 * (not an aggrprobe). Remove from the hash list.",
        "		 */",
        "		goto disarmed;",
        "",
        "	/* Following process expects this probe is an aggrprobe */",
        "	WARN_ON(!kprobe_aggrprobe(ap));",
        "",
        "	if (list_is_singular(&ap->list) && kprobe_disarmed(ap))",
        "		/*",
        "		 * !disarmed could be happen if the probe is under delayed",
        "		 * unoptimizing.",
        "		 */",
        "		goto disarmed;",
        "	else {",
        "		/* If disabling probe has special handlers, update aggrprobe */",
        "		if (p->post_handler && !kprobe_gone(p)) {",
        "			list_for_each_entry(list_p, &ap->list, list) {",
        "				if ((list_p != p) && (list_p->post_handler))",
        "					goto noclean;",
        "			}",
        "			/*",
        "			 * For the kprobe-on-ftrace case, we keep the",
        "			 * post_handler setting to identify this aggrprobe",
        "			 * armed with kprobe_ipmodify_ops.",
        "			 */",
        "			if (!kprobe_ftrace(ap))",
        "				ap->post_handler = NULL;",
        "		}",
        "noclean:",
        "		/*",
        "		 * Remove from the aggrprobe: this path will do nothing in",
        "		 * __unregister_kprobe_bottom().",
        "		 */",
        "		list_del_rcu(&p->list);",
        "		if (!kprobe_disabled(ap) && !kprobes_all_disarmed)",
        "			/*",
        "			 * Try to optimize this probe again, because post",
        "			 * handler may have been changed.",
        "			 */",
        "			optimize_kprobe(ap);",
        "	}",
        "	return 0;",
        "",
        "disarmed:",
        "	hlist_del_rcu(&ap->hlist);",
        "	return 0;",
        "}",
        "",
        "static void __unregister_kprobe_bottom(struct kprobe *p)",
        "{",
        "	struct kprobe *ap;",
        "",
        "	if (list_empty(&p->list))",
        "		/* This is an independent kprobe */",
        "		arch_remove_kprobe(p);",
        "	else if (list_is_singular(&p->list)) {",
        "		/* This is the last child of an aggrprobe */",
        "		ap = list_entry(p->list.next, struct kprobe, list);",
        "		list_del(&p->list);",
        "		free_aggr_kprobe(ap);",
        "	}",
        "	/* Otherwise, do nothing. */",
        "}",
        "",
        "int register_kprobes(struct kprobe **kps, int num)",
        "{",
        "	int i, ret = 0;",
        "",
        "	if (num <= 0)",
        "		return -EINVAL;",
        "	for (i = 0; i < num; i++) {",
        "		ret = register_kprobe(kps[i]);",
        "		if (ret < 0) {",
        "			if (i > 0)",
        "				unregister_kprobes(kps, i);",
        "			break;",
        "		}",
        "	}",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kprobes);",
        "",
        "void unregister_kprobe(struct kprobe *p)",
        "{",
        "	unregister_kprobes(&p, 1);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kprobe);",
        "",
        "void unregister_kprobes(struct kprobe **kps, int num)",
        "{",
        "	int i;",
        "",
        "	if (num <= 0)",
        "		return;",
        "	mutex_lock(&kprobe_mutex);",
        "	for (i = 0; i < num; i++)",
        "		if (__unregister_kprobe_top(kps[i]) < 0)",
        "			kps[i]->addr = NULL;",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	synchronize_rcu();",
        "	for (i = 0; i < num; i++)",
        "		if (kps[i]->addr)",
        "			__unregister_kprobe_bottom(kps[i]);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kprobes);",
        "",
        "int __weak kprobe_exceptions_notify(struct notifier_block *self,",
        "					unsigned long val, void *data)",
        "{",
        "	return NOTIFY_DONE;",
        "}",
        "NOKPROBE_SYMBOL(kprobe_exceptions_notify);",
        "",
        "static struct notifier_block kprobe_exceptions_nb = {",
        "	.notifier_call = kprobe_exceptions_notify,",
        "	.priority = 0x7fffffff /* we need to be notified first */",
        "};",
        "",
        "#ifdef CONFIG_KRETPROBES",
        "",
        "#if !defined(CONFIG_KRETPROBE_ON_RETHOOK)",
        "",
        "/* callbacks for objpool of kretprobe instances */",
        "static int kretprobe_init_inst(void *nod, void *context)",
        "{",
        "	struct kretprobe_instance *ri = nod;",
        "",
        "	ri->rph = context;",
        "	return 0;",
        "}",
        "static int kretprobe_fini_pool(struct objpool_head *head, void *context)",
        "{",
        "	kfree(context);",
        "	return 0;",
        "}",
        "",
        "static void free_rp_inst_rcu(struct rcu_head *head)",
        "{",
        "	struct kretprobe_instance *ri = container_of(head, struct kretprobe_instance, rcu);",
        "	struct kretprobe_holder *rph = ri->rph;",
        "",
        "	objpool_drop(ri, &rph->pool);",
        "}",
        "NOKPROBE_SYMBOL(free_rp_inst_rcu);",
        "",
        "static void recycle_rp_inst(struct kretprobe_instance *ri)",
        "{",
        "	struct kretprobe *rp = get_kretprobe(ri);",
        "",
        "	if (likely(rp))",
        "		objpool_push(ri, &rp->rph->pool);",
        "	else",
        "		call_rcu(&ri->rcu, free_rp_inst_rcu);",
        "}",
        "NOKPROBE_SYMBOL(recycle_rp_inst);",
        "",
        "/*",
        " * This function is called from delayed_put_task_struct() when a task is",
        " * dead and cleaned up to recycle any kretprobe instances associated with",
        " * this task. These left over instances represent probed functions that",
        " * have been called but will never return.",
        " */",
        "void kprobe_flush_task(struct task_struct *tk)",
        "{",
        "	struct kretprobe_instance *ri;",
        "	struct llist_node *node;",
        "",
        "	/* Early boot, not yet initialized. */",
        "	if (unlikely(!kprobes_initialized))",
        "		return;",
        "",
        "	kprobe_busy_begin();",
        "",
        "	node = __llist_del_all(&tk->kretprobe_instances);",
        "	while (node) {",
        "		ri = container_of(node, struct kretprobe_instance, llist);",
        "		node = node->next;",
        "",
        "		recycle_rp_inst(ri);",
        "	}",
        "",
        "	kprobe_busy_end();",
        "}",
        "NOKPROBE_SYMBOL(kprobe_flush_task);",
        "",
        "static inline void free_rp_inst(struct kretprobe *rp)",
        "{",
        "	struct kretprobe_holder *rph = rp->rph;",
        "",
        "	if (!rph)",
        "		return;",
        "	rp->rph = NULL;",
        "	objpool_fini(&rph->pool);",
        "}",
        "",
        "/* This assumes the 'tsk' is the current task or the is not running. */",
        "static kprobe_opcode_t *__kretprobe_find_ret_addr(struct task_struct *tsk,",
        "						  struct llist_node **cur)",
        "{",
        "	struct kretprobe_instance *ri = NULL;",
        "	struct llist_node *node = *cur;",
        "",
        "	if (!node)",
        "		node = tsk->kretprobe_instances.first;",
        "	else",
        "		node = node->next;",
        "",
        "	while (node) {",
        "		ri = container_of(node, struct kretprobe_instance, llist);",
        "		if (ri->ret_addr != kretprobe_trampoline_addr()) {",
        "			*cur = node;",
        "			return ri->ret_addr;",
        "		}",
        "		node = node->next;",
        "	}",
        "	return NULL;",
        "}",
        "NOKPROBE_SYMBOL(__kretprobe_find_ret_addr);",
        "",
        "/**",
        " * kretprobe_find_ret_addr -- Find correct return address modified by kretprobe",
        " * @tsk: Target task",
        " * @fp: A frame pointer",
        " * @cur: a storage of the loop cursor llist_node pointer for next call",
        " *",
        " * Find the correct return address modified by a kretprobe on @tsk in unsigned",
        " * long type. If it finds the return address, this returns that address value,",
        " * or this returns 0.",
        " * The @tsk must be 'current' or a task which is not running. @fp is a hint",
        " * to get the currect return address - which is compared with the",
        " * kretprobe_instance::fp field. The @cur is a loop cursor for searching the",
        " * kretprobe return addresses on the @tsk. The '*@cur' should be NULL at the",
        " * first call, but '@cur' itself must NOT NULL.",
        " */",
        "unsigned long kretprobe_find_ret_addr(struct task_struct *tsk, void *fp,",
        "				      struct llist_node **cur)",
        "{",
        "	struct kretprobe_instance *ri;",
        "	kprobe_opcode_t *ret;",
        "",
        "	if (WARN_ON_ONCE(!cur))",
        "		return 0;",
        "",
        "	do {",
        "		ret = __kretprobe_find_ret_addr(tsk, cur);",
        "		if (!ret)",
        "			break;",
        "		ri = container_of(*cur, struct kretprobe_instance, llist);",
        "	} while (ri->fp != fp);",
        "",
        "	return (unsigned long)ret;",
        "}",
        "NOKPROBE_SYMBOL(kretprobe_find_ret_addr);",
        "",
        "void __weak arch_kretprobe_fixup_return(struct pt_regs *regs,",
        "					kprobe_opcode_t *correct_ret_addr)",
        "{",
        "	/*",
        "	 * Do nothing by default. Please fill this to update the fake return",
        "	 * address on the stack with the correct one on each arch if possible.",
        "	 */",
        "}",
        "",
        "unsigned long __kretprobe_trampoline_handler(struct pt_regs *regs,",
        "					     void *frame_pointer)",
        "{",
        "	struct kretprobe_instance *ri = NULL;",
        "	struct llist_node *first, *node = NULL;",
        "	kprobe_opcode_t *correct_ret_addr;",
        "	struct kretprobe *rp;",
        "",
        "	/* Find correct address and all nodes for this frame. */",
        "	correct_ret_addr = __kretprobe_find_ret_addr(current, &node);",
        "	if (!correct_ret_addr) {",
        "		pr_err(\"kretprobe: Return address not found, not execute handler. Maybe there is a bug in the kernel.\\n\");",
        "		BUG_ON(1);",
        "	}",
        "",
        "	/*",
        "	 * Set the return address as the instruction pointer, because if the",
        "	 * user handler calls stack_trace_save_regs() with this 'regs',",
        "	 * the stack trace will start from the instruction pointer.",
        "	 */",
        "	instruction_pointer_set(regs, (unsigned long)correct_ret_addr);",
        "",
        "	/* Run the user handler of the nodes. */",
        "	first = current->kretprobe_instances.first;",
        "	while (first) {",
        "		ri = container_of(first, struct kretprobe_instance, llist);",
        "",
        "		if (WARN_ON_ONCE(ri->fp != frame_pointer))",
        "			break;",
        "",
        "		rp = get_kretprobe(ri);",
        "		if (rp && rp->handler) {",
        "			struct kprobe *prev = kprobe_running();",
        "",
        "			__this_cpu_write(current_kprobe, &rp->kp);",
        "			ri->ret_addr = correct_ret_addr;",
        "			rp->handler(ri, regs);",
        "			__this_cpu_write(current_kprobe, prev);",
        "		}",
        "		if (first == node)",
        "			break;",
        "",
        "		first = first->next;",
        "	}",
        "",
        "	arch_kretprobe_fixup_return(regs, correct_ret_addr);",
        "",
        "	/* Unlink all nodes for this frame. */",
        "	first = current->kretprobe_instances.first;",
        "	current->kretprobe_instances.first = node->next;",
        "	node->next = NULL;",
        "",
        "	/* Recycle free instances. */",
        "	while (first) {",
        "		ri = container_of(first, struct kretprobe_instance, llist);",
        "		first = first->next;",
        "",
        "		recycle_rp_inst(ri);",
        "	}",
        "",
        "	return (unsigned long)correct_ret_addr;",
        "}",
        "NOKPROBE_SYMBOL(__kretprobe_trampoline_handler)",
        "",
        "/*",
        " * This kprobe pre_handler is registered with every kretprobe. When probe",
        " * hits it will set up the return probe.",
        " */",
        "static int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)",
        "{",
        "	struct kretprobe *rp = container_of(p, struct kretprobe, kp);",
        "	struct kretprobe_holder *rph = rp->rph;",
        "	struct kretprobe_instance *ri;",
        "",
        "	ri = objpool_pop(&rph->pool);",
        "	if (!ri) {",
        "		rp->nmissed++;",
        "		return 0;",
        "	}",
        "",
        "	if (rp->entry_handler && rp->entry_handler(ri, regs)) {",
        "		objpool_push(ri, &rph->pool);",
        "		return 0;",
        "	}",
        "",
        "	arch_prepare_kretprobe(ri, regs);",
        "",
        "	__llist_add(&ri->llist, &current->kretprobe_instances);",
        "",
        "	return 0;",
        "}",
        "NOKPROBE_SYMBOL(pre_handler_kretprobe);",
        "#else /* CONFIG_KRETPROBE_ON_RETHOOK */",
        "/*",
        " * This kprobe pre_handler is registered with every kretprobe. When probe",
        " * hits it will set up the return probe.",
        " */",
        "static int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)",
        "{",
        "	struct kretprobe *rp = container_of(p, struct kretprobe, kp);",
        "	struct kretprobe_instance *ri;",
        "	struct rethook_node *rhn;",
        "",
        "	rhn = rethook_try_get(rp->rh);",
        "	if (!rhn) {",
        "		rp->nmissed++;",
        "		return 0;",
        "	}",
        "",
        "	ri = container_of(rhn, struct kretprobe_instance, node);",
        "",
        "	if (rp->entry_handler && rp->entry_handler(ri, regs))",
        "		rethook_recycle(rhn);",
        "	else",
        "		rethook_hook(rhn, regs, kprobe_ftrace(p));",
        "",
        "	return 0;",
        "}",
        "NOKPROBE_SYMBOL(pre_handler_kretprobe);",
        "",
        "static void kretprobe_rethook_handler(struct rethook_node *rh, void *data,",
        "				      unsigned long ret_addr,",
        "				      struct pt_regs *regs)",
        "{",
        "	struct kretprobe *rp = (struct kretprobe *)data;",
        "	struct kretprobe_instance *ri;",
        "	struct kprobe_ctlblk *kcb;",
        "",
        "	/* The data must NOT be null. This means rethook data structure is broken. */",
        "	if (WARN_ON_ONCE(!data) || !rp->handler)",
        "		return;",
        "",
        "	__this_cpu_write(current_kprobe, &rp->kp);",
        "	kcb = get_kprobe_ctlblk();",
        "	kcb->kprobe_status = KPROBE_HIT_ACTIVE;",
        "",
        "	ri = container_of(rh, struct kretprobe_instance, node);",
        "	rp->handler(ri, regs);",
        "",
        "	__this_cpu_write(current_kprobe, NULL);",
        "}",
        "NOKPROBE_SYMBOL(kretprobe_rethook_handler);",
        "",
        "#endif /* !CONFIG_KRETPROBE_ON_RETHOOK */",
        "",
        "/**",
        " * kprobe_on_func_entry() -- check whether given address is function entry",
        " * @addr: Target address",
        " * @sym:  Target symbol name",
        " * @offset: The offset from the symbol or the address",
        " *",
        " * This checks whether the given @addr+@offset or @sym+@offset is on the",
        " * function entry address or not.",
        " * This returns 0 if it is the function entry, or -EINVAL if it is not.",
        " * And also it returns -ENOENT if it fails the symbol or address lookup.",
        " * Caller must pass @addr or @sym (either one must be NULL), or this",
        " * returns -EINVAL.",
        " */",
        "int kprobe_on_func_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset)",
        "{",
        "	bool on_func_entry;",
        "	kprobe_opcode_t *kp_addr = _kprobe_addr(addr, sym, offset, &on_func_entry);",
        "",
        "	if (IS_ERR(kp_addr))",
        "		return PTR_ERR(kp_addr);",
        "",
        "	if (!on_func_entry)",
        "		return -EINVAL;",
        "",
        "	return 0;",
        "}",
        "",
        "int register_kretprobe(struct kretprobe *rp)",
        "{",
        "	int ret;",
        "	int i;",
        "	void *addr;",
        "",
        "	ret = kprobe_on_func_entry(rp->kp.addr, rp->kp.symbol_name, rp->kp.offset);",
        "	if (ret)",
        "		return ret;",
        "",
        "	/* If only 'rp->kp.addr' is specified, check reregistering kprobes */",
        "	if (rp->kp.addr && warn_kprobe_rereg(&rp->kp))",
        "		return -EINVAL;",
        "",
        "	if (kretprobe_blacklist_size) {",
        "		addr = kprobe_addr(&rp->kp);",
        "		if (IS_ERR(addr))",
        "			return PTR_ERR(addr);",
        "",
        "		for (i = 0; kretprobe_blacklist[i].name != NULL; i++) {",
        "			if (kretprobe_blacklist[i].addr == addr)",
        "				return -EINVAL;",
        "		}",
        "	}",
        "",
        "	if (rp->data_size > KRETPROBE_MAX_DATA_SIZE)",
        "		return -E2BIG;",
        "",
        "	rp->kp.pre_handler = pre_handler_kretprobe;",
        "	rp->kp.post_handler = NULL;",
        "",
        "	/* Pre-allocate memory for max kretprobe instances */",
        "	if (rp->maxactive <= 0)",
        "		rp->maxactive = max_t(unsigned int, 10, 2*num_possible_cpus());",
        "",
        "#ifdef CONFIG_KRETPROBE_ON_RETHOOK",
        "	rp->rh = rethook_alloc((void *)rp, kretprobe_rethook_handler,",
        "				sizeof(struct kretprobe_instance) +",
        "				rp->data_size, rp->maxactive);",
        "	if (IS_ERR(rp->rh))",
        "		return PTR_ERR(rp->rh);",
        "",
        "	rp->nmissed = 0;",
        "	/* Establish function entry probe point */",
        "	ret = register_kprobe(&rp->kp);",
        "	if (ret != 0) {",
        "		rethook_free(rp->rh);",
        "		rp->rh = NULL;",
        "	}",
        "#else	/* !CONFIG_KRETPROBE_ON_RETHOOK */",
        "	rp->rph = kzalloc(sizeof(struct kretprobe_holder), GFP_KERNEL);",
        "	if (!rp->rph)",
        "		return -ENOMEM;",
        "",
        "	if (objpool_init(&rp->rph->pool, rp->maxactive, rp->data_size +",
        "			sizeof(struct kretprobe_instance), GFP_KERNEL,",
        "			rp->rph, kretprobe_init_inst, kretprobe_fini_pool)) {",
        "		kfree(rp->rph);",
        "		rp->rph = NULL;",
        "		return -ENOMEM;",
        "	}",
        "	rcu_assign_pointer(rp->rph->rp, rp);",
        "	rp->nmissed = 0;",
        "	/* Establish function entry probe point */",
        "	ret = register_kprobe(&rp->kp);",
        "	if (ret != 0)",
        "		free_rp_inst(rp);",
        "#endif",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kretprobe);",
        "",
        "int register_kretprobes(struct kretprobe **rps, int num)",
        "{",
        "	int ret = 0, i;",
        "",
        "	if (num <= 0)",
        "		return -EINVAL;",
        "	for (i = 0; i < num; i++) {",
        "		ret = register_kretprobe(rps[i]);",
        "		if (ret < 0) {",
        "			if (i > 0)",
        "				unregister_kretprobes(rps, i);",
        "			break;",
        "		}",
        "	}",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kretprobes);",
        "",
        "void unregister_kretprobe(struct kretprobe *rp)",
        "{",
        "	unregister_kretprobes(&rp, 1);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kretprobe);",
        "",
        "void unregister_kretprobes(struct kretprobe **rps, int num)",
        "{",
        "	int i;",
        "",
        "	if (num <= 0)",
        "		return;",
        "	mutex_lock(&kprobe_mutex);",
        "	for (i = 0; i < num; i++) {",
        "		if (__unregister_kprobe_top(&rps[i]->kp) < 0)",
        "			rps[i]->kp.addr = NULL;",
        "#ifdef CONFIG_KRETPROBE_ON_RETHOOK",
        "		rethook_free(rps[i]->rh);",
        "#else",
        "		rcu_assign_pointer(rps[i]->rph->rp, NULL);",
        "#endif",
        "	}",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	synchronize_rcu();",
        "	for (i = 0; i < num; i++) {",
        "		if (rps[i]->kp.addr) {",
        "			__unregister_kprobe_bottom(&rps[i]->kp);",
        "#ifndef CONFIG_KRETPROBE_ON_RETHOOK",
        "			free_rp_inst(rps[i]);",
        "#endif",
        "		}",
        "	}",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kretprobes);",
        "",
        "#else /* CONFIG_KRETPROBES */",
        "int register_kretprobe(struct kretprobe *rp)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kretprobe);",
        "",
        "int register_kretprobes(struct kretprobe **rps, int num)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "EXPORT_SYMBOL_GPL(register_kretprobes);",
        "",
        "void unregister_kretprobe(struct kretprobe *rp)",
        "{",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kretprobe);",
        "",
        "void unregister_kretprobes(struct kretprobe **rps, int num)",
        "{",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_kretprobes);",
        "",
        "static int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)",
        "{",
        "	return 0;",
        "}",
        "NOKPROBE_SYMBOL(pre_handler_kretprobe);",
        "",
        "#endif /* CONFIG_KRETPROBES */",
        "",
        "/* Set the kprobe gone and remove its instruction buffer. */",
        "static void kill_kprobe(struct kprobe *p)",
        "{",
        "	struct kprobe *kp;",
        "",
        "	lockdep_assert_held(&kprobe_mutex);",
        "",
        "	/*",
        "	 * The module is going away. We should disarm the kprobe which",
        "	 * is using ftrace, because ftrace framework is still available at",
        "	 * 'MODULE_STATE_GOING' notification.",
        "	 */",
        "	if (kprobe_ftrace(p) && !kprobe_disabled(p) && !kprobes_all_disarmed)",
        "		disarm_kprobe_ftrace(p);",
        "",
        "	p->flags |= KPROBE_FLAG_GONE;",
        "	if (kprobe_aggrprobe(p)) {",
        "		/*",
        "		 * If this is an aggr_kprobe, we have to list all the",
        "		 * chained probes and mark them GONE.",
        "		 */",
        "		list_for_each_entry(kp, &p->list, list)",
        "			kp->flags |= KPROBE_FLAG_GONE;",
        "		p->post_handler = NULL;",
        "		kill_optimized_kprobe(p);",
        "	}",
        "	/*",
        "	 * Here, we can remove insn_slot safely, because no thread calls",
        "	 * the original probed function (which will be freed soon) any more.",
        "	 */",
        "	arch_remove_kprobe(p);",
        "}",
        "",
        "/* Disable one kprobe */",
        "int disable_kprobe(struct kprobe *kp)",
        "{",
        "	int ret = 0;",
        "	struct kprobe *p;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	/* Disable this kprobe */",
        "	p = __disable_kprobe(kp);",
        "	if (IS_ERR(p))",
        "		ret = PTR_ERR(p);",
        "",
        "	mutex_unlock(&kprobe_mutex);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(disable_kprobe);",
        "",
        "/* Enable one kprobe */",
        "int enable_kprobe(struct kprobe *kp)",
        "{",
        "	int ret = 0;",
        "	struct kprobe *p;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	/* Check whether specified probe is valid. */",
        "	p = __get_valid_kprobe(kp);",
        "	if (unlikely(p == NULL)) {",
        "		ret = -EINVAL;",
        "		goto out;",
        "	}",
        "",
        "	if (kprobe_gone(kp)) {",
        "		/* This kprobe has gone, we couldn't enable it. */",
        "		ret = -EINVAL;",
        "		goto out;",
        "	}",
        "",
        "	if (p != kp)",
        "		kp->flags &= ~KPROBE_FLAG_DISABLED;",
        "",
        "	if (!kprobes_all_disarmed && kprobe_disabled(p)) {",
        "		p->flags &= ~KPROBE_FLAG_DISABLED;",
        "		ret = arm_kprobe(p);",
        "		if (ret) {",
        "			p->flags |= KPROBE_FLAG_DISABLED;",
        "			if (p != kp)",
        "				kp->flags |= KPROBE_FLAG_DISABLED;",
        "		}",
        "	}",
        "out:",
        "	mutex_unlock(&kprobe_mutex);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(enable_kprobe);",
        "",
        "/* Caller must NOT call this in usual path. This is only for critical case */",
        "void dump_kprobe(struct kprobe *kp)",
        "{",
        "	pr_err(\"Dump kprobe:\\n.symbol_name = %s, .offset = %x, .addr = %pS\\n\",",
        "	       kp->symbol_name, kp->offset, kp->addr);",
        "}",
        "NOKPROBE_SYMBOL(dump_kprobe);",
        "",
        "int kprobe_add_ksym_blacklist(unsigned long entry)",
        "{",
        "	struct kprobe_blacklist_entry *ent;",
        "	unsigned long offset = 0, size = 0;",
        "",
        "	if (!kernel_text_address(entry) ||",
        "	    !kallsyms_lookup_size_offset(entry, &size, &offset))",
        "		return -EINVAL;",
        "",
        "	ent = kmalloc(sizeof(*ent), GFP_KERNEL);",
        "	if (!ent)",
        "		return -ENOMEM;",
        "	ent->start_addr = entry;",
        "	ent->end_addr = entry + size;",
        "	INIT_LIST_HEAD(&ent->list);",
        "	list_add_tail(&ent->list, &kprobe_blacklist);",
        "",
        "	return (int)size;",
        "}",
        "",
        "/* Add all symbols in given area into kprobe blacklist */",
        "int kprobe_add_area_blacklist(unsigned long start, unsigned long end)",
        "{",
        "	unsigned long entry;",
        "	int ret = 0;",
        "",
        "	for (entry = start; entry < end; entry += ret) {",
        "		ret = kprobe_add_ksym_blacklist(entry);",
        "		if (ret < 0)",
        "			return ret;",
        "		if (ret == 0)	/* In case of alias symbol */",
        "			ret = 1;",
        "	}",
        "	return 0;",
        "}",
        "",
        "int __weak arch_kprobe_get_kallsym(unsigned int *symnum, unsigned long *value,",
        "				   char *type, char *sym)",
        "{",
        "	return -ERANGE;",
        "}",
        "",
        "int kprobe_get_kallsym(unsigned int symnum, unsigned long *value, char *type,",
        "		       char *sym)",
        "{",
        "#ifdef __ARCH_WANT_KPROBES_INSN_SLOT",
        "	if (!kprobe_cache_get_kallsym(&kprobe_insn_slots, &symnum, value, type, sym))",
        "		return 0;",
        "#ifdef CONFIG_OPTPROBES",
        "	if (!kprobe_cache_get_kallsym(&kprobe_optinsn_slots, &symnum, value, type, sym))",
        "		return 0;",
        "#endif",
        "#endif",
        "	if (!arch_kprobe_get_kallsym(&symnum, value, type, sym))",
        "		return 0;",
        "	return -ERANGE;",
        "}",
        "",
        "int __init __weak arch_populate_kprobe_blacklist(void)",
        "{",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Lookup and populate the kprobe_blacklist.",
        " *",
        " * Unlike the kretprobe blacklist, we'll need to determine",
        " * the range of addresses that belong to the said functions,",
        " * since a kprobe need not necessarily be at the beginning",
        " * of a function.",
        " */",
        "static int __init populate_kprobe_blacklist(unsigned long *start,",
        "					     unsigned long *end)",
        "{",
        "	unsigned long entry;",
        "	unsigned long *iter;",
        "	int ret;",
        "",
        "	for (iter = start; iter < end; iter++) {",
        "		entry = (unsigned long)dereference_symbol_descriptor((void *)*iter);",
        "		ret = kprobe_add_ksym_blacklist(entry);",
        "		if (ret == -EINVAL)",
        "			continue;",
        "		if (ret < 0)",
        "			return ret;",
        "	}",
        "",
        "	/* Symbols in '__kprobes_text' are blacklisted */",
        "	ret = kprobe_add_area_blacklist((unsigned long)__kprobes_text_start,",
        "					(unsigned long)__kprobes_text_end);",
        "	if (ret)",
        "		return ret;",
        "",
        "	/* Symbols in 'noinstr' section are blacklisted */",
        "	ret = kprobe_add_area_blacklist((unsigned long)__noinstr_text_start,",
        "					(unsigned long)__noinstr_text_end);",
        "",
        "	return ret ? : arch_populate_kprobe_blacklist();",
        "}",
        "",
        "#ifdef CONFIG_MODULES",
        "/* Remove all symbols in given area from kprobe blacklist */",
        "static void kprobe_remove_area_blacklist(unsigned long start, unsigned long end)",
        "{",
        "	struct kprobe_blacklist_entry *ent, *n;",
        "",
        "	list_for_each_entry_safe(ent, n, &kprobe_blacklist, list) {",
        "		if (ent->start_addr < start || ent->start_addr >= end)",
        "			continue;",
        "		list_del(&ent->list);",
        "		kfree(ent);",
        "	}",
        "}",
        "",
        "static void kprobe_remove_ksym_blacklist(unsigned long entry)",
        "{",
        "	kprobe_remove_area_blacklist(entry, entry + 1);",
        "}",
        "",
        "static void add_module_kprobe_blacklist(struct module *mod)",
        "{",
        "	unsigned long start, end;",
        "	int i;",
        "",
        "	if (mod->kprobe_blacklist) {",
        "		for (i = 0; i < mod->num_kprobe_blacklist; i++)",
        "			kprobe_add_ksym_blacklist(mod->kprobe_blacklist[i]);",
        "	}",
        "",
        "	start = (unsigned long)mod->kprobes_text_start;",
        "	if (start) {",
        "		end = start + mod->kprobes_text_size;",
        "		kprobe_add_area_blacklist(start, end);",
        "	}",
        "",
        "	start = (unsigned long)mod->noinstr_text_start;",
        "	if (start) {",
        "		end = start + mod->noinstr_text_size;",
        "		kprobe_add_area_blacklist(start, end);",
        "	}",
        "}",
        "",
        "static void remove_module_kprobe_blacklist(struct module *mod)",
        "{",
        "	unsigned long start, end;",
        "	int i;",
        "",
        "	if (mod->kprobe_blacklist) {",
        "		for (i = 0; i < mod->num_kprobe_blacklist; i++)",
        "			kprobe_remove_ksym_blacklist(mod->kprobe_blacklist[i]);",
        "	}",
        "",
        "	start = (unsigned long)mod->kprobes_text_start;",
        "	if (start) {",
        "		end = start + mod->kprobes_text_size;",
        "		kprobe_remove_area_blacklist(start, end);",
        "	}",
        "",
        "	start = (unsigned long)mod->noinstr_text_start;",
        "	if (start) {",
        "		end = start + mod->noinstr_text_size;",
        "		kprobe_remove_area_blacklist(start, end);",
        "	}",
        "}",
        "",
        "/* Module notifier call back, checking kprobes on the module */",
        "static int kprobes_module_callback(struct notifier_block *nb,",
        "				   unsigned long val, void *data)",
        "{",
        "	struct module *mod = data;",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	unsigned int i;",
        "	int checkcore = (val == MODULE_STATE_GOING);",
        "",
        "	if (val == MODULE_STATE_COMING) {",
        "		mutex_lock(&kprobe_mutex);",
        "		add_module_kprobe_blacklist(mod);",
        "		mutex_unlock(&kprobe_mutex);",
        "	}",
        "	if (val != MODULE_STATE_GOING && val != MODULE_STATE_LIVE)",
        "		return NOTIFY_DONE;",
        "",
        "	/*",
        "	 * When 'MODULE_STATE_GOING' was notified, both of module '.text' and",
        "	 * '.init.text' sections would be freed. When 'MODULE_STATE_LIVE' was",
        "	 * notified, only '.init.text' section would be freed. We need to",
        "	 * disable kprobes which have been inserted in the sections.",
        "	 */",
        "	mutex_lock(&kprobe_mutex);",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		hlist_for_each_entry(p, head, hlist)",
        "			if (within_module_init((unsigned long)p->addr, mod) ||",
        "			    (checkcore &&",
        "			     within_module_core((unsigned long)p->addr, mod))) {",
        "				/*",
        "				 * The vaddr this probe is installed will soon",
        "				 * be vfreed buy not synced to disk. Hence,",
        "				 * disarming the breakpoint isn't needed.",
        "				 *",
        "				 * Note, this will also move any optimized probes",
        "				 * that are pending to be removed from their",
        "				 * corresponding lists to the 'freeing_list' and",
        "				 * will not be touched by the delayed",
        "				 * kprobe_optimizer() work handler.",
        "				 */",
        "				kill_kprobe(p);",
        "			}",
        "	}",
        "	if (val == MODULE_STATE_GOING)",
        "		remove_module_kprobe_blacklist(mod);",
        "	mutex_unlock(&kprobe_mutex);",
        "	return NOTIFY_DONE;",
        "}",
        "",
        "static struct notifier_block kprobe_module_nb = {",
        "	.notifier_call = kprobes_module_callback,",
        "	.priority = 0",
        "};",
        "",
        "static int kprobe_register_module_notifier(void)",
        "{",
        "	return register_module_notifier(&kprobe_module_nb);",
        "}",
        "#else",
        "static int kprobe_register_module_notifier(void)",
        "{",
        "	return 0;",
        "}",
        "#endif /* CONFIG_MODULES */",
        "",
        "void kprobe_free_init_mem(void)",
        "{",
        "	void *start = (void *)(&__init_begin);",
        "	void *end = (void *)(&__init_end);",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	int i;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	/* Kill all kprobes on initmem because the target code has been freed. */",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		hlist_for_each_entry(p, head, hlist) {",
        "			if (start <= (void *)p->addr && (void *)p->addr < end)",
        "				kill_kprobe(p);",
        "		}",
        "	}",
        "",
        "	mutex_unlock(&kprobe_mutex);",
        "}",
        "",
        "static int __init init_kprobes(void)",
        "{",
        "	int i, err;",
        "",
        "	/* FIXME allocate the probe table, currently defined statically */",
        "	/* initialize all list heads */",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++)",
        "		INIT_HLIST_HEAD(&kprobe_table[i]);",
        "",
        "	err = populate_kprobe_blacklist(__start_kprobe_blacklist,",
        "					__stop_kprobe_blacklist);",
        "	if (err)",
        "		pr_err(\"Failed to populate blacklist (error %d), kprobes not restricted, be careful using them!\\n\", err);",
        "",
        "	if (kretprobe_blacklist_size) {",
        "		/* lookup the function address from its name */",
        "		for (i = 0; kretprobe_blacklist[i].name != NULL; i++) {",
        "			kretprobe_blacklist[i].addr =",
        "				kprobe_lookup_name(kretprobe_blacklist[i].name, 0);",
        "			if (!kretprobe_blacklist[i].addr)",
        "				pr_err(\"Failed to lookup symbol '%s' for kretprobe blacklist. Maybe the target function is removed or renamed.\\n\",",
        "				       kretprobe_blacklist[i].name);",
        "		}",
        "	}",
        "",
        "	/* By default, kprobes are armed */",
        "	kprobes_all_disarmed = false;",
        "",
        "#if defined(CONFIG_OPTPROBES) && defined(__ARCH_WANT_KPROBES_INSN_SLOT)",
        "	/* Init 'kprobe_optinsn_slots' for allocation */",
        "	kprobe_optinsn_slots.insn_size = MAX_OPTINSN_SIZE;",
        "#endif",
        "",
        "	err = arch_init_kprobes();",
        "	if (!err)",
        "		err = register_die_notifier(&kprobe_exceptions_nb);",
        "	if (!err)",
        "		err = kprobe_register_module_notifier();",
        "",
        "	kprobes_initialized = (err == 0);",
        "	kprobe_sysctls_init();",
        "	return err;",
        "}",
        "early_initcall(init_kprobes);",
        "",
        "#if defined(CONFIG_OPTPROBES)",
        "static int __init init_optprobes(void)",
        "{",
        "	/*",
        "	 * Enable kprobe optimization - this kicks the optimizer which",
        "	 * depends on synchronize_rcu_tasks() and ksoftirqd, that is",
        "	 * not spawned in early initcall. So delay the optimization.",
        "	 */",
        "	optimize_all_kprobes();",
        "",
        "	return 0;",
        "}",
        "subsys_initcall(init_optprobes);",
        "#endif",
        "",
        "#ifdef CONFIG_DEBUG_FS",
        "static void report_probe(struct seq_file *pi, struct kprobe *p,",
        "		const char *sym, int offset, char *modname, struct kprobe *pp)",
        "{",
        "	char *kprobe_type;",
        "	void *addr = p->addr;",
        "",
        "	if (p->pre_handler == pre_handler_kretprobe)",
        "		kprobe_type = \"r\";",
        "	else",
        "		kprobe_type = \"k\";",
        "",
        "	if (!kallsyms_show_value(pi->file->f_cred))",
        "		addr = NULL;",
        "",
        "	if (sym)",
        "		seq_printf(pi, \"%px  %s  %s+0x%x  %s \",",
        "			addr, kprobe_type, sym, offset,",
        "			(modname ? modname : \" \"));",
        "	else	/* try to use %pS */",
        "		seq_printf(pi, \"%px  %s  %pS \",",
        "			addr, kprobe_type, p->addr);",
        "",
        "	if (!pp)",
        "		pp = p;",
        "	seq_printf(pi, \"%s%s%s%s\\n\",",
        "		(kprobe_gone(p) ? \"[GONE]\" : \"\"),",
        "		((kprobe_disabled(p) && !kprobe_gone(p)) ?  \"[DISABLED]\" : \"\"),",
        "		(kprobe_optimized(pp) ? \"[OPTIMIZED]\" : \"\"),",
        "		(kprobe_ftrace(pp) ? \"[FTRACE]\" : \"\"));",
        "}",
        "",
        "static void *kprobe_seq_start(struct seq_file *f, loff_t *pos)",
        "{",
        "	return (*pos < KPROBE_TABLE_SIZE) ? pos : NULL;",
        "}",
        "",
        "static void *kprobe_seq_next(struct seq_file *f, void *v, loff_t *pos)",
        "{",
        "	(*pos)++;",
        "	if (*pos >= KPROBE_TABLE_SIZE)",
        "		return NULL;",
        "	return pos;",
        "}",
        "",
        "static void kprobe_seq_stop(struct seq_file *f, void *v)",
        "{",
        "	/* Nothing to do */",
        "}",
        "",
        "static int show_kprobe_addr(struct seq_file *pi, void *v)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p, *kp;",
        "	const char *sym;",
        "	unsigned int i = *(loff_t *) v;",
        "	unsigned long offset = 0;",
        "	char *modname, namebuf[KSYM_NAME_LEN];",
        "",
        "	head = &kprobe_table[i];",
        "	preempt_disable();",
        "	hlist_for_each_entry_rcu(p, head, hlist) {",
        "		sym = kallsyms_lookup((unsigned long)p->addr, NULL,",
        "					&offset, &modname, namebuf);",
        "		if (kprobe_aggrprobe(p)) {",
        "			list_for_each_entry_rcu(kp, &p->list, list)",
        "				report_probe(pi, kp, sym, offset, modname, p);",
        "		} else",
        "			report_probe(pi, p, sym, offset, modname, NULL);",
        "	}",
        "	preempt_enable();",
        "	return 0;",
        "}",
        "",
        "static const struct seq_operations kprobes_sops = {",
        "	.start = kprobe_seq_start,",
        "	.next  = kprobe_seq_next,",
        "	.stop  = kprobe_seq_stop,",
        "	.show  = show_kprobe_addr",
        "};",
        "",
        "DEFINE_SEQ_ATTRIBUTE(kprobes);",
        "",
        "/* kprobes/blacklist -- shows which functions can not be probed */",
        "static void *kprobe_blacklist_seq_start(struct seq_file *m, loff_t *pos)",
        "{",
        "	mutex_lock(&kprobe_mutex);",
        "	return seq_list_start(&kprobe_blacklist, *pos);",
        "}",
        "",
        "static void *kprobe_blacklist_seq_next(struct seq_file *m, void *v, loff_t *pos)",
        "{",
        "	return seq_list_next(v, &kprobe_blacklist, pos);",
        "}",
        "",
        "static int kprobe_blacklist_seq_show(struct seq_file *m, void *v)",
        "{",
        "	struct kprobe_blacklist_entry *ent =",
        "		list_entry(v, struct kprobe_blacklist_entry, list);",
        "",
        "	/*",
        "	 * If '/proc/kallsyms' is not showing kernel address, we won't",
        "	 * show them here either.",
        "	 */",
        "	if (!kallsyms_show_value(m->file->f_cred))",
        "		seq_printf(m, \"0x%px-0x%px\\t%ps\\n\", NULL, NULL,",
        "			   (void *)ent->start_addr);",
        "	else",
        "		seq_printf(m, \"0x%px-0x%px\\t%ps\\n\", (void *)ent->start_addr,",
        "			   (void *)ent->end_addr, (void *)ent->start_addr);",
        "	return 0;",
        "}",
        "",
        "static void kprobe_blacklist_seq_stop(struct seq_file *f, void *v)",
        "{",
        "	mutex_unlock(&kprobe_mutex);",
        "}",
        "",
        "static const struct seq_operations kprobe_blacklist_sops = {",
        "	.start = kprobe_blacklist_seq_start,",
        "	.next  = kprobe_blacklist_seq_next,",
        "	.stop  = kprobe_blacklist_seq_stop,",
        "	.show  = kprobe_blacklist_seq_show,",
        "};",
        "DEFINE_SEQ_ATTRIBUTE(kprobe_blacklist);",
        "",
        "static int arm_all_kprobes(void)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	unsigned int i, total = 0, errors = 0;",
        "	int err, ret = 0;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	/* If kprobes are armed, just return */",
        "	if (!kprobes_all_disarmed)",
        "		goto already_enabled;",
        "",
        "	/*",
        "	 * optimize_kprobe() called by arm_kprobe() checks",
        "	 * kprobes_all_disarmed, so set kprobes_all_disarmed before",
        "	 * arm_kprobe.",
        "	 */",
        "	kprobes_all_disarmed = false;",
        "	/* Arming kprobes doesn't optimize kprobe itself */",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		/* Arm all kprobes on a best-effort basis */",
        "		hlist_for_each_entry(p, head, hlist) {",
        "			if (!kprobe_disabled(p)) {",
        "				err = arm_kprobe(p);",
        "				if (err)  {",
        "					errors++;",
        "					ret = err;",
        "				}",
        "				total++;",
        "			}",
        "		}",
        "	}",
        "",
        "	if (errors)",
        "		pr_warn(\"Kprobes globally enabled, but failed to enable %d out of %d probes. Please check which kprobes are kept disabled via debugfs.\\n\",",
        "			errors, total);",
        "	else",
        "		pr_info(\"Kprobes globally enabled\\n\");",
        "",
        "already_enabled:",
        "	mutex_unlock(&kprobe_mutex);",
        "	return ret;",
        "}",
        "",
        "static int disarm_all_kprobes(void)",
        "{",
        "	struct hlist_head *head;",
        "	struct kprobe *p;",
        "	unsigned int i, total = 0, errors = 0;",
        "	int err, ret = 0;",
        "",
        "	mutex_lock(&kprobe_mutex);",
        "",
        "	/* If kprobes are already disarmed, just return */",
        "	if (kprobes_all_disarmed) {",
        "		mutex_unlock(&kprobe_mutex);",
        "		return 0;",
        "	}",
        "",
        "	kprobes_all_disarmed = true;",
        "",
        "	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {",
        "		head = &kprobe_table[i];",
        "		/* Disarm all kprobes on a best-effort basis */",
        "		hlist_for_each_entry(p, head, hlist) {",
        "			if (!arch_trampoline_kprobe(p) && !kprobe_disabled(p)) {",
        "				err = disarm_kprobe(p, false);",
        "				if (err) {",
        "					errors++;",
        "					ret = err;",
        "				}",
        "				total++;",
        "			}",
        "		}",
        "	}",
        "",
        "	if (errors)",
        "		pr_warn(\"Kprobes globally disabled, but failed to disable %d out of %d probes. Please check which kprobes are kept enabled via debugfs.\\n\",",
        "			errors, total);",
        "	else",
        "		pr_info(\"Kprobes globally disabled\\n\");",
        "",
        "	mutex_unlock(&kprobe_mutex);",
        "",
        "	/* Wait for disarming all kprobes by optimizer */",
        "	wait_for_kprobe_optimizer();",
        "",
        "	return ret;",
        "}",
        "",
        "/*",
        " * XXX: The debugfs bool file interface doesn't allow for callbacks",
        " * when the bool state is switched. We can reuse that facility when",
        " * available",
        " */",
        "static ssize_t read_enabled_file_bool(struct file *file,",
        "	       char __user *user_buf, size_t count, loff_t *ppos)",
        "{",
        "	char buf[3];",
        "",
        "	if (!kprobes_all_disarmed)",
        "		buf[0] = '1';",
        "	else",
        "		buf[0] = '0';",
        "	buf[1] = '\\n';",
        "	buf[2] = 0x00;",
        "	return simple_read_from_buffer(user_buf, count, ppos, buf, 2);",
        "}",
        "",
        "static ssize_t write_enabled_file_bool(struct file *file,",
        "	       const char __user *user_buf, size_t count, loff_t *ppos)",
        "{",
        "	bool enable;",
        "	int ret;",
        "",
        "	ret = kstrtobool_from_user(user_buf, count, &enable);",
        "	if (ret)",
        "		return ret;",
        "",
        "	ret = enable ? arm_all_kprobes() : disarm_all_kprobes();",
        "	if (ret)",
        "		return ret;",
        "",
        "	return count;",
        "}",
        "",
        "static const struct file_operations fops_kp = {",
        "	.read =         read_enabled_file_bool,",
        "	.write =        write_enabled_file_bool,",
        "	.llseek =	default_llseek,",
        "};",
        "",
        "static int __init debugfs_kprobe_init(void)",
        "{",
        "	struct dentry *dir;",
        "",
        "	dir = debugfs_create_dir(\"kprobes\", NULL);",
        "",
        "	debugfs_create_file(\"list\", 0400, dir, NULL, &kprobes_fops);",
        "",
        "	debugfs_create_file(\"enabled\", 0600, dir, NULL, &fops_kp);",
        "",
        "	debugfs_create_file(\"blacklist\", 0400, dir, NULL,",
        "			    &kprobe_blacklist_fops);",
        "",
        "	return 0;",
        "}",
        "",
        "late_initcall(debugfs_kprobe_init);",
        "#endif /* CONFIG_DEBUG_FS */"
    ]
  },
  "fs_anon_inodes_c": {
    path: "fs/anon_inodes.c",
    covered: [87, 240, 269, 234, 268, 114],
    totalLines: 321,
    coveredCount: 6,
    coveragePct: 1.9,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " *  fs/anon_inodes.c",
        " *",
        " *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>",
        " *",
        " *  Thanks to Arnd Bergmann for code review and suggestions.",
        " *  More changes for Thomas Gleixner suggestions.",
        " *",
        " */",
        "",
        "#include <linux/cred.h>",
        "#include <linux/file.h>",
        "#include <linux/poll.h>",
        "#include <linux/sched.h>",
        "#include <linux/init.h>",
        "#include <linux/fs.h>",
        "#include <linux/mount.h>",
        "#include <linux/module.h>",
        "#include <linux/kernel.h>",
        "#include <linux/magic.h>",
        "#include <linux/anon_inodes.h>",
        "#include <linux/pseudo_fs.h>",
        "",
        "#include <linux/uaccess.h>",
        "",
        "static struct vfsmount *anon_inode_mnt __ro_after_init;",
        "static struct inode *anon_inode_inode __ro_after_init;",
        "",
        "/*",
        " * anon_inodefs_dname() is called from d_path().",
        " */",
        "static char *anon_inodefs_dname(struct dentry *dentry, char *buffer, int buflen)",
        "{",
        "	return dynamic_dname(buffer, buflen, \"anon_inode:%s\",",
        "				dentry->d_name.name);",
        "}",
        "",
        "static const struct dentry_operations anon_inodefs_dentry_operations = {",
        "	.d_dname	= anon_inodefs_dname,",
        "};",
        "",
        "static int anon_inodefs_init_fs_context(struct fs_context *fc)",
        "{",
        "	struct pseudo_fs_context *ctx = init_pseudo(fc, ANON_INODE_FS_MAGIC);",
        "	if (!ctx)",
        "		return -ENOMEM;",
        "	ctx->dops = &anon_inodefs_dentry_operations;",
        "	return 0;",
        "}",
        "",
        "static struct file_system_type anon_inode_fs_type = {",
        "	.name		= \"anon_inodefs\",",
        "	.init_fs_context = anon_inodefs_init_fs_context,",
        "	.kill_sb	= kill_anon_super,",
        "};",
        "",
        "static struct inode *anon_inode_make_secure_inode(",
        "	const char *name,",
        "	const struct inode *context_inode)",
        "{",
        "	struct inode *inode;",
        "	const struct qstr qname = QSTR_INIT(name, strlen(name));",
        "	int error;",
        "",
        "	inode = alloc_anon_inode(anon_inode_mnt->mnt_sb);",
        "	if (IS_ERR(inode))",
        "		return inode;",
        "	inode->i_flags &= ~S_PRIVATE;",
        "	error =	security_inode_init_security_anon(inode, &qname, context_inode);",
        "	if (error) {",
        "		iput(inode);",
        "		return ERR_PTR(error);",
        "	}",
        "	return inode;",
        "}",
        "",
        "static struct file *__anon_inode_getfile(const char *name,",
        "					 const struct file_operations *fops,",
        "					 void *priv, int flags,",
        "					 const struct inode *context_inode,",
        "					 bool make_inode)",
        "{",
        "	struct inode *inode;",
        "	struct file *file;",
        "",
        "	if (fops->owner && !try_module_get(fops->owner))",
        "		return ERR_PTR(-ENOENT);",
        "",
        "	if (make_inode) {",
        "		inode =	anon_inode_make_secure_inode(name, context_inode);",
        "		if (IS_ERR(inode)) {",
        "			file = ERR_CAST(inode);",
        "			goto err;",
        "		}",
        "	} else {",
        "		inode =	anon_inode_inode;",
        "		if (IS_ERR(inode)) {",
        "			file = ERR_PTR(-ENODEV);",
        "			goto err;",
        "		}",
        "		/*",
        "		 * We know the anon_inode inode count is always",
        "		 * greater than zero, so ihold() is safe.",
        "		 */",
        "		ihold(inode);",
        "	}",
        "",
        "	file = alloc_file_pseudo(inode, anon_inode_mnt, name,",
        "				 flags & (O_ACCMODE | O_NONBLOCK), fops);",
        "	if (IS_ERR(file))",
        "		goto err_iput;",
        "",
        "	file->f_mapping = inode->i_mapping;",
        "",
        "	file->private_data = priv;",
        "",
        "	return file;",
        "",
        "err_iput:",
        "	iput(inode);",
        "err:",
        "	module_put(fops->owner);",
        "	return file;",
        "}",
        "",
        "/**",
        " * anon_inode_getfile - creates a new file instance by hooking it up to an",
        " *                      anonymous inode, and a dentry that describe the \"class\"",
        " *                      of the file",
        " *",
        " * @name:    [in]    name of the \"class\" of the new file",
        " * @fops:    [in]    file operations for the new file",
        " * @priv:    [in]    private data for the new file (will be file's private_data)",
        " * @flags:   [in]    flags",
        " *",
        " * Creates a new file by hooking it on a single inode. This is useful for files",
        " * that do not need to have a full-fledged inode in order to operate correctly.",
        " * All the files created with anon_inode_getfile() will share a single inode,",
        " * hence saving memory and avoiding code duplication for the file/inode/dentry",
        " * setup.  Returns the newly created file* or an error pointer.",
        " */",
        "struct file *anon_inode_getfile(const char *name,",
        "				const struct file_operations *fops,",
        "				void *priv, int flags)",
        "{",
        "	return __anon_inode_getfile(name, fops, priv, flags, NULL, false);",
        "}",
        "EXPORT_SYMBOL_GPL(anon_inode_getfile);",
        "",
        "/**",
        " * anon_inode_getfile_fmode - creates a new file instance by hooking it up to an",
        " *                      anonymous inode, and a dentry that describe the \"class\"",
        " *                      of the file",
        " *",
        " * @name:    [in]    name of the \"class\" of the new file",
        " * @fops:    [in]    file operations for the new file",
        " * @priv:    [in]    private data for the new file (will be file's private_data)",
        " * @flags:   [in]    flags",
        " * @f_mode:  [in]    fmode",
        " *",
        " * Creates a new file by hooking it on a single inode. This is useful for files",
        " * that do not need to have a full-fledged inode in order to operate correctly.",
        " * All the files created with anon_inode_getfile() will share a single inode,",
        " * hence saving memory and avoiding code duplication for the file/inode/dentry",
        " * setup. Allows setting the fmode. Returns the newly created file* or an error",
        " * pointer.",
        " */",
        "struct file *anon_inode_getfile_fmode(const char *name,",
        "				const struct file_operations *fops,",
        "				void *priv, int flags, fmode_t f_mode)",
        "{",
        "	struct file *file;",
        "",
        "	file = __anon_inode_getfile(name, fops, priv, flags, NULL, false);",
        "	if (!IS_ERR(file))",
        "		file->f_mode |= f_mode;",
        "",
        "	return file;",
        "}",
        "EXPORT_SYMBOL_GPL(anon_inode_getfile_fmode);",
        "",
        "/**",
        " * anon_inode_create_getfile - Like anon_inode_getfile(), but creates a new",
        " *                             !S_PRIVATE anon inode rather than reuse the",
        " *                             singleton anon inode and calls the",
        " *                             inode_init_security_anon() LSM hook.",
        " *",
        " * @name:    [in]    name of the \"class\" of the new file",
        " * @fops:    [in]    file operations for the new file",
        " * @priv:    [in]    private data for the new file (will be file's private_data)",
        " * @flags:   [in]    flags",
        " * @context_inode:",
        " *           [in]    the logical relationship with the new inode (optional)",
        " *",
        " * Create a new anonymous inode and file pair.  This can be done for two",
        " * reasons:",
        " *",
        " * - for the inode to have its own security context, so that LSMs can enforce",
        " *   policy on the inode's creation;",
        " *",
        " * - if the caller needs a unique inode, for example in order to customize",
        " *   the size returned by fstat()",
        " *",
        " * The LSM may use @context_inode in inode_init_security_anon(), but a",
        " * reference to it is not held.",
        " *",
        " * Returns the newly created file* or an error pointer.",
        " */",
        "struct file *anon_inode_create_getfile(const char *name,",
        "				       const struct file_operations *fops,",
        "				       void *priv, int flags,",
        "				       const struct inode *context_inode)",
        "{",
        "	return __anon_inode_getfile(name, fops, priv, flags,",
        "				    context_inode, true);",
        "}",
        "EXPORT_SYMBOL_GPL(anon_inode_create_getfile);",
        "",
        "static int __anon_inode_getfd(const char *name,",
        "			      const struct file_operations *fops,",
        "			      void *priv, int flags,",
        "			      const struct inode *context_inode,",
        "			      bool make_inode)",
        "{",
        "	int error, fd;",
        "	struct file *file;",
        "",
        "	error = get_unused_fd_flags(flags);",
        "	if (error < 0)",
        "		return error;",
        "	fd = error;",
        "",
        "	file = __anon_inode_getfile(name, fops, priv, flags, context_inode,",
        "				    make_inode);",
        "	if (IS_ERR(file)) {",
        "		error = PTR_ERR(file);",
        "		goto err_put_unused_fd;",
        "	}",
        "	fd_install(fd, file);",
        "",
        "	return fd;",
        "",
        "err_put_unused_fd:",
        "	put_unused_fd(fd);",
        "	return error;",
        "}",
        "",
        "/**",
        " * anon_inode_getfd - creates a new file instance by hooking it up to",
        " *                    an anonymous inode and a dentry that describe",
        " *                    the \"class\" of the file",
        " *",
        " * @name:    [in]    name of the \"class\" of the new file",
        " * @fops:    [in]    file operations for the new file",
        " * @priv:    [in]    private data for the new file (will be file's private_data)",
        " * @flags:   [in]    flags",
        " *",
        " * Creates a new file by hooking it on a single inode. This is",
        " * useful for files that do not need to have a full-fledged inode in",
        " * order to operate correctly.  All the files created with",
        " * anon_inode_getfd() will use the same singleton inode, reducing",
        " * memory use and avoiding code duplication for the file/inode/dentry",
        " * setup.  Returns a newly created file descriptor or an error code.",
        " */",
        "int anon_inode_getfd(const char *name, const struct file_operations *fops,",
        "		     void *priv, int flags)",
        "{",
        "	return __anon_inode_getfd(name, fops, priv, flags, NULL, false);",
        "}",
        "EXPORT_SYMBOL_GPL(anon_inode_getfd);",
        "",
        "/**",
        " * anon_inode_create_getfd - Like anon_inode_getfd(), but creates a new",
        " * !S_PRIVATE anon inode rather than reuse the singleton anon inode, and calls",
        " * the inode_init_security_anon() LSM hook.",
        " *",
        " * @name:    [in]    name of the \"class\" of the new file",
        " * @fops:    [in]    file operations for the new file",
        " * @priv:    [in]    private data for the new file (will be file's private_data)",
        " * @flags:   [in]    flags",
        " * @context_inode:",
        " *           [in]    the logical relationship with the new inode (optional)",
        " *",
        " * Create a new anonymous inode and file pair.  This can be done for two",
        " * reasons:",
        " *",
        " * - for the inode to have its own security context, so that LSMs can enforce",
        " *   policy on the inode's creation;",
        " *",
        " * - if the caller needs a unique inode, for example in order to customize",
        " *   the size returned by fstat()",
        " *",
        " * The LSM may use @context_inode in inode_init_security_anon(), but a",
        " * reference to it is not held.",
        " *",
        " * Returns a newly created file descriptor or an error code.",
        " */",
        "int anon_inode_create_getfd(const char *name, const struct file_operations *fops,",
        "			    void *priv, int flags,",
        "			    const struct inode *context_inode)",
        "{",
        "	return __anon_inode_getfd(name, fops, priv, flags, context_inode, true);",
        "}",
        "",
        "",
        "static int __init anon_inode_init(void)",
        "{",
        "	anon_inode_mnt = kern_mount(&anon_inode_fs_type);",
        "	if (IS_ERR(anon_inode_mnt))",
        "		panic(\"anon_inode_init() kernel mount failed (%ld)\\n\", PTR_ERR(anon_inode_mnt));",
        "",
        "	anon_inode_inode = alloc_anon_inode(anon_inode_mnt->mnt_sb);",
        "	if (IS_ERR(anon_inode_inode))",
        "		panic(\"anon_inode_init() inode allocation failed (%ld)\\n\", PTR_ERR(anon_inode_inode));",
        "",
        "	return 0;",
        "}",
        "",
        "fs_initcall(anon_inode_init);",
        ""
    ]
  },
  "arch_x86_include_asm_pgtable_h": {
    path: "arch/x86/include/asm/pgtable.h",
    covered: [769, 1224, 762, 1681, 1213, 1204, 965, 1025, 789, 162, 1185, 1677, 218],
    totalLines: 1817,
    coveredCount: 13,
    coveragePct: 0.7,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _ASM_X86_PGTABLE_H",
        "#define _ASM_X86_PGTABLE_H",
        "",
        "#include <linux/mem_encrypt.h>",
        "#include <asm/page.h>",
        "#include <asm/pgtable_types.h>",
        "",
        "/*",
        " * Macro to mark a page protection value as UC-",
        " */",
        "#define pgprot_noncached(prot)						\\",
        "	((boot_cpu_data.x86 > 3)					\\",
        "	 ? (__pgprot(pgprot_val(prot) |					\\",
        "		     cachemode2protval(_PAGE_CACHE_MODE_UC_MINUS)))	\\",
        "	 : (prot))",
        "",
        "#ifndef __ASSEMBLY__",
        "#include <linux/spinlock.h>",
        "#include <asm/x86_init.h>",
        "#include <asm/pkru.h>",
        "#include <asm/fpu/api.h>",
        "#include <asm/coco.h>",
        "#include <asm-generic/pgtable_uffd.h>",
        "#include <linux/page_table_check.h>",
        "",
        "extern pgd_t early_top_pgt[PTRS_PER_PGD];",
        "bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);",
        "",
        "struct seq_file;",
        "void ptdump_walk_pgd_level(struct seq_file *m, struct mm_struct *mm);",
        "void ptdump_walk_pgd_level_debugfs(struct seq_file *m, struct mm_struct *mm,",
        "				   bool user);",
        "bool ptdump_walk_pgd_level_checkwx(void);",
        "#define ptdump_check_wx ptdump_walk_pgd_level_checkwx",
        "void ptdump_walk_user_pgd_level_checkwx(void);",
        "",
        "/*",
        " * Macros to add or remove encryption attribute",
        " */",
        "#define pgprot_encrypted(prot)	__pgprot(cc_mkenc(pgprot_val(prot)))",
        "#define pgprot_decrypted(prot)	__pgprot(cc_mkdec(pgprot_val(prot)))",
        "",
        "#ifdef CONFIG_DEBUG_WX",
        "#define debug_checkwx_user()	ptdump_walk_user_pgd_level_checkwx()",
        "#else",
        "#define debug_checkwx_user()	do { } while (0)",
        "#endif",
        "",
        "/*",
        " * ZERO_PAGE is a global shared page that is always zero: used",
        " * for zero-mapped memory areas etc..",
        " */",
        "extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]",
        "	__visible;",
        "#define ZERO_PAGE(vaddr) ((void)(vaddr),virt_to_page(empty_zero_page))",
        "",
        "extern spinlock_t pgd_lock;",
        "extern struct list_head pgd_list;",
        "",
        "extern struct mm_struct *pgd_page_get_mm(struct page *page);",
        "",
        "extern pmdval_t early_pmd_flags;",
        "",
        "#ifdef CONFIG_PARAVIRT_XXL",
        "#include <asm/paravirt.h>",
        "#else  /* !CONFIG_PARAVIRT_XXL */",
        "#define set_pte(ptep, pte)		native_set_pte(ptep, pte)",
        "",
        "#define set_pte_atomic(ptep, pte)					\\",
        "	native_set_pte_atomic(ptep, pte)",
        "",
        "#define set_pmd(pmdp, pmd)		native_set_pmd(pmdp, pmd)",
        "",
        "#ifndef __PAGETABLE_P4D_FOLDED",
        "#define set_pgd(pgdp, pgd)		native_set_pgd(pgdp, pgd)",
        "#define pgd_clear(pgd)			(pgtable_l5_enabled() ? native_pgd_clear(pgd) : 0)",
        "#endif",
        "",
        "#ifndef set_p4d",
        "# define set_p4d(p4dp, p4d)		native_set_p4d(p4dp, p4d)",
        "#endif",
        "",
        "#ifndef __PAGETABLE_PUD_FOLDED",
        "#define p4d_clear(p4d)			native_p4d_clear(p4d)",
        "#endif",
        "",
        "#ifndef set_pud",
        "# define set_pud(pudp, pud)		native_set_pud(pudp, pud)",
        "#endif",
        "",
        "#ifndef __PAGETABLE_PUD_FOLDED",
        "#define pud_clear(pud)			native_pud_clear(pud)",
        "#endif",
        "",
        "#define pte_clear(mm, addr, ptep)	native_pte_clear(mm, addr, ptep)",
        "#define pmd_clear(pmd)			native_pmd_clear(pmd)",
        "",
        "#define pgd_val(x)	native_pgd_val(x)",
        "#define __pgd(x)	native_make_pgd(x)",
        "",
        "#ifndef __PAGETABLE_P4D_FOLDED",
        "#define p4d_val(x)	native_p4d_val(x)",
        "#define __p4d(x)	native_make_p4d(x)",
        "#endif",
        "",
        "#ifndef __PAGETABLE_PUD_FOLDED",
        "#define pud_val(x)	native_pud_val(x)",
        "#define __pud(x)	native_make_pud(x)",
        "#endif",
        "",
        "#ifndef __PAGETABLE_PMD_FOLDED",
        "#define pmd_val(x)	native_pmd_val(x)",
        "#define __pmd(x)	native_make_pmd(x)",
        "#endif",
        "",
        "#define pte_val(x)	native_pte_val(x)",
        "#define __pte(x)	native_make_pte(x)",
        "",
        "#define arch_end_context_switch(prev)	do {} while(0)",
        "#endif	/* CONFIG_PARAVIRT_XXL */",
        "",
        "static inline pmd_t pmd_set_flags(pmd_t pmd, pmdval_t set)",
        "{",
        "	pmdval_t v = native_pmd_val(pmd);",
        "",
        "	return native_make_pmd(v | set);",
        "}",
        "",
        "static inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)",
        "{",
        "	pmdval_t v = native_pmd_val(pmd);",
        "",
        "	return native_make_pmd(v & ~clear);",
        "}",
        "",
        "static inline pud_t pud_set_flags(pud_t pud, pudval_t set)",
        "{",
        "	pudval_t v = native_pud_val(pud);",
        "",
        "	return native_make_pud(v | set);",
        "}",
        "",
        "static inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)",
        "{",
        "	pudval_t v = native_pud_val(pud);",
        "",
        "	return native_make_pud(v & ~clear);",
        "}",
        "",
        "/*",
        " * The following only work if pte_present() is true.",
        " * Undefined behaviour if not..",
        " */",
        "static inline bool pte_dirty(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_DIRTY_BITS;",
        "}",
        "",
        "static inline bool pte_shstk(pte_t pte)",
        "{",
        "	return cpu_feature_enabled(X86_FEATURE_SHSTK) &&",
        "	       (pte_flags(pte) & (_PAGE_RW | _PAGE_DIRTY)) == _PAGE_DIRTY;",
        "}",
        "",
        "static inline int pte_young(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_ACCESSED;",
        "}",
        "",
        "static inline bool pte_decrypted(pte_t pte)",
        "{",
        "	return cc_mkdec(pte_val(pte)) == pte_val(pte);",
        "}",
        "",
        "#define pmd_dirty pmd_dirty",
        "static inline bool pmd_dirty(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_DIRTY_BITS;",
        "}",
        "",
        "static inline bool pmd_shstk(pmd_t pmd)",
        "{",
        "	return cpu_feature_enabled(X86_FEATURE_SHSTK) &&",
        "	       (pmd_flags(pmd) & (_PAGE_RW | _PAGE_DIRTY | _PAGE_PSE)) ==",
        "	       (_PAGE_DIRTY | _PAGE_PSE);",
        "}",
        "",
        "#define pmd_young pmd_young",
        "static inline int pmd_young(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_ACCESSED;",
        "}",
        "",
        "static inline bool pud_dirty(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_DIRTY_BITS;",
        "}",
        "",
        "static inline int pud_young(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_ACCESSED;",
        "}",
        "",
        "static inline bool pud_shstk(pud_t pud)",
        "{",
        "	return cpu_feature_enabled(X86_FEATURE_SHSTK) &&",
        "	       (pud_flags(pud) & (_PAGE_RW | _PAGE_DIRTY | _PAGE_PSE)) ==",
        "	       (_PAGE_DIRTY | _PAGE_PSE);",
        "}",
        "",
        "static inline int pte_write(pte_t pte)",
        "{",
        "	/*",
        "	 * Shadow stack pages are logically writable, but do not have",
        "	 * _PAGE_RW.  Check for them separately from _PAGE_RW itself.",
        "	 */",
        "	return (pte_flags(pte) & _PAGE_RW) || pte_shstk(pte);",
        "}",
        "",
        "#define pmd_write pmd_write",
        "static inline int pmd_write(pmd_t pmd)",
        "{",
        "	/*",
        "	 * Shadow stack pages are logically writable, but do not have",
        "	 * _PAGE_RW.  Check for them separately from _PAGE_RW itself.",
        "	 */",
        "	return (pmd_flags(pmd) & _PAGE_RW) || pmd_shstk(pmd);",
        "}",
        "",
        "#define pud_write pud_write",
        "static inline int pud_write(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_RW;",
        "}",
        "",
        "static inline int pte_huge(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_PSE;",
        "}",
        "",
        "static inline int pte_global(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_GLOBAL;",
        "}",
        "",
        "static inline int pte_exec(pte_t pte)",
        "{",
        "	return !(pte_flags(pte) & _PAGE_NX);",
        "}",
        "",
        "static inline int pte_special(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_SPECIAL;",
        "}",
        "",
        "/* Entries that were set to PROT_NONE are inverted */",
        "",
        "static inline u64 protnone_mask(u64 val);",
        "",
        "#define PFN_PTE_SHIFT	PAGE_SHIFT",
        "",
        "static inline unsigned long pte_pfn(pte_t pte)",
        "{",
        "	phys_addr_t pfn = pte_val(pte);",
        "	pfn ^= protnone_mask(pfn);",
        "	return (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;",
        "}",
        "",
        "static inline unsigned long pmd_pfn(pmd_t pmd)",
        "{",
        "	phys_addr_t pfn = pmd_val(pmd);",
        "	pfn ^= protnone_mask(pfn);",
        "	return (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;",
        "}",
        "",
        "#define pud_pfn pud_pfn",
        "static inline unsigned long pud_pfn(pud_t pud)",
        "{",
        "	phys_addr_t pfn = pud_val(pud);",
        "	pfn ^= protnone_mask(pfn);",
        "	return (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;",
        "}",
        "",
        "static inline unsigned long p4d_pfn(p4d_t p4d)",
        "{",
        "	return (p4d_val(p4d) & p4d_pfn_mask(p4d)) >> PAGE_SHIFT;",
        "}",
        "",
        "static inline unsigned long pgd_pfn(pgd_t pgd)",
        "{",
        "	return (pgd_val(pgd) & PTE_PFN_MASK) >> PAGE_SHIFT;",
        "}",
        "",
        "#define p4d_leaf p4d_leaf",
        "static inline bool p4d_leaf(p4d_t p4d)",
        "{",
        "	/* No 512 GiB pages yet */",
        "	return 0;",
        "}",
        "",
        "#define pte_page(pte)	pfn_to_page(pte_pfn(pte))",
        "",
        "#define pmd_leaf pmd_leaf",
        "static inline bool pmd_leaf(pmd_t pte)",
        "{",
        "	return pmd_flags(pte) & _PAGE_PSE;",
        "}",
        "",
        "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
        "/* NOTE: when predicate huge page, consider also pmd_devmap, or use pmd_leaf */",
        "static inline int pmd_trans_huge(pmd_t pmd)",
        "{",
        "	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD",
        "static inline int pud_trans_huge(pud_t pud)",
        "{",
        "	return (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;",
        "}",
        "#endif",
        "",
        "#define has_transparent_hugepage has_transparent_hugepage",
        "static inline int has_transparent_hugepage(void)",
        "{",
        "	return boot_cpu_has(X86_FEATURE_PSE);",
        "}",
        "",
        "#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP",
        "static inline int pmd_devmap(pmd_t pmd)",
        "{",
        "	return !!(pmd_val(pmd) & _PAGE_DEVMAP);",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD",
        "static inline int pud_devmap(pud_t pud)",
        "{",
        "	return !!(pud_val(pud) & _PAGE_DEVMAP);",
        "}",
        "#else",
        "static inline int pud_devmap(pud_t pud)",
        "{",
        "	return 0;",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_ARCH_SUPPORTS_PMD_PFNMAP",
        "static inline bool pmd_special(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_SPECIAL;",
        "}",
        "",
        "static inline pmd_t pmd_mkspecial(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_SPECIAL);",
        "}",
        "#endif	/* CONFIG_ARCH_SUPPORTS_PMD_PFNMAP */",
        "",
        "#ifdef CONFIG_ARCH_SUPPORTS_PUD_PFNMAP",
        "static inline bool pud_special(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_SPECIAL;",
        "}",
        "",
        "static inline pud_t pud_mkspecial(pud_t pud)",
        "{",
        "	return pud_set_flags(pud, _PAGE_SPECIAL);",
        "}",
        "#endif	/* CONFIG_ARCH_SUPPORTS_PUD_PFNMAP */",
        "",
        "static inline int pgd_devmap(pgd_t pgd)",
        "{",
        "	return 0;",
        "}",
        "#endif",
        "#endif /* CONFIG_TRANSPARENT_HUGEPAGE */",
        "",
        "static inline pte_t pte_set_flags(pte_t pte, pteval_t set)",
        "{",
        "	pteval_t v = native_pte_val(pte);",
        "",
        "	return native_make_pte(v | set);",
        "}",
        "",
        "static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)",
        "{",
        "	pteval_t v = native_pte_val(pte);",
        "",
        "	return native_make_pte(v & ~clear);",
        "}",
        "",
        "/*",
        " * Write protection operations can result in Dirty=1,Write=0 PTEs. But in the",
        " * case of X86_FEATURE_USER_SHSTK, these PTEs denote shadow stack memory. So",
        " * when creating dirty, write-protected memory, a software bit is used:",
        " * _PAGE_BIT_SAVED_DIRTY. The following functions take a PTE and transition the",
        " * Dirty bit to SavedDirty, and vice-vesra.",
        " *",
        " * This shifting is only done if needed. In the case of shifting",
        " * Dirty->SavedDirty, the condition is if the PTE is Write=0. In the case of",
        " * shifting SavedDirty->Dirty, the condition is Write=1.",
        " */",
        "static inline pgprotval_t mksaveddirty_shift(pgprotval_t v)",
        "{",
        "	pgprotval_t cond = (~v >> _PAGE_BIT_RW) & 1;",
        "",
        "	v |= ((v >> _PAGE_BIT_DIRTY) & cond) << _PAGE_BIT_SAVED_DIRTY;",
        "	v &= ~(cond << _PAGE_BIT_DIRTY);",
        "",
        "	return v;",
        "}",
        "",
        "static inline pgprotval_t clear_saveddirty_shift(pgprotval_t v)",
        "{",
        "	pgprotval_t cond = (v >> _PAGE_BIT_RW) & 1;",
        "",
        "	v |= ((v >> _PAGE_BIT_SAVED_DIRTY) & cond) << _PAGE_BIT_DIRTY;",
        "	v &= ~(cond << _PAGE_BIT_SAVED_DIRTY);",
        "",
        "	return v;",
        "}",
        "",
        "static inline pte_t pte_mksaveddirty(pte_t pte)",
        "{",
        "	pteval_t v = native_pte_val(pte);",
        "",
        "	v = mksaveddirty_shift(v);",
        "	return native_make_pte(v);",
        "}",
        "",
        "static inline pte_t pte_clear_saveddirty(pte_t pte)",
        "{",
        "	pteval_t v = native_pte_val(pte);",
        "",
        "	v = clear_saveddirty_shift(v);",
        "	return native_make_pte(v);",
        "}",
        "",
        "static inline pte_t pte_wrprotect(pte_t pte)",
        "{",
        "	pte = pte_clear_flags(pte, _PAGE_RW);",
        "",
        "	/*",
        "	 * Blindly clearing _PAGE_RW might accidentally create",
        "	 * a shadow stack PTE (Write=0,Dirty=1). Move the hardware",
        "	 * dirty value to the software bit, if present.",
        "	 */",
        "	return pte_mksaveddirty(pte);",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP",
        "static inline int pte_uffd_wp(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_UFFD_WP;",
        "}",
        "",
        "static inline pte_t pte_mkuffd_wp(pte_t pte)",
        "{",
        "	return pte_wrprotect(pte_set_flags(pte, _PAGE_UFFD_WP));",
        "}",
        "",
        "static inline pte_t pte_clear_uffd_wp(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_UFFD_WP);",
        "}",
        "#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */",
        "",
        "static inline pte_t pte_mkclean(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_DIRTY_BITS);",
        "}",
        "",
        "static inline pte_t pte_mkold(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pte_t pte_mkexec(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_NX);",
        "}",
        "",
        "static inline pte_t pte_mkdirty(pte_t pte)",
        "{",
        "	pte = pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);",
        "",
        "	return pte_mksaveddirty(pte);",
        "}",
        "",
        "static inline pte_t pte_mkwrite_shstk(pte_t pte)",
        "{",
        "	pte = pte_clear_flags(pte, _PAGE_RW);",
        "",
        "	return pte_set_flags(pte, _PAGE_DIRTY);",
        "}",
        "",
        "static inline pte_t pte_mkyoung(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pte_t pte_mkwrite_novma(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_RW);",
        "}",
        "",
        "struct vm_area_struct;",
        "pte_t pte_mkwrite(pte_t pte, struct vm_area_struct *vma);",
        "#define pte_mkwrite pte_mkwrite",
        "",
        "static inline pte_t pte_mkhuge(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_PSE);",
        "}",
        "",
        "static inline pte_t pte_clrhuge(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_PSE);",
        "}",
        "",
        "static inline pte_t pte_mkglobal(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_GLOBAL);",
        "}",
        "",
        "static inline pte_t pte_clrglobal(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_GLOBAL);",
        "}",
        "",
        "static inline pte_t pte_mkspecial(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SPECIAL);",
        "}",
        "",
        "static inline pte_t pte_mkdevmap(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SPECIAL|_PAGE_DEVMAP);",
        "}",
        "",
        "/* See comments above mksaveddirty_shift() */",
        "static inline pmd_t pmd_mksaveddirty(pmd_t pmd)",
        "{",
        "	pmdval_t v = native_pmd_val(pmd);",
        "",
        "	v = mksaveddirty_shift(v);",
        "	return native_make_pmd(v);",
        "}",
        "",
        "/* See comments above mksaveddirty_shift() */",
        "static inline pmd_t pmd_clear_saveddirty(pmd_t pmd)",
        "{",
        "	pmdval_t v = native_pmd_val(pmd);",
        "",
        "	v = clear_saveddirty_shift(v);",
        "	return native_make_pmd(v);",
        "}",
        "",
        "static inline pmd_t pmd_wrprotect(pmd_t pmd)",
        "{",
        "	pmd = pmd_clear_flags(pmd, _PAGE_RW);",
        "",
        "	/*",
        "	 * Blindly clearing _PAGE_RW might accidentally create",
        "	 * a shadow stack PMD (RW=0, Dirty=1). Move the hardware",
        "	 * dirty value to the software bit.",
        "	 */",
        "	return pmd_mksaveddirty(pmd);",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP",
        "static inline int pmd_uffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_UFFD_WP;",
        "}",
        "",
        "static inline pmd_t pmd_mkuffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_wrprotect(pmd_set_flags(pmd, _PAGE_UFFD_WP));",
        "}",
        "",
        "static inline pmd_t pmd_clear_uffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_UFFD_WP);",
        "}",
        "#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */",
        "",
        "static inline pmd_t pmd_mkold(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pmd_t pmd_mkclean(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_DIRTY_BITS);",
        "}",
        "",
        "static inline pmd_t pmd_mkdirty(pmd_t pmd)",
        "{",
        "	pmd = pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);",
        "",
        "	return pmd_mksaveddirty(pmd);",
        "}",
        "",
        "static inline pmd_t pmd_mkwrite_shstk(pmd_t pmd)",
        "{",
        "	pmd = pmd_clear_flags(pmd, _PAGE_RW);",
        "",
        "	return pmd_set_flags(pmd, _PAGE_DIRTY);",
        "}",
        "",
        "static inline pmd_t pmd_mkdevmap(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_DEVMAP);",
        "}",
        "",
        "static inline pmd_t pmd_mkhuge(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_PSE);",
        "}",
        "",
        "static inline pmd_t pmd_mkyoung(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pmd_t pmd_mkwrite_novma(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_RW);",
        "}",
        "",
        "pmd_t pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);",
        "#define pmd_mkwrite pmd_mkwrite",
        "",
        "/* See comments above mksaveddirty_shift() */",
        "static inline pud_t pud_mksaveddirty(pud_t pud)",
        "{",
        "	pudval_t v = native_pud_val(pud);",
        "",
        "	v = mksaveddirty_shift(v);",
        "	return native_make_pud(v);",
        "}",
        "",
        "/* See comments above mksaveddirty_shift() */",
        "static inline pud_t pud_clear_saveddirty(pud_t pud)",
        "{",
        "	pudval_t v = native_pud_val(pud);",
        "",
        "	v = clear_saveddirty_shift(v);",
        "	return native_make_pud(v);",
        "}",
        "",
        "static inline pud_t pud_mkold(pud_t pud)",
        "{",
        "	return pud_clear_flags(pud, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pud_t pud_mkclean(pud_t pud)",
        "{",
        "	return pud_clear_flags(pud, _PAGE_DIRTY_BITS);",
        "}",
        "",
        "static inline pud_t pud_wrprotect(pud_t pud)",
        "{",
        "	pud = pud_clear_flags(pud, _PAGE_RW);",
        "",
        "	/*",
        "	 * Blindly clearing _PAGE_RW might accidentally create",
        "	 * a shadow stack PUD (RW=0, Dirty=1). Move the hardware",
        "	 * dirty value to the software bit.",
        "	 */",
        "	return pud_mksaveddirty(pud);",
        "}",
        "",
        "static inline pud_t pud_mkdirty(pud_t pud)",
        "{",
        "	pud = pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);",
        "",
        "	return pud_mksaveddirty(pud);",
        "}",
        "",
        "static inline pud_t pud_mkdevmap(pud_t pud)",
        "{",
        "	return pud_set_flags(pud, _PAGE_DEVMAP);",
        "}",
        "",
        "static inline pud_t pud_mkhuge(pud_t pud)",
        "{",
        "	return pud_set_flags(pud, _PAGE_PSE);",
        "}",
        "",
        "static inline pud_t pud_mkyoung(pud_t pud)",
        "{",
        "	return pud_set_flags(pud, _PAGE_ACCESSED);",
        "}",
        "",
        "static inline pud_t pud_mkwrite(pud_t pud)",
        "{",
        "	pud = pud_set_flags(pud, _PAGE_RW);",
        "",
        "	return pud_clear_saveddirty(pud);",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY",
        "static inline int pte_soft_dirty(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_SOFT_DIRTY;",
        "}",
        "",
        "static inline int pmd_soft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_SOFT_DIRTY;",
        "}",
        "",
        "static inline int pud_soft_dirty(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_SOFT_DIRTY;",
        "}",
        "",
        "static inline pte_t pte_mksoft_dirty(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "static inline pud_t pud_mksoft_dirty(pud_t pud)",
        "{",
        "	return pud_set_flags(pud, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "static inline pte_t pte_clear_soft_dirty(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "static inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "static inline pud_t pud_clear_soft_dirty(pud_t pud)",
        "{",
        "	return pud_clear_flags(pud, _PAGE_SOFT_DIRTY);",
        "}",
        "",
        "#endif /* CONFIG_HAVE_ARCH_SOFT_DIRTY */",
        "",
        "/*",
        " * Mask out unsupported bits in a present pgprot.  Non-present pgprots",
        " * can use those bits for other purposes, so leave them be.",
        " */",
        "static inline pgprotval_t massage_pgprot(pgprot_t pgprot)",
        "{",
        "	pgprotval_t protval = pgprot_val(pgprot);",
        "",
        "	if (protval & _PAGE_PRESENT)",
        "		protval &= __supported_pte_mask;",
        "",
        "	return protval;",
        "}",
        "",
        "static inline pgprotval_t check_pgprot(pgprot_t pgprot)",
        "{",
        "	pgprotval_t massaged_val = massage_pgprot(pgprot);",
        "",
        "	/* mmdebug.h can not be included here because of dependencies */",
        "#ifdef CONFIG_DEBUG_VM",
        "	WARN_ONCE(pgprot_val(pgprot) != massaged_val,",
        "		  \"attempted to set unsupported pgprot: %016llx \"",
        "		  \"bits: %016llx supported: %016llx\\n\",",
        "		  (u64)pgprot_val(pgprot),",
        "		  (u64)pgprot_val(pgprot) ^ massaged_val,",
        "		  (u64)__supported_pte_mask);",
        "#endif",
        "",
        "	return massaged_val;",
        "}",
        "",
        "static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)",
        "{",
        "	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;",
        "	pfn ^= protnone_mask(pgprot_val(pgprot));",
        "	pfn &= PTE_PFN_MASK;",
        "	return __pte(pfn | check_pgprot(pgprot));",
        "}",
        "",
        "static inline pmd_t pfn_pmd(unsigned long page_nr, pgprot_t pgprot)",
        "{",
        "	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;",
        "	pfn ^= protnone_mask(pgprot_val(pgprot));",
        "	pfn &= PHYSICAL_PMD_PAGE_MASK;",
        "	return __pmd(pfn | check_pgprot(pgprot));",
        "}",
        "",
        "static inline pud_t pfn_pud(unsigned long page_nr, pgprot_t pgprot)",
        "{",
        "	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;",
        "	pfn ^= protnone_mask(pgprot_val(pgprot));",
        "	pfn &= PHYSICAL_PUD_PAGE_MASK;",
        "	return __pud(pfn | check_pgprot(pgprot));",
        "}",
        "",
        "static inline pmd_t pmd_mkinvalid(pmd_t pmd)",
        "{",
        "	return pfn_pmd(pmd_pfn(pmd),",
        "		      __pgprot(pmd_flags(pmd) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));",
        "}",
        "",
        "static inline pud_t pud_mkinvalid(pud_t pud)",
        "{",
        "	return pfn_pud(pud_pfn(pud),",
        "		       __pgprot(pud_flags(pud) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));",
        "}",
        "",
        "static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);",
        "",
        "static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)",
        "{",
        "	pteval_t val = pte_val(pte), oldval = val;",
        "	pte_t pte_result;",
        "",
        "	/*",
        "	 * Chop off the NX bit (if present), and add the NX portion of",
        "	 * the newprot (if present):",
        "	 */",
        "	val &= _PAGE_CHG_MASK;",
        "	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;",
        "	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);",
        "",
        "	pte_result = __pte(val);",
        "",
        "	/*",
        "	 * To avoid creating Write=0,Dirty=1 PTEs, pte_modify() needs to avoid:",
        "	 *  1. Marking Write=0 PTEs Dirty=1",
        "	 *  2. Marking Dirty=1 PTEs Write=0",
        "	 *",
        "	 * The first case cannot happen because the _PAGE_CHG_MASK will filter",
        "	 * out any Dirty bit passed in newprot. Handle the second case by",
        "	 * going through the mksaveddirty exercise. Only do this if the old",
        "	 * value was Write=1 to avoid doing this on Shadow Stack PTEs.",
        "	 */",
        "	if (oldval & _PAGE_RW)",
        "		pte_result = pte_mksaveddirty(pte_result);",
        "	else",
        "		pte_result = pte_clear_saveddirty(pte_result);",
        "",
        "	return pte_result;",
        "}",
        "",
        "static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)",
        "{",
        "	pmdval_t val = pmd_val(pmd), oldval = val;",
        "	pmd_t pmd_result;",
        "",
        "	val &= (_HPAGE_CHG_MASK & ~_PAGE_DIRTY);",
        "	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;",
        "	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);",
        "",
        "	pmd_result = __pmd(val);",
        "",
        "	/*",
        "	 * Avoid creating shadow stack PMD by accident.  See comment in",
        "	 * pte_modify().",
        "	 */",
        "	if (oldval & _PAGE_RW)",
        "		pmd_result = pmd_mksaveddirty(pmd_result);",
        "	else",
        "		pmd_result = pmd_clear_saveddirty(pmd_result);",
        "",
        "	return pmd_result;",
        "}",
        "",
        "static inline pud_t pud_modify(pud_t pud, pgprot_t newprot)",
        "{",
        "	pudval_t val = pud_val(pud), oldval = val;",
        "	pud_t pud_result;",
        "",
        "	val &= _HPAGE_CHG_MASK;",
        "	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;",
        "	val = flip_protnone_guard(oldval, val, PHYSICAL_PUD_PAGE_MASK);",
        "",
        "	pud_result = __pud(val);",
        "",
        "	/*",
        "	 * Avoid creating shadow stack PUD by accident.  See comment in",
        "	 * pte_modify().",
        "	 */",
        "	if (oldval & _PAGE_RW)",
        "		pud_result = pud_mksaveddirty(pud_result);",
        "	else",
        "		pud_result = pud_clear_saveddirty(pud_result);",
        "",
        "	return pud_result;",
        "}",
        "",
        "/*",
        " * mprotect needs to preserve PAT and encryption bits when updating",
        " * vm_page_prot",
        " */",
        "#define pgprot_modify pgprot_modify",
        "static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)",
        "{",
        "	pgprotval_t preservebits = pgprot_val(oldprot) & _PAGE_CHG_MASK;",
        "	pgprotval_t addbits = pgprot_val(newprot) & ~_PAGE_CHG_MASK;",
        "	return __pgprot(preservebits | addbits);",
        "}",
        "",
        "#define pte_pgprot(x) __pgprot(pte_flags(x))",
        "#define pmd_pgprot(x) __pgprot(pmd_flags(x))",
        "#define pud_pgprot(x) __pgprot(pud_flags(x))",
        "#define p4d_pgprot(x) __pgprot(p4d_flags(x))",
        "",
        "#define canon_pgprot(p) __pgprot(massage_pgprot(p))",
        "",
        "static inline int is_new_memtype_allowed(u64 paddr, unsigned long size,",
        "					 enum page_cache_mode pcm,",
        "					 enum page_cache_mode new_pcm)",
        "{",
        "	/*",
        "	 * PAT type is always WB for untracked ranges, so no need to check.",
        "	 */",
        "	if (x86_platform.is_untracked_pat_range(paddr, paddr + size))",
        "		return 1;",
        "",
        "	/*",
        "	 * Certain new memtypes are not allowed with certain",
        "	 * requested memtype:",
        "	 * - request is uncached, return cannot be write-back",
        "	 * - request is write-combine, return cannot be write-back",
        "	 * - request is write-through, return cannot be write-back",
        "	 * - request is write-through, return cannot be write-combine",
        "	 */",
        "	if ((pcm == _PAGE_CACHE_MODE_UC_MINUS &&",
        "	     new_pcm == _PAGE_CACHE_MODE_WB) ||",
        "	    (pcm == _PAGE_CACHE_MODE_WC &&",
        "	     new_pcm == _PAGE_CACHE_MODE_WB) ||",
        "	    (pcm == _PAGE_CACHE_MODE_WT &&",
        "	     new_pcm == _PAGE_CACHE_MODE_WB) ||",
        "	    (pcm == _PAGE_CACHE_MODE_WT &&",
        "	     new_pcm == _PAGE_CACHE_MODE_WC)) {",
        "		return 0;",
        "	}",
        "",
        "	return 1;",
        "}",
        "",
        "pmd_t *populate_extra_pmd(unsigned long vaddr);",
        "pte_t *populate_extra_pte(unsigned long vaddr);",
        "",
        "#ifdef CONFIG_MITIGATION_PAGE_TABLE_ISOLATION",
        "pgd_t __pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd);",
        "",
        "/*",
        " * Take a PGD location (pgdp) and a pgd value that needs to be set there.",
        " * Populates the user and returns the resulting PGD that must be set in",
        " * the kernel copy of the page tables.",
        " */",
        "static inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)",
        "{",
        "	if (!static_cpu_has(X86_FEATURE_PTI))",
        "		return pgd;",
        "	return __pti_set_user_pgtbl(pgdp, pgd);",
        "}",
        "#else   /* CONFIG_MITIGATION_PAGE_TABLE_ISOLATION */",
        "static inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)",
        "{",
        "	return pgd;",
        "}",
        "#endif  /* CONFIG_MITIGATION_PAGE_TABLE_ISOLATION */",
        "",
        "#endif	/* __ASSEMBLY__ */",
        "",
        "",
        "#ifdef CONFIG_X86_32",
        "# include <asm/pgtable_32.h>",
        "#else",
        "# include <asm/pgtable_64.h>",
        "#endif",
        "",
        "#ifndef __ASSEMBLY__",
        "#include <linux/mm_types.h>",
        "#include <linux/mmdebug.h>",
        "#include <linux/log2.h>",
        "#include <asm/fixmap.h>",
        "",
        "static inline int pte_none(pte_t pte)",
        "{",
        "	return !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));",
        "}",
        "",
        "#define __HAVE_ARCH_PTE_SAME",
        "static inline int pte_same(pte_t a, pte_t b)",
        "{",
        "	return a.pte == b.pte;",
        "}",
        "",
        "static inline pte_t pte_advance_pfn(pte_t pte, unsigned long nr)",
        "{",
        "	if (__pte_needs_invert(pte_val(pte)))",
        "		return __pte(pte_val(pte) - (nr << PFN_PTE_SHIFT));",
        "	return __pte(pte_val(pte) + (nr << PFN_PTE_SHIFT));",
        "}",
        "#define pte_advance_pfn	pte_advance_pfn",
        "",
        "static inline int pte_present(pte_t a)",
        "{",
        "	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);",
        "}",
        "",
        "#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP",
        "static inline int pte_devmap(pte_t a)",
        "{",
        "	return (pte_flags(a) & _PAGE_DEVMAP) == _PAGE_DEVMAP;",
        "}",
        "#endif",
        "",
        "#define pte_accessible pte_accessible",
        "static inline bool pte_accessible(struct mm_struct *mm, pte_t a)",
        "{",
        "	if (pte_flags(a) & _PAGE_PRESENT)",
        "		return true;",
        "",
        "	if ((pte_flags(a) & _PAGE_PROTNONE) &&",
        "			atomic_read(&mm->tlb_flush_pending))",
        "		return true;",
        "",
        "	return false;",
        "}",
        "",
        "static inline int pmd_present(pmd_t pmd)",
        "{",
        "	/*",
        "	 * Checking for _PAGE_PSE is needed too because",
        "	 * split_huge_page will temporarily clear the present bit (but",
        "	 * the _PAGE_PSE flag will remain set at all times while the",
        "	 * _PAGE_PRESENT bit is clear).",
        "	 */",
        "	return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);",
        "}",
        "",
        "#ifdef CONFIG_NUMA_BALANCING",
        "/*",
        " * These work without NUMA balancing but the kernel does not care. See the",
        " * comment in include/linux/pgtable.h",
        " */",
        "static inline int pte_protnone(pte_t pte)",
        "{",
        "	return (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))",
        "		== _PAGE_PROTNONE;",
        "}",
        "",
        "static inline int pmd_protnone(pmd_t pmd)",
        "{",
        "	return (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))",
        "		== _PAGE_PROTNONE;",
        "}",
        "#endif /* CONFIG_NUMA_BALANCING */",
        "",
        "static inline int pmd_none(pmd_t pmd)",
        "{",
        "	/* Only check low word on 32-bit platforms, since it might be",
        "	   out of sync with upper half. */",
        "	unsigned long val = native_pmd_val(pmd);",
        "	return (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;",
        "}",
        "",
        "static inline unsigned long pmd_page_vaddr(pmd_t pmd)",
        "{",
        "	return (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));",
        "}",
        "",
        "/*",
        " * Currently stuck as a macro due to indirect forward reference to",
        " * linux/mmzone.h's __section_mem_map_addr() definition:",
        " */",
        "#define pmd_page(pmd)	pfn_to_page(pmd_pfn(pmd))",
        "",
        "/*",
        " * Conversion functions: convert a page and protection to a page entry,",
        " * and a page entry and page directory to the page they refer to.",
        " *",
        " * (Currently stuck as a macro because of indirect forward reference",
        " * to linux/mm.h:page_to_nid())",
        " */",
        "#define mk_pte(page, pgprot)						  \\",
        "({									  \\",
        "	pgprot_t __pgprot = pgprot;					  \\",
        "									  \\",
        "	WARN_ON_ONCE((pgprot_val(__pgprot) & (_PAGE_DIRTY | _PAGE_RW)) == \\",
        "		    _PAGE_DIRTY);					  \\",
        "	pfn_pte(page_to_pfn(page), __pgprot);				  \\",
        "})",
        "",
        "static inline int pmd_bad(pmd_t pmd)",
        "{",
        "	return (pmd_flags(pmd) & ~(_PAGE_USER | _PAGE_ACCESSED)) !=",
        "	       (_KERNPG_TABLE & ~_PAGE_ACCESSED);",
        "}",
        "",
        "static inline unsigned long pages_to_mb(unsigned long npg)",
        "{",
        "	return npg >> (20 - PAGE_SHIFT);",
        "}",
        "",
        "#if CONFIG_PGTABLE_LEVELS > 2",
        "static inline int pud_none(pud_t pud)",
        "{",
        "	return (native_pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;",
        "}",
        "",
        "static inline int pud_present(pud_t pud)",
        "{",
        "	return pud_flags(pud) & _PAGE_PRESENT;",
        "}",
        "",
        "static inline pmd_t *pud_pgtable(pud_t pud)",
        "{",
        "	return (pmd_t *)__va(pud_val(pud) & pud_pfn_mask(pud));",
        "}",
        "",
        "/*",
        " * Currently stuck as a macro due to indirect forward reference to",
        " * linux/mmzone.h's __section_mem_map_addr() definition:",
        " */",
        "#define pud_page(pud)	pfn_to_page(pud_pfn(pud))",
        "",
        "#define pud_leaf pud_leaf",
        "static inline bool pud_leaf(pud_t pud)",
        "{",
        "	return pud_val(pud) & _PAGE_PSE;",
        "}",
        "",
        "static inline int pud_bad(pud_t pud)",
        "{",
        "	return (pud_flags(pud) & ~(_KERNPG_TABLE | _PAGE_USER)) != 0;",
        "}",
        "#endif	/* CONFIG_PGTABLE_LEVELS > 2 */",
        "",
        "#if CONFIG_PGTABLE_LEVELS > 3",
        "static inline int p4d_none(p4d_t p4d)",
        "{",
        "	return (native_p4d_val(p4d) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;",
        "}",
        "",
        "static inline int p4d_present(p4d_t p4d)",
        "{",
        "	return p4d_flags(p4d) & _PAGE_PRESENT;",
        "}",
        "",
        "static inline pud_t *p4d_pgtable(p4d_t p4d)",
        "{",
        "	return (pud_t *)__va(p4d_val(p4d) & p4d_pfn_mask(p4d));",
        "}",
        "",
        "/*",
        " * Currently stuck as a macro due to indirect forward reference to",
        " * linux/mmzone.h's __section_mem_map_addr() definition:",
        " */",
        "#define p4d_page(p4d)	pfn_to_page(p4d_pfn(p4d))",
        "",
        "static inline int p4d_bad(p4d_t p4d)",
        "{",
        "	unsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;",
        "",
        "	if (IS_ENABLED(CONFIG_MITIGATION_PAGE_TABLE_ISOLATION))",
        "		ignore_flags |= _PAGE_NX;",
        "",
        "	return (p4d_flags(p4d) & ~ignore_flags) != 0;",
        "}",
        "#endif  /* CONFIG_PGTABLE_LEVELS > 3 */",
        "",
        "static inline unsigned long p4d_index(unsigned long address)",
        "{",
        "	return (address >> P4D_SHIFT) & (PTRS_PER_P4D - 1);",
        "}",
        "",
        "#if CONFIG_PGTABLE_LEVELS > 4",
        "static inline int pgd_present(pgd_t pgd)",
        "{",
        "	if (!pgtable_l5_enabled())",
        "		return 1;",
        "	return pgd_flags(pgd) & _PAGE_PRESENT;",
        "}",
        "",
        "static inline unsigned long pgd_page_vaddr(pgd_t pgd)",
        "{",
        "	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);",
        "}",
        "",
        "/*",
        " * Currently stuck as a macro due to indirect forward reference to",
        " * linux/mmzone.h's __section_mem_map_addr() definition:",
        " */",
        "#define pgd_page(pgd)	pfn_to_page(pgd_pfn(pgd))",
        "",
        "/* to find an entry in a page-table-directory. */",
        "static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)",
        "{",
        "	if (!pgtable_l5_enabled())",
        "		return (p4d_t *)pgd;",
        "	return (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);",
        "}",
        "",
        "static inline int pgd_bad(pgd_t pgd)",
        "{",
        "	unsigned long ignore_flags = _PAGE_USER;",
        "",
        "	if (!pgtable_l5_enabled())",
        "		return 0;",
        "",
        "	if (IS_ENABLED(CONFIG_MITIGATION_PAGE_TABLE_ISOLATION))",
        "		ignore_flags |= _PAGE_NX;",
        "",
        "	return (pgd_flags(pgd) & ~ignore_flags) != _KERNPG_TABLE;",
        "}",
        "",
        "static inline int pgd_none(pgd_t pgd)",
        "{",
        "	if (!pgtable_l5_enabled())",
        "		return 0;",
        "	/*",
        "	 * There is no need to do a workaround for the KNL stray",
        "	 * A/D bit erratum here.  PGDs only point to page tables",
        "	 * except on 32-bit non-PAE which is not supported on",
        "	 * KNL.",
        "	 */",
        "	return !native_pgd_val(pgd);",
        "}",
        "#endif	/* CONFIG_PGTABLE_LEVELS > 4 */",
        "",
        "#endif	/* __ASSEMBLY__ */",
        "",
        "#define KERNEL_PGD_BOUNDARY	pgd_index(PAGE_OFFSET)",
        "#define KERNEL_PGD_PTRS		(PTRS_PER_PGD - KERNEL_PGD_BOUNDARY)",
        "",
        "#ifndef __ASSEMBLY__",
        "",
        "extern int direct_gbpages;",
        "void init_mem_mapping(void);",
        "void early_alloc_pgt_buf(void);",
        "void __init poking_init(void);",
        "unsigned long init_memory_mapping(unsigned long start,",
        "				  unsigned long end, pgprot_t prot);",
        "",
        "#ifdef CONFIG_X86_64",
        "extern pgd_t trampoline_pgd_entry;",
        "#endif",
        "",
        "/* local pte updates need not use xchg for locking */",
        "static inline pte_t native_local_ptep_get_and_clear(pte_t *ptep)",
        "{",
        "	pte_t res = *ptep;",
        "",
        "	/* Pure native function needs no input for mm, addr */",
        "	native_pte_clear(NULL, 0, ptep);",
        "	return res;",
        "}",
        "",
        "static inline pmd_t native_local_pmdp_get_and_clear(pmd_t *pmdp)",
        "{",
        "	pmd_t res = *pmdp;",
        "",
        "	native_pmd_clear(pmdp);",
        "	return res;",
        "}",
        "",
        "static inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)",
        "{",
        "	pud_t res = *pudp;",
        "",
        "	native_pud_clear(pudp);",
        "	return res;",
        "}",
        "",
        "static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,",
        "			      pmd_t *pmdp, pmd_t pmd)",
        "{",
        "	page_table_check_pmd_set(mm, pmdp, pmd);",
        "	set_pmd(pmdp, pmd);",
        "}",
        "",
        "static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,",
        "			      pud_t *pudp, pud_t pud)",
        "{",
        "	page_table_check_pud_set(mm, pudp, pud);",
        "	native_set_pud(pudp, pud);",
        "}",
        "",
        "/*",
        " * We only update the dirty/accessed state if we set",
        " * the dirty bit by hand in the kernel, since the hardware",
        " * will do the accessed bit for us, and we don't want to",
        " * race with other CPU's that might be updating the dirty",
        " * bit at the same time.",
        " */",
        "struct vm_area_struct;",
        "",
        "#define  __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS",
        "extern int ptep_set_access_flags(struct vm_area_struct *vma,",
        "				 unsigned long address, pte_t *ptep,",
        "				 pte_t entry, int dirty);",
        "",
        "#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG",
        "extern int ptep_test_and_clear_young(struct vm_area_struct *vma,",
        "				     unsigned long addr, pte_t *ptep);",
        "",
        "#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH",
        "extern int ptep_clear_flush_young(struct vm_area_struct *vma,",
        "				  unsigned long address, pte_t *ptep);",
        "",
        "#define __HAVE_ARCH_PTEP_GET_AND_CLEAR",
        "static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,",
        "				       pte_t *ptep)",
        "{",
        "	pte_t pte = native_ptep_get_and_clear(ptep);",
        "	page_table_check_pte_clear(mm, pte);",
        "	return pte;",
        "}",
        "",
        "#define __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL",
        "static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,",
        "					    unsigned long addr, pte_t *ptep,",
        "					    int full)",
        "{",
        "	pte_t pte;",
        "	if (full) {",
        "		/*",
        "		 * Full address destruction in progress; paravirt does not",
        "		 * care about updates and native needs no locking",
        "		 */",
        "		pte = native_local_ptep_get_and_clear(ptep);",
        "		page_table_check_pte_clear(mm, pte);",
        "	} else {",
        "		pte = ptep_get_and_clear(mm, addr, ptep);",
        "	}",
        "	return pte;",
        "}",
        "",
        "#define __HAVE_ARCH_PTEP_SET_WRPROTECT",
        "static inline void ptep_set_wrprotect(struct mm_struct *mm,",
        "				      unsigned long addr, pte_t *ptep)",
        "{",
        "	/*",
        "	 * Avoid accidentally creating shadow stack PTEs",
        "	 * (Write=0,Dirty=1).  Use cmpxchg() to prevent races with",
        "	 * the hardware setting Dirty=1.",
        "	 */",
        "	pte_t old_pte, new_pte;",
        "",
        "	old_pte = READ_ONCE(*ptep);",
        "	do {",
        "		new_pte = pte_wrprotect(old_pte);",
        "	} while (!try_cmpxchg((long *)&ptep->pte, (long *)&old_pte, *(long *)&new_pte));",
        "}",
        "",
        "#define flush_tlb_fix_spurious_fault(vma, address, ptep) do { } while (0)",
        "",
        "#define mk_pmd(page, pgprot)   pfn_pmd(page_to_pfn(page), (pgprot))",
        "",
        "#define  __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS",
        "extern int pmdp_set_access_flags(struct vm_area_struct *vma,",
        "				 unsigned long address, pmd_t *pmdp,",
        "				 pmd_t entry, int dirty);",
        "extern int pudp_set_access_flags(struct vm_area_struct *vma,",
        "				 unsigned long address, pud_t *pudp,",
        "				 pud_t entry, int dirty);",
        "",
        "#define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG",
        "extern int pmdp_test_and_clear_young(struct vm_area_struct *vma,",
        "				     unsigned long addr, pmd_t *pmdp);",
        "extern int pudp_test_and_clear_young(struct vm_area_struct *vma,",
        "				     unsigned long addr, pud_t *pudp);",
        "",
        "#define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH",
        "extern int pmdp_clear_flush_young(struct vm_area_struct *vma,",
        "				  unsigned long address, pmd_t *pmdp);",
        "",
        "",
        "#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR",
        "static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,",
        "				       pmd_t *pmdp)",
        "{",
        "	pmd_t pmd = native_pmdp_get_and_clear(pmdp);",
        "",
        "	page_table_check_pmd_clear(mm, pmd);",
        "",
        "	return pmd;",
        "}",
        "",
        "#define __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR",
        "static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,",
        "					unsigned long addr, pud_t *pudp)",
        "{",
        "	pud_t pud = native_pudp_get_and_clear(pudp);",
        "",
        "	page_table_check_pud_clear(mm, pud);",
        "",
        "	return pud;",
        "}",
        "",
        "#define __HAVE_ARCH_PMDP_SET_WRPROTECT",
        "static inline void pmdp_set_wrprotect(struct mm_struct *mm,",
        "				      unsigned long addr, pmd_t *pmdp)",
        "{",
        "	/*",
        "	 * Avoid accidentally creating shadow stack PTEs",
        "	 * (Write=0,Dirty=1).  Use cmpxchg() to prevent races with",
        "	 * the hardware setting Dirty=1.",
        "	 */",
        "	pmd_t old_pmd, new_pmd;",
        "",
        "	old_pmd = READ_ONCE(*pmdp);",
        "	do {",
        "		new_pmd = pmd_wrprotect(old_pmd);",
        "	} while (!try_cmpxchg((long *)pmdp, (long *)&old_pmd, *(long *)&new_pmd));",
        "}",
        "",
        "#ifndef pmdp_establish",
        "#define pmdp_establish pmdp_establish",
        "static inline pmd_t pmdp_establish(struct vm_area_struct *vma,",
        "		unsigned long address, pmd_t *pmdp, pmd_t pmd)",
        "{",
        "	page_table_check_pmd_set(vma->vm_mm, pmdp, pmd);",
        "	if (IS_ENABLED(CONFIG_SMP)) {",
        "		return xchg(pmdp, pmd);",
        "	} else {",
        "		pmd_t old = *pmdp;",
        "		WRITE_ONCE(*pmdp, pmd);",
        "		return old;",
        "	}",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD",
        "static inline pud_t pudp_establish(struct vm_area_struct *vma,",
        "		unsigned long address, pud_t *pudp, pud_t pud)",
        "{",
        "	page_table_check_pud_set(vma->vm_mm, pudp, pud);",
        "	if (IS_ENABLED(CONFIG_SMP)) {",
        "		return xchg(pudp, pud);",
        "	} else {",
        "		pud_t old = *pudp;",
        "		WRITE_ONCE(*pudp, pud);",
        "		return old;",
        "	}",
        "}",
        "#endif",
        "",
        "#define __HAVE_ARCH_PMDP_INVALIDATE_AD",
        "extern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,",
        "				unsigned long address, pmd_t *pmdp);",
        "",
        "pud_t pudp_invalidate(struct vm_area_struct *vma, unsigned long address,",
        "		      pud_t *pudp);",
        "",
        "/*",
        " * Page table pages are page-aligned.  The lower half of the top",
        " * level is used for userspace and the top half for the kernel.",
        " *",
        " * Returns true for parts of the PGD that map userspace and",
        " * false for the parts that map the kernel.",
        " */",
        "static inline bool pgdp_maps_userspace(void *__ptr)",
        "{",
        "	unsigned long ptr = (unsigned long)__ptr;",
        "",
        "	return (((ptr & ~PAGE_MASK) / sizeof(pgd_t)) < PGD_KERNEL_START);",
        "}",
        "",
        "#define pgd_leaf	pgd_leaf",
        "static inline bool pgd_leaf(pgd_t pgd) { return false; }",
        "",
        "#ifdef CONFIG_MITIGATION_PAGE_TABLE_ISOLATION",
        "/*",
        " * All top-level MITIGATION_PAGE_TABLE_ISOLATION page tables are order-1 pages",
        " * (8k-aligned and 8k in size).  The kernel one is at the beginning 4k and",
        " * the user one is in the last 4k.  To switch between them, you",
        " * just need to flip the 12th bit in their addresses.",
        " */",
        "#define PTI_PGTABLE_SWITCH_BIT	PAGE_SHIFT",
        "",
        "/*",
        " * This generates better code than the inline assembly in",
        " * __set_bit().",
        " */",
        "static inline void *ptr_set_bit(void *ptr, int bit)",
        "{",
        "	unsigned long __ptr = (unsigned long)ptr;",
        "",
        "	__ptr |= BIT(bit);",
        "	return (void *)__ptr;",
        "}",
        "static inline void *ptr_clear_bit(void *ptr, int bit)",
        "{",
        "	unsigned long __ptr = (unsigned long)ptr;",
        "",
        "	__ptr &= ~BIT(bit);",
        "	return (void *)__ptr;",
        "}",
        "",
        "static inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)",
        "{",
        "	return ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);",
        "}",
        "",
        "static inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)",
        "{",
        "	return ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);",
        "}",
        "",
        "static inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)",
        "{",
        "	return ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);",
        "}",
        "",
        "static inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)",
        "{",
        "	return ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);",
        "}",
        "#endif /* CONFIG_MITIGATION_PAGE_TABLE_ISOLATION */",
        "",
        "/*",
        " * clone_pgd_range(pgd_t *dst, pgd_t *src, int count);",
        " *",
        " *  dst - pointer to pgd range anywhere on a pgd page",
        " *  src - \"\"",
        " *  count - the number of pgds to copy.",
        " *",
        " * dst and src can be on the same page, but the range must not overlap,",
        " * and must not cross a page boundary.",
        " */",
        "static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)",
        "{",
        "	memcpy(dst, src, count * sizeof(pgd_t));",
        "#ifdef CONFIG_MITIGATION_PAGE_TABLE_ISOLATION",
        "	if (!static_cpu_has(X86_FEATURE_PTI))",
        "		return;",
        "	/* Clone the user space pgd as well */",
        "	memcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),",
        "	       count * sizeof(pgd_t));",
        "#endif",
        "}",
        "",
        "#define PTE_SHIFT ilog2(PTRS_PER_PTE)",
        "static inline int page_level_shift(enum pg_level level)",
        "{",
        "	return (PAGE_SHIFT - PTE_SHIFT) + level * PTE_SHIFT;",
        "}",
        "static inline unsigned long page_level_size(enum pg_level level)",
        "{",
        "	return 1UL << page_level_shift(level);",
        "}",
        "static inline unsigned long page_level_mask(enum pg_level level)",
        "{",
        "	return ~(page_level_size(level) - 1);",
        "}",
        "",
        "/*",
        " * The x86 doesn't have any external MMU info: the kernel page",
        " * tables contain all the necessary information.",
        " */",
        "static inline void update_mmu_cache(struct vm_area_struct *vma,",
        "		unsigned long addr, pte_t *ptep)",
        "{",
        "}",
        "static inline void update_mmu_cache_range(struct vm_fault *vmf,",
        "		struct vm_area_struct *vma, unsigned long addr,",
        "		pte_t *ptep, unsigned int nr)",
        "{",
        "}",
        "static inline void update_mmu_cache_pmd(struct vm_area_struct *vma,",
        "		unsigned long addr, pmd_t *pmd)",
        "{",
        "}",
        "static inline void update_mmu_cache_pud(struct vm_area_struct *vma,",
        "		unsigned long addr, pud_t *pud)",
        "{",
        "}",
        "static inline pte_t pte_swp_mkexclusive(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SWP_EXCLUSIVE);",
        "}",
        "",
        "static inline int pte_swp_exclusive(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_SWP_EXCLUSIVE;",
        "}",
        "",
        "static inline pte_t pte_swp_clear_exclusive(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_SWP_EXCLUSIVE);",
        "}",
        "",
        "#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY",
        "static inline pte_t pte_swp_mksoft_dirty(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);",
        "}",
        "",
        "static inline int pte_swp_soft_dirty(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;",
        "}",
        "",
        "static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);",
        "}",
        "",
        "#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION",
        "static inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_SWP_SOFT_DIRTY);",
        "}",
        "",
        "static inline int pmd_swp_soft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_SWP_SOFT_DIRTY;",
        "}",
        "",
        "static inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_SWP_SOFT_DIRTY);",
        "}",
        "#endif",
        "#endif",
        "",
        "#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP",
        "static inline pte_t pte_swp_mkuffd_wp(pte_t pte)",
        "{",
        "	return pte_set_flags(pte, _PAGE_SWP_UFFD_WP);",
        "}",
        "",
        "static inline int pte_swp_uffd_wp(pte_t pte)",
        "{",
        "	return pte_flags(pte) & _PAGE_SWP_UFFD_WP;",
        "}",
        "",
        "static inline pte_t pte_swp_clear_uffd_wp(pte_t pte)",
        "{",
        "	return pte_clear_flags(pte, _PAGE_SWP_UFFD_WP);",
        "}",
        "",
        "static inline pmd_t pmd_swp_mkuffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_set_flags(pmd, _PAGE_SWP_UFFD_WP);",
        "}",
        "",
        "static inline int pmd_swp_uffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_flags(pmd) & _PAGE_SWP_UFFD_WP;",
        "}",
        "",
        "static inline pmd_t pmd_swp_clear_uffd_wp(pmd_t pmd)",
        "{",
        "	return pmd_clear_flags(pmd, _PAGE_SWP_UFFD_WP);",
        "}",
        "#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */",
        "",
        "static inline u16 pte_flags_pkey(unsigned long pte_flags)",
        "{",
        "#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS",
        "	/* ifdef to avoid doing 59-bit shift on 32-bit values */",
        "	return (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;",
        "#else",
        "	return 0;",
        "#endif",
        "}",
        "",
        "static inline bool __pkru_allows_pkey(u16 pkey, bool write)",
        "{",
        "	u32 pkru = read_pkru();",
        "",
        "	if (!__pkru_allows_read(pkru, pkey))",
        "		return false;",
        "	if (write && !__pkru_allows_write(pkru, pkey))",
        "		return false;",
        "",
        "	return true;",
        "}",
        "",
        "/*",
        " * 'pteval' can come from a PTE, PMD or PUD.  We only check",
        " * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the",
        " * same value on all 3 types.",
        " */",
        "static inline bool __pte_access_permitted(unsigned long pteval, bool write)",
        "{",
        "	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;",
        "",
        "	/*",
        "	 * Write=0,Dirty=1 PTEs are shadow stack, which the kernel",
        "	 * shouldn't generally allow access to, but since they",
        "	 * are already Write=0, the below logic covers both cases.",
        "	 */",
        "	if (write)",
        "		need_pte_bits |= _PAGE_RW;",
        "",
        "	if ((pteval & need_pte_bits) != need_pte_bits)",
        "		return 0;",
        "",
        "	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);",
        "}",
        "",
        "#define pte_access_permitted pte_access_permitted",
        "static inline bool pte_access_permitted(pte_t pte, bool write)",
        "{",
        "	return __pte_access_permitted(pte_val(pte), write);",
        "}",
        "",
        "#define pmd_access_permitted pmd_access_permitted",
        "static inline bool pmd_access_permitted(pmd_t pmd, bool write)",
        "{",
        "	return __pte_access_permitted(pmd_val(pmd), write);",
        "}",
        "",
        "#define pud_access_permitted pud_access_permitted",
        "static inline bool pud_access_permitted(pud_t pud, bool write)",
        "{",
        "	return __pte_access_permitted(pud_val(pud), write);",
        "}",
        "",
        "#define __HAVE_ARCH_PFN_MODIFY_ALLOWED 1",
        "extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);",
        "",
        "static inline bool arch_has_pfn_modify_check(void)",
        "{",
        "	return boot_cpu_has_bug(X86_BUG_L1TF);",
        "}",
        "",
        "#define arch_check_zapped_pte arch_check_zapped_pte",
        "void arch_check_zapped_pte(struct vm_area_struct *vma, pte_t pte);",
        "",
        "#define arch_check_zapped_pmd arch_check_zapped_pmd",
        "void arch_check_zapped_pmd(struct vm_area_struct *vma, pmd_t pmd);",
        "",
        "#define arch_check_zapped_pud arch_check_zapped_pud",
        "void arch_check_zapped_pud(struct vm_area_struct *vma, pud_t pud);",
        "",
        "#ifdef CONFIG_XEN_PV",
        "#define arch_has_hw_nonleaf_pmd_young arch_has_hw_nonleaf_pmd_young",
        "static inline bool arch_has_hw_nonleaf_pmd_young(void)",
        "{",
        "	return !cpu_feature_enabled(X86_FEATURE_XENPV);",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_PAGE_TABLE_CHECK",
        "static inline bool pte_user_accessible_page(pte_t pte)",
        "{",
        "	return (pte_val(pte) & _PAGE_PRESENT) && (pte_val(pte) & _PAGE_USER);",
        "}",
        "",
        "static inline bool pmd_user_accessible_page(pmd_t pmd)",
        "{",
        "	return pmd_leaf(pmd) && (pmd_val(pmd) & _PAGE_PRESENT) && (pmd_val(pmd) & _PAGE_USER);",
        "}",
        "",
        "static inline bool pud_user_accessible_page(pud_t pud)",
        "{",
        "	return pud_leaf(pud) && (pud_val(pud) & _PAGE_PRESENT) && (pud_val(pud) & _PAGE_USER);",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_X86_SGX",
        "int arch_memory_failure(unsigned long pfn, int flags);",
        "#define arch_memory_failure arch_memory_failure",
        "",
        "bool arch_is_platform_page(u64 paddr);",
        "#define arch_is_platform_page arch_is_platform_page",
        "#endif",
        "",
        "/*",
        " * Use set_p*_safe(), and elide TLB flushing, when confident that *no*",
        " * TLB flush will be required as a result of the \"set\". For example, use",
        " * in scenarios where it is known ahead of time that the routine is",
        " * setting non-present entries, or re-setting an existing entry to the",
        " * same value. Otherwise, use the typical \"set\" helpers and flush the",
        " * TLB.",
        " */",
        "#define set_pte_safe(ptep, pte) \\",
        "({ \\",
        "	WARN_ON_ONCE(pte_present(*ptep) && !pte_same(*ptep, pte)); \\",
        "	set_pte(ptep, pte); \\",
        "})",
        "",
        "#define set_pmd_safe(pmdp, pmd) \\",
        "({ \\",
        "	WARN_ON_ONCE(pmd_present(*pmdp) && !pmd_same(*pmdp, pmd)); \\",
        "	set_pmd(pmdp, pmd); \\",
        "})",
        "",
        "#define set_pud_safe(pudp, pud) \\",
        "({ \\",
        "	WARN_ON_ONCE(pud_present(*pudp) && !pud_same(*pudp, pud)); \\",
        "	set_pud(pudp, pud); \\",
        "})",
        "",
        "#define set_p4d_safe(p4dp, p4d) \\",
        "({ \\",
        "	WARN_ON_ONCE(p4d_present(*p4dp) && !p4d_same(*p4dp, p4d)); \\",
        "	set_p4d(p4dp, p4d); \\",
        "})",
        "",
        "#define set_pgd_safe(pgdp, pgd) \\",
        "({ \\",
        "	WARN_ON_ONCE(pgd_present(*pgdp) && !pgd_same(*pgdp, pgd)); \\",
        "	set_pgd(pgdp, pgd); \\",
        "})",
        "#endif	/* __ASSEMBLY__ */",
        "",
        "#endif /* _ASM_X86_PGTABLE_H */"
    ]
  },
  "include_linux_mmzone_h": {
    path: "include/linux/mmzone.h",
    covered: [1882, 1889, 2045, 2057, 2032, 2047, 1994, 2058],
    totalLines: 2106,
    coveredCount: 8,
    coveragePct: 0.4,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_MMZONE_H",
        "#define _LINUX_MMZONE_H",
        "",
        "#ifndef __ASSEMBLY__",
        "#ifndef __GENERATING_BOUNDS_H",
        "",
        "#include <linux/spinlock.h>",
        "#include <linux/list.h>",
        "#include <linux/list_nulls.h>",
        "#include <linux/wait.h>",
        "#include <linux/bitops.h>",
        "#include <linux/cache.h>",
        "#include <linux/threads.h>",
        "#include <linux/numa.h>",
        "#include <linux/init.h>",
        "#include <linux/seqlock.h>",
        "#include <linux/nodemask.h>",
        "#include <linux/pageblock-flags.h>",
        "#include <linux/page-flags-layout.h>",
        "#include <linux/atomic.h>",
        "#include <linux/mm_types.h>",
        "#include <linux/page-flags.h>",
        "#include <linux/local_lock.h>",
        "#include <linux/zswap.h>",
        "#include <asm/page.h>",
        "",
        "/* Free memory management - zoned buddy allocator.  */",
        "#ifndef CONFIG_ARCH_FORCE_MAX_ORDER",
        "#define MAX_PAGE_ORDER 10",
        "#else",
        "#define MAX_PAGE_ORDER CONFIG_ARCH_FORCE_MAX_ORDER",
        "#endif",
        "#define MAX_ORDER_NR_PAGES (1 << MAX_PAGE_ORDER)",
        "",
        "#define IS_MAX_ORDER_ALIGNED(pfn) IS_ALIGNED(pfn, MAX_ORDER_NR_PAGES)",
        "",
        "#define NR_PAGE_ORDERS (MAX_PAGE_ORDER + 1)",
        "",
        "/*",
        " * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed",
        " * costly to service.  That is between allocation orders which should",
        " * coalesce naturally under reasonable reclaim pressure and those which",
        " * will not.",
        " */",
        "#define PAGE_ALLOC_COSTLY_ORDER 3",
        "",
        "enum migratetype {",
        "	MIGRATE_UNMOVABLE,",
        "	MIGRATE_MOVABLE,",
        "	MIGRATE_RECLAIMABLE,",
        "	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */",
        "	MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,",
        "#ifdef CONFIG_CMA",
        "	/*",
        "	 * MIGRATE_CMA migration type is designed to mimic the way",
        "	 * ZONE_MOVABLE works.  Only movable pages can be allocated",
        "	 * from MIGRATE_CMA pageblocks and page allocator never",
        "	 * implicitly change migration type of MIGRATE_CMA pageblock.",
        "	 *",
        "	 * The way to use it is to change migratetype of a range of",
        "	 * pageblocks to MIGRATE_CMA which can be done by",
        "	 * __free_pageblock_cma() function.",
        "	 */",
        "	MIGRATE_CMA,",
        "#endif",
        "#ifdef CONFIG_MEMORY_ISOLATION",
        "	MIGRATE_ISOLATE,	/* can't allocate from here */",
        "#endif",
        "	MIGRATE_TYPES",
        "};",
        "",
        "/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */",
        "extern const char * const migratetype_names[MIGRATE_TYPES];",
        "",
        "#ifdef CONFIG_CMA",
        "#  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)",
        "#  define is_migrate_cma_page(_page) (get_pageblock_migratetype(_page) == MIGRATE_CMA)",
        "#  define is_migrate_cma_folio(folio, pfn)	(MIGRATE_CMA ==		\\",
        "	get_pfnblock_flags_mask(&folio->page, pfn, MIGRATETYPE_MASK))",
        "#else",
        "#  define is_migrate_cma(migratetype) false",
        "#  define is_migrate_cma_page(_page) false",
        "#  define is_migrate_cma_folio(folio, pfn) false",
        "#endif",
        "",
        "static inline bool is_migrate_movable(int mt)",
        "{",
        "	return is_migrate_cma(mt) || mt == MIGRATE_MOVABLE;",
        "}",
        "",
        "/*",
        " * Check whether a migratetype can be merged with another migratetype.",
        " *",
        " * It is only mergeable when it can fall back to other migratetypes for",
        " * allocation. See fallbacks[MIGRATE_TYPES][3] in page_alloc.c.",
        " */",
        "static inline bool migratetype_is_mergeable(int mt)",
        "{",
        "	return mt < MIGRATE_PCPTYPES;",
        "}",
        "",
        "#define for_each_migratetype_order(order, type) \\",
        "	for (order = 0; order < NR_PAGE_ORDERS; order++) \\",
        "		for (type = 0; type < MIGRATE_TYPES; type++)",
        "",
        "extern int page_group_by_mobility_disabled;",
        "",
        "#define MIGRATETYPE_MASK ((1UL << PB_migratetype_bits) - 1)",
        "",
        "#define get_pageblock_migratetype(page)					\\",
        "	get_pfnblock_flags_mask(page, page_to_pfn(page), MIGRATETYPE_MASK)",
        "",
        "#define folio_migratetype(folio)				\\",
        "	get_pfnblock_flags_mask(&folio->page, folio_pfn(folio),		\\",
        "			MIGRATETYPE_MASK)",
        "struct free_area {",
        "	struct list_head	free_list[MIGRATE_TYPES];",
        "	unsigned long		nr_free;",
        "};",
        "",
        "struct pglist_data;",
        "",
        "#ifdef CONFIG_NUMA",
        "enum numa_stat_item {",
        "	NUMA_HIT,		/* allocated in intended node */",
        "	NUMA_MISS,		/* allocated in non intended node */",
        "	NUMA_FOREIGN,		/* was intended here, hit elsewhere */",
        "	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */",
        "	NUMA_LOCAL,		/* allocation from local node */",
        "	NUMA_OTHER,		/* allocation from other node */",
        "	NR_VM_NUMA_EVENT_ITEMS",
        "};",
        "#else",
        "#define NR_VM_NUMA_EVENT_ITEMS 0",
        "#endif",
        "",
        "enum zone_stat_item {",
        "	/* First 128 byte cacheline (assuming 64 bit words) */",
        "	NR_FREE_PAGES,",
        "	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */",
        "	NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,",
        "	NR_ZONE_ACTIVE_ANON,",
        "	NR_ZONE_INACTIVE_FILE,",
        "	NR_ZONE_ACTIVE_FILE,",
        "	NR_ZONE_UNEVICTABLE,",
        "	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */",
        "	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */",
        "	/* Second 128 byte cacheline */",
        "	NR_BOUNCE,",
        "#if IS_ENABLED(CONFIG_ZSMALLOC)",
        "	NR_ZSPAGES,		/* allocated in zsmalloc */",
        "#endif",
        "	NR_FREE_CMA_PAGES,",
        "#ifdef CONFIG_UNACCEPTED_MEMORY",
        "	NR_UNACCEPTED,",
        "#endif",
        "	NR_VM_ZONE_STAT_ITEMS };",
        "",
        "enum node_stat_item {",
        "	NR_LRU_BASE,",
        "	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */",
        "	NR_ACTIVE_ANON,		/*  \"     \"     \"   \"       \"         */",
        "	NR_INACTIVE_FILE,	/*  \"     \"     \"   \"       \"         */",
        "	NR_ACTIVE_FILE,		/*  \"     \"     \"   \"       \"         */",
        "	NR_UNEVICTABLE,		/*  \"     \"     \"   \"       \"         */",
        "	NR_SLAB_RECLAIMABLE_B,",
        "	NR_SLAB_UNRECLAIMABLE_B,",
        "	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */",
        "	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */",
        "	WORKINGSET_NODES,",
        "	WORKINGSET_REFAULT_BASE,",
        "	WORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,",
        "	WORKINGSET_REFAULT_FILE,",
        "	WORKINGSET_ACTIVATE_BASE,",
        "	WORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,",
        "	WORKINGSET_ACTIVATE_FILE,",
        "	WORKINGSET_RESTORE_BASE,",
        "	WORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,",
        "	WORKINGSET_RESTORE_FILE,",
        "	WORKINGSET_NODERECLAIM,",
        "	NR_ANON_MAPPED,	/* Mapped anonymous pages */",
        "	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.",
        "			   only modified from process context */",
        "	NR_FILE_PAGES,",
        "	NR_FILE_DIRTY,",
        "	NR_WRITEBACK,",
        "	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */",
        "	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */",
        "	NR_SHMEM_THPS,",
        "	NR_SHMEM_PMDMAPPED,",
        "	NR_FILE_THPS,",
        "	NR_FILE_PMDMAPPED,",
        "	NR_ANON_THPS,",
        "	NR_VMSCAN_WRITE,",
        "	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */",
        "	NR_DIRTIED,		/* page dirtyings since bootup */",
        "	NR_WRITTEN,		/* page writings since bootup */",
        "	NR_THROTTLED_WRITTEN,	/* NR_WRITTEN while reclaim throttled */",
        "	NR_KERNEL_MISC_RECLAIMABLE,	/* reclaimable non-slab kernel pages */",
        "	NR_FOLL_PIN_ACQUIRED,	/* via: pin_user_page(), gup flag: FOLL_PIN */",
        "	NR_FOLL_PIN_RELEASED,	/* pages returned via unpin_user_page() */",
        "	NR_KERNEL_STACK_KB,	/* measured in KiB */",
        "#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)",
        "	NR_KERNEL_SCS_KB,	/* measured in KiB */",
        "#endif",
        "	NR_PAGETABLE,		/* used for pagetables */",
        "	NR_SECONDARY_PAGETABLE, /* secondary pagetables, KVM & IOMMU */",
        "#ifdef CONFIG_IOMMU_SUPPORT",
        "	NR_IOMMU_PAGES,		/* # of pages allocated by IOMMU */",
        "#endif",
        "#ifdef CONFIG_SWAP",
        "	NR_SWAPCACHE,",
        "#endif",
        "#ifdef CONFIG_NUMA_BALANCING",
        "	PGPROMOTE_SUCCESS,	/* promote successfully */",
        "	PGPROMOTE_CANDIDATE,	/* candidate pages to promote */",
        "#endif",
        "	/* PGDEMOTE_*: pages demoted */",
        "	PGDEMOTE_KSWAPD,",
        "	PGDEMOTE_DIRECT,",
        "	PGDEMOTE_KHUGEPAGED,",
        "#ifdef CONFIG_HUGETLB_PAGE",
        "	NR_HUGETLB,",
        "#endif",
        "	NR_VM_NODE_STAT_ITEMS",
        "};",
        "",
        "/*",
        " * Returns true if the item should be printed in THPs (/proc/vmstat",
        " * currently prints number of anon, file and shmem THPs. But the item",
        " * is charged in pages).",
        " */",
        "static __always_inline bool vmstat_item_print_in_thp(enum node_stat_item item)",
        "{",
        "	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))",
        "		return false;",
        "",
        "	return item == NR_ANON_THPS ||",
        "	       item == NR_FILE_THPS ||",
        "	       item == NR_SHMEM_THPS ||",
        "	       item == NR_SHMEM_PMDMAPPED ||",
        "	       item == NR_FILE_PMDMAPPED;",
        "}",
        "",
        "/*",
        " * Returns true if the value is measured in bytes (most vmstat values are",
        " * measured in pages). This defines the API part, the internal representation",
        " * might be different.",
        " */",
        "static __always_inline bool vmstat_item_in_bytes(int idx)",
        "{",
        "	/*",
        "	 * Global and per-node slab counters track slab pages.",
        "	 * It's expected that changes are multiples of PAGE_SIZE.",
        "	 * Internally values are stored in pages.",
        "	 *",
        "	 * Per-memcg and per-lruvec counters track memory, consumed",
        "	 * by individual slab objects. These counters are actually",
        "	 * byte-precise.",
        "	 */",
        "	return (idx == NR_SLAB_RECLAIMABLE_B ||",
        "		idx == NR_SLAB_UNRECLAIMABLE_B);",
        "}",
        "",
        "/*",
        " * We do arithmetic on the LRU lists in various places in the code,",
        " * so it is important to keep the active lists LRU_ACTIVE higher in",
        " * the array than the corresponding inactive lists, and to keep",
        " * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.",
        " *",
        " * This has to be kept in sync with the statistics in zone_stat_item",
        " * above and the descriptions in vmstat_text in mm/vmstat.c",
        " */",
        "#define LRU_BASE 0",
        "#define LRU_ACTIVE 1",
        "#define LRU_FILE 2",
        "",
        "enum lru_list {",
        "	LRU_INACTIVE_ANON = LRU_BASE,",
        "	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,",
        "	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,",
        "	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,",
        "	LRU_UNEVICTABLE,",
        "	NR_LRU_LISTS",
        "};",
        "",
        "enum vmscan_throttle_state {",
        "	VMSCAN_THROTTLE_WRITEBACK,",
        "	VMSCAN_THROTTLE_ISOLATED,",
        "	VMSCAN_THROTTLE_NOPROGRESS,",
        "	VMSCAN_THROTTLE_CONGESTED,",
        "	NR_VMSCAN_THROTTLE,",
        "};",
        "",
        "#define for_each_lru(lru) for (lru = 0; lru < NR_LRU_LISTS; lru++)",
        "",
        "#define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)",
        "",
        "static inline bool is_file_lru(enum lru_list lru)",
        "{",
        "	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);",
        "}",
        "",
        "static inline bool is_active_lru(enum lru_list lru)",
        "{",
        "	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);",
        "}",
        "",
        "#define WORKINGSET_ANON 0",
        "#define WORKINGSET_FILE 1",
        "#define ANON_AND_FILE 2",
        "",
        "enum lruvec_flags {",
        "	/*",
        "	 * An lruvec has many dirty pages backed by a congested BDI:",
        "	 * 1. LRUVEC_CGROUP_CONGESTED is set by cgroup-level reclaim.",
        "	 *    It can be cleared by cgroup reclaim or kswapd.",
        "	 * 2. LRUVEC_NODE_CONGESTED is set by kswapd node-level reclaim.",
        "	 *    It can only be cleared by kswapd.",
        "	 *",
        "	 * Essentially, kswapd can unthrottle an lruvec throttled by cgroup",
        "	 * reclaim, but not vice versa. This only applies to the root cgroup.",
        "	 * The goal is to prevent cgroup reclaim on the root cgroup (e.g.",
        "	 * memory.reclaim) to unthrottle an unbalanced node (that was throttled",
        "	 * by kswapd).",
        "	 */",
        "	LRUVEC_CGROUP_CONGESTED,",
        "	LRUVEC_NODE_CONGESTED,",
        "};",
        "",
        "#endif /* !__GENERATING_BOUNDS_H */",
        "",
        "/*",
        " * Evictable pages are divided into multiple generations. The youngest and the",
        " * oldest generation numbers, max_seq and min_seq, are monotonically increasing.",
        " * They form a sliding window of a variable size [MIN_NR_GENS, MAX_NR_GENS]. An",
        " * offset within MAX_NR_GENS, i.e., gen, indexes the LRU list of the",
        " * corresponding generation. The gen counter in folio->flags stores gen+1 while",
        " * a page is on one of lrugen->folios[]. Otherwise it stores 0.",
        " *",
        " * A page is added to the youngest generation on faulting. The aging needs to",
        " * check the accessed bit at least twice before handing this page over to the",
        " * eviction. The first check takes care of the accessed bit set on the initial",
        " * fault; the second check makes sure this page hasn't been used since then.",
        " * This process, AKA second chance, requires a minimum of two generations,",
        " * hence MIN_NR_GENS. And to maintain ABI compatibility with the active/inactive",
        " * LRU, e.g., /proc/vmstat, these two generations are considered active; the",
        " * rest of generations, if they exist, are considered inactive. See",
        " * lru_gen_is_active().",
        " *",
        " * PG_active is always cleared while a page is on one of lrugen->folios[] so",
        " * that the aging needs not to worry about it. And it's set again when a page",
        " * considered active is isolated for non-reclaiming purposes, e.g., migration.",
        " * See lru_gen_add_folio() and lru_gen_del_folio().",
        " *",
        " * MAX_NR_GENS is set to 4 so that the multi-gen LRU can support twice the",
        " * number of categories of the active/inactive LRU when keeping track of",
        " * accesses through page tables. This requires order_base_2(MAX_NR_GENS+1) bits",
        " * in folio->flags.",
        " */",
        "#define MIN_NR_GENS		2U",
        "#define MAX_NR_GENS		4U",
        "",
        "/*",
        " * Each generation is divided into multiple tiers. A page accessed N times",
        " * through file descriptors is in tier order_base_2(N). A page in the first tier",
        " * (N=0,1) is marked by PG_referenced unless it was faulted in through page",
        " * tables or read ahead. A page in any other tier (N>1) is marked by",
        " * PG_referenced and PG_workingset. This implies a minimum of two tiers is",
        " * supported without using additional bits in folio->flags.",
        " *",
        " * In contrast to moving across generations which requires the LRU lock, moving",
        " * across tiers only involves atomic operations on folio->flags and therefore",
        " * has a negligible cost in the buffered access path. In the eviction path,",
        " * comparisons of refaulted/(evicted+protected) from the first tier and the",
        " * rest infer whether pages accessed multiple times through file descriptors",
        " * are statistically hot and thus worth protecting.",
        " *",
        " * MAX_NR_TIERS is set to 4 so that the multi-gen LRU can support twice the",
        " * number of categories of the active/inactive LRU when keeping track of",
        " * accesses through file descriptors. This uses MAX_NR_TIERS-2 spare bits in",
        " * folio->flags.",
        " */",
        "#define MAX_NR_TIERS		4U",
        "",
        "#ifndef __GENERATING_BOUNDS_H",
        "",
        "struct lruvec;",
        "struct page_vma_mapped_walk;",
        "",
        "#define LRU_GEN_MASK		((BIT(LRU_GEN_WIDTH) - 1) << LRU_GEN_PGOFF)",
        "#define LRU_REFS_MASK		((BIT(LRU_REFS_WIDTH) - 1) << LRU_REFS_PGOFF)",
        "",
        "#ifdef CONFIG_LRU_GEN",
        "",
        "enum {",
        "	LRU_GEN_ANON,",
        "	LRU_GEN_FILE,",
        "};",
        "",
        "enum {",
        "	LRU_GEN_CORE,",
        "	LRU_GEN_MM_WALK,",
        "	LRU_GEN_NONLEAF_YOUNG,",
        "	NR_LRU_GEN_CAPS",
        "};",
        "",
        "#define LRU_REFS_FLAGS		(BIT(PG_referenced) | BIT(PG_workingset))",
        "",
        "#define MIN_LRU_BATCH		BITS_PER_LONG",
        "#define MAX_LRU_BATCH		(MIN_LRU_BATCH * 64)",
        "",
        "/* whether to keep historical stats from evicted generations */",
        "#ifdef CONFIG_LRU_GEN_STATS",
        "#define NR_HIST_GENS		MAX_NR_GENS",
        "#else",
        "#define NR_HIST_GENS		1U",
        "#endif",
        "",
        "/*",
        " * The youngest generation number is stored in max_seq for both anon and file",
        " * types as they are aged on an equal footing. The oldest generation numbers are",
        " * stored in min_seq[] separately for anon and file types as clean file pages",
        " * can be evicted regardless of swap constraints.",
        " *",
        " * Normally anon and file min_seq are in sync. But if swapping is constrained,",
        " * e.g., out of swap space, file min_seq is allowed to advance and leave anon",
        " * min_seq behind.",
        " *",
        " * The number of pages in each generation is eventually consistent and therefore",
        " * can be transiently negative when reset_batch_size() is pending.",
        " */",
        "struct lru_gen_folio {",
        "	/* the aging increments the youngest generation number */",
        "	unsigned long max_seq;",
        "	/* the eviction increments the oldest generation numbers */",
        "	unsigned long min_seq[ANON_AND_FILE];",
        "	/* the birth time of each generation in jiffies */",
        "	unsigned long timestamps[MAX_NR_GENS];",
        "	/* the multi-gen LRU lists, lazily sorted on eviction */",
        "	struct list_head folios[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];",
        "	/* the multi-gen LRU sizes, eventually consistent */",
        "	long nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];",
        "	/* the exponential moving average of refaulted */",
        "	unsigned long avg_refaulted[ANON_AND_FILE][MAX_NR_TIERS];",
        "	/* the exponential moving average of evicted+protected */",
        "	unsigned long avg_total[ANON_AND_FILE][MAX_NR_TIERS];",
        "	/* the first tier doesn't need protection, hence the minus one */",
        "	unsigned long protected[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS - 1];",
        "	/* can be modified without holding the LRU lock */",
        "	atomic_long_t evicted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];",
        "	atomic_long_t refaulted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];",
        "	/* whether the multi-gen LRU is enabled */",
        "	bool enabled;",
        "	/* the memcg generation this lru_gen_folio belongs to */",
        "	u8 gen;",
        "	/* the list segment this lru_gen_folio belongs to */",
        "	u8 seg;",
        "	/* per-node lru_gen_folio list for global reclaim */",
        "	struct hlist_nulls_node list;",
        "};",
        "",
        "enum {",
        "	MM_LEAF_TOTAL,		/* total leaf entries */",
        "	MM_LEAF_YOUNG,		/* young leaf entries */",
        "	MM_NONLEAF_FOUND,	/* non-leaf entries found in Bloom filters */",
        "	MM_NONLEAF_ADDED,	/* non-leaf entries added to Bloom filters */",
        "	NR_MM_STATS",
        "};",
        "",
        "/* double-buffering Bloom filters */",
        "#define NR_BLOOM_FILTERS	2",
        "",
        "struct lru_gen_mm_state {",
        "	/* synced with max_seq after each iteration */",
        "	unsigned long seq;",
        "	/* where the current iteration continues after */",
        "	struct list_head *head;",
        "	/* where the last iteration ended before */",
        "	struct list_head *tail;",
        "	/* Bloom filters flip after each iteration */",
        "	unsigned long *filters[NR_BLOOM_FILTERS];",
        "	/* the mm stats for debugging */",
        "	unsigned long stats[NR_HIST_GENS][NR_MM_STATS];",
        "};",
        "",
        "struct lru_gen_mm_walk {",
        "	/* the lruvec under reclaim */",
        "	struct lruvec *lruvec;",
        "	/* max_seq from lru_gen_folio: can be out of date */",
        "	unsigned long seq;",
        "	/* the next address within an mm to scan */",
        "	unsigned long next_addr;",
        "	/* to batch promoted pages */",
        "	int nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];",
        "	/* to batch the mm stats */",
        "	int mm_stats[NR_MM_STATS];",
        "	/* total batched items */",
        "	int batched;",
        "	bool can_swap;",
        "	bool force_scan;",
        "};",
        "",
        "/*",
        " * For each node, memcgs are divided into two generations: the old and the",
        " * young. For each generation, memcgs are randomly sharded into multiple bins",
        " * to improve scalability. For each bin, the hlist_nulls is virtually divided",
        " * into three segments: the head, the tail and the default.",
        " *",
        " * An onlining memcg is added to the tail of a random bin in the old generation.",
        " * The eviction starts at the head of a random bin in the old generation. The",
        " * per-node memcg generation counter, whose reminder (mod MEMCG_NR_GENS) indexes",
        " * the old generation, is incremented when all its bins become empty.",
        " *",
        " * There are four operations:",
        " * 1. MEMCG_LRU_HEAD, which moves a memcg to the head of a random bin in its",
        " *    current generation (old or young) and updates its \"seg\" to \"head\";",
        " * 2. MEMCG_LRU_TAIL, which moves a memcg to the tail of a random bin in its",
        " *    current generation (old or young) and updates its \"seg\" to \"tail\";",
        " * 3. MEMCG_LRU_OLD, which moves a memcg to the head of a random bin in the old",
        " *    generation, updates its \"gen\" to \"old\" and resets its \"seg\" to \"default\";",
        " * 4. MEMCG_LRU_YOUNG, which moves a memcg to the tail of a random bin in the",
        " *    young generation, updates its \"gen\" to \"young\" and resets its \"seg\" to",
        " *    \"default\".",
        " *",
        " * The events that trigger the above operations are:",
        " * 1. Exceeding the soft limit, which triggers MEMCG_LRU_HEAD;",
        " * 2. The first attempt to reclaim a memcg below low, which triggers",
        " *    MEMCG_LRU_TAIL;",
        " * 3. The first attempt to reclaim a memcg offlined or below reclaimable size",
        " *    threshold, which triggers MEMCG_LRU_TAIL;",
        " * 4. The second attempt to reclaim a memcg offlined or below reclaimable size",
        " *    threshold, which triggers MEMCG_LRU_YOUNG;",
        " * 5. Attempting to reclaim a memcg below min, which triggers MEMCG_LRU_YOUNG;",
        " * 6. Finishing the aging on the eviction path, which triggers MEMCG_LRU_YOUNG;",
        " * 7. Offlining a memcg, which triggers MEMCG_LRU_OLD.",
        " *",
        " * Notes:",
        " * 1. Memcg LRU only applies to global reclaim, and the round-robin incrementing",
        " *    of their max_seq counters ensures the eventual fairness to all eligible",
        " *    memcgs. For memcg reclaim, it still relies on mem_cgroup_iter().",
        " * 2. There are only two valid generations: old (seq) and young (seq+1).",
        " *    MEMCG_NR_GENS is set to three so that when reading the generation counter",
        " *    locklessly, a stale value (seq-1) does not wraparound to young.",
        " */",
        "#define MEMCG_NR_GENS	3",
        "#define MEMCG_NR_BINS	8",
        "",
        "struct lru_gen_memcg {",
        "	/* the per-node memcg generation counter */",
        "	unsigned long seq;",
        "	/* each memcg has one lru_gen_folio per node */",
        "	unsigned long nr_memcgs[MEMCG_NR_GENS];",
        "	/* per-node lru_gen_folio list for global reclaim */",
        "	struct hlist_nulls_head	fifo[MEMCG_NR_GENS][MEMCG_NR_BINS];",
        "	/* protects the above */",
        "	spinlock_t lock;",
        "};",
        "",
        "void lru_gen_init_pgdat(struct pglist_data *pgdat);",
        "void lru_gen_init_lruvec(struct lruvec *lruvec);",
        "bool lru_gen_look_around(struct page_vma_mapped_walk *pvmw);",
        "",
        "void lru_gen_init_memcg(struct mem_cgroup *memcg);",
        "void lru_gen_exit_memcg(struct mem_cgroup *memcg);",
        "void lru_gen_online_memcg(struct mem_cgroup *memcg);",
        "void lru_gen_offline_memcg(struct mem_cgroup *memcg);",
        "void lru_gen_release_memcg(struct mem_cgroup *memcg);",
        "void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid);",
        "",
        "#else /* !CONFIG_LRU_GEN */",
        "",
        "static inline void lru_gen_init_pgdat(struct pglist_data *pgdat)",
        "{",
        "}",
        "",
        "static inline void lru_gen_init_lruvec(struct lruvec *lruvec)",
        "{",
        "}",
        "",
        "static inline bool lru_gen_look_around(struct page_vma_mapped_walk *pvmw)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline void lru_gen_init_memcg(struct mem_cgroup *memcg)",
        "{",
        "}",
        "",
        "static inline void lru_gen_exit_memcg(struct mem_cgroup *memcg)",
        "{",
        "}",
        "",
        "static inline void lru_gen_online_memcg(struct mem_cgroup *memcg)",
        "{",
        "}",
        "",
        "static inline void lru_gen_offline_memcg(struct mem_cgroup *memcg)",
        "{",
        "}",
        "",
        "static inline void lru_gen_release_memcg(struct mem_cgroup *memcg)",
        "{",
        "}",
        "",
        "static inline void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)",
        "{",
        "}",
        "",
        "#endif /* CONFIG_LRU_GEN */",
        "",
        "struct lruvec {",
        "	struct list_head		lists[NR_LRU_LISTS];",
        "	/* per lruvec lru_lock for memcg */",
        "	spinlock_t			lru_lock;",
        "	/*",
        "	 * These track the cost of reclaiming one LRU - file or anon -",
        "	 * over the other. As the observed cost of reclaiming one LRU",
        "	 * increases, the reclaim scan balance tips toward the other.",
        "	 */",
        "	unsigned long			anon_cost;",
        "	unsigned long			file_cost;",
        "	/* Non-resident age, driven by LRU movement */",
        "	atomic_long_t			nonresident_age;",
        "	/* Refaults at the time of last reclaim cycle */",
        "	unsigned long			refaults[ANON_AND_FILE];",
        "	/* Various lruvec state flags (enum lruvec_flags) */",
        "	unsigned long			flags;",
        "#ifdef CONFIG_LRU_GEN",
        "	/* evictable pages divided into generations */",
        "	struct lru_gen_folio		lrugen;",
        "#ifdef CONFIG_LRU_GEN_WALKS_MMU",
        "	/* to concurrently iterate lru_gen_mm_list */",
        "	struct lru_gen_mm_state		mm_state;",
        "#endif",
        "#endif /* CONFIG_LRU_GEN */",
        "#ifdef CONFIG_MEMCG",
        "	struct pglist_data *pgdat;",
        "#endif",
        "	struct zswap_lruvec_state zswap_lruvec_state;",
        "};",
        "",
        "/* Isolate for asynchronous migration */",
        "#define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x4)",
        "/* Isolate unevictable pages */",
        "#define ISOLATE_UNEVICTABLE	((__force isolate_mode_t)0x8)",
        "",
        "/* LRU Isolation modes. */",
        "typedef unsigned __bitwise isolate_mode_t;",
        "",
        "enum zone_watermarks {",
        "	WMARK_MIN,",
        "	WMARK_LOW,",
        "	WMARK_HIGH,",
        "	WMARK_PROMO,",
        "	NR_WMARK",
        "};",
        "",
        "/*",
        " * One per migratetype for each PAGE_ALLOC_COSTLY_ORDER. Two additional lists",
        " * are added for THP. One PCP list is used by GPF_MOVABLE, and the other PCP list",
        " * is used by GFP_UNMOVABLE and GFP_RECLAIMABLE.",
        " */",
        "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
        "#define NR_PCP_THP 2",
        "#else",
        "#define NR_PCP_THP 0",
        "#endif",
        "#define NR_LOWORDER_PCP_LISTS (MIGRATE_PCPTYPES * (PAGE_ALLOC_COSTLY_ORDER + 1))",
        "#define NR_PCP_LISTS (NR_LOWORDER_PCP_LISTS + NR_PCP_THP)",
        "",
        "/*",
        " * Flags used in pcp->flags field.",
        " *",
        " * PCPF_PREV_FREE_HIGH_ORDER: a high-order page is freed in the",
        " * previous page freeing.  To avoid to drain PCP for an accident",
        " * high-order page freeing.",
        " *",
        " * PCPF_FREE_HIGH_BATCH: preserve \"pcp->batch\" pages in PCP before",
        " * draining PCP for consecutive high-order pages freeing without",
        " * allocation if data cache slice of CPU is large enough.  To reduce",
        " * zone lock contention and keep cache-hot pages reusing.",
        " */",
        "#define	PCPF_PREV_FREE_HIGH_ORDER	BIT(0)",
        "#define	PCPF_FREE_HIGH_BATCH		BIT(1)",
        "",
        "struct per_cpu_pages {",
        "	spinlock_t lock;	/* Protects lists field */",
        "	int count;		/* number of pages in the list */",
        "	int high;		/* high watermark, emptying needed */",
        "	int high_min;		/* min high watermark */",
        "	int high_max;		/* max high watermark */",
        "	int batch;		/* chunk size for buddy add/remove */",
        "	u8 flags;		/* protected by pcp->lock */",
        "	u8 alloc_factor;	/* batch scaling factor during allocate */",
        "#ifdef CONFIG_NUMA",
        "	u8 expire;		/* When 0, remote pagesets are drained */",
        "#endif",
        "	short free_count;	/* consecutive free count */",
        "",
        "	/* Lists of pages, one per migrate type stored on the pcp-lists */",
        "	struct list_head lists[NR_PCP_LISTS];",
        "} ____cacheline_aligned_in_smp;",
        "",
        "struct per_cpu_zonestat {",
        "#ifdef CONFIG_SMP",
        "	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];",
        "	s8 stat_threshold;",
        "#endif",
        "#ifdef CONFIG_NUMA",
        "	/*",
        "	 * Low priority inaccurate counters that are only folded",
        "	 * on demand. Use a large type to avoid the overhead of",
        "	 * folding during refresh_cpu_vm_stats.",
        "	 */",
        "	unsigned long vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];",
        "#endif",
        "};",
        "",
        "struct per_cpu_nodestat {",
        "	s8 stat_threshold;",
        "	s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];",
        "};",
        "",
        "#endif /* !__GENERATING_BOUNDS.H */",
        "",
        "enum zone_type {",
        "	/*",
        "	 * ZONE_DMA and ZONE_DMA32 are used when there are peripherals not able",
        "	 * to DMA to all of the addressable memory (ZONE_NORMAL).",
        "	 * On architectures where this area covers the whole 32 bit address",
        "	 * space ZONE_DMA32 is used. ZONE_DMA is left for the ones with smaller",
        "	 * DMA addressing constraints. This distinction is important as a 32bit",
        "	 * DMA mask is assumed when ZONE_DMA32 is defined. Some 64-bit",
        "	 * platforms may need both zones as they support peripherals with",
        "	 * different DMA addressing limitations.",
        "	 */",
        "#ifdef CONFIG_ZONE_DMA",
        "	ZONE_DMA,",
        "#endif",
        "#ifdef CONFIG_ZONE_DMA32",
        "	ZONE_DMA32,",
        "#endif",
        "	/*",
        "	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be",
        "	 * performed on pages in ZONE_NORMAL if the DMA devices support",
        "	 * transfers to all addressable memory.",
        "	 */",
        "	ZONE_NORMAL,",
        "#ifdef CONFIG_HIGHMEM",
        "	/*",
        "	 * A memory area that is only addressable by the kernel through",
        "	 * mapping portions into its own address space. This is for example",
        "	 * used by i386 to allow the kernel to address the memory beyond",
        "	 * 900MB. The kernel will set up special mappings (page",
        "	 * table entries on i386) for each page that the kernel needs to",
        "	 * access.",
        "	 */",
        "	ZONE_HIGHMEM,",
        "#endif",
        "	/*",
        "	 * ZONE_MOVABLE is similar to ZONE_NORMAL, except that it contains",
        "	 * movable pages with few exceptional cases described below. Main use",
        "	 * cases for ZONE_MOVABLE are to make memory offlining/unplug more",
        "	 * likely to succeed, and to locally limit unmovable allocations - e.g.,",
        "	 * to increase the number of THP/huge pages. Notable special cases are:",
        "	 *",
        "	 * 1. Pinned pages: (long-term) pinning of movable pages might",
        "	 *    essentially turn such pages unmovable. Therefore, we do not allow",
        "	 *    pinning long-term pages in ZONE_MOVABLE. When pages are pinned and",
        "	 *    faulted, they come from the right zone right away. However, it is",
        "	 *    still possible that address space already has pages in",
        "	 *    ZONE_MOVABLE at the time when pages are pinned (i.e. user has",
        "	 *    touches that memory before pinning). In such case we migrate them",
        "	 *    to a different zone. When migration fails - pinning fails.",
        "	 * 2. memblock allocations: kernelcore/movablecore setups might create",
        "	 *    situations where ZONE_MOVABLE contains unmovable allocations",
        "	 *    after boot. Memory offlining and allocations fail early.",
        "	 * 3. Memory holes: kernelcore/movablecore setups might create very rare",
        "	 *    situations where ZONE_MOVABLE contains memory holes after boot,",
        "	 *    for example, if we have sections that are only partially",
        "	 *    populated. Memory offlining and allocations fail early.",
        "	 * 4. PG_hwpoison pages: while poisoned pages can be skipped during",
        "	 *    memory offlining, such pages cannot be allocated.",
        "	 * 5. Unmovable PG_offline pages: in paravirtualized environments,",
        "	 *    hotplugged memory blocks might only partially be managed by the",
        "	 *    buddy (e.g., via XEN-balloon, Hyper-V balloon, virtio-mem). The",
        "	 *    parts not manged by the buddy are unmovable PG_offline pages. In",
        "	 *    some cases (virtio-mem), such pages can be skipped during",
        "	 *    memory offlining, however, cannot be moved/allocated. These",
        "	 *    techniques might use alloc_contig_range() to hide previously",
        "	 *    exposed pages from the buddy again (e.g., to implement some sort",
        "	 *    of memory unplug in virtio-mem).",
        "	 * 6. ZERO_PAGE(0), kernelcore/movablecore setups might create",
        "	 *    situations where ZERO_PAGE(0) which is allocated differently",
        "	 *    on different platforms may end up in a movable zone. ZERO_PAGE(0)",
        "	 *    cannot be migrated.",
        "	 * 7. Memory-hotplug: when using memmap_on_memory and onlining the",
        "	 *    memory to the MOVABLE zone, the vmemmap pages are also placed in",
        "	 *    such zone. Such pages cannot be really moved around as they are",
        "	 *    self-stored in the range, but they are treated as movable when",
        "	 *    the range they describe is about to be offlined.",
        "	 *",
        "	 * In general, no unmovable allocations that degrade memory offlining",
        "	 * should end up in ZONE_MOVABLE. Allocators (like alloc_contig_range())",
        "	 * have to expect that migrating pages in ZONE_MOVABLE can fail (even",
        "	 * if has_unmovable_pages() states that there are no unmovable pages,",
        "	 * there can be false negatives).",
        "	 */",
        "	ZONE_MOVABLE,",
        "#ifdef CONFIG_ZONE_DEVICE",
        "	ZONE_DEVICE,",
        "#endif",
        "	__MAX_NR_ZONES",
        "",
        "};",
        "",
        "#ifndef __GENERATING_BOUNDS_H",
        "",
        "#define ASYNC_AND_SYNC 2",
        "",
        "struct zone {",
        "	/* Read-mostly fields */",
        "",
        "	/* zone watermarks, access with *_wmark_pages(zone) macros */",
        "	unsigned long _watermark[NR_WMARK];",
        "	unsigned long watermark_boost;",
        "",
        "	unsigned long nr_reserved_highatomic;",
        "	unsigned long nr_free_highatomic;",
        "",
        "	/*",
        "	 * We don't know if the memory that we're going to allocate will be",
        "	 * freeable or/and it will be released eventually, so to avoid totally",
        "	 * wasting several GB of ram we must reserve some of the lower zone",
        "	 * memory (otherwise we risk to run OOM on the lower zones despite",
        "	 * there being tons of freeable ram on the higher zones).  This array is",
        "	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl",
        "	 * changes.",
        "	 */",
        "	long lowmem_reserve[MAX_NR_ZONES];",
        "",
        "#ifdef CONFIG_NUMA",
        "	int node;",
        "#endif",
        "	struct pglist_data	*zone_pgdat;",
        "	struct per_cpu_pages	__percpu *per_cpu_pageset;",
        "	struct per_cpu_zonestat	__percpu *per_cpu_zonestats;",
        "	/*",
        "	 * the high and batch values are copied to individual pagesets for",
        "	 * faster access",
        "	 */",
        "	int pageset_high_min;",
        "	int pageset_high_max;",
        "	int pageset_batch;",
        "",
        "#ifndef CONFIG_SPARSEMEM",
        "	/*",
        "	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.",
        "	 * In SPARSEMEM, this map is stored in struct mem_section",
        "	 */",
        "	unsigned long		*pageblock_flags;",
        "#endif /* CONFIG_SPARSEMEM */",
        "",
        "	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */",
        "	unsigned long		zone_start_pfn;",
        "",
        "	/*",
        "	 * spanned_pages is the total pages spanned by the zone, including",
        "	 * holes, which is calculated as:",
        "	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;",
        "	 *",
        "	 * present_pages is physical pages existing within the zone, which",
        "	 * is calculated as:",
        "	 *	present_pages = spanned_pages - absent_pages(pages in holes);",
        "	 *",
        "	 * present_early_pages is present pages existing within the zone",
        "	 * located on memory available since early boot, excluding hotplugged",
        "	 * memory.",
        "	 *",
        "	 * managed_pages is present pages managed by the buddy system, which",
        "	 * is calculated as (reserved_pages includes pages allocated by the",
        "	 * bootmem allocator):",
        "	 *	managed_pages = present_pages - reserved_pages;",
        "	 *",
        "	 * cma pages is present pages that are assigned for CMA use",
        "	 * (MIGRATE_CMA).",
        "	 *",
        "	 * So present_pages may be used by memory hotplug or memory power",
        "	 * management logic to figure out unmanaged pages by checking",
        "	 * (present_pages - managed_pages). And managed_pages should be used",
        "	 * by page allocator and vm scanner to calculate all kinds of watermarks",
        "	 * and thresholds.",
        "	 *",
        "	 * Locking rules:",
        "	 *",
        "	 * zone_start_pfn and spanned_pages are protected by span_seqlock.",
        "	 * It is a seqlock because it has to be read outside of zone->lock,",
        "	 * and it is done in the main allocator path.  But, it is written",
        "	 * quite infrequently.",
        "	 *",
        "	 * The span_seq lock is declared along with zone->lock because it is",
        "	 * frequently read in proximity to zone->lock.  It's good to",
        "	 * give them a chance of being in the same cacheline.",
        "	 *",
        "	 * Write access to present_pages at runtime should be protected by",
        "	 * mem_hotplug_begin/done(). Any reader who can't tolerant drift of",
        "	 * present_pages should use get_online_mems() to get a stable value.",
        "	 */",
        "	atomic_long_t		managed_pages;",
        "	unsigned long		spanned_pages;",
        "	unsigned long		present_pages;",
        "#if defined(CONFIG_MEMORY_HOTPLUG)",
        "	unsigned long		present_early_pages;",
        "#endif",
        "#ifdef CONFIG_CMA",
        "	unsigned long		cma_pages;",
        "#endif",
        "",
        "	const char		*name;",
        "",
        "#ifdef CONFIG_MEMORY_ISOLATION",
        "	/*",
        "	 * Number of isolated pageblock. It is used to solve incorrect",
        "	 * freepage counting problem due to racy retrieving migratetype",
        "	 * of pageblock. Protected by zone->lock.",
        "	 */",
        "	unsigned long		nr_isolate_pageblock;",
        "#endif",
        "",
        "#ifdef CONFIG_MEMORY_HOTPLUG",
        "	/* see spanned/present_pages for more description */",
        "	seqlock_t		span_seqlock;",
        "#endif",
        "",
        "	int initialized;",
        "",
        "	/* Write-intensive fields used from the page allocator */",
        "	CACHELINE_PADDING(_pad1_);",
        "",
        "	/* free areas of different sizes */",
        "	struct free_area	free_area[NR_PAGE_ORDERS];",
        "",
        "#ifdef CONFIG_UNACCEPTED_MEMORY",
        "	/* Pages to be accepted. All pages on the list are MAX_PAGE_ORDER */",
        "	struct list_head	unaccepted_pages;",
        "#endif",
        "",
        "	/* zone flags, see below */",
        "	unsigned long		flags;",
        "",
        "	/* Primarily protects free_area */",
        "	spinlock_t		lock;",
        "",
        "	/* Write-intensive fields used by compaction and vmstats. */",
        "	CACHELINE_PADDING(_pad2_);",
        "",
        "	/*",
        "	 * When free pages are below this point, additional steps are taken",
        "	 * when reading the number of free pages to avoid per-cpu counter",
        "	 * drift allowing watermarks to be breached",
        "	 */",
        "	unsigned long percpu_drift_mark;",
        "",
        "#if defined CONFIG_COMPACTION || defined CONFIG_CMA",
        "	/* pfn where compaction free scanner should start */",
        "	unsigned long		compact_cached_free_pfn;",
        "	/* pfn where compaction migration scanner should start */",
        "	unsigned long		compact_cached_migrate_pfn[ASYNC_AND_SYNC];",
        "	unsigned long		compact_init_migrate_pfn;",
        "	unsigned long		compact_init_free_pfn;",
        "#endif",
        "",
        "#ifdef CONFIG_COMPACTION",
        "	/*",
        "	 * On compaction failure, 1<<compact_defer_shift compactions",
        "	 * are skipped before trying again. The number attempted since",
        "	 * last failure is tracked with compact_considered.",
        "	 * compact_order_failed is the minimum compaction failed order.",
        "	 */",
        "	unsigned int		compact_considered;",
        "	unsigned int		compact_defer_shift;",
        "	int			compact_order_failed;",
        "#endif",
        "",
        "#if defined CONFIG_COMPACTION || defined CONFIG_CMA",
        "	/* Set to true when the PG_migrate_skip bits should be cleared */",
        "	bool			compact_blockskip_flush;",
        "#endif",
        "",
        "	bool			contiguous;",
        "",
        "	CACHELINE_PADDING(_pad3_);",
        "	/* Zone statistics */",
        "	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];",
        "	atomic_long_t		vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];",
        "} ____cacheline_internodealigned_in_smp;",
        "",
        "enum pgdat_flags {",
        "	PGDAT_DIRTY,			/* reclaim scanning has recently found",
        "					 * many dirty file pages at the tail",
        "					 * of the LRU.",
        "					 */",
        "	PGDAT_WRITEBACK,		/* reclaim scanning has recently found",
        "					 * many pages under writeback",
        "					 */",
        "	PGDAT_RECLAIM_LOCKED,		/* prevents concurrent reclaim */",
        "};",
        "",
        "enum zone_flags {",
        "	ZONE_BOOSTED_WATERMARK,		/* zone recently boosted watermarks.",
        "					 * Cleared when kswapd is woken.",
        "					 */",
        "	ZONE_RECLAIM_ACTIVE,		/* kswapd may be scanning the zone. */",
        "	ZONE_BELOW_HIGH,		/* zone is below high watermark. */",
        "};",
        "",
        "static inline unsigned long wmark_pages(const struct zone *z,",
        "					enum zone_watermarks w)",
        "{",
        "	return z->_watermark[w] + z->watermark_boost;",
        "}",
        "",
        "static inline unsigned long min_wmark_pages(const struct zone *z)",
        "{",
        "	return wmark_pages(z, WMARK_MIN);",
        "}",
        "",
        "static inline unsigned long low_wmark_pages(const struct zone *z)",
        "{",
        "	return wmark_pages(z, WMARK_LOW);",
        "}",
        "",
        "static inline unsigned long high_wmark_pages(const struct zone *z)",
        "{",
        "	return wmark_pages(z, WMARK_HIGH);",
        "}",
        "",
        "static inline unsigned long promo_wmark_pages(const struct zone *z)",
        "{",
        "	return wmark_pages(z, WMARK_PROMO);",
        "}",
        "",
        "static inline unsigned long zone_managed_pages(struct zone *zone)",
        "{",
        "	return (unsigned long)atomic_long_read(&zone->managed_pages);",
        "}",
        "",
        "static inline unsigned long zone_cma_pages(struct zone *zone)",
        "{",
        "#ifdef CONFIG_CMA",
        "	return zone->cma_pages;",
        "#else",
        "	return 0;",
        "#endif",
        "}",
        "",
        "static inline unsigned long zone_end_pfn(const struct zone *zone)",
        "{",
        "	return zone->zone_start_pfn + zone->spanned_pages;",
        "}",
        "",
        "static inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)",
        "{",
        "	return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);",
        "}",
        "",
        "static inline bool zone_is_initialized(struct zone *zone)",
        "{",
        "	return zone->initialized;",
        "}",
        "",
        "static inline bool zone_is_empty(struct zone *zone)",
        "{",
        "	return zone->spanned_pages == 0;",
        "}",
        "",
        "#ifndef BUILD_VDSO32_64",
        "/*",
        " * The zone field is never updated after free_area_init_core()",
        " * sets it, so none of the operations on it need to be atomic.",
        " */",
        "",
        "/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */",
        "#define SECTIONS_PGOFF		((sizeof(unsigned long)*8) - SECTIONS_WIDTH)",
        "#define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)",
        "#define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)",
        "#define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)",
        "#define KASAN_TAG_PGOFF		(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)",
        "#define LRU_GEN_PGOFF		(KASAN_TAG_PGOFF - LRU_GEN_WIDTH)",
        "#define LRU_REFS_PGOFF		(LRU_GEN_PGOFF - LRU_REFS_WIDTH)",
        "",
        "/*",
        " * Define the bit shifts to access each section.  For non-existent",
        " * sections we define the shift as 0; that plus a 0 mask ensures",
        " * the compiler will optimise away reference to them.",
        " */",
        "#define SECTIONS_PGSHIFT	(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))",
        "#define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))",
        "#define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))",
        "#define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))",
        "#define KASAN_TAG_PGSHIFT	(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))",
        "",
        "/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */",
        "#ifdef NODE_NOT_IN_PAGE_FLAGS",
        "#define ZONEID_SHIFT		(SECTIONS_SHIFT + ZONES_SHIFT)",
        "#define ZONEID_PGOFF		((SECTIONS_PGOFF < ZONES_PGOFF) ? \\",
        "						SECTIONS_PGOFF : ZONES_PGOFF)",
        "#else",
        "#define ZONEID_SHIFT		(NODES_SHIFT + ZONES_SHIFT)",
        "#define ZONEID_PGOFF		((NODES_PGOFF < ZONES_PGOFF) ? \\",
        "						NODES_PGOFF : ZONES_PGOFF)",
        "#endif",
        "",
        "#define ZONEID_PGSHIFT		(ZONEID_PGOFF * (ZONEID_SHIFT != 0))",
        "",
        "#define ZONES_MASK		((1UL << ZONES_WIDTH) - 1)",
        "#define NODES_MASK		((1UL << NODES_WIDTH) - 1)",
        "#define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)",
        "#define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_SHIFT) - 1)",
        "#define KASAN_TAG_MASK		((1UL << KASAN_TAG_WIDTH) - 1)",
        "#define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)",
        "",
        "static inline enum zone_type page_zonenum(const struct page *page)",
        "{",
        "	ASSERT_EXCLUSIVE_BITS(page->flags, ZONES_MASK << ZONES_PGSHIFT);",
        "	return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;",
        "}",
        "",
        "static inline enum zone_type folio_zonenum(const struct folio *folio)",
        "{",
        "	return page_zonenum(&folio->page);",
        "}",
        "",
        "#ifdef CONFIG_ZONE_DEVICE",
        "static inline bool is_zone_device_page(const struct page *page)",
        "{",
        "	return page_zonenum(page) == ZONE_DEVICE;",
        "}",
        "",
        "/*",
        " * Consecutive zone device pages should not be merged into the same sgl",
        " * or bvec segment with other types of pages or if they belong to different",
        " * pgmaps. Otherwise getting the pgmap of a given segment is not possible",
        " * without scanning the entire segment. This helper returns true either if",
        " * both pages are not zone device pages or both pages are zone device pages",
        " * with the same pgmap.",
        " */",
        "static inline bool zone_device_pages_have_same_pgmap(const struct page *a,",
        "						     const struct page *b)",
        "{",
        "	if (is_zone_device_page(a) != is_zone_device_page(b))",
        "		return false;",
        "	if (!is_zone_device_page(a))",
        "		return true;",
        "	return a->pgmap == b->pgmap;",
        "}",
        "",
        "extern void memmap_init_zone_device(struct zone *, unsigned long,",
        "				    unsigned long, struct dev_pagemap *);",
        "#else",
        "static inline bool is_zone_device_page(const struct page *page)",
        "{",
        "	return false;",
        "}",
        "static inline bool zone_device_pages_have_same_pgmap(const struct page *a,",
        "						     const struct page *b)",
        "{",
        "	return true;",
        "}",
        "#endif",
        "",
        "static inline bool folio_is_zone_device(const struct folio *folio)",
        "{",
        "	return is_zone_device_page(&folio->page);",
        "}",
        "",
        "static inline bool is_zone_movable_page(const struct page *page)",
        "{",
        "	return page_zonenum(page) == ZONE_MOVABLE;",
        "}",
        "",
        "static inline bool folio_is_zone_movable(const struct folio *folio)",
        "{",
        "	return folio_zonenum(folio) == ZONE_MOVABLE;",
        "}",
        "#endif",
        "",
        "/*",
        " * Return true if [start_pfn, start_pfn + nr_pages) range has a non-empty",
        " * intersection with the given zone",
        " */",
        "static inline bool zone_intersects(struct zone *zone,",
        "		unsigned long start_pfn, unsigned long nr_pages)",
        "{",
        "	if (zone_is_empty(zone))",
        "		return false;",
        "	if (start_pfn >= zone_end_pfn(zone) ||",
        "	    start_pfn + nr_pages <= zone->zone_start_pfn)",
        "		return false;",
        "",
        "	return true;",
        "}",
        "",
        "/*",
        " * The \"priority\" of VM scanning is how much of the queues we will scan in one",
        " * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the",
        " * queues (\"queue_length >> 12\") during an aging round.",
        " */",
        "#define DEF_PRIORITY 12",
        "",
        "/* Maximum number of zones on a zonelist */",
        "#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)",
        "",
        "enum {",
        "	ZONELIST_FALLBACK,	/* zonelist with fallback */",
        "#ifdef CONFIG_NUMA",
        "	/*",
        "	 * The NUMA zonelists are doubled because we need zonelists that",
        "	 * restrict the allocations to a single node for __GFP_THISNODE.",
        "	 */",
        "	ZONELIST_NOFALLBACK,	/* zonelist without fallback (__GFP_THISNODE) */",
        "#endif",
        "	MAX_ZONELISTS",
        "};",
        "",
        "/*",
        " * This struct contains information about a zone in a zonelist. It is stored",
        " * here to avoid dereferences into large structures and lookups of tables",
        " */",
        "struct zoneref {",
        "	struct zone *zone;	/* Pointer to actual zone */",
        "	int zone_idx;		/* zone_idx(zoneref->zone) */",
        "};",
        "",
        "/*",
        " * One allocation request operates on a zonelist. A zonelist",
        " * is a list of zones, the first one is the 'goal' of the",
        " * allocation, the other zones are fallback zones, in decreasing",
        " * priority.",
        " *",
        " * To speed the reading of the zonelist, the zonerefs contain the zone index",
        " * of the entry being read. Helper functions to access information given",
        " * a struct zoneref are",
        " *",
        " * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs",
        " * zonelist_zone_idx()	- Return the index of the zone for an entry",
        " * zonelist_node_idx()	- Return the index of the node for an entry",
        " */",
        "struct zonelist {",
        "	struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];",
        "};",
        "",
        "/*",
        " * The array of struct pages for flatmem.",
        " * It must be declared for SPARSEMEM as well because there are configurations",
        " * that rely on that.",
        " */",
        "extern struct page *mem_map;",
        "",
        "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
        "struct deferred_split {",
        "	spinlock_t split_queue_lock;",
        "	struct list_head split_queue;",
        "	unsigned long split_queue_len;",
        "};",
        "#endif",
        "",
        "#ifdef CONFIG_MEMORY_FAILURE",
        "/*",
        " * Per NUMA node memory failure handling statistics.",
        " */",
        "struct memory_failure_stats {",
        "	/*",
        "	 * Number of raw pages poisoned.",
        "	 * Cases not accounted: memory outside kernel control, offline page,",
        "	 * arch-specific memory_failure (SGX), hwpoison_filter() filtered",
        "	 * error events, and unpoison actions from hwpoison_unpoison.",
        "	 */",
        "	unsigned long total;",
        "	/*",
        "	 * Recovery results of poisoned raw pages handled by memory_failure,",
        "	 * in sync with mf_result.",
        "	 * total = ignored + failed + delayed + recovered.",
        "	 * total * PAGE_SIZE * #nodes = /proc/meminfo/HardwareCorrupted.",
        "	 */",
        "	unsigned long ignored;",
        "	unsigned long failed;",
        "	unsigned long delayed;",
        "	unsigned long recovered;",
        "};",
        "#endif",
        "",
        "/*",
        " * On NUMA machines, each NUMA node would have a pg_data_t to describe",
        " * it's memory layout. On UMA machines there is a single pglist_data which",
        " * describes the whole memory.",
        " *",
        " * Memory statistics and page replacement data structures are maintained on a",
        " * per-zone basis.",
        " */",
        "typedef struct pglist_data {",
        "	/*",
        "	 * node_zones contains just the zones for THIS node. Not all of the",
        "	 * zones may be populated, but it is the full list. It is referenced by",
        "	 * this node's node_zonelists as well as other node's node_zonelists.",
        "	 */",
        "	struct zone node_zones[MAX_NR_ZONES];",
        "",
        "	/*",
        "	 * node_zonelists contains references to all zones in all nodes.",
        "	 * Generally the first zones will be references to this node's",
        "	 * node_zones.",
        "	 */",
        "	struct zonelist node_zonelists[MAX_ZONELISTS];",
        "",
        "	int nr_zones; /* number of populated zones in this node */",
        "#ifdef CONFIG_FLATMEM	/* means !SPARSEMEM */",
        "	struct page *node_mem_map;",
        "#ifdef CONFIG_PAGE_EXTENSION",
        "	struct page_ext *node_page_ext;",
        "#endif",
        "#endif",
        "#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)",
        "	/*",
        "	 * Must be held any time you expect node_start_pfn,",
        "	 * node_present_pages, node_spanned_pages or nr_zones to stay constant.",
        "	 * Also synchronizes pgdat->first_deferred_pfn during deferred page",
        "	 * init.",
        "	 *",
        "	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to",
        "	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG",
        "	 * or CONFIG_DEFERRED_STRUCT_PAGE_INIT.",
        "	 *",
        "	 * Nests above zone->lock and zone->span_seqlock",
        "	 */",
        "	spinlock_t node_size_lock;",
        "#endif",
        "	unsigned long node_start_pfn;",
        "	unsigned long node_present_pages; /* total number of physical pages */",
        "	unsigned long node_spanned_pages; /* total size of physical page",
        "					     range, including holes */",
        "	int node_id;",
        "	wait_queue_head_t kswapd_wait;",
        "	wait_queue_head_t pfmemalloc_wait;",
        "",
        "	/* workqueues for throttling reclaim for different reasons. */",
        "	wait_queue_head_t reclaim_wait[NR_VMSCAN_THROTTLE];",
        "",
        "	atomic_t nr_writeback_throttled;/* nr of writeback-throttled tasks */",
        "	unsigned long nr_reclaim_start;	/* nr pages written while throttled",
        "					 * when throttling started. */",
        "#ifdef CONFIG_MEMORY_HOTPLUG",
        "	struct mutex kswapd_lock;",
        "#endif",
        "	struct task_struct *kswapd;	/* Protected by kswapd_lock */",
        "	int kswapd_order;",
        "	enum zone_type kswapd_highest_zoneidx;",
        "",
        "	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */",
        "",
        "#ifdef CONFIG_COMPACTION",
        "	int kcompactd_max_order;",
        "	enum zone_type kcompactd_highest_zoneidx;",
        "	wait_queue_head_t kcompactd_wait;",
        "	struct task_struct *kcompactd;",
        "	bool proactive_compact_trigger;",
        "#endif",
        "	/*",
        "	 * This is a per-node reserve of pages that are not available",
        "	 * to userspace allocations.",
        "	 */",
        "	unsigned long		totalreserve_pages;",
        "",
        "#ifdef CONFIG_NUMA",
        "	/*",
        "	 * node reclaim becomes active if more unmapped pages exist.",
        "	 */",
        "	unsigned long		min_unmapped_pages;",
        "	unsigned long		min_slab_pages;",
        "#endif /* CONFIG_NUMA */",
        "",
        "	/* Write-intensive fields used by page reclaim */",
        "	CACHELINE_PADDING(_pad1_);",
        "",
        "#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT",
        "	/*",
        "	 * If memory initialisation on large machines is deferred then this",
        "	 * is the first PFN that needs to be initialised.",
        "	 */",
        "	unsigned long first_deferred_pfn;",
        "#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */",
        "",
        "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
        "	struct deferred_split deferred_split_queue;",
        "#endif",
        "",
        "#ifdef CONFIG_NUMA_BALANCING",
        "	/* start time in ms of current promote rate limit period */",
        "	unsigned int nbp_rl_start;",
        "	/* number of promote candidate pages at start time of current rate limit period */",
        "	unsigned long nbp_rl_nr_cand;",
        "	/* promote threshold in ms */",
        "	unsigned int nbp_threshold;",
        "	/* start time in ms of current promote threshold adjustment period */",
        "	unsigned int nbp_th_start;",
        "	/*",
        "	 * number of promote candidate pages at start time of current promote",
        "	 * threshold adjustment period",
        "	 */",
        "	unsigned long nbp_th_nr_cand;",
        "#endif",
        "	/* Fields commonly accessed by the page reclaim scanner */",
        "",
        "	/*",
        "	 * NOTE: THIS IS UNUSED IF MEMCG IS ENABLED.",
        "	 *",
        "	 * Use mem_cgroup_lruvec() to look up lruvecs.",
        "	 */",
        "	struct lruvec		__lruvec;",
        "",
        "	unsigned long		flags;",
        "",
        "#ifdef CONFIG_LRU_GEN",
        "	/* kswap mm walk data */",
        "	struct lru_gen_mm_walk mm_walk;",
        "	/* lru_gen_folio list */",
        "	struct lru_gen_memcg memcg_lru;",
        "#endif",
        "",
        "	CACHELINE_PADDING(_pad2_);",
        "",
        "	/* Per-node vmstats */",
        "	struct per_cpu_nodestat __percpu *per_cpu_nodestats;",
        "	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];",
        "#ifdef CONFIG_NUMA",
        "	struct memory_tier __rcu *memtier;",
        "#endif",
        "#ifdef CONFIG_MEMORY_FAILURE",
        "	struct memory_failure_stats mf_stats;",
        "#endif",
        "} pg_data_t;",
        "",
        "#define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)",
        "#define node_spanned_pages(nid)	(NODE_DATA(nid)->node_spanned_pages)",
        "",
        "#define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)",
        "#define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))",
        "",
        "static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)",
        "{",
        "	return pgdat->node_start_pfn + pgdat->node_spanned_pages;",
        "}",
        "",
        "#include <linux/memory_hotplug.h>",
        "",
        "void build_all_zonelists(pg_data_t *pgdat);",
        "void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,",
        "		   enum zone_type highest_zoneidx);",
        "bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,",
        "			 int highest_zoneidx, unsigned int alloc_flags,",
        "			 long free_pages);",
        "bool zone_watermark_ok(struct zone *z, unsigned int order,",
        "		unsigned long mark, int highest_zoneidx,",
        "		unsigned int alloc_flags);",
        "bool zone_watermark_ok_safe(struct zone *z, unsigned int order,",
        "		unsigned long mark, int highest_zoneidx);",
        "/*",
        " * Memory initialization context, use to differentiate memory added by",
        " * the platform statically or via memory hotplug interface.",
        " */",
        "enum meminit_context {",
        "	MEMINIT_EARLY,",
        "	MEMINIT_HOTPLUG,",
        "};",
        "",
        "extern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,",
        "				     unsigned long size);",
        "",
        "extern void lruvec_init(struct lruvec *lruvec);",
        "",
        "static inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)",
        "{",
        "#ifdef CONFIG_MEMCG",
        "	return lruvec->pgdat;",
        "#else",
        "	return container_of(lruvec, struct pglist_data, __lruvec);",
        "#endif",
        "}",
        "",
        "#ifdef CONFIG_HAVE_MEMORYLESS_NODES",
        "int local_memory_node(int node_id);",
        "#else",
        "static inline int local_memory_node(int node_id) { return node_id; };",
        "#endif",
        "",
        "/*",
        " * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.",
        " */",
        "#define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)",
        "",
        "#ifdef CONFIG_ZONE_DEVICE",
        "static inline bool zone_is_zone_device(struct zone *zone)",
        "{",
        "	return zone_idx(zone) == ZONE_DEVICE;",
        "}",
        "#else",
        "static inline bool zone_is_zone_device(struct zone *zone)",
        "{",
        "	return false;",
        "}",
        "#endif",
        "",
        "/*",
        " * Returns true if a zone has pages managed by the buddy allocator.",
        " * All the reclaim decisions have to use this function rather than",
        " * populated_zone(). If the whole zone is reserved then we can easily",
        " * end up with populated_zone() && !managed_zone().",
        " */",
        "static inline bool managed_zone(struct zone *zone)",
        "{",
        "	return zone_managed_pages(zone);",
        "}",
        "",
        "/* Returns true if a zone has memory */",
        "static inline bool populated_zone(struct zone *zone)",
        "{",
        "	return zone->present_pages;",
        "}",
        "",
        "#ifdef CONFIG_NUMA",
        "static inline int zone_to_nid(struct zone *zone)",
        "{",
        "	return zone->node;",
        "}",
        "",
        "static inline void zone_set_nid(struct zone *zone, int nid)",
        "{",
        "	zone->node = nid;",
        "}",
        "#else",
        "static inline int zone_to_nid(struct zone *zone)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void zone_set_nid(struct zone *zone, int nid) {}",
        "#endif",
        "",
        "extern int movable_zone;",
        "",
        "static inline int is_highmem_idx(enum zone_type idx)",
        "{",
        "#ifdef CONFIG_HIGHMEM",
        "	return (idx == ZONE_HIGHMEM ||",
        "		(idx == ZONE_MOVABLE && movable_zone == ZONE_HIGHMEM));",
        "#else",
        "	return 0;",
        "#endif",
        "}",
        "",
        "/**",
        " * is_highmem - helper function to quickly check if a struct zone is a",
        " *              highmem zone or not.  This is an attempt to keep references",
        " *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.",
        " * @zone: pointer to struct zone variable",
        " * Return: 1 for a highmem zone, 0 otherwise",
        " */",
        "static inline int is_highmem(struct zone *zone)",
        "{",
        "	return is_highmem_idx(zone_idx(zone));",
        "}",
        "",
        "#ifdef CONFIG_ZONE_DMA",
        "bool has_managed_dma(void);",
        "#else",
        "static inline bool has_managed_dma(void)",
        "{",
        "	return false;",
        "}",
        "#endif",
        "",
        "",
        "#ifndef CONFIG_NUMA",
        "",
        "extern struct pglist_data contig_page_data;",
        "static inline struct pglist_data *NODE_DATA(int nid)",
        "{",
        "	return &contig_page_data;",
        "}",
        "",
        "#else /* CONFIG_NUMA */",
        "",
        "#include <asm/mmzone.h>",
        "",
        "#endif /* !CONFIG_NUMA */",
        "",
        "extern struct pglist_data *first_online_pgdat(void);",
        "extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);",
        "extern struct zone *next_zone(struct zone *zone);",
        "",
        "/**",
        " * for_each_online_pgdat - helper macro to iterate over all online nodes",
        " * @pgdat: pointer to a pg_data_t variable",
        " */",
        "#define for_each_online_pgdat(pgdat)			\\",
        "	for (pgdat = first_online_pgdat();		\\",
        "	     pgdat;					\\",
        "	     pgdat = next_online_pgdat(pgdat))",
        "/**",
        " * for_each_zone - helper macro to iterate over all memory zones",
        " * @zone: pointer to struct zone variable",
        " *",
        " * The user only needs to declare the zone variable, for_each_zone",
        " * fills it in.",
        " */",
        "#define for_each_zone(zone)			        \\",
        "	for (zone = (first_online_pgdat())->node_zones; \\",
        "	     zone;					\\",
        "	     zone = next_zone(zone))",
        "",
        "#define for_each_populated_zone(zone)		        \\",
        "	for (zone = (first_online_pgdat())->node_zones; \\",
        "	     zone;					\\",
        "	     zone = next_zone(zone))			\\",
        "		if (!populated_zone(zone))		\\",
        "			; /* do nothing */		\\",
        "		else",
        "",
        "static inline struct zone *zonelist_zone(struct zoneref *zoneref)",
        "{",
        "	return zoneref->zone;",
        "}",
        "",
        "static inline int zonelist_zone_idx(struct zoneref *zoneref)",
        "{",
        "	return zoneref->zone_idx;",
        "}",
        "",
        "static inline int zonelist_node_idx(struct zoneref *zoneref)",
        "{",
        "	return zone_to_nid(zoneref->zone);",
        "}",
        "",
        "struct zoneref *__next_zones_zonelist(struct zoneref *z,",
        "					enum zone_type highest_zoneidx,",
        "					nodemask_t *nodes);",
        "",
        "/**",
        " * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point",
        " * @z: The cursor used as a starting point for the search",
        " * @highest_zoneidx: The zone index of the highest zone to return",
        " * @nodes: An optional nodemask to filter the zonelist with",
        " *",
        " * This function returns the next zone at or below a given zone index that is",
        " * within the allowed nodemask using a cursor as the starting point for the",
        " * search. The zoneref returned is a cursor that represents the current zone",
        " * being examined. It should be advanced by one before calling",
        " * next_zones_zonelist again.",
        " *",
        " * Return: the next zone at or below highest_zoneidx within the allowed",
        " * nodemask using a cursor within a zonelist as a starting point",
        " */",
        "static __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,",
        "					enum zone_type highest_zoneidx,",
        "					nodemask_t *nodes)",
        "{",
        "	if (likely(!nodes && zonelist_zone_idx(z) <= highest_zoneidx))",
        "		return z;",
        "	return __next_zones_zonelist(z, highest_zoneidx, nodes);",
        "}",
        "",
        "/**",
        " * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist",
        " * @zonelist: The zonelist to search for a suitable zone",
        " * @highest_zoneidx: The zone index of the highest zone to return",
        " * @nodes: An optional nodemask to filter the zonelist with",
        " *",
        " * This function returns the first zone at or below a given zone index that is",
        " * within the allowed nodemask. The zoneref returned is a cursor that can be",
        " * used to iterate the zonelist with next_zones_zonelist by advancing it by",
        " * one before calling.",
        " *",
        " * When no eligible zone is found, zoneref->zone is NULL (zoneref itself is",
        " * never NULL). This may happen either genuinely, or due to concurrent nodemask",
        " * update due to cpuset modification.",
        " *",
        " * Return: Zoneref pointer for the first suitable zone found",
        " */",
        "static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,",
        "					enum zone_type highest_zoneidx,",
        "					nodemask_t *nodes)",
        "{",
        "	return next_zones_zonelist(zonelist->_zonerefs,",
        "							highest_zoneidx, nodes);",
        "}",
        "",
        "/**",
        " * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask",
        " * @zone: The current zone in the iterator",
        " * @z: The current pointer within zonelist->_zonerefs being iterated",
        " * @zlist: The zonelist being iterated",
        " * @highidx: The zone index of the highest zone to return",
        " * @nodemask: Nodemask allowed by the allocator",
        " *",
        " * This iterator iterates though all zones at or below a given zone index and",
        " * within a given nodemask",
        " */",
        "#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \\",
        "	for (z = first_zones_zonelist(zlist, highidx, nodemask), zone = zonelist_zone(z);	\\",
        "		zone;							\\",
        "		z = next_zones_zonelist(++z, highidx, nodemask),	\\",
        "			zone = zonelist_zone(z))",
        "",
        "#define for_next_zone_zonelist_nodemask(zone, z, highidx, nodemask) \\",
        "	for (zone = zonelist_zone(z);	\\",
        "		zone;							\\",
        "		z = next_zones_zonelist(++z, highidx, nodemask),	\\",
        "			zone = zonelist_zone(z))",
        "",
        "",
        "/**",
        " * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index",
        " * @zone: The current zone in the iterator",
        " * @z: The current pointer within zonelist->zones being iterated",
        " * @zlist: The zonelist being iterated",
        " * @highidx: The zone index of the highest zone to return",
        " *",
        " * This iterator iterates though all zones at or below a given zone index.",
        " */",
        "#define for_each_zone_zonelist(zone, z, zlist, highidx) \\",
        "	for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, NULL)",
        "",
        "/* Whether the 'nodes' are all movable nodes */",
        "static inline bool movable_only_nodes(nodemask_t *nodes)",
        "{",
        "	struct zonelist *zonelist;",
        "	struct zoneref *z;",
        "	int nid;",
        "",
        "	if (nodes_empty(*nodes))",
        "		return false;",
        "",
        "	/*",
        "	 * We can chose arbitrary node from the nodemask to get a",
        "	 * zonelist as they are interlinked. We just need to find",
        "	 * at least one zone that can satisfy kernel allocations.",
        "	 */",
        "	nid = first_node(*nodes);",
        "	zonelist = &NODE_DATA(nid)->node_zonelists[ZONELIST_FALLBACK];",
        "	z = first_zones_zonelist(zonelist, ZONE_NORMAL,	nodes);",
        "	return (!zonelist_zone(z)) ? true : false;",
        "}",
        "",
        "",
        "#ifdef CONFIG_SPARSEMEM",
        "#include <asm/sparsemem.h>",
        "#endif",
        "",
        "#ifdef CONFIG_FLATMEM",
        "#define pfn_to_nid(pfn)		(0)",
        "#endif",
        "",
        "#ifdef CONFIG_SPARSEMEM",
        "",
        "/*",
        " * PA_SECTION_SHIFT		physical address to/from section number",
        " * PFN_SECTION_SHIFT		pfn to/from section number",
        " */",
        "#define PA_SECTION_SHIFT	(SECTION_SIZE_BITS)",
        "#define PFN_SECTION_SHIFT	(SECTION_SIZE_BITS - PAGE_SHIFT)",
        "",
        "#define NR_MEM_SECTIONS		(1UL << SECTIONS_SHIFT)",
        "",
        "#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)",
        "#define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))",
        "",
        "#define SECTION_BLOCKFLAGS_BITS \\",
        "	((1UL << (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)",
        "",
        "#if (MAX_PAGE_ORDER + PAGE_SHIFT) > SECTION_SIZE_BITS",
        "#error Allocator MAX_PAGE_ORDER exceeds SECTION_SIZE",
        "#endif",
        "",
        "static inline unsigned long pfn_to_section_nr(unsigned long pfn)",
        "{",
        "	return pfn >> PFN_SECTION_SHIFT;",
        "}",
        "static inline unsigned long section_nr_to_pfn(unsigned long sec)",
        "{",
        "	return sec << PFN_SECTION_SHIFT;",
        "}",
        "",
        "#define SECTION_ALIGN_UP(pfn)	(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)",
        "#define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)",
        "",
        "#define SUBSECTION_SHIFT 21",
        "#define SUBSECTION_SIZE (1UL << SUBSECTION_SHIFT)",
        "",
        "#define PFN_SUBSECTION_SHIFT (SUBSECTION_SHIFT - PAGE_SHIFT)",
        "#define PAGES_PER_SUBSECTION (1UL << PFN_SUBSECTION_SHIFT)",
        "#define PAGE_SUBSECTION_MASK (~(PAGES_PER_SUBSECTION-1))",
        "",
        "#if SUBSECTION_SHIFT > SECTION_SIZE_BITS",
        "#error Subsection size exceeds section size",
        "#else",
        "#define SUBSECTIONS_PER_SECTION (1UL << (SECTION_SIZE_BITS - SUBSECTION_SHIFT))",
        "#endif",
        "",
        "#define SUBSECTION_ALIGN_UP(pfn) ALIGN((pfn), PAGES_PER_SUBSECTION)",
        "#define SUBSECTION_ALIGN_DOWN(pfn) ((pfn) & PAGE_SUBSECTION_MASK)",
        "",
        "struct mem_section_usage {",
        "	struct rcu_head rcu;",
        "#ifdef CONFIG_SPARSEMEM_VMEMMAP",
        "	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);",
        "#endif",
        "	/* See declaration of similar field in struct zone */",
        "	unsigned long pageblock_flags[0];",
        "};",
        "",
        "void subsection_map_init(unsigned long pfn, unsigned long nr_pages);",
        "",
        "struct page;",
        "struct page_ext;",
        "struct mem_section {",
        "	/*",
        "	 * This is, logically, a pointer to an array of struct",
        "	 * pages.  However, it is stored with some other magic.",
        "	 * (see sparse.c::sparse_init_one_section())",
        "	 *",
        "	 * Additionally during early boot we encode node id of",
        "	 * the location of the section here to guide allocation.",
        "	 * (see sparse.c::memory_present())",
        "	 *",
        "	 * Making it a UL at least makes someone do a cast",
        "	 * before using it wrong.",
        "	 */",
        "	unsigned long section_mem_map;",
        "",
        "	struct mem_section_usage *usage;",
        "#ifdef CONFIG_PAGE_EXTENSION",
        "	/*",
        "	 * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use",
        "	 * section. (see page_ext.h about this.)",
        "	 */",
        "	struct page_ext *page_ext;",
        "	unsigned long pad;",
        "#endif",
        "	/*",
        "	 * WARNING: mem_section must be a power-of-2 in size for the",
        "	 * calculation and use of SECTION_ROOT_MASK to make sense.",
        "	 */",
        "};",
        "",
        "#ifdef CONFIG_SPARSEMEM_EXTREME",
        "#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))",
        "#else",
        "#define SECTIONS_PER_ROOT	1",
        "#endif",
        "",
        "#define SECTION_NR_TO_ROOT(sec)	((sec) / SECTIONS_PER_ROOT)",
        "#define NR_SECTION_ROOTS	DIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)",
        "#define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)",
        "",
        "#ifdef CONFIG_SPARSEMEM_EXTREME",
        "extern struct mem_section **mem_section;",
        "#else",
        "extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];",
        "#endif",
        "",
        "static inline unsigned long *section_to_usemap(struct mem_section *ms)",
        "{",
        "	return ms->usage->pageblock_flags;",
        "}",
        "",
        "static inline struct mem_section *__nr_to_section(unsigned long nr)",
        "{",
        "	unsigned long root = SECTION_NR_TO_ROOT(nr);",
        "",
        "	if (unlikely(root >= NR_SECTION_ROOTS))",
        "		return NULL;",
        "",
        "#ifdef CONFIG_SPARSEMEM_EXTREME",
        "	if (!mem_section || !mem_section[root])",
        "		return NULL;",
        "#endif",
        "	return &mem_section[root][nr & SECTION_ROOT_MASK];",
        "}",
        "extern size_t mem_section_usage_size(void);",
        "",
        "/*",
        " * We use the lower bits of the mem_map pointer to store",
        " * a little bit of information.  The pointer is calculated",
        " * as mem_map - section_nr_to_pfn(pnum).  The result is",
        " * aligned to the minimum alignment of the two values:",
        " *   1. All mem_map arrays are page-aligned.",
        " *   2. section_nr_to_pfn() always clears PFN_SECTION_SHIFT",
        " *      lowest bits.  PFN_SECTION_SHIFT is arch-specific",
        " *      (equal SECTION_SIZE_BITS - PAGE_SHIFT), and the",
        " *      worst combination is powerpc with 256k pages,",
        " *      which results in PFN_SECTION_SHIFT equal 6.",
        " * To sum it up, at least 6 bits are available on all architectures.",
        " * However, we can exceed 6 bits on some other architectures except",
        " * powerpc (e.g. 15 bits are available on x86_64, 13 bits are available",
        " * with the worst case of 64K pages on arm64) if we make sure the",
        " * exceeded bit is not applicable to powerpc.",
        " */",
        "enum {",
        "	SECTION_MARKED_PRESENT_BIT,",
        "	SECTION_HAS_MEM_MAP_BIT,",
        "	SECTION_IS_ONLINE_BIT,",
        "	SECTION_IS_EARLY_BIT,",
        "#ifdef CONFIG_ZONE_DEVICE",
        "	SECTION_TAINT_ZONE_DEVICE_BIT,",
        "#endif",
        "	SECTION_MAP_LAST_BIT,",
        "};",
        "",
        "#define SECTION_MARKED_PRESENT		BIT(SECTION_MARKED_PRESENT_BIT)",
        "#define SECTION_HAS_MEM_MAP		BIT(SECTION_HAS_MEM_MAP_BIT)",
        "#define SECTION_IS_ONLINE		BIT(SECTION_IS_ONLINE_BIT)",
        "#define SECTION_IS_EARLY		BIT(SECTION_IS_EARLY_BIT)",
        "#ifdef CONFIG_ZONE_DEVICE",
        "#define SECTION_TAINT_ZONE_DEVICE	BIT(SECTION_TAINT_ZONE_DEVICE_BIT)",
        "#endif",
        "#define SECTION_MAP_MASK		(~(BIT(SECTION_MAP_LAST_BIT) - 1))",
        "#define SECTION_NID_SHIFT		SECTION_MAP_LAST_BIT",
        "",
        "static inline struct page *__section_mem_map_addr(struct mem_section *section)",
        "{",
        "	unsigned long map = section->section_mem_map;",
        "	map &= SECTION_MAP_MASK;",
        "	return (struct page *)map;",
        "}",
        "",
        "static inline int present_section(struct mem_section *section)",
        "{",
        "	return (section && (section->section_mem_map & SECTION_MARKED_PRESENT));",
        "}",
        "",
        "static inline int present_section_nr(unsigned long nr)",
        "{",
        "	return present_section(__nr_to_section(nr));",
        "}",
        "",
        "static inline int valid_section(struct mem_section *section)",
        "{",
        "	return (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));",
        "}",
        "",
        "static inline int early_section(struct mem_section *section)",
        "{",
        "	return (section && (section->section_mem_map & SECTION_IS_EARLY));",
        "}",
        "",
        "static inline int valid_section_nr(unsigned long nr)",
        "{",
        "	return valid_section(__nr_to_section(nr));",
        "}",
        "",
        "static inline int online_section(struct mem_section *section)",
        "{",
        "	return (section && (section->section_mem_map & SECTION_IS_ONLINE));",
        "}",
        "",
        "#ifdef CONFIG_ZONE_DEVICE",
        "static inline int online_device_section(struct mem_section *section)",
        "{",
        "	unsigned long flags = SECTION_IS_ONLINE | SECTION_TAINT_ZONE_DEVICE;",
        "",
        "	return section && ((section->section_mem_map & flags) == flags);",
        "}",
        "#else",
        "static inline int online_device_section(struct mem_section *section)",
        "{",
        "	return 0;",
        "}",
        "#endif",
        "",
        "static inline int online_section_nr(unsigned long nr)",
        "{",
        "	return online_section(__nr_to_section(nr));",
        "}",
        "",
        "#ifdef CONFIG_MEMORY_HOTPLUG",
        "void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);",
        "void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);",
        "#endif",
        "",
        "static inline struct mem_section *__pfn_to_section(unsigned long pfn)",
        "{",
        "	return __nr_to_section(pfn_to_section_nr(pfn));",
        "}",
        "",
        "extern unsigned long __highest_present_section_nr;",
        "",
        "static inline int subsection_map_index(unsigned long pfn)",
        "{",
        "	return (pfn & ~(PAGE_SECTION_MASK)) / PAGES_PER_SUBSECTION;",
        "}",
        "",
        "#ifdef CONFIG_SPARSEMEM_VMEMMAP",
        "static inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)",
        "{",
        "	int idx = subsection_map_index(pfn);",
        "	struct mem_section_usage *usage = READ_ONCE(ms->usage);",
        "",
        "	return usage ? test_bit(idx, usage->subsection_map) : 0;",
        "}",
        "#else",
        "static inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)",
        "{",
        "	return 1;",
        "}",
        "#endif",
        "",
        "#ifndef CONFIG_HAVE_ARCH_PFN_VALID",
        "/**",
        " * pfn_valid - check if there is a valid memory map entry for a PFN",
        " * @pfn: the page frame number to check",
        " *",
        " * Check if there is a valid memory map entry aka struct page for the @pfn.",
        " * Note, that availability of the memory map entry does not imply that",
        " * there is actual usable memory at that @pfn. The struct page may",
        " * represent a hole or an unusable page frame.",
        " *",
        " * Return: 1 for PFNs that have memory map entries and 0 otherwise",
        " */",
        "static inline int pfn_valid(unsigned long pfn)",
        "{",
        "	struct mem_section *ms;",
        "	int ret;",
        "",
        "	/*",
        "	 * Ensure the upper PAGE_SHIFT bits are clear in the",
        "	 * pfn. Else it might lead to false positives when",
        "	 * some of the upper bits are set, but the lower bits",
        "	 * match a valid pfn.",
        "	 */",
        "	if (PHYS_PFN(PFN_PHYS(pfn)) != pfn)",
        "		return 0;",
        "",
        "	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)",
        "		return 0;",
        "	ms = __pfn_to_section(pfn);",
        "	rcu_read_lock_sched();",
        "	if (!valid_section(ms)) {",
        "		rcu_read_unlock_sched();",
        "		return 0;",
        "	}",
        "	/*",
        "	 * Traditionally early sections always returned pfn_valid() for",
        "	 * the entire section-sized span.",
        "	 */",
        "	ret = early_section(ms) || pfn_section_valid(ms, pfn);",
        "	rcu_read_unlock_sched();",
        "",
        "	return ret;",
        "}",
        "#endif",
        "",
        "static inline int pfn_in_present_section(unsigned long pfn)",
        "{",
        "	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)",
        "		return 0;",
        "	return present_section(__pfn_to_section(pfn));",
        "}",
        "",
        "static inline unsigned long next_present_section_nr(unsigned long section_nr)",
        "{",
        "	while (++section_nr <= __highest_present_section_nr) {",
        "		if (present_section_nr(section_nr))",
        "			return section_nr;",
        "	}",
        "",
        "	return -1;",
        "}",
        "",
        "/*",
        " * These are _only_ used during initialisation, therefore they",
        " * can use __initdata ...  They could have names to indicate",
        " * this restriction.",
        " */",
        "#ifdef CONFIG_NUMA",
        "#define pfn_to_nid(pfn)							\\",
        "({									\\",
        "	unsigned long __pfn_to_nid_pfn = (pfn);				\\",
        "	page_to_nid(pfn_to_page(__pfn_to_nid_pfn));			\\",
        "})",
        "#else",
        "#define pfn_to_nid(pfn)		(0)",
        "#endif",
        "",
        "void sparse_init(void);",
        "#else",
        "#define sparse_init()	do {} while (0)",
        "#define sparse_index_init(_sec, _nid)  do {} while (0)",
        "#define pfn_in_present_section pfn_valid",
        "#define subsection_map_init(_pfn, _nr_pages) do {} while (0)",
        "#endif /* CONFIG_SPARSEMEM */",
        "",
        "#endif /* !__GENERATING_BOUNDS.H */",
        "#endif /* !__ASSEMBLY__ */",
        "#endif /* _LINUX_MMZONE_H */"
    ]
  },
  "arch_x86_kernel_ldt_c": {
    path: "arch/x86/kernel/ldt.c",
    covered: [91, 109, 113],
    totalLines: 696,
    coveredCount: 3,
    coveragePct: 0.4,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/*",
        " * Copyright (C) 1992 Krishna Balasubramanian and Linus Torvalds",
        " * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>",
        " * Copyright (C) 2002 Andi Kleen",
        " *",
        " * This handles calls from both 32bit and 64bit mode.",
        " *",
        " * Lock order:",
        " *	context.ldt_usr_sem",
        " *	  mmap_lock",
        " *	    context.lock",
        " */",
        "",
        "#include <linux/errno.h>",
        "#include <linux/gfp.h>",
        "#include <linux/sched.h>",
        "#include <linux/string.h>",
        "#include <linux/mm.h>",
        "#include <linux/smp.h>",
        "#include <linux/syscalls.h>",
        "#include <linux/slab.h>",
        "#include <linux/vmalloc.h>",
        "#include <linux/uaccess.h>",
        "",
        "#include <asm/ldt.h>",
        "#include <asm/tlb.h>",
        "#include <asm/desc.h>",
        "#include <asm/mmu_context.h>",
        "#include <asm/pgtable_areas.h>",
        "",
        "#include <xen/xen.h>",
        "",
        "/* This is a multiple of PAGE_SIZE. */",
        "#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)",
        "",
        "static inline void *ldt_slot_va(int slot)",
        "{",
        "	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);",
        "}",
        "",
        "void load_mm_ldt(struct mm_struct *mm)",
        "{",
        "	struct ldt_struct *ldt;",
        "",
        "	/* READ_ONCE synchronizes with smp_store_release */",
        "	ldt = READ_ONCE(mm->context.ldt);",
        "",
        "	/*",
        "	 * Any change to mm->context.ldt is followed by an IPI to all",
        "	 * CPUs with the mm active.  The LDT will not be freed until",
        "	 * after the IPI is handled by all such CPUs.  This means that",
        "	 * if the ldt_struct changes before we return, the values we see",
        "	 * will be safe, and the new values will be loaded before we run",
        "	 * any user code.",
        "	 *",
        "	 * NB: don't try to convert this to use RCU without extreme care.",
        "	 * We would still need IRQs off, because we don't want to change",
        "	 * the local LDT after an IPI loaded a newer value than the one",
        "	 * that we can see.",
        "	 */",
        "",
        "	if (unlikely(ldt)) {",
        "		if (static_cpu_has(X86_FEATURE_PTI)) {",
        "			if (WARN_ON_ONCE((unsigned long)ldt->slot > 1)) {",
        "				/*",
        "				 * Whoops -- either the new LDT isn't mapped",
        "				 * (if slot == -1) or is mapped into a bogus",
        "				 * slot (if slot > 1).",
        "				 */",
        "				clear_LDT();",
        "				return;",
        "			}",
        "",
        "			/*",
        "			 * If page table isolation is enabled, ldt->entries",
        "			 * will not be mapped in the userspace pagetables.",
        "			 * Tell the CPU to access the LDT through the alias",
        "			 * at ldt_slot_va(ldt->slot).",
        "			 */",
        "			set_ldt(ldt_slot_va(ldt->slot), ldt->nr_entries);",
        "		} else {",
        "			set_ldt(ldt->entries, ldt->nr_entries);",
        "		}",
        "	} else {",
        "		clear_LDT();",
        "	}",
        "}",
        "",
        "void switch_ldt(struct mm_struct *prev, struct mm_struct *next)",
        "{",
        "	/*",
        "	 * Load the LDT if either the old or new mm had an LDT.",
        "	 *",
        "	 * An mm will never go from having an LDT to not having an LDT.  Two",
        "	 * mms never share an LDT, so we don't gain anything by checking to",
        "	 * see whether the LDT changed.  There's also no guarantee that",
        "	 * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,",
        "	 * then prev->context.ldt will also be non-NULL.",
        "	 *",
        "	 * If we really cared, we could optimize the case where prev == next",
        "	 * and we're exiting lazy mode.  Most of the time, if this happens,",
        "	 * we don't actually need to reload LDTR, but modify_ldt() is mostly",
        "	 * used by legacy code and emulators where we don't need this level of",
        "	 * performance.",
        "	 *",
        "	 * This uses | instead of || because it generates better code.",
        "	 */",
        "	if (unlikely((unsigned long)prev->context.ldt |",
        "		     (unsigned long)next->context.ldt))",
        "		load_mm_ldt(next);",
        "",
        "	DEBUG_LOCKS_WARN_ON(preemptible());",
        "}",
        "",
        "static void refresh_ldt_segments(void)",
        "{",
        "#ifdef CONFIG_X86_64",
        "	unsigned short sel;",
        "",
        "	/*",
        "	 * Make sure that the cached DS and ES descriptors match the updated",
        "	 * LDT.",
        "	 */",
        "	savesegment(ds, sel);",
        "	if ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT)",
        "		loadsegment(ds, sel);",
        "",
        "	savesegment(es, sel);",
        "	if ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT)",
        "		loadsegment(es, sel);",
        "#endif",
        "}",
        "",
        "/* context.lock is held by the task which issued the smp function call */",
        "static void flush_ldt(void *__mm)",
        "{",
        "	struct mm_struct *mm = __mm;",
        "",
        "	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)",
        "		return;",
        "",
        "	load_mm_ldt(mm);",
        "",
        "	refresh_ldt_segments();",
        "}",
        "",
        "/* The caller must call finalize_ldt_struct on the result. LDT starts zeroed. */",
        "static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)",
        "{",
        "	struct ldt_struct *new_ldt;",
        "	unsigned int alloc_size;",
        "",
        "	if (num_entries > LDT_ENTRIES)",
        "		return NULL;",
        "",
        "	new_ldt = kmalloc(sizeof(struct ldt_struct), GFP_KERNEL_ACCOUNT);",
        "	if (!new_ldt)",
        "		return NULL;",
        "",
        "	BUILD_BUG_ON(LDT_ENTRY_SIZE != sizeof(struct desc_struct));",
        "	alloc_size = num_entries * LDT_ENTRY_SIZE;",
        "",
        "	/*",
        "	 * Xen is very picky: it requires a page-aligned LDT that has no",
        "	 * trailing nonzero bytes in any page that contains LDT descriptors.",
        "	 * Keep it simple: zero the whole allocation and never allocate less",
        "	 * than PAGE_SIZE.",
        "	 */",
        "	if (alloc_size > PAGE_SIZE)",
        "		new_ldt->entries = __vmalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);",
        "	else",
        "		new_ldt->entries = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);",
        "",
        "	if (!new_ldt->entries) {",
        "		kfree(new_ldt);",
        "		return NULL;",
        "	}",
        "",
        "	/* The new LDT isn't aliased for PTI yet. */",
        "	new_ldt->slot = -1;",
        "",
        "	new_ldt->nr_entries = num_entries;",
        "	return new_ldt;",
        "}",
        "",
        "#ifdef CONFIG_MITIGATION_PAGE_TABLE_ISOLATION",
        "",
        "static void do_sanity_check(struct mm_struct *mm,",
        "			    bool had_kernel_mapping,",
        "			    bool had_user_mapping)",
        "{",
        "	if (mm->context.ldt) {",
        "		/*",
        "		 * We already had an LDT.  The top-level entry should already",
        "		 * have been allocated and synchronized with the usermode",
        "		 * tables.",
        "		 */",
        "		WARN_ON(!had_kernel_mapping);",
        "		if (boot_cpu_has(X86_FEATURE_PTI))",
        "			WARN_ON(!had_user_mapping);",
        "	} else {",
        "		/*",
        "		 * This is the first time we're mapping an LDT for this process.",
        "		 * Sync the pgd to the usermode tables.",
        "		 */",
        "		WARN_ON(had_kernel_mapping);",
        "		if (boot_cpu_has(X86_FEATURE_PTI))",
        "			WARN_ON(had_user_mapping);",
        "	}",
        "}",
        "",
        "#ifdef CONFIG_X86_PAE",
        "",
        "static pmd_t *pgd_to_pmd_walk(pgd_t *pgd, unsigned long va)",
        "{",
        "	p4d_t *p4d;",
        "	pud_t *pud;",
        "",
        "	if (pgd->pgd == 0)",
        "		return NULL;",
        "",
        "	p4d = p4d_offset(pgd, va);",
        "	if (p4d_none(*p4d))",
        "		return NULL;",
        "",
        "	pud = pud_offset(p4d, va);",
        "	if (pud_none(*pud))",
        "		return NULL;",
        "",
        "	return pmd_offset(pud, va);",
        "}",
        "",
        "static void map_ldt_struct_to_user(struct mm_struct *mm)",
        "{",
        "	pgd_t *k_pgd = pgd_offset(mm, LDT_BASE_ADDR);",
        "	pgd_t *u_pgd = kernel_to_user_pgdp(k_pgd);",
        "	pmd_t *k_pmd, *u_pmd;",
        "",
        "	k_pmd = pgd_to_pmd_walk(k_pgd, LDT_BASE_ADDR);",
        "	u_pmd = pgd_to_pmd_walk(u_pgd, LDT_BASE_ADDR);",
        "",
        "	if (boot_cpu_has(X86_FEATURE_PTI) && !mm->context.ldt)",
        "		set_pmd(u_pmd, *k_pmd);",
        "}",
        "",
        "static void sanity_check_ldt_mapping(struct mm_struct *mm)",
        "{",
        "	pgd_t *k_pgd = pgd_offset(mm, LDT_BASE_ADDR);",
        "	pgd_t *u_pgd = kernel_to_user_pgdp(k_pgd);",
        "	bool had_kernel, had_user;",
        "	pmd_t *k_pmd, *u_pmd;",
        "",
        "	k_pmd      = pgd_to_pmd_walk(k_pgd, LDT_BASE_ADDR);",
        "	u_pmd      = pgd_to_pmd_walk(u_pgd, LDT_BASE_ADDR);",
        "	had_kernel = (k_pmd->pmd != 0);",
        "	had_user   = (u_pmd->pmd != 0);",
        "",
        "	do_sanity_check(mm, had_kernel, had_user);",
        "}",
        "",
        "#else /* !CONFIG_X86_PAE */",
        "",
        "static void map_ldt_struct_to_user(struct mm_struct *mm)",
        "{",
        "	pgd_t *pgd = pgd_offset(mm, LDT_BASE_ADDR);",
        "",
        "	if (boot_cpu_has(X86_FEATURE_PTI) && !mm->context.ldt)",
        "		set_pgd(kernel_to_user_pgdp(pgd), *pgd);",
        "}",
        "",
        "static void sanity_check_ldt_mapping(struct mm_struct *mm)",
        "{",
        "	pgd_t *pgd = pgd_offset(mm, LDT_BASE_ADDR);",
        "	bool had_kernel = (pgd->pgd != 0);",
        "	bool had_user   = (kernel_to_user_pgdp(pgd)->pgd != 0);",
        "",
        "	do_sanity_check(mm, had_kernel, had_user);",
        "}",
        "",
        "#endif /* CONFIG_X86_PAE */",
        "",
        "/*",
        " * If PTI is enabled, this maps the LDT into the kernelmode and",
        " * usermode tables for the given mm.",
        " */",
        "static int",
        "map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)",
        "{",
        "	unsigned long va;",
        "	bool is_vmalloc;",
        "	spinlock_t *ptl;",
        "	int i, nr_pages;",
        "",
        "	if (!boot_cpu_has(X86_FEATURE_PTI))",
        "		return 0;",
        "",
        "	/*",
        "	 * Any given ldt_struct should have map_ldt_struct() called at most",
        "	 * once.",
        "	 */",
        "	WARN_ON(ldt->slot != -1);",
        "",
        "	/* Check if the current mappings are sane */",
        "	sanity_check_ldt_mapping(mm);",
        "",
        "	is_vmalloc = is_vmalloc_addr(ldt->entries);",
        "",
        "	nr_pages = DIV_ROUND_UP(ldt->nr_entries * LDT_ENTRY_SIZE, PAGE_SIZE);",
        "",
        "	for (i = 0; i < nr_pages; i++) {",
        "		unsigned long offset = i << PAGE_SHIFT;",
        "		const void *src = (char *)ldt->entries + offset;",
        "		unsigned long pfn;",
        "		pgprot_t pte_prot;",
        "		pte_t pte, *ptep;",
        "",
        "		va = (unsigned long)ldt_slot_va(slot) + offset;",
        "		pfn = is_vmalloc ? vmalloc_to_pfn(src) :",
        "			page_to_pfn(virt_to_page(src));",
        "		/*",
        "		 * Treat the PTI LDT range as a *userspace* range.",
        "		 * get_locked_pte() will allocate all needed pagetables",
        "		 * and account for them in this mm.",
        "		 */",
        "		ptep = get_locked_pte(mm, va, &ptl);",
        "		if (!ptep)",
        "			return -ENOMEM;",
        "		/*",
        "		 * Map it RO so the easy to find address is not a primary",
        "		 * target via some kernel interface which misses a",
        "		 * permission check.",
        "		 */",
        "		pte_prot = __pgprot(__PAGE_KERNEL_RO & ~_PAGE_GLOBAL);",
        "		/* Filter out unsuppored __PAGE_KERNEL* bits: */",
        "		pgprot_val(pte_prot) &= __supported_pte_mask;",
        "		pte = pfn_pte(pfn, pte_prot);",
        "		set_pte_at(mm, va, ptep, pte);",
        "		pte_unmap_unlock(ptep, ptl);",
        "	}",
        "",
        "	/* Propagate LDT mapping to the user page-table */",
        "	map_ldt_struct_to_user(mm);",
        "",
        "	ldt->slot = slot;",
        "	return 0;",
        "}",
        "",
        "static void unmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt)",
        "{",
        "	unsigned long va;",
        "	int i, nr_pages;",
        "",
        "	if (!ldt)",
        "		return;",
        "",
        "	/* LDT map/unmap is only required for PTI */",
        "	if (!boot_cpu_has(X86_FEATURE_PTI))",
        "		return;",
        "",
        "	nr_pages = DIV_ROUND_UP(ldt->nr_entries * LDT_ENTRY_SIZE, PAGE_SIZE);",
        "",
        "	for (i = 0; i < nr_pages; i++) {",
        "		unsigned long offset = i << PAGE_SHIFT;",
        "		spinlock_t *ptl;",
        "		pte_t *ptep;",
        "",
        "		va = (unsigned long)ldt_slot_va(ldt->slot) + offset;",
        "		ptep = get_locked_pte(mm, va, &ptl);",
        "		if (!WARN_ON_ONCE(!ptep)) {",
        "			pte_clear(mm, va, ptep);",
        "			pte_unmap_unlock(ptep, ptl);",
        "		}",
        "	}",
        "",
        "	va = (unsigned long)ldt_slot_va(ldt->slot);",
        "	flush_tlb_mm_range(mm, va, va + nr_pages * PAGE_SIZE, PAGE_SHIFT, false);",
        "}",
        "",
        "#else /* !CONFIG_MITIGATION_PAGE_TABLE_ISOLATION */",
        "",
        "static int",
        "map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)",
        "{",
        "	return 0;",
        "}",
        "",
        "static void unmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt)",
        "{",
        "}",
        "#endif /* CONFIG_MITIGATION_PAGE_TABLE_ISOLATION */",
        "",
        "static void free_ldt_pgtables(struct mm_struct *mm)",
        "{",
        "#ifdef CONFIG_MITIGATION_PAGE_TABLE_ISOLATION",
        "	struct mmu_gather tlb;",
        "	unsigned long start = LDT_BASE_ADDR;",
        "	unsigned long end = LDT_END_ADDR;",
        "",
        "	if (!boot_cpu_has(X86_FEATURE_PTI))",
        "		return;",
        "",
        "	/*",
        "	 * Although free_pgd_range() is intended for freeing user",
        "	 * page-tables, it also works out for kernel mappings on x86.",
        "	 * We use tlb_gather_mmu_fullmm() to avoid confusing the",
        "	 * range-tracking logic in __tlb_adjust_range().",
        "	 */",
        "	tlb_gather_mmu_fullmm(&tlb, mm);",
        "	free_pgd_range(&tlb, start, end, start, end);",
        "	tlb_finish_mmu(&tlb);",
        "#endif",
        "}",
        "",
        "/* After calling this, the LDT is immutable. */",
        "static void finalize_ldt_struct(struct ldt_struct *ldt)",
        "{",
        "	paravirt_alloc_ldt(ldt->entries, ldt->nr_entries);",
        "}",
        "",
        "static void install_ldt(struct mm_struct *mm, struct ldt_struct *ldt)",
        "{",
        "	mutex_lock(&mm->context.lock);",
        "",
        "	/* Synchronizes with READ_ONCE in load_mm_ldt. */",
        "	smp_store_release(&mm->context.ldt, ldt);",
        "",
        "	/* Activate the LDT for all CPUs using currents mm. */",
        "	on_each_cpu_mask(mm_cpumask(mm), flush_ldt, mm, true);",
        "",
        "	mutex_unlock(&mm->context.lock);",
        "}",
        "",
        "static void free_ldt_struct(struct ldt_struct *ldt)",
        "{",
        "	if (likely(!ldt))",
        "		return;",
        "",
        "	paravirt_free_ldt(ldt->entries, ldt->nr_entries);",
        "	if (ldt->nr_entries * LDT_ENTRY_SIZE > PAGE_SIZE)",
        "		vfree_atomic(ldt->entries);",
        "	else",
        "		free_page((unsigned long)ldt->entries);",
        "	kfree(ldt);",
        "}",
        "",
        "/*",
        " * Called on fork from arch_dup_mmap(). Just copy the current LDT state,",
        " * the new task is not running, so nothing can be installed.",
        " */",
        "int ldt_dup_context(struct mm_struct *old_mm, struct mm_struct *mm)",
        "{",
        "	struct ldt_struct *new_ldt;",
        "	int retval = 0;",
        "",
        "	if (!old_mm)",
        "		return 0;",
        "",
        "	mutex_lock(&old_mm->context.lock);",
        "	if (!old_mm->context.ldt)",
        "		goto out_unlock;",
        "",
        "	new_ldt = alloc_ldt_struct(old_mm->context.ldt->nr_entries);",
        "	if (!new_ldt) {",
        "		retval = -ENOMEM;",
        "		goto out_unlock;",
        "	}",
        "",
        "	memcpy(new_ldt->entries, old_mm->context.ldt->entries,",
        "	       new_ldt->nr_entries * LDT_ENTRY_SIZE);",
        "	finalize_ldt_struct(new_ldt);",
        "",
        "	retval = map_ldt_struct(mm, new_ldt, 0);",
        "	if (retval) {",
        "		free_ldt_pgtables(mm);",
        "		free_ldt_struct(new_ldt);",
        "		goto out_unlock;",
        "	}",
        "	mm->context.ldt = new_ldt;",
        "",
        "out_unlock:",
        "	mutex_unlock(&old_mm->context.lock);",
        "	return retval;",
        "}",
        "",
        "/*",
        " * No need to lock the MM as we are the last user",
        " *",
        " * 64bit: Don't touch the LDT register - we're already in the next thread.",
        " */",
        "void destroy_context_ldt(struct mm_struct *mm)",
        "{",
        "	free_ldt_struct(mm->context.ldt);",
        "	mm->context.ldt = NULL;",
        "}",
        "",
        "void ldt_arch_exit_mmap(struct mm_struct *mm)",
        "{",
        "	free_ldt_pgtables(mm);",
        "}",
        "",
        "static int read_ldt(void __user *ptr, unsigned long bytecount)",
        "{",
        "	struct mm_struct *mm = current->mm;",
        "	unsigned long entries_size;",
        "	int retval;",
        "",
        "	down_read(&mm->context.ldt_usr_sem);",
        "",
        "	if (!mm->context.ldt) {",
        "		retval = 0;",
        "		goto out_unlock;",
        "	}",
        "",
        "	if (bytecount > LDT_ENTRY_SIZE * LDT_ENTRIES)",
        "		bytecount = LDT_ENTRY_SIZE * LDT_ENTRIES;",
        "",
        "	entries_size = mm->context.ldt->nr_entries * LDT_ENTRY_SIZE;",
        "	if (entries_size > bytecount)",
        "		entries_size = bytecount;",
        "",
        "	if (copy_to_user(ptr, mm->context.ldt->entries, entries_size)) {",
        "		retval = -EFAULT;",
        "		goto out_unlock;",
        "	}",
        "",
        "	if (entries_size != bytecount) {",
        "		/* Zero-fill the rest and pretend we read bytecount bytes. */",
        "		if (clear_user(ptr + entries_size, bytecount - entries_size)) {",
        "			retval = -EFAULT;",
        "			goto out_unlock;",
        "		}",
        "	}",
        "	retval = bytecount;",
        "",
        "out_unlock:",
        "	up_read(&mm->context.ldt_usr_sem);",
        "	return retval;",
        "}",
        "",
        "static int read_default_ldt(void __user *ptr, unsigned long bytecount)",
        "{",
        "	/* CHECKME: Can we use _one_ random number ? */",
        "#ifdef CONFIG_X86_32",
        "	unsigned long size = 5 * sizeof(struct desc_struct);",
        "#else",
        "	unsigned long size = 128;",
        "#endif",
        "	if (bytecount > size)",
        "		bytecount = size;",
        "	if (clear_user(ptr, bytecount))",
        "		return -EFAULT;",
        "	return bytecount;",
        "}",
        "",
        "static bool allow_16bit_segments(void)",
        "{",
        "	if (!IS_ENABLED(CONFIG_X86_16BIT))",
        "		return false;",
        "",
        "#ifdef CONFIG_XEN_PV",
        "	/*",
        "	 * Xen PV does not implement ESPFIX64, which means that 16-bit",
        "	 * segments will not work correctly.  Until either Xen PV implements",
        "	 * ESPFIX64 and can signal this fact to the guest or unless someone",
        "	 * provides compelling evidence that allowing broken 16-bit segments",
        "	 * is worthwhile, disallow 16-bit segments under Xen PV.",
        "	 */",
        "	if (xen_pv_domain()) {",
        "		pr_info_once(\"Warning: 16-bit segments do not work correctly in a Xen PV guest\\n\");",
        "		return false;",
        "	}",
        "#endif",
        "",
        "	return true;",
        "}",
        "",
        "static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)",
        "{",
        "	struct mm_struct *mm = current->mm;",
        "	struct ldt_struct *new_ldt, *old_ldt;",
        "	unsigned int old_nr_entries, new_nr_entries;",
        "	struct user_desc ldt_info;",
        "	struct desc_struct ldt;",
        "	int error;",
        "",
        "	error = -EINVAL;",
        "	if (bytecount != sizeof(ldt_info))",
        "		goto out;",
        "	error = -EFAULT;",
        "	if (copy_from_user(&ldt_info, ptr, sizeof(ldt_info)))",
        "		goto out;",
        "",
        "	error = -EINVAL;",
        "	if (ldt_info.entry_number >= LDT_ENTRIES)",
        "		goto out;",
        "	if (ldt_info.contents == 3) {",
        "		if (oldmode)",
        "			goto out;",
        "		if (ldt_info.seg_not_present == 0)",
        "			goto out;",
        "	}",
        "",
        "	if ((oldmode && !ldt_info.base_addr && !ldt_info.limit) ||",
        "	    LDT_empty(&ldt_info)) {",
        "		/* The user wants to clear the entry. */",
        "		memset(&ldt, 0, sizeof(ldt));",
        "	} else {",
        "		if (!ldt_info.seg_32bit && !allow_16bit_segments()) {",
        "			error = -EINVAL;",
        "			goto out;",
        "		}",
        "",
        "		fill_ldt(&ldt, &ldt_info);",
        "		if (oldmode)",
        "			ldt.avl = 0;",
        "	}",
        "",
        "	if (down_write_killable(&mm->context.ldt_usr_sem))",
        "		return -EINTR;",
        "",
        "	old_ldt       = mm->context.ldt;",
        "	old_nr_entries = old_ldt ? old_ldt->nr_entries : 0;",
        "	new_nr_entries = max(ldt_info.entry_number + 1, old_nr_entries);",
        "",
        "	error = -ENOMEM;",
        "	new_ldt = alloc_ldt_struct(new_nr_entries);",
        "	if (!new_ldt)",
        "		goto out_unlock;",
        "",
        "	if (old_ldt)",
        "		memcpy(new_ldt->entries, old_ldt->entries, old_nr_entries * LDT_ENTRY_SIZE);",
        "",
        "	new_ldt->entries[ldt_info.entry_number] = ldt;",
        "	finalize_ldt_struct(new_ldt);",
        "",
        "	/*",
        "	 * If we are using PTI, map the new LDT into the userspace pagetables.",
        "	 * If there is already an LDT, use the other slot so that other CPUs",
        "	 * will continue to use the old LDT until install_ldt() switches",
        "	 * them over to the new LDT.",
        "	 */",
        "	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt->slot : 0);",
        "	if (error) {",
        "		/*",
        "		 * This only can fail for the first LDT setup. If an LDT is",
        "		 * already installed then the PTE page is already",
        "		 * populated. Mop up a half populated page table.",
        "		 */",
        "		if (!WARN_ON_ONCE(old_ldt))",
        "			free_ldt_pgtables(mm);",
        "		free_ldt_struct(new_ldt);",
        "		goto out_unlock;",
        "	}",
        "",
        "	install_ldt(mm, new_ldt);",
        "	unmap_ldt_struct(mm, old_ldt);",
        "	free_ldt_struct(old_ldt);",
        "	error = 0;",
        "",
        "out_unlock:",
        "	up_write(&mm->context.ldt_usr_sem);",
        "out:",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE3(modify_ldt, int , func , void __user * , ptr ,",
        "		unsigned long , bytecount)",
        "{",
        "	int ret = -ENOSYS;",
        "",
        "	switch (func) {",
        "	case 0:",
        "		ret = read_ldt(ptr, bytecount);",
        "		break;",
        "	case 1:",
        "		ret = write_ldt(ptr, bytecount, 1);",
        "		break;",
        "	case 2:",
        "		ret = read_default_ldt(ptr, bytecount);",
        "		break;",
        "	case 0x11:",
        "		ret = write_ldt(ptr, bytecount, 0);",
        "		break;",
        "	}",
        "	/*",
        "	 * The SYSCALL_DEFINE() macros give us an 'unsigned long'",
        "	 * return type, but the ABI for sys_modify_ldt() expects",
        "	 * 'int'.  This cast gives us an int-sized value in %rax",
        "	 * for the return code.  The 'unsigned' is necessary so",
        "	 * the compiler does not try to sign-extend the negative",
        "	 * return codes into the high half of the register when",
        "	 * taking the value from int->long.",
        "	 */",
        "	return (unsigned int)ret;",
        "}"
    ]
  },
  "include_asm-generic_pgalloc_h": {
    path: "include/asm-generic/pgalloc.h",
    covered: [73, 141, 178, 195],
    totalLines: 227,
    coveredCount: 4,
    coveragePct: 1.8,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef __ASM_GENERIC_PGALLOC_H",
        "#define __ASM_GENERIC_PGALLOC_H",
        "",
        "#ifdef CONFIG_MMU",
        "",
        "#define GFP_PGTABLE_KERNEL	(GFP_KERNEL | __GFP_ZERO)",
        "#define GFP_PGTABLE_USER	(GFP_PGTABLE_KERNEL | __GFP_ACCOUNT)",
        "",
        "/**",
        " * __pte_alloc_one_kernel - allocate memory for a PTE-level kernel page table",
        " * @mm: the mm_struct of the current context",
        " *",
        " * This function is intended for architectures that need",
        " * anything beyond simple page allocation.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "static inline pte_t *__pte_alloc_one_kernel_noprof(struct mm_struct *mm)",
        "{",
        "	struct ptdesc *ptdesc = pagetable_alloc_noprof(GFP_PGTABLE_KERNEL &",
        "			~__GFP_HIGHMEM, 0);",
        "",
        "	if (!ptdesc)",
        "		return NULL;",
        "	return ptdesc_address(ptdesc);",
        "}",
        "#define __pte_alloc_one_kernel(...)	alloc_hooks(__pte_alloc_one_kernel_noprof(__VA_ARGS__))",
        "",
        "#ifndef __HAVE_ARCH_PTE_ALLOC_ONE_KERNEL",
        "/**",
        " * pte_alloc_one_kernel - allocate memory for a PTE-level kernel page table",
        " * @mm: the mm_struct of the current context",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "static inline pte_t *pte_alloc_one_kernel_noprof(struct mm_struct *mm)",
        "{",
        "	return __pte_alloc_one_kernel_noprof(mm);",
        "}",
        "#define pte_alloc_one_kernel(...)	alloc_hooks(pte_alloc_one_kernel_noprof(__VA_ARGS__))",
        "#endif",
        "",
        "/**",
        " * pte_free_kernel - free PTE-level kernel page table memory",
        " * @mm: the mm_struct of the current context",
        " * @pte: pointer to the memory containing the page table",
        " */",
        "static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)",
        "{",
        "	pagetable_free(virt_to_ptdesc(pte));",
        "}",
        "",
        "/**",
        " * __pte_alloc_one - allocate memory for a PTE-level user page table",
        " * @mm: the mm_struct of the current context",
        " * @gfp: GFP flags to use for the allocation",
        " *",
        " * Allocate memory for a page table and ptdesc and runs pagetable_pte_ctor().",
        " *",
        " * This function is intended for architectures that need",
        " * anything beyond simple page allocation or must have custom GFP flags.",
        " *",
        " * Return: `struct page` referencing the ptdesc or %NULL on error",
        " */",
        "static inline pgtable_t __pte_alloc_one_noprof(struct mm_struct *mm, gfp_t gfp)",
        "{",
        "	struct ptdesc *ptdesc;",
        "",
        "	ptdesc = pagetable_alloc_noprof(gfp, 0);",
        "	if (!ptdesc)",
        "		return NULL;",
        "	if (!pagetable_pte_ctor(ptdesc)) {",
        "		pagetable_free(ptdesc);",
        "		return NULL;",
        "	}",
        "",
        "	return ptdesc_page(ptdesc);",
        "}",
        "#define __pte_alloc_one(...)	alloc_hooks(__pte_alloc_one_noprof(__VA_ARGS__))",
        "",
        "#ifndef __HAVE_ARCH_PTE_ALLOC_ONE",
        "/**",
        " * pte_alloc_one - allocate a page for PTE-level user page table",
        " * @mm: the mm_struct of the current context",
        " *",
        " * Allocate memory for a page table and ptdesc and runs pagetable_pte_ctor().",
        " *",
        " * Return: `struct page` referencing the ptdesc or %NULL on error",
        " */",
        "static inline pgtable_t pte_alloc_one_noprof(struct mm_struct *mm)",
        "{",
        "	return __pte_alloc_one_noprof(mm, GFP_PGTABLE_USER);",
        "}",
        "#define pte_alloc_one(...)	alloc_hooks(pte_alloc_one_noprof(__VA_ARGS__))",
        "#endif",
        "",
        "/*",
        " * Should really implement gc for free page table pages. This could be",
        " * done with a reference count in struct page.",
        " */",
        "",
        "/**",
        " * pte_free - free PTE-level user page table memory",
        " * @mm: the mm_struct of the current context",
        " * @pte_page: the `struct page` referencing the ptdesc",
        " */",
        "static inline void pte_free(struct mm_struct *mm, struct page *pte_page)",
        "{",
        "	struct ptdesc *ptdesc = page_ptdesc(pte_page);",
        "",
        "	pagetable_pte_dtor(ptdesc);",
        "	pagetable_free(ptdesc);",
        "}",
        "",
        "",
        "#if CONFIG_PGTABLE_LEVELS > 2",
        "",
        "#ifndef __HAVE_ARCH_PMD_ALLOC_ONE",
        "/**",
        " * pmd_alloc_one - allocate memory for a PMD-level page table",
        " * @mm: the mm_struct of the current context",
        " *",
        " * Allocate memory for a page table and ptdesc and runs pagetable_pmd_ctor().",
        " *",
        " * Allocations use %GFP_PGTABLE_USER in user context and",
        " * %GFP_PGTABLE_KERNEL in kernel context.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "static inline pmd_t *pmd_alloc_one_noprof(struct mm_struct *mm, unsigned long addr)",
        "{",
        "	struct ptdesc *ptdesc;",
        "	gfp_t gfp = GFP_PGTABLE_USER;",
        "",
        "	if (mm == &init_mm)",
        "		gfp = GFP_PGTABLE_KERNEL;",
        "	ptdesc = pagetable_alloc_noprof(gfp, 0);",
        "	if (!ptdesc)",
        "		return NULL;",
        "	if (!pagetable_pmd_ctor(ptdesc)) {",
        "		pagetable_free(ptdesc);",
        "		return NULL;",
        "	}",
        "	return ptdesc_address(ptdesc);",
        "}",
        "#define pmd_alloc_one(...)	alloc_hooks(pmd_alloc_one_noprof(__VA_ARGS__))",
        "#endif",
        "",
        "#ifndef __HAVE_ARCH_PMD_FREE",
        "static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)",
        "{",
        "	struct ptdesc *ptdesc = virt_to_ptdesc(pmd);",
        "",
        "	BUG_ON((unsigned long)pmd & (PAGE_SIZE-1));",
        "	pagetable_pmd_dtor(ptdesc);",
        "	pagetable_free(ptdesc);",
        "}",
        "#endif",
        "",
        "#endif /* CONFIG_PGTABLE_LEVELS > 2 */",
        "",
        "#if CONFIG_PGTABLE_LEVELS > 3",
        "",
        "static inline pud_t *__pud_alloc_one_noprof(struct mm_struct *mm, unsigned long addr)",
        "{",
        "	gfp_t gfp = GFP_PGTABLE_USER;",
        "	struct ptdesc *ptdesc;",
        "",
        "	if (mm == &init_mm)",
        "		gfp = GFP_PGTABLE_KERNEL;",
        "	gfp &= ~__GFP_HIGHMEM;",
        "",
        "	ptdesc = pagetable_alloc_noprof(gfp, 0);",
        "	if (!ptdesc)",
        "		return NULL;",
        "",
        "	pagetable_pud_ctor(ptdesc);",
        "	return ptdesc_address(ptdesc);",
        "}",
        "#define __pud_alloc_one(...)	alloc_hooks(__pud_alloc_one_noprof(__VA_ARGS__))",
        "",
        "#ifndef __HAVE_ARCH_PUD_ALLOC_ONE",
        "/**",
        " * pud_alloc_one - allocate memory for a PUD-level page table",
        " * @mm: the mm_struct of the current context",
        " *",
        " * Allocate memory for a page table using %GFP_PGTABLE_USER for user context",
        " * and %GFP_PGTABLE_KERNEL for kernel context.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "static inline pud_t *pud_alloc_one_noprof(struct mm_struct *mm, unsigned long addr)",
        "{",
        "	return __pud_alloc_one_noprof(mm, addr);",
        "}",
        "#define pud_alloc_one(...)	alloc_hooks(pud_alloc_one_noprof(__VA_ARGS__))",
        "#endif",
        "",
        "static inline void __pud_free(struct mm_struct *mm, pud_t *pud)",
        "{",
        "	struct ptdesc *ptdesc = virt_to_ptdesc(pud);",
        "",
        "	BUG_ON((unsigned long)pud & (PAGE_SIZE-1));",
        "	pagetable_pud_dtor(ptdesc);",
        "	pagetable_free(ptdesc);",
        "}",
        "",
        "#ifndef __HAVE_ARCH_PUD_FREE",
        "static inline void pud_free(struct mm_struct *mm, pud_t *pud)",
        "{",
        "	__pud_free(mm, pud);",
        "}",
        "#endif",
        "",
        "#endif /* CONFIG_PGTABLE_LEVELS > 3 */",
        "",
        "#ifndef __HAVE_ARCH_PGD_FREE",
        "static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)",
        "{",
        "	pagetable_free(virt_to_ptdesc(pgd));",
        "}",
        "#endif",
        "",
        "#endif /* CONFIG_MMU */",
        "",
        "#endif /* __ASM_GENERIC_PGALLOC_H */"
    ]
  },
  "include_linux_swapops_h": {
    path: "include/linux/swapops.h",
    covered: [506],
    totalLines: 622,
    coveredCount: 1,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_SWAPOPS_H",
        "#define _LINUX_SWAPOPS_H",
        "",
        "#include <linux/radix-tree.h>",
        "#include <linux/bug.h>",
        "#include <linux/mm_types.h>",
        "",
        "#ifdef CONFIG_MMU",
        "",
        "#ifdef CONFIG_SWAP",
        "#include <linux/swapfile.h>",
        "#endif	/* CONFIG_SWAP */",
        "",
        "/*",
        " * swapcache pages are stored in the swapper_space radix tree.  We want to",
        " * get good packing density in that tree, so the index should be dense in",
        " * the low-order bits.",
        " *",
        " * We arrange the `type' and `offset' fields so that `type' is at the six",
        " * high-order bits of the swp_entry_t and `offset' is right-aligned in the",
        " * remaining bits.  Although `type' itself needs only five bits, we allow for",
        " * shmem/tmpfs to shift it all up a further one bit: see swp_to_radix_entry().",
        " *",
        " * swp_entry_t's are *never* stored anywhere in their arch-dependent format.",
        " */",
        "#define SWP_TYPE_SHIFT	(BITS_PER_XA_VALUE - MAX_SWAPFILES_SHIFT)",
        "#define SWP_OFFSET_MASK	((1UL << SWP_TYPE_SHIFT) - 1)",
        "",
        "/*",
        " * Definitions only for PFN swap entries (see is_pfn_swap_entry()).  To",
        " * store PFN, we only need SWP_PFN_BITS bits.  Each of the pfn swap entries",
        " * can use the extra bits to store other information besides PFN.",
        " */",
        "#ifdef MAX_PHYSMEM_BITS",
        "#define SWP_PFN_BITS		(MAX_PHYSMEM_BITS - PAGE_SHIFT)",
        "#else  /* MAX_PHYSMEM_BITS */",
        "#define SWP_PFN_BITS		min_t(int, \\",
        "				      sizeof(phys_addr_t) * 8 - PAGE_SHIFT, \\",
        "				      SWP_TYPE_SHIFT)",
        "#endif	/* MAX_PHYSMEM_BITS */",
        "#define SWP_PFN_MASK		(BIT(SWP_PFN_BITS) - 1)",
        "",
        "/**",
        " * Migration swap entry specific bitfield definitions.  Layout:",
        " *",
        " *   |----------+--------------------|",
        " *   | swp_type | swp_offset         |",
        " *   |----------+--------+-+-+-------|",
        " *   |          | resv   |D|A|  PFN  |",
        " *   |----------+--------+-+-+-------|",
        " *",
        " * @SWP_MIG_YOUNG_BIT: Whether the page used to have young bit set (bit A)",
        " * @SWP_MIG_DIRTY_BIT: Whether the page used to have dirty bit set (bit D)",
        " *",
        " * Note: A/D bits will be stored in migration entries iff there're enough",
        " * free bits in arch specific swp offset.  By default we'll ignore A/D bits",
        " * when migrating a page.  Please refer to migration_entry_supports_ad()",
        " * for more information.  If there're more bits besides PFN and A/D bits,",
        " * they should be reserved and always be zeros.",
        " */",
        "#define SWP_MIG_YOUNG_BIT		(SWP_PFN_BITS)",
        "#define SWP_MIG_DIRTY_BIT		(SWP_PFN_BITS + 1)",
        "#define SWP_MIG_TOTAL_BITS		(SWP_PFN_BITS + 2)",
        "",
        "#define SWP_MIG_YOUNG			BIT(SWP_MIG_YOUNG_BIT)",
        "#define SWP_MIG_DIRTY			BIT(SWP_MIG_DIRTY_BIT)",
        "",
        "static inline bool is_pfn_swap_entry(swp_entry_t entry);",
        "",
        "/* Clear all flags but only keep swp_entry_t related information */",
        "static inline pte_t pte_swp_clear_flags(pte_t pte)",
        "{",
        "	if (pte_swp_exclusive(pte))",
        "		pte = pte_swp_clear_exclusive(pte);",
        "	if (pte_swp_soft_dirty(pte))",
        "		pte = pte_swp_clear_soft_dirty(pte);",
        "	if (pte_swp_uffd_wp(pte))",
        "		pte = pte_swp_clear_uffd_wp(pte);",
        "	return pte;",
        "}",
        "",
        "/*",
        " * Store a type+offset into a swp_entry_t in an arch-independent format",
        " */",
        "static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)",
        "{",
        "	swp_entry_t ret;",
        "",
        "	ret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Extract the `type' field from a swp_entry_t.  The swp_entry_t is in",
        " * arch-independent format",
        " */",
        "static inline unsigned swp_type(swp_entry_t entry)",
        "{",
        "	return (entry.val >> SWP_TYPE_SHIFT);",
        "}",
        "",
        "/*",
        " * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in",
        " * arch-independent format",
        " */",
        "static inline pgoff_t swp_offset(swp_entry_t entry)",
        "{",
        "	return entry.val & SWP_OFFSET_MASK;",
        "}",
        "",
        "/*",
        " * This should only be called upon a pfn swap entry to get the PFN stored",
        " * in the swap entry.  Please refers to is_pfn_swap_entry() for definition",
        " * of pfn swap entry.",
        " */",
        "static inline unsigned long swp_offset_pfn(swp_entry_t entry)",
        "{",
        "	VM_BUG_ON(!is_pfn_swap_entry(entry));",
        "	return swp_offset(entry) & SWP_PFN_MASK;",
        "}",
        "",
        "/* check whether a pte points to a swap entry */",
        "static inline int is_swap_pte(pte_t pte)",
        "{",
        "	return !pte_none(pte) && !pte_present(pte);",
        "}",
        "",
        "/*",
        " * Convert the arch-dependent pte representation of a swp_entry_t into an",
        " * arch-independent swp_entry_t.",
        " */",
        "static inline swp_entry_t pte_to_swp_entry(pte_t pte)",
        "{",
        "	swp_entry_t arch_entry;",
        "",
        "	pte = pte_swp_clear_flags(pte);",
        "	arch_entry = __pte_to_swp_entry(pte);",
        "	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));",
        "}",
        "",
        "/*",
        " * Convert the arch-independent representation of a swp_entry_t into the",
        " * arch-dependent pte representation.",
        " */",
        "static inline pte_t swp_entry_to_pte(swp_entry_t entry)",
        "{",
        "	swp_entry_t arch_entry;",
        "",
        "	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));",
        "	return __swp_entry_to_pte(arch_entry);",
        "}",
        "",
        "static inline swp_entry_t radix_to_swp_entry(void *arg)",
        "{",
        "	swp_entry_t entry;",
        "",
        "	entry.val = xa_to_value(arg);",
        "	return entry;",
        "}",
        "",
        "static inline void *swp_to_radix_entry(swp_entry_t entry)",
        "{",
        "	return xa_mk_value(entry.val);",
        "}",
        "",
        "#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)",
        "static inline swp_entry_t make_readable_device_private_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_DEVICE_READ, offset);",
        "}",
        "",
        "static inline swp_entry_t make_writable_device_private_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_DEVICE_WRITE, offset);",
        "}",
        "",
        "static inline bool is_device_private_entry(swp_entry_t entry)",
        "{",
        "	int type = swp_type(entry);",
        "	return type == SWP_DEVICE_READ || type == SWP_DEVICE_WRITE;",
        "}",
        "",
        "static inline bool is_writable_device_private_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);",
        "}",
        "",
        "static inline swp_entry_t make_readable_device_exclusive_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_DEVICE_EXCLUSIVE_READ, offset);",
        "}",
        "",
        "static inline swp_entry_t make_writable_device_exclusive_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_DEVICE_EXCLUSIVE_WRITE, offset);",
        "}",
        "",
        "static inline bool is_device_exclusive_entry(swp_entry_t entry)",
        "{",
        "	return swp_type(entry) == SWP_DEVICE_EXCLUSIVE_READ ||",
        "		swp_type(entry) == SWP_DEVICE_EXCLUSIVE_WRITE;",
        "}",
        "",
        "static inline bool is_writable_device_exclusive_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_DEVICE_EXCLUSIVE_WRITE);",
        "}",
        "#else /* CONFIG_DEVICE_PRIVATE */",
        "static inline swp_entry_t make_readable_device_private_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline swp_entry_t make_writable_device_private_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline bool is_device_private_entry(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline bool is_writable_device_private_entry(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline swp_entry_t make_readable_device_exclusive_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline swp_entry_t make_writable_device_exclusive_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline bool is_device_exclusive_entry(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline bool is_writable_device_exclusive_entry(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "#endif /* CONFIG_DEVICE_PRIVATE */",
        "",
        "#ifdef CONFIG_MIGRATION",
        "static inline int is_migration_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_MIGRATION_READ ||",
        "			swp_type(entry) == SWP_MIGRATION_READ_EXCLUSIVE ||",
        "			swp_type(entry) == SWP_MIGRATION_WRITE);",
        "}",
        "",
        "static inline int is_writable_migration_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);",
        "}",
        "",
        "static inline int is_readable_migration_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_MIGRATION_READ);",
        "}",
        "",
        "static inline int is_readable_exclusive_migration_entry(swp_entry_t entry)",
        "{",
        "	return unlikely(swp_type(entry) == SWP_MIGRATION_READ_EXCLUSIVE);",
        "}",
        "",
        "static inline swp_entry_t make_readable_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_MIGRATION_READ, offset);",
        "}",
        "",
        "static inline swp_entry_t make_readable_exclusive_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_MIGRATION_READ_EXCLUSIVE, offset);",
        "}",
        "",
        "static inline swp_entry_t make_writable_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(SWP_MIGRATION_WRITE, offset);",
        "}",
        "",
        "/*",
        " * Returns whether the host has large enough swap offset field to support",
        " * carrying over pgtable A/D bits for page migrations.  The result is",
        " * pretty much arch specific.",
        " */",
        "static inline bool migration_entry_supports_ad(void)",
        "{",
        "#ifdef CONFIG_SWAP",
        "	return swap_migration_ad_supported;",
        "#else  /* CONFIG_SWAP */",
        "	return false;",
        "#endif	/* CONFIG_SWAP */",
        "}",
        "",
        "static inline swp_entry_t make_migration_entry_young(swp_entry_t entry)",
        "{",
        "	if (migration_entry_supports_ad())",
        "		return swp_entry(swp_type(entry),",
        "				 swp_offset(entry) | SWP_MIG_YOUNG);",
        "	return entry;",
        "}",
        "",
        "static inline bool is_migration_entry_young(swp_entry_t entry)",
        "{",
        "	if (migration_entry_supports_ad())",
        "		return swp_offset(entry) & SWP_MIG_YOUNG;",
        "	/* Keep the old behavior of aging page after migration */",
        "	return false;",
        "}",
        "",
        "static inline swp_entry_t make_migration_entry_dirty(swp_entry_t entry)",
        "{",
        "	if (migration_entry_supports_ad())",
        "		return swp_entry(swp_type(entry),",
        "				 swp_offset(entry) | SWP_MIG_DIRTY);",
        "	return entry;",
        "}",
        "",
        "static inline bool is_migration_entry_dirty(swp_entry_t entry)",
        "{",
        "	if (migration_entry_supports_ad())",
        "		return swp_offset(entry) & SWP_MIG_DIRTY;",
        "	/* Keep the old behavior of clean page after migration */",
        "	return false;",
        "}",
        "",
        "extern void migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,",
        "					unsigned long address);",
        "extern void migration_entry_wait_huge(struct vm_area_struct *vma, unsigned long addr, pte_t *pte);",
        "#else  /* CONFIG_MIGRATION */",
        "static inline swp_entry_t make_readable_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline swp_entry_t make_readable_exclusive_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline swp_entry_t make_writable_migration_entry(pgoff_t offset)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline int is_migration_entry(swp_entry_t swp)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,",
        "					unsigned long address) { }",
        "static inline void migration_entry_wait_huge(struct vm_area_struct *vma,",
        "					     unsigned long addr, pte_t *pte) { }",
        "static inline int is_writable_migration_entry(swp_entry_t entry)",
        "{",
        "	return 0;",
        "}",
        "static inline int is_readable_migration_entry(swp_entry_t entry)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline swp_entry_t make_migration_entry_young(swp_entry_t entry)",
        "{",
        "	return entry;",
        "}",
        "",
        "static inline bool is_migration_entry_young(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline swp_entry_t make_migration_entry_dirty(swp_entry_t entry)",
        "{",
        "	return entry;",
        "}",
        "",
        "static inline bool is_migration_entry_dirty(swp_entry_t entry)",
        "{",
        "	return false;",
        "}",
        "#endif	/* CONFIG_MIGRATION */",
        "",
        "#ifdef CONFIG_MEMORY_FAILURE",
        "",
        "/*",
        " * Support for hardware poisoned pages",
        " */",
        "static inline swp_entry_t make_hwpoison_entry(struct page *page)",
        "{",
        "	BUG_ON(!PageLocked(page));",
        "	return swp_entry(SWP_HWPOISON, page_to_pfn(page));",
        "}",
        "",
        "static inline int is_hwpoison_entry(swp_entry_t entry)",
        "{",
        "	return swp_type(entry) == SWP_HWPOISON;",
        "}",
        "",
        "#else",
        "",
        "static inline swp_entry_t make_hwpoison_entry(struct page *page)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline int is_hwpoison_entry(swp_entry_t swp)",
        "{",
        "	return 0;",
        "}",
        "#endif",
        "",
        "typedef unsigned long pte_marker;",
        "",
        "#define  PTE_MARKER_UFFD_WP			BIT(0)",
        "/*",
        " * \"Poisoned\" here is meant in the very general sense of \"future accesses are",
        " * invalid\", instead of referring very specifically to hardware memory errors.",
        " * This marker is meant to represent any of various different causes of this.",
        " *",
        " * Note that, when encountered by the faulting logic, PTEs with this marker will",
        " * result in VM_FAULT_HWPOISON and thus regardless trigger hardware memory error",
        " * logic.",
        " */",
        "#define  PTE_MARKER_POISONED			BIT(1)",
        "/*",
        " * Indicates that, on fault, this PTE will case a SIGSEGV signal to be",
        " * sent. This means guard markers behave in effect as if the region were mapped",
        " * PROT_NONE, rather than if they were a memory hole or equivalent.",
        " */",
        "#define  PTE_MARKER_GUARD			BIT(2)",
        "#define  PTE_MARKER_MASK			(BIT(3) - 1)",
        "",
        "static inline swp_entry_t make_pte_marker_entry(pte_marker marker)",
        "{",
        "	return swp_entry(SWP_PTE_MARKER, marker);",
        "}",
        "",
        "static inline bool is_pte_marker_entry(swp_entry_t entry)",
        "{",
        "	return swp_type(entry) == SWP_PTE_MARKER;",
        "}",
        "",
        "static inline pte_marker pte_marker_get(swp_entry_t entry)",
        "{",
        "	return swp_offset(entry) & PTE_MARKER_MASK;",
        "}",
        "",
        "static inline bool is_pte_marker(pte_t pte)",
        "{",
        "	return is_swap_pte(pte) && is_pte_marker_entry(pte_to_swp_entry(pte));",
        "}",
        "",
        "static inline pte_t make_pte_marker(pte_marker marker)",
        "{",
        "	return swp_entry_to_pte(make_pte_marker_entry(marker));",
        "}",
        "",
        "static inline swp_entry_t make_poisoned_swp_entry(void)",
        "{",
        "	return make_pte_marker_entry(PTE_MARKER_POISONED);",
        "}",
        "",
        "static inline int is_poisoned_swp_entry(swp_entry_t entry)",
        "{",
        "	return is_pte_marker_entry(entry) &&",
        "	    (pte_marker_get(entry) & PTE_MARKER_POISONED);",
        "",
        "}",
        "",
        "static inline swp_entry_t make_guard_swp_entry(void)",
        "{",
        "	return make_pte_marker_entry(PTE_MARKER_GUARD);",
        "}",
        "",
        "static inline int is_guard_swp_entry(swp_entry_t entry)",
        "{",
        "	return is_pte_marker_entry(entry) &&",
        "		(pte_marker_get(entry) & PTE_MARKER_GUARD);",
        "}",
        "",
        "/*",
        " * This is a special version to check pte_none() just to cover the case when",
        " * the pte is a pte marker.  It existed because in many cases the pte marker",
        " * should be seen as a none pte; it's just that we have stored some information",
        " * onto the none pte so it becomes not-none any more.",
        " *",
        " * It should be used when the pte is file-backed, ram-based and backing",
        " * userspace pages, like shmem.  It is not needed upon pgtables that do not",
        " * support pte markers at all.  For example, it's not needed on anonymous",
        " * memory, kernel-only memory (including when the system is during-boot),",
        " * non-ram based generic file-system.  It's fine to be used even there, but the",
        " * extra pte marker check will be pure overhead.",
        " */",
        "static inline int pte_none_mostly(pte_t pte)",
        "{",
        "	return pte_none(pte) || is_pte_marker(pte);",
        "}",
        "",
        "static inline struct page *pfn_swap_entry_to_page(swp_entry_t entry)",
        "{",
        "	struct page *p = pfn_to_page(swp_offset_pfn(entry));",
        "",
        "	/*",
        "	 * Any use of migration entries may only occur while the",
        "	 * corresponding page is locked",
        "	 */",
        "	BUG_ON(is_migration_entry(entry) && !PageLocked(p));",
        "",
        "	return p;",
        "}",
        "",
        "static inline struct folio *pfn_swap_entry_folio(swp_entry_t entry)",
        "{",
        "	struct folio *folio = pfn_folio(swp_offset_pfn(entry));",
        "",
        "	/*",
        "	 * Any use of migration entries may only occur while the",
        "	 * corresponding folio is locked",
        "	 */",
        "	BUG_ON(is_migration_entry(entry) && !folio_test_locked(folio));",
        "",
        "	return folio;",
        "}",
        "",
        "/*",
        " * A pfn swap entry is a special type of swap entry that always has a pfn stored",
        " * in the swap offset. They can either be used to represent unaddressable device",
        " * memory, to restrict access to a page undergoing migration or to represent a",
        " * pfn which has been hwpoisoned and unmapped.",
        " */",
        "static inline bool is_pfn_swap_entry(swp_entry_t entry)",
        "{",
        "	/* Make sure the swp offset can always store the needed fields */",
        "	BUILD_BUG_ON(SWP_TYPE_SHIFT < SWP_PFN_BITS);",
        "",
        "	return is_migration_entry(entry) || is_device_private_entry(entry) ||",
        "	       is_device_exclusive_entry(entry) || is_hwpoison_entry(entry);",
        "}",
        "",
        "struct page_vma_mapped_walk;",
        "",
        "#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION",
        "extern int set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,",
        "		struct page *page);",
        "",
        "extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,",
        "		struct page *new);",
        "",
        "extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);",
        "",
        "static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)",
        "{",
        "	swp_entry_t arch_entry;",
        "",
        "	if (pmd_swp_soft_dirty(pmd))",
        "		pmd = pmd_swp_clear_soft_dirty(pmd);",
        "	if (pmd_swp_uffd_wp(pmd))",
        "		pmd = pmd_swp_clear_uffd_wp(pmd);",
        "	arch_entry = __pmd_to_swp_entry(pmd);",
        "	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));",
        "}",
        "",
        "static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)",
        "{",
        "	swp_entry_t arch_entry;",
        "",
        "	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));",
        "	return __swp_entry_to_pmd(arch_entry);",
        "}",
        "",
        "static inline int is_pmd_migration_entry(pmd_t pmd)",
        "{",
        "	return is_swap_pmd(pmd) && is_migration_entry(pmd_to_swp_entry(pmd));",
        "}",
        "#else  /* CONFIG_ARCH_ENABLE_THP_MIGRATION */",
        "static inline int set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,",
        "		struct page *page)",
        "{",
        "	BUILD_BUG();",
        "}",
        "",
        "static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,",
        "		struct page *new)",
        "{",
        "	BUILD_BUG();",
        "}",
        "",
        "static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }",
        "",
        "static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)",
        "{",
        "	return swp_entry(0, 0);",
        "}",
        "",
        "static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)",
        "{",
        "	return __pmd(0);",
        "}",
        "",
        "static inline int is_pmd_migration_entry(pmd_t pmd)",
        "{",
        "	return 0;",
        "}",
        "#endif  /* CONFIG_ARCH_ENABLE_THP_MIGRATION */",
        "",
        "static inline int non_swap_entry(swp_entry_t entry)",
        "{",
        "	return swp_type(entry) >= MAX_SWAPFILES;",
        "}",
        "",
        "#endif /* CONFIG_MMU */",
        "#endif /* _LINUX_SWAPOPS_H */"
    ]
  },
  "arch_x86_kernel_jump_label_c": {
    path: "arch/x86/kernel/jump_label.c",
    covered: [53, 144, 125, 37, 48, 136, 45, 76, 25],
    totalLines: 148,
    coveredCount: 9,
    coveragePct: 6.1,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/*",
        " * jump label x86 support",
        " *",
        " * Copyright (C) 2009 Jason Baron <jbaron@redhat.com>",
        " *",
        " */",
        "#include <linux/jump_label.h>",
        "#include <linux/memory.h>",
        "#include <linux/uaccess.h>",
        "#include <linux/module.h>",
        "#include <linux/list.h>",
        "#include <linux/jhash.h>",
        "#include <linux/cpu.h>",
        "#include <asm/kprobes.h>",
        "#include <asm/alternative.h>",
        "#include <asm/text-patching.h>",
        "#include <asm/insn.h>",
        "",
        "int arch_jump_entry_size(struct jump_entry *entry)",
        "{",
        "	struct insn insn = {};",
        "",
        "	insn_decode_kernel(&insn, (void *)jump_entry_code(entry));",
        "	BUG_ON(insn.length != 2 && insn.length != 5);",
        "",
        "	return insn.length;",
        "}",
        "",
        "struct jump_label_patch {",
        "	const void *code;",
        "	int size;",
        "};",
        "",
        "static struct jump_label_patch",
        "__jump_label_patch(struct jump_entry *entry, enum jump_label_type type)",
        "{",
        "	const void *expect, *code, *nop;",
        "	const void *addr, *dest;",
        "	int size;",
        "",
        "	addr = (void *)jump_entry_code(entry);",
        "	dest = (void *)jump_entry_target(entry);",
        "",
        "	size = arch_jump_entry_size(entry);",
        "	switch (size) {",
        "	case JMP8_INSN_SIZE:",
        "		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);",
        "		nop = x86_nops[size];",
        "		break;",
        "",
        "	case JMP32_INSN_SIZE:",
        "		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);",
        "		nop = x86_nops[size];",
        "		break;",
        "",
        "	default: BUG();",
        "	}",
        "",
        "	if (type == JUMP_LABEL_JMP)",
        "		expect = nop;",
        "	else",
        "		expect = code;",
        "",
        "	if (memcmp(addr, expect, size)) {",
        "		/*",
        "		 * The location is not an op that we were expecting.",
        "		 * Something went wrong. Crash the box, as something could be",
        "		 * corrupting the kernel.",
        "		 */",
        "		pr_crit(\"jump_label: Fatal kernel bug, unexpected op at %pS [%p] (%5ph != %5ph)) size:%d type:%d\\n\",",
        "				addr, addr, addr, expect, size, type);",
        "		BUG();",
        "	}",
        "",
        "	if (type == JUMP_LABEL_NOP)",
        "		code = nop;",
        "",
        "	return (struct jump_label_patch){.code = code, .size = size};",
        "}",
        "",
        "static __always_inline void",
        "__jump_label_transform(struct jump_entry *entry,",
        "		       enum jump_label_type type,",
        "		       int init)",
        "{",
        "	const struct jump_label_patch jlp = __jump_label_patch(entry, type);",
        "",
        "	/*",
        "	 * As long as only a single processor is running and the code is still",
        "	 * not marked as RO, text_poke_early() can be used; Checking that",
        "	 * system_state is SYSTEM_BOOTING guarantees it. It will be set to",
        "	 * SYSTEM_SCHEDULING before other cores are awaken and before the",
        "	 * code is write-protected.",
        "	 *",
        "	 * At the time the change is being done, just ignore whether we",
        "	 * are doing nop -> jump or jump -> nop transition, and assume",
        "	 * always nop being the 'currently valid' instruction",
        "	 */",
        "	if (init || system_state == SYSTEM_BOOTING) {",
        "		text_poke_early((void *)jump_entry_code(entry), jlp.code, jlp.size);",
        "		return;",
        "	}",
        "",
        "	text_poke_bp((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);",
        "}",
        "",
        "static void __ref jump_label_transform(struct jump_entry *entry,",
        "				       enum jump_label_type type,",
        "				       int init)",
        "{",
        "	mutex_lock(&text_mutex);",
        "	__jump_label_transform(entry, type, init);",
        "	mutex_unlock(&text_mutex);",
        "}",
        "",
        "void arch_jump_label_transform(struct jump_entry *entry,",
        "			       enum jump_label_type type)",
        "{",
        "	jump_label_transform(entry, type, 0);",
        "}",
        "",
        "bool arch_jump_label_transform_queue(struct jump_entry *entry,",
        "				     enum jump_label_type type)",
        "{",
        "	struct jump_label_patch jlp;",
        "",
        "	if (system_state == SYSTEM_BOOTING) {",
        "		/*",
        "		 * Fallback to the non-batching mode.",
        "		 */",
        "		arch_jump_label_transform(entry, type);",
        "		return true;",
        "	}",
        "",
        "	mutex_lock(&text_mutex);",
        "	jlp = __jump_label_patch(entry, type);",
        "	text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);",
        "	mutex_unlock(&text_mutex);",
        "	return true;",
        "}",
        "",
        "void arch_jump_label_transform_apply(void)",
        "{",
        "	mutex_lock(&text_mutex);",
        "	text_poke_finish();",
        "	mutex_unlock(&text_mutex);",
        "}"
    ]
  },
  "fs_userfaultfd_c": {
    path: "fs/userfaultfd.c",
    covered: [154, 380, 501, 220, 499, 289, 496, 292, 294, 523, 478, 167, 486, 364, 166, 504, 395, 465, 461, 329, 327],
    totalLines: 2202,
    coveredCount: 21,
    coveragePct: 1.0,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " *  fs/userfaultfd.c",
        " *",
        " *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>",
        " *  Copyright (C) 2008-2009 Red Hat, Inc.",
        " *  Copyright (C) 2015  Red Hat, Inc.",
        " *",
        " *  Some part derived from fs/eventfd.c (anon inode setup) and",
        " *  mm/ksm.c (mm hashing).",
        " */",
        "",
        "#include <linux/list.h>",
        "#include <linux/hashtable.h>",
        "#include <linux/sched/signal.h>",
        "#include <linux/sched/mm.h>",
        "#include <linux/mm.h>",
        "#include <linux/mm_inline.h>",
        "#include <linux/mmu_notifier.h>",
        "#include <linux/poll.h>",
        "#include <linux/slab.h>",
        "#include <linux/seq_file.h>",
        "#include <linux/file.h>",
        "#include <linux/bug.h>",
        "#include <linux/anon_inodes.h>",
        "#include <linux/syscalls.h>",
        "#include <linux/userfaultfd_k.h>",
        "#include <linux/mempolicy.h>",
        "#include <linux/ioctl.h>",
        "#include <linux/security.h>",
        "#include <linux/hugetlb.h>",
        "#include <linux/swapops.h>",
        "#include <linux/miscdevice.h>",
        "#include <linux/uio.h>",
        "",
        "static int sysctl_unprivileged_userfaultfd __read_mostly;",
        "",
        "#ifdef CONFIG_SYSCTL",
        "static struct ctl_table vm_userfaultfd_table[] = {",
        "	{",
        "		.procname	= \"unprivileged_userfaultfd\",",
        "		.data		= &sysctl_unprivileged_userfaultfd,",
        "		.maxlen		= sizeof(sysctl_unprivileged_userfaultfd),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_dointvec_minmax,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_ONE,",
        "	},",
        "};",
        "#endif",
        "",
        "static struct kmem_cache *userfaultfd_ctx_cachep __ro_after_init;",
        "",
        "struct userfaultfd_fork_ctx {",
        "	struct userfaultfd_ctx *orig;",
        "	struct userfaultfd_ctx *new;",
        "	struct list_head list;",
        "};",
        "",
        "struct userfaultfd_unmap_ctx {",
        "	struct userfaultfd_ctx *ctx;",
        "	unsigned long start;",
        "	unsigned long end;",
        "	struct list_head list;",
        "};",
        "",
        "struct userfaultfd_wait_queue {",
        "	struct uffd_msg msg;",
        "	wait_queue_entry_t wq;",
        "	struct userfaultfd_ctx *ctx;",
        "	bool waken;",
        "};",
        "",
        "struct userfaultfd_wake_range {",
        "	unsigned long start;",
        "	unsigned long len;",
        "};",
        "",
        "/* internal indication that UFFD_API ioctl was successfully executed */",
        "#define UFFD_FEATURE_INITIALIZED		(1u << 31)",
        "",
        "static bool userfaultfd_is_initialized(struct userfaultfd_ctx *ctx)",
        "{",
        "	return ctx->features & UFFD_FEATURE_INITIALIZED;",
        "}",
        "",
        "static bool userfaultfd_wp_async_ctx(struct userfaultfd_ctx *ctx)",
        "{",
        "	return ctx && (ctx->features & UFFD_FEATURE_WP_ASYNC);",
        "}",
        "",
        "/*",
        " * Whether WP_UNPOPULATED is enabled on the uffd context.  It is only",
        " * meaningful when userfaultfd_wp()==true on the vma and when it's",
        " * anonymous.",
        " */",
        "bool userfaultfd_wp_unpopulated(struct vm_area_struct *vma)",
        "{",
        "	struct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;",
        "",
        "	if (!ctx)",
        "		return false;",
        "",
        "	return ctx->features & UFFD_FEATURE_WP_UNPOPULATED;",
        "}",
        "",
        "static int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,",
        "				     int wake_flags, void *key)",
        "{",
        "	struct userfaultfd_wake_range *range = key;",
        "	int ret;",
        "	struct userfaultfd_wait_queue *uwq;",
        "	unsigned long start, len;",
        "",
        "	uwq = container_of(wq, struct userfaultfd_wait_queue, wq);",
        "	ret = 0;",
        "	/* len == 0 means wake all */",
        "	start = range->start;",
        "	len = range->len;",
        "	if (len && (start > uwq->msg.arg.pagefault.address ||",
        "		    start + len <= uwq->msg.arg.pagefault.address))",
        "		goto out;",
        "	WRITE_ONCE(uwq->waken, true);",
        "	/*",
        "	 * The Program-Order guarantees provided by the scheduler",
        "	 * ensure uwq->waken is visible before the task is woken.",
        "	 */",
        "	ret = wake_up_state(wq->private, mode);",
        "	if (ret) {",
        "		/*",
        "		 * Wake only once, autoremove behavior.",
        "		 *",
        "		 * After the effect of list_del_init is visible to the other",
        "		 * CPUs, the waitqueue may disappear from under us, see the",
        "		 * !list_empty_careful() in handle_userfault().",
        "		 *",
        "		 * try_to_wake_up() has an implicit smp_mb(), and the",
        "		 * wq->private is read before calling the extern function",
        "		 * \"wake_up_state\" (which in turns calls try_to_wake_up).",
        "		 */",
        "		list_del_init(&wq->entry);",
        "	}",
        "out:",
        "	return ret;",
        "}",
        "",
        "/**",
        " * userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd",
        " * context.",
        " * @ctx: [in] Pointer to the userfaultfd context.",
        " */",
        "static void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)",
        "{",
        "	refcount_inc(&ctx->refcount);",
        "}",
        "",
        "/**",
        " * userfaultfd_ctx_put - Releases a reference to the internal userfaultfd",
        " * context.",
        " * @ctx: [in] Pointer to userfaultfd context.",
        " *",
        " * The userfaultfd context reference must have been previously acquired either",
        " * with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().",
        " */",
        "static void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)",
        "{",
        "	if (refcount_dec_and_test(&ctx->refcount)) {",
        "		VM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));",
        "		VM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));",
        "		VM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));",
        "		VM_BUG_ON(waitqueue_active(&ctx->fault_wqh));",
        "		VM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));",
        "		VM_BUG_ON(waitqueue_active(&ctx->event_wqh));",
        "		VM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));",
        "		VM_BUG_ON(waitqueue_active(&ctx->fd_wqh));",
        "		mmdrop(ctx->mm);",
        "		kmem_cache_free(userfaultfd_ctx_cachep, ctx);",
        "	}",
        "}",
        "",
        "static inline void msg_init(struct uffd_msg *msg)",
        "{",
        "	BUILD_BUG_ON(sizeof(struct uffd_msg) != 32);",
        "	/*",
        "	 * Must use memset to zero out the paddings or kernel data is",
        "	 * leaked to userland.",
        "	 */",
        "	memset(msg, 0, sizeof(struct uffd_msg));",
        "}",
        "",
        "static inline struct uffd_msg userfault_msg(unsigned long address,",
        "					    unsigned long real_address,",
        "					    unsigned int flags,",
        "					    unsigned long reason,",
        "					    unsigned int features)",
        "{",
        "	struct uffd_msg msg;",
        "",
        "	msg_init(&msg);",
        "	msg.event = UFFD_EVENT_PAGEFAULT;",
        "",
        "	msg.arg.pagefault.address = (features & UFFD_FEATURE_EXACT_ADDRESS) ?",
        "				    real_address : address;",
        "",
        "	/*",
        "	 * These flags indicate why the userfault occurred:",
        "	 * - UFFD_PAGEFAULT_FLAG_WP indicates a write protect fault.",
        "	 * - UFFD_PAGEFAULT_FLAG_MINOR indicates a minor fault.",
        "	 * - Neither of these flags being set indicates a MISSING fault.",
        "	 *",
        "	 * Separately, UFFD_PAGEFAULT_FLAG_WRITE indicates it was a write",
        "	 * fault. Otherwise, it was a read fault.",
        "	 */",
        "	if (flags & FAULT_FLAG_WRITE)",
        "		msg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;",
        "	if (reason & VM_UFFD_WP)",
        "		msg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;",
        "	if (reason & VM_UFFD_MINOR)",
        "		msg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_MINOR;",
        "	if (features & UFFD_FEATURE_THREAD_ID)",
        "		msg.arg.pagefault.feat.ptid = task_pid_vnr(current);",
        "	return msg;",
        "}",
        "",
        "#ifdef CONFIG_HUGETLB_PAGE",
        "/*",
        " * Same functionality as userfaultfd_must_wait below with modifications for",
        " * hugepmd ranges.",
        " */",
        "static inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,",
        "					      struct vm_fault *vmf,",
        "					      unsigned long reason)",
        "{",
        "	struct vm_area_struct *vma = vmf->vma;",
        "	pte_t *ptep, pte;",
        "	bool ret = true;",
        "",
        "	assert_fault_locked(vmf);",
        "",
        "	ptep = hugetlb_walk(vma, vmf->address, vma_mmu_pagesize(vma));",
        "	if (!ptep)",
        "		goto out;",
        "",
        "	ret = false;",
        "	pte = huge_ptep_get(vma->vm_mm, vmf->address, ptep);",
        "",
        "	/*",
        "	 * Lockless access: we're in a wait_event so it's ok if it",
        "	 * changes under us.  PTE markers should be handled the same as none",
        "	 * ptes here.",
        "	 */",
        "	if (huge_pte_none_mostly(pte))",
        "		ret = true;",
        "	if (!huge_pte_write(pte) && (reason & VM_UFFD_WP))",
        "		ret = true;",
        "out:",
        "	return ret;",
        "}",
        "#else",
        "static inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,",
        "					      struct vm_fault *vmf,",
        "					      unsigned long reason)",
        "{",
        "	return false;	/* should never get here */",
        "}",
        "#endif /* CONFIG_HUGETLB_PAGE */",
        "",
        "/*",
        " * Verify the pagetables are still not ok after having reigstered into",
        " * the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any",
        " * userfault that has already been resolved, if userfaultfd_read_iter and",
        " * UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different",
        " * threads.",
        " */",
        "static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,",
        "					 struct vm_fault *vmf,",
        "					 unsigned long reason)",
        "{",
        "	struct mm_struct *mm = ctx->mm;",
        "	unsigned long address = vmf->address;",
        "	pgd_t *pgd;",
        "	p4d_t *p4d;",
        "	pud_t *pud;",
        "	pmd_t *pmd, _pmd;",
        "	pte_t *pte;",
        "	pte_t ptent;",
        "	bool ret = true;",
        "",
        "	assert_fault_locked(vmf);",
        "",
        "	pgd = pgd_offset(mm, address);",
        "	if (!pgd_present(*pgd))",
        "		goto out;",
        "	p4d = p4d_offset(pgd, address);",
        "	if (!p4d_present(*p4d))",
        "		goto out;",
        "	pud = pud_offset(p4d, address);",
        "	if (!pud_present(*pud))",
        "		goto out;",
        "	pmd = pmd_offset(pud, address);",
        "again:",
        "	_pmd = pmdp_get_lockless(pmd);",
        "	if (pmd_none(_pmd))",
        "		goto out;",
        "",
        "	ret = false;",
        "	if (!pmd_present(_pmd) || pmd_devmap(_pmd))",
        "		goto out;",
        "",
        "	if (pmd_trans_huge(_pmd)) {",
        "		if (!pmd_write(_pmd) && (reason & VM_UFFD_WP))",
        "			ret = true;",
        "		goto out;",
        "	}",
        "",
        "	pte = pte_offset_map(pmd, address);",
        "	if (!pte) {",
        "		ret = true;",
        "		goto again;",
        "	}",
        "	/*",
        "	 * Lockless access: we're in a wait_event so it's ok if it",
        "	 * changes under us.  PTE markers should be handled the same as none",
        "	 * ptes here.",
        "	 */",
        "	ptent = ptep_get(pte);",
        "	if (pte_none_mostly(ptent))",
        "		ret = true;",
        "	if (!pte_write(ptent) && (reason & VM_UFFD_WP))",
        "		ret = true;",
        "	pte_unmap(pte);",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)",
        "{",
        "	if (flags & FAULT_FLAG_INTERRUPTIBLE)",
        "		return TASK_INTERRUPTIBLE;",
        "",
        "	if (flags & FAULT_FLAG_KILLABLE)",
        "		return TASK_KILLABLE;",
        "",
        "	return TASK_UNINTERRUPTIBLE;",
        "}",
        "",
        "/*",
        " * The locking rules involved in returning VM_FAULT_RETRY depending on",
        " * FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and",
        " * FAULT_FLAG_KILLABLE are not straightforward. The \"Caution\"",
        " * recommendation in __lock_page_or_retry is not an understatement.",
        " *",
        " * If FAULT_FLAG_ALLOW_RETRY is set, the mmap_lock must be released",
        " * before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is",
        " * not set.",
        " *",
        " * If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not",
        " * set, VM_FAULT_RETRY can still be returned if and only if there are",
        " * fatal_signal_pending()s, and the mmap_lock must be released before",
        " * returning it.",
        " */",
        "vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)",
        "{",
        "	struct vm_area_struct *vma = vmf->vma;",
        "	struct mm_struct *mm = vma->vm_mm;",
        "	struct userfaultfd_ctx *ctx;",
        "	struct userfaultfd_wait_queue uwq;",
        "	vm_fault_t ret = VM_FAULT_SIGBUS;",
        "	bool must_wait;",
        "	unsigned int blocking_state;",
        "",
        "	/*",
        "	 * We don't do userfault handling for the final child pid update",
        "	 * and when coredumping (faults triggered by get_dump_page()).",
        "	 */",
        "	if (current->flags & (PF_EXITING|PF_DUMPCORE))",
        "		goto out;",
        "",
        "	assert_fault_locked(vmf);",
        "",
        "	ctx = vma->vm_userfaultfd_ctx.ctx;",
        "	if (!ctx)",
        "		goto out;",
        "",
        "	BUG_ON(ctx->mm != mm);",
        "",
        "	/* Any unrecognized flag is a bug. */",
        "	VM_BUG_ON(reason & ~__VM_UFFD_FLAGS);",
        "	/* 0 or > 1 flags set is a bug; we expect exactly 1. */",
        "	VM_BUG_ON(!reason || (reason & (reason - 1)));",
        "",
        "	if (ctx->features & UFFD_FEATURE_SIGBUS)",
        "		goto out;",
        "	if (!(vmf->flags & FAULT_FLAG_USER) && (ctx->flags & UFFD_USER_MODE_ONLY))",
        "		goto out;",
        "",
        "	/*",
        "	 * If it's already released don't get it. This avoids to loop",
        "	 * in __get_user_pages if userfaultfd_release waits on the",
        "	 * caller of handle_userfault to release the mmap_lock.",
        "	 */",
        "	if (unlikely(READ_ONCE(ctx->released))) {",
        "		/*",
        "		 * Don't return VM_FAULT_SIGBUS in this case, so a non",
        "		 * cooperative manager can close the uffd after the",
        "		 * last UFFDIO_COPY, without risking to trigger an",
        "		 * involuntary SIGBUS if the process was starting the",
        "		 * userfaultfd while the userfaultfd was still armed",
        "		 * (but after the last UFFDIO_COPY). If the uffd",
        "		 * wasn't already closed when the userfault reached",
        "		 * this point, that would normally be solved by",
        "		 * userfaultfd_must_wait returning 'false'.",
        "		 *",
        "		 * If we were to return VM_FAULT_SIGBUS here, the non",
        "		 * cooperative manager would be instead forced to",
        "		 * always call UFFDIO_UNREGISTER before it can safely",
        "		 * close the uffd.",
        "		 */",
        "		ret = VM_FAULT_NOPAGE;",
        "		goto out;",
        "	}",
        "",
        "	/*",
        "	 * Check that we can return VM_FAULT_RETRY.",
        "	 *",
        "	 * NOTE: it should become possible to return VM_FAULT_RETRY",
        "	 * even if FAULT_FLAG_TRIED is set without leading to gup()",
        "	 * -EBUSY failures, if the userfaultfd is to be extended for",
        "	 * VM_UFFD_WP tracking and we intend to arm the userfault",
        "	 * without first stopping userland access to the memory. For",
        "	 * VM_UFFD_MISSING userfaults this is enough for now.",
        "	 */",
        "	if (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {",
        "		/*",
        "		 * Validate the invariant that nowait must allow retry",
        "		 * to be sure not to return SIGBUS erroneously on",
        "		 * nowait invocations.",
        "		 */",
        "		BUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);",
        "#ifdef CONFIG_DEBUG_VM",
        "		if (printk_ratelimit()) {",
        "			printk(KERN_WARNING",
        "			       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",",
        "			       vmf->flags);",
        "			dump_stack();",
        "		}",
        "#endif",
        "		goto out;",
        "	}",
        "",
        "	/*",
        "	 * Handle nowait, not much to do other than tell it to retry",
        "	 * and wait.",
        "	 */",
        "	ret = VM_FAULT_RETRY;",
        "	if (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)",
        "		goto out;",
        "",
        "	/* take the reference before dropping the mmap_lock */",
        "	userfaultfd_ctx_get(ctx);",
        "",
        "	init_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);",
        "	uwq.wq.private = current;",
        "	uwq.msg = userfault_msg(vmf->address, vmf->real_address, vmf->flags,",
        "				reason, ctx->features);",
        "	uwq.ctx = ctx;",
        "	uwq.waken = false;",
        "",
        "	blocking_state = userfaultfd_get_blocking_state(vmf->flags);",
        "",
        "        /*",
        "         * Take the vma lock now, in order to safely call",
        "         * userfaultfd_huge_must_wait() later. Since acquiring the",
        "         * (sleepable) vma lock can modify the current task state, that",
        "         * must be before explicitly calling set_current_state().",
        "         */",
        "	if (is_vm_hugetlb_page(vma))",
        "		hugetlb_vma_lock_read(vma);",
        "",
        "	spin_lock_irq(&ctx->fault_pending_wqh.lock);",
        "	/*",
        "	 * After the __add_wait_queue the uwq is visible to userland",
        "	 * through poll/read().",
        "	 */",
        "	__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);",
        "	/*",
        "	 * The smp_mb() after __set_current_state prevents the reads",
        "	 * following the spin_unlock to happen before the list_add in",
        "	 * __add_wait_queue.",
        "	 */",
        "	set_current_state(blocking_state);",
        "	spin_unlock_irq(&ctx->fault_pending_wqh.lock);",
        "",
        "	if (!is_vm_hugetlb_page(vma))",
        "		must_wait = userfaultfd_must_wait(ctx, vmf, reason);",
        "	else",
        "		must_wait = userfaultfd_huge_must_wait(ctx, vmf, reason);",
        "	if (is_vm_hugetlb_page(vma))",
        "		hugetlb_vma_unlock_read(vma);",
        "	release_fault_lock(vmf);",
        "",
        "	if (likely(must_wait && !READ_ONCE(ctx->released))) {",
        "		wake_up_poll(&ctx->fd_wqh, EPOLLIN);",
        "		schedule();",
        "	}",
        "",
        "	__set_current_state(TASK_RUNNING);",
        "",
        "	/*",
        "	 * Here we race with the list_del; list_add in",
        "	 * userfaultfd_ctx_read(), however because we don't ever run",
        "	 * list_del_init() to refile across the two lists, the prev",
        "	 * and next pointers will never point to self. list_add also",
        "	 * would never let any of the two pointers to point to",
        "	 * self. So list_empty_careful won't risk to see both pointers",
        "	 * pointing to self at any time during the list refile. The",
        "	 * only case where list_del_init() is called is the full",
        "	 * removal in the wake function and there we don't re-list_add",
        "	 * and it's fine not to block on the spinlock. The uwq on this",
        "	 * kernel stack can be released after the list_del_init.",
        "	 */",
        "	if (!list_empty_careful(&uwq.wq.entry)) {",
        "		spin_lock_irq(&ctx->fault_pending_wqh.lock);",
        "		/*",
        "		 * No need of list_del_init(), the uwq on the stack",
        "		 * will be freed shortly anyway.",
        "		 */",
        "		list_del(&uwq.wq.entry);",
        "		spin_unlock_irq(&ctx->fault_pending_wqh.lock);",
        "	}",
        "",
        "	/*",
        "	 * ctx may go away after this if the userfault pseudo fd is",
        "	 * already released.",
        "	 */",
        "	userfaultfd_ctx_put(ctx);",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,",
        "					      struct userfaultfd_wait_queue *ewq)",
        "{",
        "	struct userfaultfd_ctx *release_new_ctx;",
        "",
        "	if (WARN_ON_ONCE(current->flags & PF_EXITING))",
        "		goto out;",
        "",
        "	ewq->ctx = ctx;",
        "	init_waitqueue_entry(&ewq->wq, current);",
        "	release_new_ctx = NULL;",
        "",
        "	spin_lock_irq(&ctx->event_wqh.lock);",
        "	/*",
        "	 * After the __add_wait_queue the uwq is visible to userland",
        "	 * through poll/read().",
        "	 */",
        "	__add_wait_queue(&ctx->event_wqh, &ewq->wq);",
        "	for (;;) {",
        "		set_current_state(TASK_KILLABLE);",
        "		if (ewq->msg.event == 0)",
        "			break;",
        "		if (READ_ONCE(ctx->released) ||",
        "		    fatal_signal_pending(current)) {",
        "			/*",
        "			 * &ewq->wq may be queued in fork_event, but",
        "			 * __remove_wait_queue ignores the head",
        "			 * parameter. It would be a problem if it",
        "			 * didn't.",
        "			 */",
        "			__remove_wait_queue(&ctx->event_wqh, &ewq->wq);",
        "			if (ewq->msg.event == UFFD_EVENT_FORK) {",
        "				struct userfaultfd_ctx *new;",
        "",
        "				new = (struct userfaultfd_ctx *)",
        "					(unsigned long)",
        "					ewq->msg.arg.reserved.reserved1;",
        "				release_new_ctx = new;",
        "			}",
        "			break;",
        "		}",
        "",
        "		spin_unlock_irq(&ctx->event_wqh.lock);",
        "",
        "		wake_up_poll(&ctx->fd_wqh, EPOLLIN);",
        "		schedule();",
        "",
        "		spin_lock_irq(&ctx->event_wqh.lock);",
        "	}",
        "	__set_current_state(TASK_RUNNING);",
        "	spin_unlock_irq(&ctx->event_wqh.lock);",
        "",
        "	if (release_new_ctx) {",
        "		userfaultfd_release_new(release_new_ctx);",
        "		userfaultfd_ctx_put(release_new_ctx);",
        "	}",
        "",
        "	/*",
        "	 * ctx may go away after this if the userfault pseudo fd is",
        "	 * already released.",
        "	 */",
        "out:",
        "	atomic_dec(&ctx->mmap_changing);",
        "	VM_BUG_ON(atomic_read(&ctx->mmap_changing) < 0);",
        "	userfaultfd_ctx_put(ctx);",
        "}",
        "",
        "static void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,",
        "				       struct userfaultfd_wait_queue *ewq)",
        "{",
        "	ewq->msg.event = 0;",
        "	wake_up_locked(&ctx->event_wqh);",
        "	__remove_wait_queue(&ctx->event_wqh, &ewq->wq);",
        "}",
        "",
        "int dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)",
        "{",
        "	struct userfaultfd_ctx *ctx = NULL, *octx;",
        "	struct userfaultfd_fork_ctx *fctx;",
        "",
        "	octx = vma->vm_userfaultfd_ctx.ctx;",
        "	if (!octx)",
        "		return 0;",
        "",
        "	if (!(octx->features & UFFD_FEATURE_EVENT_FORK)) {",
        "		userfaultfd_reset_ctx(vma);",
        "		return 0;",
        "	}",
        "",
        "	list_for_each_entry(fctx, fcs, list)",
        "		if (fctx->orig == octx) {",
        "			ctx = fctx->new;",
        "			break;",
        "		}",
        "",
        "	if (!ctx) {",
        "		fctx = kmalloc(sizeof(*fctx), GFP_KERNEL);",
        "		if (!fctx)",
        "			return -ENOMEM;",
        "",
        "		ctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);",
        "		if (!ctx) {",
        "			kfree(fctx);",
        "			return -ENOMEM;",
        "		}",
        "",
        "		refcount_set(&ctx->refcount, 1);",
        "		ctx->flags = octx->flags;",
        "		ctx->features = octx->features;",
        "		ctx->released = false;",
        "		init_rwsem(&ctx->map_changing_lock);",
        "		atomic_set(&ctx->mmap_changing, 0);",
        "		ctx->mm = vma->vm_mm;",
        "		mmgrab(ctx->mm);",
        "",
        "		userfaultfd_ctx_get(octx);",
        "		down_write(&octx->map_changing_lock);",
        "		atomic_inc(&octx->mmap_changing);",
        "		up_write(&octx->map_changing_lock);",
        "		fctx->orig = octx;",
        "		fctx->new = ctx;",
        "		list_add_tail(&fctx->list, fcs);",
        "	}",
        "",
        "	vma->vm_userfaultfd_ctx.ctx = ctx;",
        "	return 0;",
        "}",
        "",
        "static void dup_fctx(struct userfaultfd_fork_ctx *fctx)",
        "{",
        "	struct userfaultfd_ctx *ctx = fctx->orig;",
        "	struct userfaultfd_wait_queue ewq;",
        "",
        "	msg_init(&ewq.msg);",
        "",
        "	ewq.msg.event = UFFD_EVENT_FORK;",
        "	ewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;",
        "",
        "	userfaultfd_event_wait_completion(ctx, &ewq);",
        "}",
        "",
        "void dup_userfaultfd_complete(struct list_head *fcs)",
        "{",
        "	struct userfaultfd_fork_ctx *fctx, *n;",
        "",
        "	list_for_each_entry_safe(fctx, n, fcs, list) {",
        "		dup_fctx(fctx);",
        "		list_del(&fctx->list);",
        "		kfree(fctx);",
        "	}",
        "}",
        "",
        "void dup_userfaultfd_fail(struct list_head *fcs)",
        "{",
        "	struct userfaultfd_fork_ctx *fctx, *n;",
        "",
        "	/*",
        "	 * An error has occurred on fork, we will tear memory down, but have",
        "	 * allocated memory for fctx's and raised reference counts for both the",
        "	 * original and child contexts (and on the mm for each as a result).",
        "	 *",
        "	 * These would ordinarily be taken care of by a user handling the event,",
        "	 * but we are no longer doing so, so manually clean up here.",
        "	 *",
        "	 * mm tear down will take care of cleaning up VMA contexts.",
        "	 */",
        "	list_for_each_entry_safe(fctx, n, fcs, list) {",
        "		struct userfaultfd_ctx *octx = fctx->orig;",
        "		struct userfaultfd_ctx *ctx = fctx->new;",
        "",
        "		atomic_dec(&octx->mmap_changing);",
        "		VM_BUG_ON(atomic_read(&octx->mmap_changing) < 0);",
        "		userfaultfd_ctx_put(octx);",
        "		userfaultfd_ctx_put(ctx);",
        "",
        "		list_del(&fctx->list);",
        "		kfree(fctx);",
        "	}",
        "}",
        "",
        "void mremap_userfaultfd_prep(struct vm_area_struct *vma,",
        "			     struct vm_userfaultfd_ctx *vm_ctx)",
        "{",
        "	struct userfaultfd_ctx *ctx;",
        "",
        "	ctx = vma->vm_userfaultfd_ctx.ctx;",
        "",
        "	if (!ctx)",
        "		return;",
        "",
        "	if (ctx->features & UFFD_FEATURE_EVENT_REMAP) {",
        "		vm_ctx->ctx = ctx;",
        "		userfaultfd_ctx_get(ctx);",
        "		down_write(&ctx->map_changing_lock);",
        "		atomic_inc(&ctx->mmap_changing);",
        "		up_write(&ctx->map_changing_lock);",
        "	} else {",
        "		/* Drop uffd context if remap feature not enabled */",
        "		userfaultfd_reset_ctx(vma);",
        "	}",
        "}",
        "",
        "void mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,",
        "				 unsigned long from, unsigned long to,",
        "				 unsigned long len)",
        "{",
        "	struct userfaultfd_ctx *ctx = vm_ctx->ctx;",
        "	struct userfaultfd_wait_queue ewq;",
        "",
        "	if (!ctx)",
        "		return;",
        "",
        "	if (to & ~PAGE_MASK) {",
        "		userfaultfd_ctx_put(ctx);",
        "		return;",
        "	}",
        "",
        "	msg_init(&ewq.msg);",
        "",
        "	ewq.msg.event = UFFD_EVENT_REMAP;",
        "	ewq.msg.arg.remap.from = from;",
        "	ewq.msg.arg.remap.to = to;",
        "	ewq.msg.arg.remap.len = len;",
        "",
        "	userfaultfd_event_wait_completion(ctx, &ewq);",
        "}",
        "",
        "bool userfaultfd_remove(struct vm_area_struct *vma,",
        "			unsigned long start, unsigned long end)",
        "{",
        "	struct mm_struct *mm = vma->vm_mm;",
        "	struct userfaultfd_ctx *ctx;",
        "	struct userfaultfd_wait_queue ewq;",
        "",
        "	ctx = vma->vm_userfaultfd_ctx.ctx;",
        "	if (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))",
        "		return true;",
        "",
        "	userfaultfd_ctx_get(ctx);",
        "	down_write(&ctx->map_changing_lock);",
        "	atomic_inc(&ctx->mmap_changing);",
        "	up_write(&ctx->map_changing_lock);",
        "	mmap_read_unlock(mm);",
        "",
        "	msg_init(&ewq.msg);",
        "",
        "	ewq.msg.event = UFFD_EVENT_REMOVE;",
        "	ewq.msg.arg.remove.start = start;",
        "	ewq.msg.arg.remove.end = end;",
        "",
        "	userfaultfd_event_wait_completion(ctx, &ewq);",
        "",
        "	return false;",
        "}",
        "",
        "static bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,",
        "			  unsigned long start, unsigned long end)",
        "{",
        "	struct userfaultfd_unmap_ctx *unmap_ctx;",
        "",
        "	list_for_each_entry(unmap_ctx, unmaps, list)",
        "		if (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&",
        "		    unmap_ctx->end == end)",
        "			return true;",
        "",
        "	return false;",
        "}",
        "",
        "int userfaultfd_unmap_prep(struct vm_area_struct *vma, unsigned long start,",
        "			   unsigned long end, struct list_head *unmaps)",
        "{",
        "	struct userfaultfd_unmap_ctx *unmap_ctx;",
        "	struct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;",
        "",
        "	if (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||",
        "	    has_unmap_ctx(ctx, unmaps, start, end))",
        "		return 0;",
        "",
        "	unmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);",
        "	if (!unmap_ctx)",
        "		return -ENOMEM;",
        "",
        "	userfaultfd_ctx_get(ctx);",
        "	down_write(&ctx->map_changing_lock);",
        "	atomic_inc(&ctx->mmap_changing);",
        "	up_write(&ctx->map_changing_lock);",
        "	unmap_ctx->ctx = ctx;",
        "	unmap_ctx->start = start;",
        "	unmap_ctx->end = end;",
        "	list_add_tail(&unmap_ctx->list, unmaps);",
        "",
        "	return 0;",
        "}",
        "",
        "void userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)",
        "{",
        "	struct userfaultfd_unmap_ctx *ctx, *n;",
        "	struct userfaultfd_wait_queue ewq;",
        "",
        "	list_for_each_entry_safe(ctx, n, uf, list) {",
        "		msg_init(&ewq.msg);",
        "",
        "		ewq.msg.event = UFFD_EVENT_UNMAP;",
        "		ewq.msg.arg.remove.start = ctx->start;",
        "		ewq.msg.arg.remove.end = ctx->end;",
        "",
        "		userfaultfd_event_wait_completion(ctx->ctx, &ewq);",
        "",
        "		list_del(&ctx->list);",
        "		kfree(ctx);",
        "	}",
        "}",
        "",
        "static int userfaultfd_release(struct inode *inode, struct file *file)",
        "{",
        "	struct userfaultfd_ctx *ctx = file->private_data;",
        "	struct mm_struct *mm = ctx->mm;",
        "	/* len == 0 means wake all */",
        "	struct userfaultfd_wake_range range = { .len = 0, };",
        "",
        "	WRITE_ONCE(ctx->released, true);",
        "",
        "	userfaultfd_release_all(mm, ctx);",
        "",
        "	/*",
        "	 * After no new page faults can wait on this fault_*wqh, flush",
        "	 * the last page faults that may have been already waiting on",
        "	 * the fault_*wqh.",
        "	 */",
        "	spin_lock_irq(&ctx->fault_pending_wqh.lock);",
        "	__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);",
        "	__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);",
        "	spin_unlock_irq(&ctx->fault_pending_wqh.lock);",
        "",
        "	/* Flush pending events that may still wait on event_wqh */",
        "	wake_up_all(&ctx->event_wqh);",
        "",
        "	wake_up_poll(&ctx->fd_wqh, EPOLLHUP);",
        "	userfaultfd_ctx_put(ctx);",
        "	return 0;",
        "}",
        "",
        "/* fault_pending_wqh.lock must be hold by the caller */",
        "static inline struct userfaultfd_wait_queue *find_userfault_in(",
        "		wait_queue_head_t *wqh)",
        "{",
        "	wait_queue_entry_t *wq;",
        "	struct userfaultfd_wait_queue *uwq;",
        "",
        "	lockdep_assert_held(&wqh->lock);",
        "",
        "	uwq = NULL;",
        "	if (!waitqueue_active(wqh))",
        "		goto out;",
        "	/* walk in reverse to provide FIFO behavior to read userfaults */",
        "	wq = list_last_entry(&wqh->head, typeof(*wq), entry);",
        "	uwq = container_of(wq, struct userfaultfd_wait_queue, wq);",
        "out:",
        "	return uwq;",
        "}",
        "",
        "static inline struct userfaultfd_wait_queue *find_userfault(",
        "		struct userfaultfd_ctx *ctx)",
        "{",
        "	return find_userfault_in(&ctx->fault_pending_wqh);",
        "}",
        "",
        "static inline struct userfaultfd_wait_queue *find_userfault_evt(",
        "		struct userfaultfd_ctx *ctx)",
        "{",
        "	return find_userfault_in(&ctx->event_wqh);",
        "}",
        "",
        "static __poll_t userfaultfd_poll(struct file *file, poll_table *wait)",
        "{",
        "	struct userfaultfd_ctx *ctx = file->private_data;",
        "	__poll_t ret;",
        "",
        "	poll_wait(file, &ctx->fd_wqh, wait);",
        "",
        "	if (!userfaultfd_is_initialized(ctx))",
        "		return EPOLLERR;",
        "",
        "	/*",
        "	 * poll() never guarantees that read won't block.",
        "	 * userfaults can be waken before they're read().",
        "	 */",
        "	if (unlikely(!(file->f_flags & O_NONBLOCK)))",
        "		return EPOLLERR;",
        "	/*",
        "	 * lockless access to see if there are pending faults",
        "	 * __pollwait last action is the add_wait_queue but",
        "	 * the spin_unlock would allow the waitqueue_active to",
        "	 * pass above the actual list_add inside",
        "	 * add_wait_queue critical section. So use a full",
        "	 * memory barrier to serialize the list_add write of",
        "	 * add_wait_queue() with the waitqueue_active read",
        "	 * below.",
        "	 */",
        "	ret = 0;",
        "	smp_mb();",
        "	if (waitqueue_active(&ctx->fault_pending_wqh))",
        "		ret = EPOLLIN;",
        "	else if (waitqueue_active(&ctx->event_wqh))",
        "		ret = EPOLLIN;",
        "",
        "	return ret;",
        "}",
        "",
        "static const struct file_operations userfaultfd_fops;",
        "",
        "static int resolve_userfault_fork(struct userfaultfd_ctx *new,",
        "				  struct inode *inode,",
        "				  struct uffd_msg *msg)",
        "{",
        "	int fd;",
        "",
        "	fd = anon_inode_create_getfd(\"[userfaultfd]\", &userfaultfd_fops, new,",
        "			O_RDONLY | (new->flags & UFFD_SHARED_FCNTL_FLAGS), inode);",
        "	if (fd < 0)",
        "		return fd;",
        "",
        "	msg->arg.reserved.reserved1 = 0;",
        "	msg->arg.fork.ufd = fd;",
        "	return 0;",
        "}",
        "",
        "static ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,",
        "				    struct uffd_msg *msg, struct inode *inode)",
        "{",
        "	ssize_t ret;",
        "	DECLARE_WAITQUEUE(wait, current);",
        "	struct userfaultfd_wait_queue *uwq;",
        "	/*",
        "	 * Handling fork event requires sleeping operations, so",
        "	 * we drop the event_wqh lock, then do these ops, then",
        "	 * lock it back and wake up the waiter. While the lock is",
        "	 * dropped the ewq may go away so we keep track of it",
        "	 * carefully.",
        "	 */",
        "	LIST_HEAD(fork_event);",
        "	struct userfaultfd_ctx *fork_nctx = NULL;",
        "",
        "	/* always take the fd_wqh lock before the fault_pending_wqh lock */",
        "	spin_lock_irq(&ctx->fd_wqh.lock);",
        "	__add_wait_queue(&ctx->fd_wqh, &wait);",
        "	for (;;) {",
        "		set_current_state(TASK_INTERRUPTIBLE);",
        "		spin_lock(&ctx->fault_pending_wqh.lock);",
        "		uwq = find_userfault(ctx);",
        "		if (uwq) {",
        "			/*",
        "			 * Use a seqcount to repeat the lockless check",
        "			 * in wake_userfault() to avoid missing",
        "			 * wakeups because during the refile both",
        "			 * waitqueue could become empty if this is the",
        "			 * only userfault.",
        "			 */",
        "			write_seqcount_begin(&ctx->refile_seq);",
        "",
        "			/*",
        "			 * The fault_pending_wqh.lock prevents the uwq",
        "			 * to disappear from under us.",
        "			 *",
        "			 * Refile this userfault from",
        "			 * fault_pending_wqh to fault_wqh, it's not",
        "			 * pending anymore after we read it.",
        "			 *",
        "			 * Use list_del() by hand (as",
        "			 * userfaultfd_wake_function also uses",
        "			 * list_del_init() by hand) to be sure nobody",
        "			 * changes __remove_wait_queue() to use",
        "			 * list_del_init() in turn breaking the",
        "			 * !list_empty_careful() check in",
        "			 * handle_userfault(). The uwq->wq.head list",
        "			 * must never be empty at any time during the",
        "			 * refile, or the waitqueue could disappear",
        "			 * from under us. The \"wait_queue_head_t\"",
        "			 * parameter of __remove_wait_queue() is unused",
        "			 * anyway.",
        "			 */",
        "			list_del(&uwq->wq.entry);",
        "			add_wait_queue(&ctx->fault_wqh, &uwq->wq);",
        "",
        "			write_seqcount_end(&ctx->refile_seq);",
        "",
        "			/* careful to always initialize msg if ret == 0 */",
        "			*msg = uwq->msg;",
        "			spin_unlock(&ctx->fault_pending_wqh.lock);",
        "			ret = 0;",
        "			break;",
        "		}",
        "		spin_unlock(&ctx->fault_pending_wqh.lock);",
        "",
        "		spin_lock(&ctx->event_wqh.lock);",
        "		uwq = find_userfault_evt(ctx);",
        "		if (uwq) {",
        "			*msg = uwq->msg;",
        "",
        "			if (uwq->msg.event == UFFD_EVENT_FORK) {",
        "				fork_nctx = (struct userfaultfd_ctx *)",
        "					(unsigned long)",
        "					uwq->msg.arg.reserved.reserved1;",
        "				list_move(&uwq->wq.entry, &fork_event);",
        "				/*",
        "				 * fork_nctx can be freed as soon as",
        "				 * we drop the lock, unless we take a",
        "				 * reference on it.",
        "				 */",
        "				userfaultfd_ctx_get(fork_nctx);",
        "				spin_unlock(&ctx->event_wqh.lock);",
        "				ret = 0;",
        "				break;",
        "			}",
        "",
        "			userfaultfd_event_complete(ctx, uwq);",
        "			spin_unlock(&ctx->event_wqh.lock);",
        "			ret = 0;",
        "			break;",
        "		}",
        "		spin_unlock(&ctx->event_wqh.lock);",
        "",
        "		if (signal_pending(current)) {",
        "			ret = -ERESTARTSYS;",
        "			break;",
        "		}",
        "		if (no_wait) {",
        "			ret = -EAGAIN;",
        "			break;",
        "		}",
        "		spin_unlock_irq(&ctx->fd_wqh.lock);",
        "		schedule();",
        "		spin_lock_irq(&ctx->fd_wqh.lock);",
        "	}",
        "	__remove_wait_queue(&ctx->fd_wqh, &wait);",
        "	__set_current_state(TASK_RUNNING);",
        "	spin_unlock_irq(&ctx->fd_wqh.lock);",
        "",
        "	if (!ret && msg->event == UFFD_EVENT_FORK) {",
        "		ret = resolve_userfault_fork(fork_nctx, inode, msg);",
        "		spin_lock_irq(&ctx->event_wqh.lock);",
        "		if (!list_empty(&fork_event)) {",
        "			/*",
        "			 * The fork thread didn't abort, so we can",
        "			 * drop the temporary refcount.",
        "			 */",
        "			userfaultfd_ctx_put(fork_nctx);",
        "",
        "			uwq = list_first_entry(&fork_event,",
        "					       typeof(*uwq),",
        "					       wq.entry);",
        "			/*",
        "			 * If fork_event list wasn't empty and in turn",
        "			 * the event wasn't already released by fork",
        "			 * (the event is allocated on fork kernel",
        "			 * stack), put the event back to its place in",
        "			 * the event_wq. fork_event head will be freed",
        "			 * as soon as we return so the event cannot",
        "			 * stay queued there no matter the current",
        "			 * \"ret\" value.",
        "			 */",
        "			list_del(&uwq->wq.entry);",
        "			__add_wait_queue(&ctx->event_wqh, &uwq->wq);",
        "",
        "			/*",
        "			 * Leave the event in the waitqueue and report",
        "			 * error to userland if we failed to resolve",
        "			 * the userfault fork.",
        "			 */",
        "			if (likely(!ret))",
        "				userfaultfd_event_complete(ctx, uwq);",
        "		} else {",
        "			/*",
        "			 * Here the fork thread aborted and the",
        "			 * refcount from the fork thread on fork_nctx",
        "			 * has already been released. We still hold",
        "			 * the reference we took before releasing the",
        "			 * lock above. If resolve_userfault_fork",
        "			 * failed we've to drop it because the",
        "			 * fork_nctx has to be freed in such case. If",
        "			 * it succeeded we'll hold it because the new",
        "			 * uffd references it.",
        "			 */",
        "			if (ret)",
        "				userfaultfd_ctx_put(fork_nctx);",
        "		}",
        "		spin_unlock_irq(&ctx->event_wqh.lock);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "static ssize_t userfaultfd_read_iter(struct kiocb *iocb, struct iov_iter *to)",
        "{",
        "	struct file *file = iocb->ki_filp;",
        "	struct userfaultfd_ctx *ctx = file->private_data;",
        "	ssize_t _ret, ret = 0;",
        "	struct uffd_msg msg;",
        "	struct inode *inode = file_inode(file);",
        "	bool no_wait;",
        "",
        "	if (!userfaultfd_is_initialized(ctx))",
        "		return -EINVAL;",
        "",
        "	no_wait = file->f_flags & O_NONBLOCK || iocb->ki_flags & IOCB_NOWAIT;",
        "	for (;;) {",
        "		if (iov_iter_count(to) < sizeof(msg))",
        "			return ret ? ret : -EINVAL;",
        "		_ret = userfaultfd_ctx_read(ctx, no_wait, &msg, inode);",
        "		if (_ret < 0)",
        "			return ret ? ret : _ret;",
        "		_ret = !copy_to_iter_full(&msg, sizeof(msg), to);",
        "		if (_ret)",
        "			return ret ? ret : -EFAULT;",
        "		ret += sizeof(msg);",
        "		/*",
        "		 * Allow to read more than one fault at time but only",
        "		 * block if waiting for the very first one.",
        "		 */",
        "		no_wait = true;",
        "	}",
        "}",
        "",
        "static void __wake_userfault(struct userfaultfd_ctx *ctx,",
        "			     struct userfaultfd_wake_range *range)",
        "{",
        "	spin_lock_irq(&ctx->fault_pending_wqh.lock);",
        "	/* wake all in the range and autoremove */",
        "	if (waitqueue_active(&ctx->fault_pending_wqh))",
        "		__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,",
        "				     range);",
        "	if (waitqueue_active(&ctx->fault_wqh))",
        "		__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);",
        "	spin_unlock_irq(&ctx->fault_pending_wqh.lock);",
        "}",
        "",
        "static __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,",
        "					   struct userfaultfd_wake_range *range)",
        "{",
        "	unsigned seq;",
        "	bool need_wakeup;",
        "",
        "	/*",
        "	 * To be sure waitqueue_active() is not reordered by the CPU",
        "	 * before the pagetable update, use an explicit SMP memory",
        "	 * barrier here. PT lock release or mmap_read_unlock(mm) still",
        "	 * have release semantics that can allow the",
        "	 * waitqueue_active() to be reordered before the pte update.",
        "	 */",
        "	smp_mb();",
        "",
        "	/*",
        "	 * Use waitqueue_active because it's very frequent to",
        "	 * change the address space atomically even if there are no",
        "	 * userfaults yet. So we take the spinlock only when we're",
        "	 * sure we've userfaults to wake.",
        "	 */",
        "	do {",
        "		seq = read_seqcount_begin(&ctx->refile_seq);",
        "		need_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||",
        "			waitqueue_active(&ctx->fault_wqh);",
        "		cond_resched();",
        "	} while (read_seqcount_retry(&ctx->refile_seq, seq));",
        "	if (need_wakeup)",
        "		__wake_userfault(ctx, range);",
        "}",
        "",
        "static __always_inline int validate_unaligned_range(",
        "	struct mm_struct *mm, __u64 start, __u64 len)",
        "{",
        "	__u64 task_size = mm->task_size;",
        "",
        "	if (len & ~PAGE_MASK)",
        "		return -EINVAL;",
        "	if (!len)",
        "		return -EINVAL;",
        "	if (start < mmap_min_addr)",
        "		return -EINVAL;",
        "	if (start >= task_size)",
        "		return -EINVAL;",
        "	if (len > task_size - start)",
        "		return -EINVAL;",
        "	if (start + len <= start)",
        "		return -EINVAL;",
        "	return 0;",
        "}",
        "",
        "static __always_inline int validate_range(struct mm_struct *mm,",
        "					  __u64 start, __u64 len)",
        "{",
        "	if (start & ~PAGE_MASK)",
        "		return -EINVAL;",
        "",
        "	return validate_unaligned_range(mm, start, len);",
        "}",
        "",
        "static int userfaultfd_register(struct userfaultfd_ctx *ctx,",
        "				unsigned long arg)",
        "{",
        "	struct mm_struct *mm = ctx->mm;",
        "	struct vm_area_struct *vma, *cur;",
        "	int ret;",
        "	struct uffdio_register uffdio_register;",
        "	struct uffdio_register __user *user_uffdio_register;",
        "	unsigned long vm_flags;",
        "	bool found;",
        "	bool basic_ioctls;",
        "	unsigned long start, end;",
        "	struct vma_iterator vmi;",
        "	bool wp_async = userfaultfd_wp_async_ctx(ctx);",
        "",
        "	user_uffdio_register = (struct uffdio_register __user *) arg;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_register, user_uffdio_register,",
        "			   sizeof(uffdio_register)-sizeof(__u64)))",
        "		goto out;",
        "",
        "	ret = -EINVAL;",
        "	if (!uffdio_register.mode)",
        "		goto out;",
        "	if (uffdio_register.mode & ~UFFD_API_REGISTER_MODES)",
        "		goto out;",
        "	vm_flags = 0;",
        "	if (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)",
        "		vm_flags |= VM_UFFD_MISSING;",
        "	if (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {",
        "#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_WP",
        "		goto out;",
        "#endif",
        "		vm_flags |= VM_UFFD_WP;",
        "	}",
        "	if (uffdio_register.mode & UFFDIO_REGISTER_MODE_MINOR) {",
        "#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR",
        "		goto out;",
        "#endif",
        "		vm_flags |= VM_UFFD_MINOR;",
        "	}",
        "",
        "	ret = validate_range(mm, uffdio_register.range.start,",
        "			     uffdio_register.range.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	start = uffdio_register.range.start;",
        "	end = start + uffdio_register.range.len;",
        "",
        "	ret = -ENOMEM;",
        "	if (!mmget_not_zero(mm))",
        "		goto out;",
        "",
        "	ret = -EINVAL;",
        "	mmap_write_lock(mm);",
        "	vma_iter_init(&vmi, mm, start);",
        "	vma = vma_find(&vmi, end);",
        "	if (!vma)",
        "		goto out_unlock;",
        "",
        "	/*",
        "	 * If the first vma contains huge pages, make sure start address",
        "	 * is aligned to huge page size.",
        "	 */",
        "	if (is_vm_hugetlb_page(vma)) {",
        "		unsigned long vma_hpagesize = vma_kernel_pagesize(vma);",
        "",
        "		if (start & (vma_hpagesize - 1))",
        "			goto out_unlock;",
        "	}",
        "",
        "	/*",
        "	 * Search for not compatible vmas.",
        "	 */",
        "	found = false;",
        "	basic_ioctls = false;",
        "	cur = vma;",
        "	do {",
        "		cond_resched();",
        "",
        "		BUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^",
        "		       !!(cur->vm_flags & __VM_UFFD_FLAGS));",
        "",
        "		/* check not compatible vmas */",
        "		ret = -EINVAL;",
        "		if (!vma_can_userfault(cur, vm_flags, wp_async))",
        "			goto out_unlock;",
        "",
        "		/*",
        "		 * UFFDIO_COPY will fill file holes even without",
        "		 * PROT_WRITE. This check enforces that if this is a",
        "		 * MAP_SHARED, the process has write permission to the backing",
        "		 * file. If VM_MAYWRITE is set it also enforces that on a",
        "		 * MAP_SHARED vma: there is no F_WRITE_SEAL and no further",
        "		 * F_WRITE_SEAL can be taken until the vma is destroyed.",
        "		 */",
        "		ret = -EPERM;",
        "		if (unlikely(!(cur->vm_flags & VM_MAYWRITE)))",
        "			goto out_unlock;",
        "",
        "		/*",
        "		 * If this vma contains ending address, and huge pages",
        "		 * check alignment.",
        "		 */",
        "		if (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&",
        "		    end > cur->vm_start) {",
        "			unsigned long vma_hpagesize = vma_kernel_pagesize(cur);",
        "",
        "			ret = -EINVAL;",
        "",
        "			if (end & (vma_hpagesize - 1))",
        "				goto out_unlock;",
        "		}",
        "		if ((vm_flags & VM_UFFD_WP) && !(cur->vm_flags & VM_MAYWRITE))",
        "			goto out_unlock;",
        "",
        "		/*",
        "		 * Check that this vma isn't already owned by a",
        "		 * different userfaultfd. We can't allow more than one",
        "		 * userfaultfd to own a single vma simultaneously or we",
        "		 * wouldn't know which one to deliver the userfaults to.",
        "		 */",
        "		ret = -EBUSY;",
        "		if (cur->vm_userfaultfd_ctx.ctx &&",
        "		    cur->vm_userfaultfd_ctx.ctx != ctx)",
        "			goto out_unlock;",
        "",
        "		/*",
        "		 * Note vmas containing huge pages",
        "		 */",
        "		if (is_vm_hugetlb_page(cur))",
        "			basic_ioctls = true;",
        "",
        "		found = true;",
        "	} for_each_vma_range(vmi, cur, end);",
        "	BUG_ON(!found);",
        "",
        "	ret = userfaultfd_register_range(ctx, vma, vm_flags, start, end,",
        "					 wp_async);",
        "",
        "out_unlock:",
        "	mmap_write_unlock(mm);",
        "	mmput(mm);",
        "	if (!ret) {",
        "		__u64 ioctls_out;",
        "",
        "		ioctls_out = basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :",
        "		    UFFD_API_RANGE_IOCTLS;",
        "",
        "		/*",
        "		 * Declare the WP ioctl only if the WP mode is",
        "		 * specified and all checks passed with the range",
        "		 */",
        "		if (!(uffdio_register.mode & UFFDIO_REGISTER_MODE_WP))",
        "			ioctls_out &= ~((__u64)1 << _UFFDIO_WRITEPROTECT);",
        "",
        "		/* CONTINUE ioctl is only supported for MINOR ranges. */",
        "		if (!(uffdio_register.mode & UFFDIO_REGISTER_MODE_MINOR))",
        "			ioctls_out &= ~((__u64)1 << _UFFDIO_CONTINUE);",
        "",
        "		/*",
        "		 * Now that we scanned all vmas we can already tell",
        "		 * userland which ioctls methods are guaranteed to",
        "		 * succeed on this range.",
        "		 */",
        "		if (put_user(ioctls_out, &user_uffdio_register->ioctls))",
        "			ret = -EFAULT;",
        "	}",
        "out:",
        "	return ret;",
        "}",
        "",
        "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,",
        "				  unsigned long arg)",
        "{",
        "	struct mm_struct *mm = ctx->mm;",
        "	struct vm_area_struct *vma, *prev, *cur;",
        "	int ret;",
        "	struct uffdio_range uffdio_unregister;",
        "	bool found;",
        "	unsigned long start, end, vma_end;",
        "	const void __user *buf = (void __user *)arg;",
        "	struct vma_iterator vmi;",
        "	bool wp_async = userfaultfd_wp_async_ctx(ctx);",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))",
        "		goto out;",
        "",
        "	ret = validate_range(mm, uffdio_unregister.start,",
        "			     uffdio_unregister.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	start = uffdio_unregister.start;",
        "	end = start + uffdio_unregister.len;",
        "",
        "	ret = -ENOMEM;",
        "	if (!mmget_not_zero(mm))",
        "		goto out;",
        "",
        "	mmap_write_lock(mm);",
        "	ret = -EINVAL;",
        "	vma_iter_init(&vmi, mm, start);",
        "	vma = vma_find(&vmi, end);",
        "	if (!vma)",
        "		goto out_unlock;",
        "",
        "	/*",
        "	 * If the first vma contains huge pages, make sure start address",
        "	 * is aligned to huge page size.",
        "	 */",
        "	if (is_vm_hugetlb_page(vma)) {",
        "		unsigned long vma_hpagesize = vma_kernel_pagesize(vma);",
        "",
        "		if (start & (vma_hpagesize - 1))",
        "			goto out_unlock;",
        "	}",
        "",
        "	/*",
        "	 * Search for not compatible vmas.",
        "	 */",
        "	found = false;",
        "	cur = vma;",
        "	do {",
        "		cond_resched();",
        "",
        "		BUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^",
        "		       !!(cur->vm_flags & __VM_UFFD_FLAGS));",
        "",
        "		/*",
        "		 * Check not compatible vmas, not strictly required",
        "		 * here as not compatible vmas cannot have an",
        "		 * userfaultfd_ctx registered on them, but this",
        "		 * provides for more strict behavior to notice",
        "		 * unregistration errors.",
        "		 */",
        "		if (!vma_can_userfault(cur, cur->vm_flags, wp_async))",
        "			goto out_unlock;",
        "",
        "		found = true;",
        "	} for_each_vma_range(vmi, cur, end);",
        "	BUG_ON(!found);",
        "",
        "	vma_iter_set(&vmi, start);",
        "	prev = vma_prev(&vmi);",
        "	if (vma->vm_start < start)",
        "		prev = vma;",
        "",
        "	ret = 0;",
        "	for_each_vma_range(vmi, vma, end) {",
        "		cond_resched();",
        "",
        "		BUG_ON(!vma_can_userfault(vma, vma->vm_flags, wp_async));",
        "",
        "		/*",
        "		 * Nothing to do: this vma is already registered into this",
        "		 * userfaultfd and with the right tracking mode too.",
        "		 */",
        "		if (!vma->vm_userfaultfd_ctx.ctx)",
        "			goto skip;",
        "",
        "		WARN_ON(!(vma->vm_flags & VM_MAYWRITE));",
        "",
        "		if (vma->vm_start > start)",
        "			start = vma->vm_start;",
        "		vma_end = min(end, vma->vm_end);",
        "",
        "		if (userfaultfd_missing(vma)) {",
        "			/*",
        "			 * Wake any concurrent pending userfault while",
        "			 * we unregister, so they will not hang",
        "			 * permanently and it avoids userland to call",
        "			 * UFFDIO_WAKE explicitly.",
        "			 */",
        "			struct userfaultfd_wake_range range;",
        "			range.start = start;",
        "			range.len = vma_end - start;",
        "			wake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);",
        "		}",
        "",
        "		vma = userfaultfd_clear_vma(&vmi, prev, vma,",
        "					    start, vma_end);",
        "		if (IS_ERR(vma)) {",
        "			ret = PTR_ERR(vma);",
        "			break;",
        "		}",
        "",
        "	skip:",
        "		prev = vma;",
        "		start = vma->vm_end;",
        "	}",
        "",
        "out_unlock:",
        "	mmap_write_unlock(mm);",
        "	mmput(mm);",
        "out:",
        "	return ret;",
        "}",
        "",
        "/*",
        " * userfaultfd_wake may be used in combination with the",
        " * UFFDIO_*_MODE_DONTWAKE to wakeup userfaults in batches.",
        " */",
        "static int userfaultfd_wake(struct userfaultfd_ctx *ctx,",
        "			    unsigned long arg)",
        "{",
        "	int ret;",
        "	struct uffdio_range uffdio_wake;",
        "	struct userfaultfd_wake_range range;",
        "	const void __user *buf = (void __user *)arg;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))",
        "		goto out;",
        "",
        "	ret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	range.start = uffdio_wake.start;",
        "	range.len = uffdio_wake.len;",
        "",
        "	/*",
        "	 * len == 0 means wake all and we don't want to wake all here,",
        "	 * so check it again to be sure.",
        "	 */",
        "	VM_BUG_ON(!range.len);",
        "",
        "	wake_userfault(ctx, &range);",
        "	ret = 0;",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "static int userfaultfd_copy(struct userfaultfd_ctx *ctx,",
        "			    unsigned long arg)",
        "{",
        "	__s64 ret;",
        "	struct uffdio_copy uffdio_copy;",
        "	struct uffdio_copy __user *user_uffdio_copy;",
        "	struct userfaultfd_wake_range range;",
        "	uffd_flags_t flags = 0;",
        "",
        "	user_uffdio_copy = (struct uffdio_copy __user *) arg;",
        "",
        "	ret = -EAGAIN;",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		goto out;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_copy, user_uffdio_copy,",
        "			   /* don't copy \"copy\" last field */",
        "			   sizeof(uffdio_copy)-sizeof(__s64)))",
        "		goto out;",
        "",
        "	ret = validate_unaligned_range(ctx->mm, uffdio_copy.src,",
        "				       uffdio_copy.len);",
        "	if (ret)",
        "		goto out;",
        "	ret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	ret = -EINVAL;",
        "	if (uffdio_copy.mode & ~(UFFDIO_COPY_MODE_DONTWAKE|UFFDIO_COPY_MODE_WP))",
        "		goto out;",
        "	if (uffdio_copy.mode & UFFDIO_COPY_MODE_WP)",
        "		flags |= MFILL_ATOMIC_WP;",
        "	if (mmget_not_zero(ctx->mm)) {",
        "		ret = mfill_atomic_copy(ctx, uffdio_copy.dst, uffdio_copy.src,",
        "					uffdio_copy.len, flags);",
        "		mmput(ctx->mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "	if (unlikely(put_user(ret, &user_uffdio_copy->copy)))",
        "		return -EFAULT;",
        "	if (ret < 0)",
        "		goto out;",
        "	BUG_ON(!ret);",
        "	/* len == 0 would wake all */",
        "	range.len = ret;",
        "	if (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {",
        "		range.start = uffdio_copy.dst;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	ret = range.len == uffdio_copy.len ? 0 : -EAGAIN;",
        "out:",
        "	return ret;",
        "}",
        "",
        "static int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,",
        "				unsigned long arg)",
        "{",
        "	__s64 ret;",
        "	struct uffdio_zeropage uffdio_zeropage;",
        "	struct uffdio_zeropage __user *user_uffdio_zeropage;",
        "	struct userfaultfd_wake_range range;",
        "",
        "	user_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;",
        "",
        "	ret = -EAGAIN;",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		goto out;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,",
        "			   /* don't copy \"zeropage\" last field */",
        "			   sizeof(uffdio_zeropage)-sizeof(__s64)))",
        "		goto out;",
        "",
        "	ret = validate_range(ctx->mm, uffdio_zeropage.range.start,",
        "			     uffdio_zeropage.range.len);",
        "	if (ret)",
        "		goto out;",
        "	ret = -EINVAL;",
        "	if (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)",
        "		goto out;",
        "",
        "	if (mmget_not_zero(ctx->mm)) {",
        "		ret = mfill_atomic_zeropage(ctx, uffdio_zeropage.range.start,",
        "					   uffdio_zeropage.range.len);",
        "		mmput(ctx->mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "	if (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))",
        "		return -EFAULT;",
        "	if (ret < 0)",
        "		goto out;",
        "	/* len == 0 would wake all */",
        "	BUG_ON(!ret);",
        "	range.len = ret;",
        "	if (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {",
        "		range.start = uffdio_zeropage.range.start;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	ret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;",
        "out:",
        "	return ret;",
        "}",
        "",
        "static int userfaultfd_writeprotect(struct userfaultfd_ctx *ctx,",
        "				    unsigned long arg)",
        "{",
        "	int ret;",
        "	struct uffdio_writeprotect uffdio_wp;",
        "	struct uffdio_writeprotect __user *user_uffdio_wp;",
        "	struct userfaultfd_wake_range range;",
        "	bool mode_wp, mode_dontwake;",
        "",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		return -EAGAIN;",
        "",
        "	user_uffdio_wp = (struct uffdio_writeprotect __user *) arg;",
        "",
        "	if (copy_from_user(&uffdio_wp, user_uffdio_wp,",
        "			   sizeof(struct uffdio_writeprotect)))",
        "		return -EFAULT;",
        "",
        "	ret = validate_range(ctx->mm, uffdio_wp.range.start,",
        "			     uffdio_wp.range.len);",
        "	if (ret)",
        "		return ret;",
        "",
        "	if (uffdio_wp.mode & ~(UFFDIO_WRITEPROTECT_MODE_DONTWAKE |",
        "			       UFFDIO_WRITEPROTECT_MODE_WP))",
        "		return -EINVAL;",
        "",
        "	mode_wp = uffdio_wp.mode & UFFDIO_WRITEPROTECT_MODE_WP;",
        "	mode_dontwake = uffdio_wp.mode & UFFDIO_WRITEPROTECT_MODE_DONTWAKE;",
        "",
        "	if (mode_wp && mode_dontwake)",
        "		return -EINVAL;",
        "",
        "	if (mmget_not_zero(ctx->mm)) {",
        "		ret = mwriteprotect_range(ctx, uffdio_wp.range.start,",
        "					  uffdio_wp.range.len, mode_wp);",
        "		mmput(ctx->mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "",
        "	if (ret)",
        "		return ret;",
        "",
        "	if (!mode_wp && !mode_dontwake) {",
        "		range.start = uffdio_wp.range.start;",
        "		range.len = uffdio_wp.range.len;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	return ret;",
        "}",
        "",
        "static int userfaultfd_continue(struct userfaultfd_ctx *ctx, unsigned long arg)",
        "{",
        "	__s64 ret;",
        "	struct uffdio_continue uffdio_continue;",
        "	struct uffdio_continue __user *user_uffdio_continue;",
        "	struct userfaultfd_wake_range range;",
        "	uffd_flags_t flags = 0;",
        "",
        "	user_uffdio_continue = (struct uffdio_continue __user *)arg;",
        "",
        "	ret = -EAGAIN;",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		goto out;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_continue, user_uffdio_continue,",
        "			   /* don't copy the output fields */",
        "			   sizeof(uffdio_continue) - (sizeof(__s64))))",
        "		goto out;",
        "",
        "	ret = validate_range(ctx->mm, uffdio_continue.range.start,",
        "			     uffdio_continue.range.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	ret = -EINVAL;",
        "	if (uffdio_continue.mode & ~(UFFDIO_CONTINUE_MODE_DONTWAKE |",
        "				     UFFDIO_CONTINUE_MODE_WP))",
        "		goto out;",
        "	if (uffdio_continue.mode & UFFDIO_CONTINUE_MODE_WP)",
        "		flags |= MFILL_ATOMIC_WP;",
        "",
        "	if (mmget_not_zero(ctx->mm)) {",
        "		ret = mfill_atomic_continue(ctx, uffdio_continue.range.start,",
        "					    uffdio_continue.range.len, flags);",
        "		mmput(ctx->mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "",
        "	if (unlikely(put_user(ret, &user_uffdio_continue->mapped)))",
        "		return -EFAULT;",
        "	if (ret < 0)",
        "		goto out;",
        "",
        "	/* len == 0 would wake all */",
        "	BUG_ON(!ret);",
        "	range.len = ret;",
        "	if (!(uffdio_continue.mode & UFFDIO_CONTINUE_MODE_DONTWAKE)) {",
        "		range.start = uffdio_continue.range.start;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	ret = range.len == uffdio_continue.range.len ? 0 : -EAGAIN;",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "static inline int userfaultfd_poison(struct userfaultfd_ctx *ctx, unsigned long arg)",
        "{",
        "	__s64 ret;",
        "	struct uffdio_poison uffdio_poison;",
        "	struct uffdio_poison __user *user_uffdio_poison;",
        "	struct userfaultfd_wake_range range;",
        "",
        "	user_uffdio_poison = (struct uffdio_poison __user *)arg;",
        "",
        "	ret = -EAGAIN;",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		goto out;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_poison, user_uffdio_poison,",
        "			   /* don't copy the output fields */",
        "			   sizeof(uffdio_poison) - (sizeof(__s64))))",
        "		goto out;",
        "",
        "	ret = validate_range(ctx->mm, uffdio_poison.range.start,",
        "			     uffdio_poison.range.len);",
        "	if (ret)",
        "		goto out;",
        "",
        "	ret = -EINVAL;",
        "	if (uffdio_poison.mode & ~UFFDIO_POISON_MODE_DONTWAKE)",
        "		goto out;",
        "",
        "	if (mmget_not_zero(ctx->mm)) {",
        "		ret = mfill_atomic_poison(ctx, uffdio_poison.range.start,",
        "					  uffdio_poison.range.len, 0);",
        "		mmput(ctx->mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "",
        "	if (unlikely(put_user(ret, &user_uffdio_poison->updated)))",
        "		return -EFAULT;",
        "	if (ret < 0)",
        "		goto out;",
        "",
        "	/* len == 0 would wake all */",
        "	BUG_ON(!ret);",
        "	range.len = ret;",
        "	if (!(uffdio_poison.mode & UFFDIO_POISON_MODE_DONTWAKE)) {",
        "		range.start = uffdio_poison.range.start;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	ret = range.len == uffdio_poison.range.len ? 0 : -EAGAIN;",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "bool userfaultfd_wp_async(struct vm_area_struct *vma)",
        "{",
        "	return userfaultfd_wp_async_ctx(vma->vm_userfaultfd_ctx.ctx);",
        "}",
        "",
        "static inline unsigned int uffd_ctx_features(__u64 user_features)",
        "{",
        "	/*",
        "	 * For the current set of features the bits just coincide. Set",
        "	 * UFFD_FEATURE_INITIALIZED to mark the features as enabled.",
        "	 */",
        "	return (unsigned int)user_features | UFFD_FEATURE_INITIALIZED;",
        "}",
        "",
        "static int userfaultfd_move(struct userfaultfd_ctx *ctx,",
        "			    unsigned long arg)",
        "{",
        "	__s64 ret;",
        "	struct uffdio_move uffdio_move;",
        "	struct uffdio_move __user *user_uffdio_move;",
        "	struct userfaultfd_wake_range range;",
        "	struct mm_struct *mm = ctx->mm;",
        "",
        "	user_uffdio_move = (struct uffdio_move __user *) arg;",
        "",
        "	if (atomic_read(&ctx->mmap_changing))",
        "		return -EAGAIN;",
        "",
        "	if (copy_from_user(&uffdio_move, user_uffdio_move,",
        "			   /* don't copy \"move\" last field */",
        "			   sizeof(uffdio_move)-sizeof(__s64)))",
        "		return -EFAULT;",
        "",
        "	/* Do not allow cross-mm moves. */",
        "	if (mm != current->mm)",
        "		return -EINVAL;",
        "",
        "	ret = validate_range(mm, uffdio_move.dst, uffdio_move.len);",
        "	if (ret)",
        "		return ret;",
        "",
        "	ret = validate_range(mm, uffdio_move.src, uffdio_move.len);",
        "	if (ret)",
        "		return ret;",
        "",
        "	if (uffdio_move.mode & ~(UFFDIO_MOVE_MODE_ALLOW_SRC_HOLES|",
        "				  UFFDIO_MOVE_MODE_DONTWAKE))",
        "		return -EINVAL;",
        "",
        "	if (mmget_not_zero(mm)) {",
        "		ret = move_pages(ctx, uffdio_move.dst, uffdio_move.src,",
        "				 uffdio_move.len, uffdio_move.mode);",
        "		mmput(mm);",
        "	} else {",
        "		return -ESRCH;",
        "	}",
        "",
        "	if (unlikely(put_user(ret, &user_uffdio_move->move)))",
        "		return -EFAULT;",
        "	if (ret < 0)",
        "		goto out;",
        "",
        "	/* len == 0 would wake all */",
        "	VM_WARN_ON(!ret);",
        "	range.len = ret;",
        "	if (!(uffdio_move.mode & UFFDIO_MOVE_MODE_DONTWAKE)) {",
        "		range.start = uffdio_move.dst;",
        "		wake_userfault(ctx, &range);",
        "	}",
        "	ret = range.len == uffdio_move.len ? 0 : -EAGAIN;",
        "",
        "out:",
        "	return ret;",
        "}",
        "",
        "/*",
        " * userland asks for a certain API version and we return which bits",
        " * and ioctl commands are implemented in this kernel for such API",
        " * version or -EINVAL if unknown.",
        " */",
        "static int userfaultfd_api(struct userfaultfd_ctx *ctx,",
        "			   unsigned long arg)",
        "{",
        "	struct uffdio_api uffdio_api;",
        "	void __user *buf = (void __user *)arg;",
        "	unsigned int ctx_features;",
        "	int ret;",
        "	__u64 features;",
        "",
        "	ret = -EFAULT;",
        "	if (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))",
        "		goto out;",
        "	features = uffdio_api.features;",
        "	ret = -EINVAL;",
        "	if (uffdio_api.api != UFFD_API)",
        "		goto err_out;",
        "	ret = -EPERM;",
        "	if ((features & UFFD_FEATURE_EVENT_FORK) && !capable(CAP_SYS_PTRACE))",
        "		goto err_out;",
        "",
        "	/* WP_ASYNC relies on WP_UNPOPULATED, choose it unconditionally */",
        "	if (features & UFFD_FEATURE_WP_ASYNC)",
        "		features |= UFFD_FEATURE_WP_UNPOPULATED;",
        "",
        "	/* report all available features and ioctls to userland */",
        "	uffdio_api.features = UFFD_API_FEATURES;",
        "#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR",
        "	uffdio_api.features &=",
        "		~(UFFD_FEATURE_MINOR_HUGETLBFS | UFFD_FEATURE_MINOR_SHMEM);",
        "#endif",
        "#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_WP",
        "	uffdio_api.features &= ~UFFD_FEATURE_PAGEFAULT_FLAG_WP;",
        "#endif",
        "#ifndef CONFIG_PTE_MARKER_UFFD_WP",
        "	uffdio_api.features &= ~UFFD_FEATURE_WP_HUGETLBFS_SHMEM;",
        "	uffdio_api.features &= ~UFFD_FEATURE_WP_UNPOPULATED;",
        "	uffdio_api.features &= ~UFFD_FEATURE_WP_ASYNC;",
        "#endif",
        "",
        "	ret = -EINVAL;",
        "	if (features & ~uffdio_api.features)",
        "		goto err_out;",
        "",
        "	uffdio_api.ioctls = UFFD_API_IOCTLS;",
        "	ret = -EFAULT;",
        "	if (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))",
        "		goto out;",
        "",
        "	/* only enable the requested features for this uffd context */",
        "	ctx_features = uffd_ctx_features(features);",
        "	ret = -EINVAL;",
        "	if (cmpxchg(&ctx->features, 0, ctx_features) != 0)",
        "		goto err_out;",
        "",
        "	ret = 0;",
        "out:",
        "	return ret;",
        "err_out:",
        "	memset(&uffdio_api, 0, sizeof(uffdio_api));",
        "	if (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))",
        "		ret = -EFAULT;",
        "	goto out;",
        "}",
        "",
        "static long userfaultfd_ioctl(struct file *file, unsigned cmd,",
        "			      unsigned long arg)",
        "{",
        "	int ret = -EINVAL;",
        "	struct userfaultfd_ctx *ctx = file->private_data;",
        "",
        "	if (cmd != UFFDIO_API && !userfaultfd_is_initialized(ctx))",
        "		return -EINVAL;",
        "",
        "	switch(cmd) {",
        "	case UFFDIO_API:",
        "		ret = userfaultfd_api(ctx, arg);",
        "		break;",
        "	case UFFDIO_REGISTER:",
        "		ret = userfaultfd_register(ctx, arg);",
        "		break;",
        "	case UFFDIO_UNREGISTER:",
        "		ret = userfaultfd_unregister(ctx, arg);",
        "		break;",
        "	case UFFDIO_WAKE:",
        "		ret = userfaultfd_wake(ctx, arg);",
        "		break;",
        "	case UFFDIO_COPY:",
        "		ret = userfaultfd_copy(ctx, arg);",
        "		break;",
        "	case UFFDIO_ZEROPAGE:",
        "		ret = userfaultfd_zeropage(ctx, arg);",
        "		break;",
        "	case UFFDIO_MOVE:",
        "		ret = userfaultfd_move(ctx, arg);",
        "		break;",
        "	case UFFDIO_WRITEPROTECT:",
        "		ret = userfaultfd_writeprotect(ctx, arg);",
        "		break;",
        "	case UFFDIO_CONTINUE:",
        "		ret = userfaultfd_continue(ctx, arg);",
        "		break;",
        "	case UFFDIO_POISON:",
        "		ret = userfaultfd_poison(ctx, arg);",
        "		break;",
        "	}",
        "	return ret;",
        "}",
        "",
        "#ifdef CONFIG_PROC_FS",
        "static void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)",
        "{",
        "	struct userfaultfd_ctx *ctx = f->private_data;",
        "	wait_queue_entry_t *wq;",
        "	unsigned long pending = 0, total = 0;",
        "",
        "	spin_lock_irq(&ctx->fault_pending_wqh.lock);",
        "	list_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {",
        "		pending++;",
        "		total++;",
        "	}",
        "	list_for_each_entry(wq, &ctx->fault_wqh.head, entry) {",
        "		total++;",
        "	}",
        "	spin_unlock_irq(&ctx->fault_pending_wqh.lock);",
        "",
        "	/*",
        "	 * If more protocols will be added, there will be all shown",
        "	 * separated by a space. Like this:",
        "	 *	protocols: aa:... bb:...",
        "	 */",
        "	seq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",",
        "		   pending, total, UFFD_API, ctx->features,",
        "		   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);",
        "}",
        "#endif",
        "",
        "static const struct file_operations userfaultfd_fops = {",
        "#ifdef CONFIG_PROC_FS",
        "	.show_fdinfo	= userfaultfd_show_fdinfo,",
        "#endif",
        "	.release	= userfaultfd_release,",
        "	.poll		= userfaultfd_poll,",
        "	.read_iter	= userfaultfd_read_iter,",
        "	.unlocked_ioctl = userfaultfd_ioctl,",
        "	.compat_ioctl	= compat_ptr_ioctl,",
        "	.llseek		= noop_llseek,",
        "};",
        "",
        "static void init_once_userfaultfd_ctx(void *mem)",
        "{",
        "	struct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;",
        "",
        "	init_waitqueue_head(&ctx->fault_pending_wqh);",
        "	init_waitqueue_head(&ctx->fault_wqh);",
        "	init_waitqueue_head(&ctx->event_wqh);",
        "	init_waitqueue_head(&ctx->fd_wqh);",
        "	seqcount_spinlock_init(&ctx->refile_seq, &ctx->fault_pending_wqh.lock);",
        "}",
        "",
        "static int new_userfaultfd(int flags)",
        "{",
        "	struct userfaultfd_ctx *ctx;",
        "	struct file *file;",
        "	int fd;",
        "",
        "	BUG_ON(!current->mm);",
        "",
        "	/* Check the UFFD_* constants for consistency.  */",
        "	BUILD_BUG_ON(UFFD_USER_MODE_ONLY & UFFD_SHARED_FCNTL_FLAGS);",
        "	BUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);",
        "	BUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);",
        "",
        "	if (flags & ~(UFFD_SHARED_FCNTL_FLAGS | UFFD_USER_MODE_ONLY))",
        "		return -EINVAL;",
        "",
        "	ctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);",
        "	if (!ctx)",
        "		return -ENOMEM;",
        "",
        "	refcount_set(&ctx->refcount, 1);",
        "	ctx->flags = flags;",
        "	ctx->features = 0;",
        "	ctx->released = false;",
        "	init_rwsem(&ctx->map_changing_lock);",
        "	atomic_set(&ctx->mmap_changing, 0);",
        "	ctx->mm = current->mm;",
        "",
        "	fd = get_unused_fd_flags(flags & UFFD_SHARED_FCNTL_FLAGS);",
        "	if (fd < 0)",
        "		goto err_out;",
        "",
        "	/* Create a new inode so that the LSM can block the creation.  */",
        "	file = anon_inode_create_getfile(\"[userfaultfd]\", &userfaultfd_fops, ctx,",
        "			O_RDONLY | (flags & UFFD_SHARED_FCNTL_FLAGS), NULL);",
        "	if (IS_ERR(file)) {",
        "		put_unused_fd(fd);",
        "		fd = PTR_ERR(file);",
        "		goto err_out;",
        "	}",
        "	/* prevent the mm struct to be freed */",
        "	mmgrab(ctx->mm);",
        "	file->f_mode |= FMODE_NOWAIT;",
        "	fd_install(fd, file);",
        "	return fd;",
        "err_out:",
        "	kmem_cache_free(userfaultfd_ctx_cachep, ctx);",
        "	return fd;",
        "}",
        "",
        "static inline bool userfaultfd_syscall_allowed(int flags)",
        "{",
        "	/* Userspace-only page faults are always allowed */",
        "	if (flags & UFFD_USER_MODE_ONLY)",
        "		return true;",
        "",
        "	/*",
        "	 * The user is requesting a userfaultfd which can handle kernel faults.",
        "	 * Privileged users are always allowed to do this.",
        "	 */",
        "	if (capable(CAP_SYS_PTRACE))",
        "		return true;",
        "",
        "	/* Otherwise, access to kernel fault handling is sysctl controlled. */",
        "	return sysctl_unprivileged_userfaultfd;",
        "}",
        "",
        "SYSCALL_DEFINE1(userfaultfd, int, flags)",
        "{",
        "	if (!userfaultfd_syscall_allowed(flags))",
        "		return -EPERM;",
        "",
        "	return new_userfaultfd(flags);",
        "}",
        "",
        "static long userfaultfd_dev_ioctl(struct file *file, unsigned int cmd, unsigned long flags)",
        "{",
        "	if (cmd != USERFAULTFD_IOC_NEW)",
        "		return -EINVAL;",
        "",
        "	return new_userfaultfd(flags);",
        "}",
        "",
        "static const struct file_operations userfaultfd_dev_fops = {",
        "	.unlocked_ioctl = userfaultfd_dev_ioctl,",
        "	.compat_ioctl = userfaultfd_dev_ioctl,",
        "	.owner = THIS_MODULE,",
        "	.llseek = noop_llseek,",
        "};",
        "",
        "static struct miscdevice userfaultfd_misc = {",
        "	.minor = MISC_DYNAMIC_MINOR,",
        "	.name = \"userfaultfd\",",
        "	.fops = &userfaultfd_dev_fops",
        "};",
        "",
        "static int __init userfaultfd_init(void)",
        "{",
        "	int ret;",
        "",
        "	ret = misc_register(&userfaultfd_misc);",
        "	if (ret)",
        "		return ret;",
        "",
        "	userfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",",
        "						sizeof(struct userfaultfd_ctx),",
        "						0,",
        "						SLAB_HWCACHE_ALIGN|SLAB_PANIC,",
        "						init_once_userfaultfd_ctx);",
        "#ifdef CONFIG_SYSCTL",
        "	register_sysctl_init(\"vm\", vm_userfaultfd_table);",
        "#endif",
        "	return 0;",
        "}",
        "__initcall(userfaultfd_init);"
    ]
  },
  "include_linux_mempolicy_h": {
    path: "include/linux/mempolicy.h",
    covered: [82, 103],
    totalLines: 302,
    coveredCount: 2,
    coveragePct: 0.7,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "/*",
        " * NUMA memory policies for Linux.",
        " * Copyright 2003,2004 Andi Kleen SuSE Labs",
        " */",
        "#ifndef _LINUX_MEMPOLICY_H",
        "#define _LINUX_MEMPOLICY_H 1",
        "",
        "#include <linux/sched.h>",
        "#include <linux/mmzone.h>",
        "#include <linux/slab.h>",
        "#include <linux/rbtree.h>",
        "#include <linux/spinlock.h>",
        "#include <linux/nodemask.h>",
        "#include <linux/pagemap.h>",
        "#include <uapi/linux/mempolicy.h>",
        "",
        "struct mm_struct;",
        "",
        "#define NO_INTERLEAVE_INDEX (-1UL)	/* use task il_prev for interleaving */",
        "",
        "#ifdef CONFIG_NUMA",
        "",
        "/*",
        " * Describe a memory policy.",
        " *",
        " * A mempolicy can be either associated with a process or with a VMA.",
        " * For VMA related allocations the VMA policy is preferred, otherwise",
        " * the process policy is used. Interrupts ignore the memory policy",
        " * of the current process.",
        " *",
        " * Locking policy for interleave:",
        " * In process context there is no locking because only the process accesses",
        " * its own state. All vma manipulation is somewhat protected by a down_read on",
        " * mmap_lock.",
        " *",
        " * Freeing policy:",
        " * Mempolicy objects are reference counted.  A mempolicy will be freed when",
        " * mpol_put() decrements the reference count to zero.",
        " *",
        " * Duplicating policy objects:",
        " * mpol_dup() allocates a new mempolicy and copies the specified mempolicy",
        " * to the new storage.  The reference count of the new object is initialized",
        " * to 1, representing the caller of mpol_dup().",
        " */",
        "struct mempolicy {",
        "	atomic_t refcnt;",
        "	unsigned short mode; 	/* See MPOL_* above */",
        "	unsigned short flags;	/* See set_mempolicy() MPOL_F_* above */",
        "	nodemask_t nodes;	/* interleave/bind/preferred/etc */",
        "	int home_node;		/* Home node to use for MPOL_BIND and MPOL_PREFERRED_MANY */",
        "",
        "	union {",
        "		nodemask_t cpuset_mems_allowed;	/* relative to these nodes */",
        "		nodemask_t user_nodemask;	/* nodemask passed by user */",
        "	} w;",
        "};",
        "",
        "/*",
        " * Support for managing mempolicy data objects (clone, copy, destroy)",
        " * The default fast path of a NULL MPOL_DEFAULT policy is always inlined.",
        " */",
        "",
        "extern void __mpol_put(struct mempolicy *pol);",
        "static inline void mpol_put(struct mempolicy *pol)",
        "{",
        "	if (pol)",
        "		__mpol_put(pol);",
        "}",
        "",
        "/*",
        " * Does mempolicy pol need explicit unref after use?",
        " * Currently only needed for shared policies.",
        " */",
        "static inline int mpol_needs_cond_ref(struct mempolicy *pol)",
        "{",
        "	return (pol && (pol->flags & MPOL_F_SHARED));",
        "}",
        "",
        "static inline void mpol_cond_put(struct mempolicy *pol)",
        "{",
        "	if (mpol_needs_cond_ref(pol))",
        "		__mpol_put(pol);",
        "}",
        "",
        "extern struct mempolicy *__mpol_dup(struct mempolicy *pol);",
        "static inline struct mempolicy *mpol_dup(struct mempolicy *pol)",
        "{",
        "	if (pol)",
        "		pol = __mpol_dup(pol);",
        "	return pol;",
        "}",
        "",
        "static inline void mpol_get(struct mempolicy *pol)",
        "{",
        "	if (pol)",
        "		atomic_inc(&pol->refcnt);",
        "}",
        "",
        "extern bool __mpol_equal(struct mempolicy *a, struct mempolicy *b);",
        "static inline bool mpol_equal(struct mempolicy *a, struct mempolicy *b)",
        "{",
        "	if (a == b)",
        "		return true;",
        "	return __mpol_equal(a, b);",
        "}",
        "",
        "/*",
        " * Tree of shared policies for a shared memory region.",
        " */",
        "struct shared_policy {",
        "	struct rb_root root;",
        "	rwlock_t lock;",
        "};",
        "struct sp_node {",
        "	struct rb_node nd;",
        "	pgoff_t start, end;",
        "	struct mempolicy *policy;",
        "};",
        "",
        "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst);",
        "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol);",
        "int mpol_set_shared_policy(struct shared_policy *sp,",
        "			   struct vm_area_struct *vma, struct mempolicy *mpol);",
        "void mpol_free_shared_policy(struct shared_policy *sp);",
        "struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,",
        "					    pgoff_t idx);",
        "",
        "struct mempolicy *get_task_policy(struct task_struct *p);",
        "struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,",
        "		unsigned long addr, pgoff_t *ilx);",
        "struct mempolicy *get_vma_policy(struct vm_area_struct *vma,",
        "		unsigned long addr, int order, pgoff_t *ilx);",
        "bool vma_policy_mof(struct vm_area_struct *vma);",
        "",
        "extern void numa_default_policy(void);",
        "extern void numa_policy_init(void);",
        "extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new);",
        "extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);",
        "",
        "extern int huge_node(struct vm_area_struct *vma,",
        "				unsigned long addr, gfp_t gfp_flags,",
        "				struct mempolicy **mpol, nodemask_t **nodemask);",
        "extern bool init_nodemask_of_mempolicy(nodemask_t *mask);",
        "extern bool mempolicy_in_oom_domain(struct task_struct *tsk,",
        "				const nodemask_t *mask);",
        "extern unsigned int mempolicy_slab_node(void);",
        "",
        "extern enum zone_type policy_zone;",
        "",
        "static inline void check_highest_zone(enum zone_type k)",
        "{",
        "	if (k > policy_zone && k != ZONE_MOVABLE)",
        "		policy_zone = k;",
        "}",
        "",
        "int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,",
        "		     const nodemask_t *to, int flags);",
        "",
        "",
        "#ifdef CONFIG_TMPFS",
        "extern int mpol_parse_str(char *str, struct mempolicy **mpol);",
        "#endif",
        "",
        "extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);",
        "",
        "/* Check if a vma is migratable */",
        "extern bool vma_migratable(struct vm_area_struct *vma);",
        "",
        "int mpol_misplaced(struct folio *folio, struct vm_fault *vmf,",
        "					unsigned long addr);",
        "extern void mpol_put_task_policy(struct task_struct *);",
        "",
        "static inline bool mpol_is_preferred_many(struct mempolicy *pol)",
        "{",
        "	return  (pol->mode == MPOL_PREFERRED_MANY);",
        "}",
        "",
        "extern bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone);",
        "",
        "#else",
        "",
        "struct mempolicy {};",
        "",
        "static inline struct mempolicy *get_task_policy(struct task_struct *p)",
        "{",
        "	return NULL;",
        "}",
        "",
        "static inline bool mpol_equal(struct mempolicy *a, struct mempolicy *b)",
        "{",
        "	return true;",
        "}",
        "",
        "static inline void mpol_put(struct mempolicy *pol)",
        "{",
        "}",
        "",
        "static inline void mpol_cond_put(struct mempolicy *pol)",
        "{",
        "}",
        "",
        "static inline void mpol_get(struct mempolicy *pol)",
        "{",
        "}",
        "",
        "struct shared_policy {};",
        "",
        "static inline void mpol_shared_policy_init(struct shared_policy *sp,",
        "						struct mempolicy *mpol)",
        "{",
        "}",
        "",
        "static inline void mpol_free_shared_policy(struct shared_policy *sp)",
        "{",
        "}",
        "",
        "static inline struct mempolicy *",
        "mpol_shared_policy_lookup(struct shared_policy *sp, pgoff_t idx)",
        "{",
        "	return NULL;",
        "}",
        "",
        "static inline struct mempolicy *get_vma_policy(struct vm_area_struct *vma,",
        "				unsigned long addr, int order, pgoff_t *ilx)",
        "{",
        "	*ilx = 0;",
        "	return NULL;",
        "}",
        "",
        "static inline int",
        "vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void numa_policy_init(void)",
        "{",
        "}",
        "",
        "static inline void numa_default_policy(void)",
        "{",
        "}",
        "",
        "static inline void mpol_rebind_task(struct task_struct *tsk,",
        "				const nodemask_t *new)",
        "{",
        "}",
        "",
        "static inline void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
        "{",
        "}",
        "",
        "static inline int huge_node(struct vm_area_struct *vma,",
        "				unsigned long addr, gfp_t gfp_flags,",
        "				struct mempolicy **mpol, nodemask_t **nodemask)",
        "{",
        "	*mpol = NULL;",
        "	*nodemask = NULL;",
        "	return 0;",
        "}",
        "",
        "static inline bool init_nodemask_of_mempolicy(nodemask_t *m)",
        "{",
        "	return false;",
        "}",
        "",
        "static inline int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,",
        "				   const nodemask_t *to, int flags)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void check_highest_zone(int k)",
        "{",
        "}",
        "",
        "#ifdef CONFIG_TMPFS",
        "static inline int mpol_parse_str(char *str, struct mempolicy **mpol)",
        "{",
        "	return 1;	/* error */",
        "}",
        "#endif",
        "",
        "static inline int mpol_misplaced(struct folio *folio,",
        "				 struct vm_fault *vmf,",
        "				 unsigned long address)",
        "{",
        "	return -1; /* no node preference */",
        "}",
        "",
        "static inline void mpol_put_task_policy(struct task_struct *task)",
        "{",
        "}",
        "",
        "static inline bool mpol_is_preferred_many(struct mempolicy *pol)",
        "{",
        "	return  false;",
        "}",
        "",
        "#endif /* CONFIG_NUMA */",
        "#endif"
    ]
  },
  "include_trace_events_vmalloc_h": {
    path: "include/trace/events/vmalloc.h",
    covered: [23, 97],
    totalLines: 123,
    coveredCount: 2,
    coveragePct: 1.6,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#undef TRACE_SYSTEM",
        "#define TRACE_SYSTEM vmalloc",
        "",
        "#if !defined(_TRACE_VMALLOC_H) || defined(TRACE_HEADER_MULTI_READ)",
        "#define _TRACE_VMALLOC_H",
        "",
        "#include <linux/tracepoint.h>",
        "",
        "/**",
        " * alloc_vmap_area - called when a new vmap allocation occurs",
        " * @addr:	an allocated address",
        " * @size:	a requested size",
        " * @align:	a requested alignment",
        " * @vstart:	a requested start range",
        " * @vend:	a requested end range",
        " * @failed:	an allocation failed or not",
        " *",
        " * This event is used for a debug purpose, it can give an extra",
        " * information for a developer about how often it occurs and which",
        " * parameters are passed for further validation.",
        " */",
        "TRACE_EVENT(alloc_vmap_area,",
        "",
        "	TP_PROTO(unsigned long addr, unsigned long size, unsigned long align,",
        "		unsigned long vstart, unsigned long vend, int failed),",
        "",
        "	TP_ARGS(addr, size, align, vstart, vend, failed),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(unsigned long, addr)",
        "		__field(unsigned long, size)",
        "		__field(unsigned long, align)",
        "		__field(unsigned long, vstart)",
        "		__field(unsigned long, vend)",
        "		__field(int, failed)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->addr = addr;",
        "		__entry->size = size;",
        "		__entry->align = align;",
        "		__entry->vstart = vstart;",
        "		__entry->vend = vend;",
        "		__entry->failed = failed;",
        "	),",
        "",
        "	TP_printk(\"va_start: %lu size=%lu align=%lu vstart=0x%lx vend=0x%lx failed=%d\",",
        "		__entry->addr, __entry->size, __entry->align,",
        "		__entry->vstart, __entry->vend, __entry->failed)",
        ");",
        "",
        "/**",
        " * purge_vmap_area_lazy - called when vmap areas were lazily freed",
        " * @start:		purging start address",
        " * @end:		purging end address",
        " * @npurged:	numbed of purged vmap areas",
        " *",
        " * This event is used for a debug purpose. It gives some",
        " * indication about start:end range and how many objects",
        " * are released.",
        " */",
        "TRACE_EVENT(purge_vmap_area_lazy,",
        "",
        "	TP_PROTO(unsigned long start, unsigned long end,",
        "		unsigned int npurged),",
        "",
        "	TP_ARGS(start, end, npurged),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(unsigned long, start)",
        "		__field(unsigned long, end)",
        "		__field(unsigned int, npurged)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->start = start;",
        "		__entry->end = end;",
        "		__entry->npurged = npurged;",
        "	),",
        "",
        "	TP_printk(\"start=0x%lx end=0x%lx num_purged=%u\",",
        "		__entry->start, __entry->end, __entry->npurged)",
        ");",
        "",
        "/**",
        " * free_vmap_area_noflush - called when a vmap area is freed",
        " * @va_start:		a start address of VA",
        " * @nr_lazy:		number of current lazy pages",
        " * @nr_lazy_max:	number of maximum lazy pages",
        " *",
        " * This event is used for a debug purpose. It gives some",
        " * indication about a VA that is released, number of current",
        " * outstanding areas and a maximum allowed threshold before",
        " * dropping all of them.",
        " */",
        "TRACE_EVENT(free_vmap_area_noflush,",
        "",
        "	TP_PROTO(unsigned long va_start, unsigned long nr_lazy,",
        "		unsigned long nr_lazy_max),",
        "",
        "	TP_ARGS(va_start, nr_lazy, nr_lazy_max),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(unsigned long, va_start)",
        "		__field(unsigned long, nr_lazy)",
        "		__field(unsigned long, nr_lazy_max)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->va_start = va_start;",
        "		__entry->nr_lazy = nr_lazy;",
        "		__entry->nr_lazy_max = nr_lazy_max;",
        "	),",
        "",
        "	TP_printk(\"va_start=0x%lx nr_lazy=%lu nr_lazy_max=%lu\",",
        "		__entry->va_start, __entry->nr_lazy, __entry->nr_lazy_max)",
        ");",
        "",
        "#endif /*  _TRACE_VMALLOC_H */",
        "",
        "/* This part must be outside protection */",
        "#include <trace/define_trace.h>"
    ]
  },
  "include_linux_mmu_notifier_h": {
    path: "include/linux/mmu_notifier.h",
    covered: [437, 471, 468],
    totalLines: 667,
    coveredCount: 3,
    coveragePct: 0.4,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_MMU_NOTIFIER_H",
        "#define _LINUX_MMU_NOTIFIER_H",
        "",
        "#include <linux/list.h>",
        "#include <linux/spinlock.h>",
        "#include <linux/mm_types.h>",
        "#include <linux/mmap_lock.h>",
        "#include <linux/srcu.h>",
        "#include <linux/interval_tree.h>",
        "",
        "struct mmu_notifier_subscriptions;",
        "struct mmu_notifier;",
        "struct mmu_notifier_range;",
        "struct mmu_interval_notifier;",
        "",
        "/**",
        " * enum mmu_notifier_event - reason for the mmu notifier callback",
        " * @MMU_NOTIFY_UNMAP: either munmap() that unmap the range or a mremap() that",
        " * move the range",
        " *",
        " * @MMU_NOTIFY_CLEAR: clear page table entry (many reasons for this like",
        " * madvise() or replacing a page by another one, ...).",
        " *",
        " * @MMU_NOTIFY_PROTECTION_VMA: update is due to protection change for the range",
        " * ie using the vma access permission (vm_page_prot) to update the whole range",
        " * is enough no need to inspect changes to the CPU page table (mprotect()",
        " * syscall)",
        " *",
        " * @MMU_NOTIFY_PROTECTION_PAGE: update is due to change in read/write flag for",
        " * pages in the range so to mirror those changes the user must inspect the CPU",
        " * page table (from the end callback).",
        " *",
        " * @MMU_NOTIFY_SOFT_DIRTY: soft dirty accounting (still same page and same",
        " * access flags). User should soft dirty the page in the end callback to make",
        " * sure that anyone relying on soft dirtiness catch pages that might be written",
        " * through non CPU mappings.",
        " *",
        " * @MMU_NOTIFY_RELEASE: used during mmu_interval_notifier invalidate to signal",
        " * that the mm refcount is zero and the range is no longer accessible.",
        " *",
        " * @MMU_NOTIFY_MIGRATE: used during migrate_vma_collect() invalidate to signal",
        " * a device driver to possibly ignore the invalidation if the",
        " * owner field matches the driver's device private pgmap owner.",
        " *",
        " * @MMU_NOTIFY_EXCLUSIVE: to signal a device driver that the device will no",
        " * longer have exclusive access to the page. When sent during creation of an",
        " * exclusive range the owner will be initialised to the value provided by the",
        " * caller of make_device_exclusive_range(), otherwise the owner will be NULL.",
        " */",
        "enum mmu_notifier_event {",
        "	MMU_NOTIFY_UNMAP = 0,",
        "	MMU_NOTIFY_CLEAR,",
        "	MMU_NOTIFY_PROTECTION_VMA,",
        "	MMU_NOTIFY_PROTECTION_PAGE,",
        "	MMU_NOTIFY_SOFT_DIRTY,",
        "	MMU_NOTIFY_RELEASE,",
        "	MMU_NOTIFY_MIGRATE,",
        "	MMU_NOTIFY_EXCLUSIVE,",
        "};",
        "",
        "#define MMU_NOTIFIER_RANGE_BLOCKABLE (1 << 0)",
        "",
        "struct mmu_notifier_ops {",
        "	/*",
        "	 * Called either by mmu_notifier_unregister or when the mm is",
        "	 * being destroyed by exit_mmap, always before all pages are",
        "	 * freed. This can run concurrently with other mmu notifier",
        "	 * methods (the ones invoked outside the mm context) and it",
        "	 * should tear down all secondary mmu mappings and freeze the",
        "	 * secondary mmu. If this method isn't implemented you've to",
        "	 * be sure that nothing could possibly write to the pages",
        "	 * through the secondary mmu by the time the last thread with",
        "	 * tsk->mm == mm exits.",
        "	 *",
        "	 * As side note: the pages freed after ->release returns could",
        "	 * be immediately reallocated by the gart at an alias physical",
        "	 * address with a different cache model, so if ->release isn't",
        "	 * implemented because all _software_ driven memory accesses",
        "	 * through the secondary mmu are terminated by the time the",
        "	 * last thread of this mm quits, you've also to be sure that",
        "	 * speculative _hardware_ operations can't allocate dirty",
        "	 * cachelines in the cpu that could not be snooped and made",
        "	 * coherent with the other read and write operations happening",
        "	 * through the gart alias address, so leading to memory",
        "	 * corruption.",
        "	 */",
        "	void (*release)(struct mmu_notifier *subscription,",
        "			struct mm_struct *mm);",
        "",
        "	/*",
        "	 * clear_flush_young is called after the VM is",
        "	 * test-and-clearing the young/accessed bitflag in the",
        "	 * pte. This way the VM will provide proper aging to the",
        "	 * accesses to the page through the secondary MMUs and not",
        "	 * only to the ones through the Linux pte.",
        "	 * Start-end is necessary in case the secondary MMU is mapping the page",
        "	 * at a smaller granularity than the primary MMU.",
        "	 */",
        "	int (*clear_flush_young)(struct mmu_notifier *subscription,",
        "				 struct mm_struct *mm,",
        "				 unsigned long start,",
        "				 unsigned long end);",
        "",
        "	/*",
        "	 * clear_young is a lightweight version of clear_flush_young. Like the",
        "	 * latter, it is supposed to test-and-clear the young/accessed bitflag",
        "	 * in the secondary pte, but it may omit flushing the secondary tlb.",
        "	 */",
        "	int (*clear_young)(struct mmu_notifier *subscription,",
        "			   struct mm_struct *mm,",
        "			   unsigned long start,",
        "			   unsigned long end);",
        "",
        "	/*",
        "	 * test_young is called to check the young/accessed bitflag in",
        "	 * the secondary pte. This is used to know if the page is",
        "	 * frequently used without actually clearing the flag or tearing",
        "	 * down the secondary mapping on the page.",
        "	 */",
        "	int (*test_young)(struct mmu_notifier *subscription,",
        "			  struct mm_struct *mm,",
        "			  unsigned long address);",
        "",
        "	/*",
        "	 * invalidate_range_start() and invalidate_range_end() must be",
        "	 * paired and are called only when the mmap_lock and/or the",
        "	 * locks protecting the reverse maps are held. If the subsystem",
        "	 * can't guarantee that no additional references are taken to",
        "	 * the pages in the range, it has to implement the",
        "	 * invalidate_range() notifier to remove any references taken",
        "	 * after invalidate_range_start().",
        "	 *",
        "	 * Invalidation of multiple concurrent ranges may be",
        "	 * optionally permitted by the driver. Either way the",
        "	 * establishment of sptes is forbidden in the range passed to",
        "	 * invalidate_range_begin/end for the whole duration of the",
        "	 * invalidate_range_begin/end critical section.",
        "	 *",
        "	 * invalidate_range_start() is called when all pages in the",
        "	 * range are still mapped and have at least a refcount of one.",
        "	 *",
        "	 * invalidate_range_end() is called when all pages in the",
        "	 * range have been unmapped and the pages have been freed by",
        "	 * the VM.",
        "	 *",
        "	 * The VM will remove the page table entries and potentially",
        "	 * the page between invalidate_range_start() and",
        "	 * invalidate_range_end(). If the page must not be freed",
        "	 * because of pending I/O or other circumstances then the",
        "	 * invalidate_range_start() callback (or the initial mapping",
        "	 * by the driver) must make sure that the refcount is kept",
        "	 * elevated.",
        "	 *",
        "	 * If the driver increases the refcount when the pages are",
        "	 * initially mapped into an address space then either",
        "	 * invalidate_range_start() or invalidate_range_end() may",
        "	 * decrease the refcount. If the refcount is decreased on",
        "	 * invalidate_range_start() then the VM can free pages as page",
        "	 * table entries are removed.  If the refcount is only",
        "	 * dropped on invalidate_range_end() then the driver itself",
        "	 * will drop the last refcount but it must take care to flush",
        "	 * any secondary tlb before doing the final free on the",
        "	 * page. Pages will no longer be referenced by the linux",
        "	 * address space but may still be referenced by sptes until",
        "	 * the last refcount is dropped.",
        "	 *",
        "	 * If blockable argument is set to false then the callback cannot",
        "	 * sleep and has to return with -EAGAIN if sleeping would be required.",
        "	 * 0 should be returned otherwise. Please note that notifiers that can",
        "	 * fail invalidate_range_start are not allowed to implement",
        "	 * invalidate_range_end, as there is no mechanism for informing the",
        "	 * notifier that its start failed.",
        "	 */",
        "	int (*invalidate_range_start)(struct mmu_notifier *subscription,",
        "				      const struct mmu_notifier_range *range);",
        "	void (*invalidate_range_end)(struct mmu_notifier *subscription,",
        "				     const struct mmu_notifier_range *range);",
        "",
        "	/*",
        "	 * arch_invalidate_secondary_tlbs() is used to manage a non-CPU TLB",
        "	 * which shares page-tables with the CPU. The",
        "	 * invalidate_range_start()/end() callbacks should not be implemented as",
        "	 * invalidate_secondary_tlbs() already catches the points in time when",
        "	 * an external TLB needs to be flushed.",
        "	 *",
        "	 * This requires arch_invalidate_secondary_tlbs() to be called while",
        "	 * holding the ptl spin-lock and therefore this callback is not allowed",
        "	 * to sleep.",
        "	 *",
        "	 * This is called by architecture code whenever invalidating a TLB",
        "	 * entry. It is assumed that any secondary TLB has the same rules for",
        "	 * when invalidations are required. If this is not the case architecture",
        "	 * code will need to call this explicitly when required for secondary",
        "	 * TLB invalidation.",
        "	 */",
        "	void (*arch_invalidate_secondary_tlbs)(",
        "					struct mmu_notifier *subscription,",
        "					struct mm_struct *mm,",
        "					unsigned long start,",
        "					unsigned long end);",
        "",
        "	/*",
        "	 * These callbacks are used with the get/put interface to manage the",
        "	 * lifetime of the mmu_notifier memory. alloc_notifier() returns a new",
        "	 * notifier for use with the mm.",
        "	 *",
        "	 * free_notifier() is only called after the mmu_notifier has been",
        "	 * fully put, calls to any ops callback are prevented and no ops",
        "	 * callbacks are currently running. It is called from a SRCU callback",
        "	 * and cannot sleep.",
        "	 */",
        "	struct mmu_notifier *(*alloc_notifier)(struct mm_struct *mm);",
        "	void (*free_notifier)(struct mmu_notifier *subscription);",
        "};",
        "",
        "/*",
        " * The notifier chains are protected by mmap_lock and/or the reverse map",
        " * semaphores. Notifier chains are only changed when all reverse maps and",
        " * the mmap_lock locks are taken.",
        " *",
        " * Therefore notifier chains can only be traversed when either",
        " *",
        " * 1. mmap_lock is held.",
        " * 2. One of the reverse map locks is held (i_mmap_rwsem or anon_vma->rwsem).",
        " * 3. No other concurrent thread can access the list (release)",
        " */",
        "struct mmu_notifier {",
        "	struct hlist_node hlist;",
        "	const struct mmu_notifier_ops *ops;",
        "	struct mm_struct *mm;",
        "	struct rcu_head rcu;",
        "	unsigned int users;",
        "};",
        "",
        "/**",
        " * struct mmu_interval_notifier_ops",
        " * @invalidate: Upon return the caller must stop using any SPTEs within this",
        " *              range. This function can sleep. Return false only if sleeping",
        " *              was required but mmu_notifier_range_blockable(range) is false.",
        " */",
        "struct mmu_interval_notifier_ops {",
        "	bool (*invalidate)(struct mmu_interval_notifier *interval_sub,",
        "			   const struct mmu_notifier_range *range,",
        "			   unsigned long cur_seq);",
        "};",
        "",
        "struct mmu_interval_notifier {",
        "	struct interval_tree_node interval_tree;",
        "	const struct mmu_interval_notifier_ops *ops;",
        "	struct mm_struct *mm;",
        "	struct hlist_node deferred_item;",
        "	unsigned long invalidate_seq;",
        "};",
        "",
        "#ifdef CONFIG_MMU_NOTIFIER",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "extern struct lockdep_map __mmu_notifier_invalidate_range_start_map;",
        "#endif",
        "",
        "struct mmu_notifier_range {",
        "	struct mm_struct *mm;",
        "	unsigned long start;",
        "	unsigned long end;",
        "	unsigned flags;",
        "	enum mmu_notifier_event event;",
        "	void *owner;",
        "};",
        "",
        "static inline int mm_has_notifiers(struct mm_struct *mm)",
        "{",
        "	return unlikely(mm->notifier_subscriptions);",
        "}",
        "",
        "struct mmu_notifier *mmu_notifier_get_locked(const struct mmu_notifier_ops *ops,",
        "					     struct mm_struct *mm);",
        "static inline struct mmu_notifier *",
        "mmu_notifier_get(const struct mmu_notifier_ops *ops, struct mm_struct *mm)",
        "{",
        "	struct mmu_notifier *ret;",
        "",
        "	mmap_write_lock(mm);",
        "	ret = mmu_notifier_get_locked(ops, mm);",
        "	mmap_write_unlock(mm);",
        "	return ret;",
        "}",
        "void mmu_notifier_put(struct mmu_notifier *subscription);",
        "void mmu_notifier_synchronize(void);",
        "",
        "extern int mmu_notifier_register(struct mmu_notifier *subscription,",
        "				 struct mm_struct *mm);",
        "extern int __mmu_notifier_register(struct mmu_notifier *subscription,",
        "				   struct mm_struct *mm);",
        "extern void mmu_notifier_unregister(struct mmu_notifier *subscription,",
        "				    struct mm_struct *mm);",
        "",
        "unsigned long",
        "mmu_interval_read_begin(struct mmu_interval_notifier *interval_sub);",
        "int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,",
        "				 struct mm_struct *mm, unsigned long start,",
        "				 unsigned long length,",
        "				 const struct mmu_interval_notifier_ops *ops);",
        "int mmu_interval_notifier_insert_locked(",
        "	struct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
        "	unsigned long start, unsigned long length,",
        "	const struct mmu_interval_notifier_ops *ops);",
        "void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub);",
        "",
        "/**",
        " * mmu_interval_set_seq - Save the invalidation sequence",
        " * @interval_sub - The subscription passed to invalidate",
        " * @cur_seq - The cur_seq passed to the invalidate() callback",
        " *",
        " * This must be called unconditionally from the invalidate callback of a",
        " * struct mmu_interval_notifier_ops under the same lock that is used to call",
        " * mmu_interval_read_retry(). It updates the sequence number for later use by",
        " * mmu_interval_read_retry(). The provided cur_seq will always be odd.",
        " *",
        " * If the caller does not call mmu_interval_read_begin() or",
        " * mmu_interval_read_retry() then this call is not required.",
        " */",
        "static inline void",
        "mmu_interval_set_seq(struct mmu_interval_notifier *interval_sub,",
        "		     unsigned long cur_seq)",
        "{",
        "	WRITE_ONCE(interval_sub->invalidate_seq, cur_seq);",
        "}",
        "",
        "/**",
        " * mmu_interval_read_retry - End a read side critical section against a VA range",
        " * interval_sub: The subscription",
        " * seq: The return of the paired mmu_interval_read_begin()",
        " *",
        " * This MUST be called under a user provided lock that is also held",
        " * unconditionally by op->invalidate() when it calls mmu_interval_set_seq().",
        " *",
        " * Each call should be paired with a single mmu_interval_read_begin() and",
        " * should be used to conclude the read side.",
        " *",
        " * Returns true if an invalidation collided with this critical section, and",
        " * the caller should retry.",
        " */",
        "static inline bool",
        "mmu_interval_read_retry(struct mmu_interval_notifier *interval_sub,",
        "			unsigned long seq)",
        "{",
        "	return interval_sub->invalidate_seq != seq;",
        "}",
        "",
        "/**",
        " * mmu_interval_check_retry - Test if a collision has occurred",
        " * interval_sub: The subscription",
        " * seq: The return of the matching mmu_interval_read_begin()",
        " *",
        " * This can be used in the critical section between mmu_interval_read_begin()",
        " * and mmu_interval_read_retry().  A return of true indicates an invalidation",
        " * has collided with this critical region and a future",
        " * mmu_interval_read_retry() will return true.",
        " *",
        " * False is not reliable and only suggests a collision may not have",
        " * occurred. It can be called many times and does not have to hold the user",
        " * provided lock.",
        " *",
        " * This call can be used as part of loops and other expensive operations to",
        " * expedite a retry.",
        " */",
        "static inline bool",
        "mmu_interval_check_retry(struct mmu_interval_notifier *interval_sub,",
        "			 unsigned long seq)",
        "{",
        "	/* Pairs with the WRITE_ONCE in mmu_interval_set_seq() */",
        "	return READ_ONCE(interval_sub->invalidate_seq) != seq;",
        "}",
        "",
        "extern void __mmu_notifier_subscriptions_destroy(struct mm_struct *mm);",
        "extern void __mmu_notifier_release(struct mm_struct *mm);",
        "extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,",
        "					  unsigned long start,",
        "					  unsigned long end);",
        "extern int __mmu_notifier_clear_young(struct mm_struct *mm,",
        "				      unsigned long start,",
        "				      unsigned long end);",
        "extern int __mmu_notifier_test_young(struct mm_struct *mm,",
        "				     unsigned long address);",
        "extern int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *r);",
        "extern void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *r);",
        "extern void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
        "					unsigned long start, unsigned long end);",
        "extern bool",
        "mmu_notifier_range_update_to_read_only(const struct mmu_notifier_range *range);",
        "",
        "static inline bool",
        "mmu_notifier_range_blockable(const struct mmu_notifier_range *range)",
        "{",
        "	return (range->flags & MMU_NOTIFIER_RANGE_BLOCKABLE);",
        "}",
        "",
        "static inline void mmu_notifier_release(struct mm_struct *mm)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		__mmu_notifier_release(mm);",
        "}",
        "",
        "static inline int mmu_notifier_clear_flush_young(struct mm_struct *mm,",
        "					  unsigned long start,",
        "					  unsigned long end)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		return __mmu_notifier_clear_flush_young(mm, start, end);",
        "	return 0;",
        "}",
        "",
        "static inline int mmu_notifier_clear_young(struct mm_struct *mm,",
        "					   unsigned long start,",
        "					   unsigned long end)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		return __mmu_notifier_clear_young(mm, start, end);",
        "	return 0;",
        "}",
        "",
        "static inline int mmu_notifier_test_young(struct mm_struct *mm,",
        "					  unsigned long address)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		return __mmu_notifier_test_young(mm, address);",
        "	return 0;",
        "}",
        "",
        "static inline void",
        "mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)",
        "{",
        "	might_sleep();",
        "",
        "	lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
        "	if (mm_has_notifiers(range->mm)) {",
        "		range->flags |= MMU_NOTIFIER_RANGE_BLOCKABLE;",
        "		__mmu_notifier_invalidate_range_start(range);",
        "	}",
        "	lock_map_release(&__mmu_notifier_invalidate_range_start_map);",
        "}",
        "",
        "/*",
        " * This version of mmu_notifier_invalidate_range_start() avoids blocking, but it",
        " * can return an error if a notifier can't proceed without blocking, in which",
        " * case you're not allowed to modify PTEs in the specified range.",
        " *",
        " * This is mainly intended for OOM handling.",
        " */",
        "static inline int __must_check",
        "mmu_notifier_invalidate_range_start_nonblock(struct mmu_notifier_range *range)",
        "{",
        "	int ret = 0;",
        "",
        "	lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
        "	if (mm_has_notifiers(range->mm)) {",
        "		range->flags &= ~MMU_NOTIFIER_RANGE_BLOCKABLE;",
        "		ret = __mmu_notifier_invalidate_range_start(range);",
        "	}",
        "	lock_map_release(&__mmu_notifier_invalidate_range_start_map);",
        "	return ret;",
        "}",
        "",
        "static inline void",
        "mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)",
        "{",
        "	if (mmu_notifier_range_blockable(range))",
        "		might_sleep();",
        "",
        "	if (mm_has_notifiers(range->mm))",
        "		__mmu_notifier_invalidate_range_end(range);",
        "}",
        "",
        "static inline void mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
        "					unsigned long start, unsigned long end)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		__mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);",
        "}",
        "",
        "static inline void mmu_notifier_subscriptions_init(struct mm_struct *mm)",
        "{",
        "	mm->notifier_subscriptions = NULL;",
        "}",
        "",
        "static inline void mmu_notifier_subscriptions_destroy(struct mm_struct *mm)",
        "{",
        "	if (mm_has_notifiers(mm))",
        "		__mmu_notifier_subscriptions_destroy(mm);",
        "}",
        "",
        "",
        "static inline void mmu_notifier_range_init(struct mmu_notifier_range *range,",
        "					   enum mmu_notifier_event event,",
        "					   unsigned flags,",
        "					   struct mm_struct *mm,",
        "					   unsigned long start,",
        "					   unsigned long end)",
        "{",
        "	range->event = event;",
        "	range->mm = mm;",
        "	range->start = start;",
        "	range->end = end;",
        "	range->flags = flags;",
        "}",
        "",
        "static inline void mmu_notifier_range_init_owner(",
        "			struct mmu_notifier_range *range,",
        "			enum mmu_notifier_event event, unsigned int flags,",
        "			struct mm_struct *mm, unsigned long start,",
        "			unsigned long end, void *owner)",
        "{",
        "	mmu_notifier_range_init(range, event, flags, mm, start, end);",
        "	range->owner = owner;",
        "}",
        "",
        "#define ptep_clear_flush_young_notify(__vma, __address, __ptep)		\\",
        "({									\\",
        "	int __young;							\\",
        "	struct vm_area_struct *___vma = __vma;				\\",
        "	unsigned long ___address = __address;				\\",
        "	__young = ptep_clear_flush_young(___vma, ___address, __ptep);	\\",
        "	__young |= mmu_notifier_clear_flush_young(___vma->vm_mm,	\\",
        "						  ___address,		\\",
        "						  ___address +		\\",
        "							PAGE_SIZE);	\\",
        "	__young;							\\",
        "})",
        "",
        "#define pmdp_clear_flush_young_notify(__vma, __address, __pmdp)		\\",
        "({									\\",
        "	int __young;							\\",
        "	struct vm_area_struct *___vma = __vma;				\\",
        "	unsigned long ___address = __address;				\\",
        "	__young = pmdp_clear_flush_young(___vma, ___address, __pmdp);	\\",
        "	__young |= mmu_notifier_clear_flush_young(___vma->vm_mm,	\\",
        "						  ___address,		\\",
        "						  ___address +		\\",
        "							PMD_SIZE);	\\",
        "	__young;							\\",
        "})",
        "",
        "#define ptep_clear_young_notify(__vma, __address, __ptep)		\\",
        "({									\\",
        "	int __young;							\\",
        "	struct vm_area_struct *___vma = __vma;				\\",
        "	unsigned long ___address = __address;				\\",
        "	__young = ptep_test_and_clear_young(___vma, ___address, __ptep);\\",
        "	__young |= mmu_notifier_clear_young(___vma->vm_mm, ___address,	\\",
        "					    ___address + PAGE_SIZE);	\\",
        "	__young;							\\",
        "})",
        "",
        "#define pmdp_clear_young_notify(__vma, __address, __pmdp)		\\",
        "({									\\",
        "	int __young;							\\",
        "	struct vm_area_struct *___vma = __vma;				\\",
        "	unsigned long ___address = __address;				\\",
        "	__young = pmdp_test_and_clear_young(___vma, ___address, __pmdp);\\",
        "	__young |= mmu_notifier_clear_young(___vma->vm_mm, ___address,	\\",
        "					    ___address + PMD_SIZE);	\\",
        "	__young;							\\",
        "})",
        "",
        "#else /* CONFIG_MMU_NOTIFIER */",
        "",
        "struct mmu_notifier_range {",
        "	unsigned long start;",
        "	unsigned long end;",
        "};",
        "",
        "static inline void _mmu_notifier_range_init(struct mmu_notifier_range *range,",
        "					    unsigned long start,",
        "					    unsigned long end)",
        "{",
        "	range->start = start;",
        "	range->end = end;",
        "}",
        "",
        "#define mmu_notifier_range_init(range,event,flags,mm,start,end)  \\",
        "	_mmu_notifier_range_init(range, start, end)",
        "#define mmu_notifier_range_init_owner(range, event, flags, mm, start, \\",
        "					end, owner) \\",
        "	_mmu_notifier_range_init(range, start, end)",
        "",
        "static inline bool",
        "mmu_notifier_range_blockable(const struct mmu_notifier_range *range)",
        "{",
        "	return true;",
        "}",
        "",
        "static inline int mm_has_notifiers(struct mm_struct *mm)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void mmu_notifier_release(struct mm_struct *mm)",
        "{",
        "}",
        "",
        "static inline int mmu_notifier_clear_flush_young(struct mm_struct *mm,",
        "					  unsigned long start,",
        "					  unsigned long end)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline int mmu_notifier_clear_young(struct mm_struct *mm,",
        "					   unsigned long start,",
        "					   unsigned long end)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline int mmu_notifier_test_young(struct mm_struct *mm,",
        "					  unsigned long address)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void",
        "mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)",
        "{",
        "}",
        "",
        "static inline int",
        "mmu_notifier_invalidate_range_start_nonblock(struct mmu_notifier_range *range)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline",
        "void mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)",
        "{",
        "}",
        "",
        "static inline void mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
        "				  unsigned long start, unsigned long end)",
        "{",
        "}",
        "",
        "static inline void mmu_notifier_subscriptions_init(struct mm_struct *mm)",
        "{",
        "}",
        "",
        "static inline void mmu_notifier_subscriptions_destroy(struct mm_struct *mm)",
        "{",
        "}",
        "",
        "#define mmu_notifier_range_update_to_read_only(r) false",
        "",
        "#define ptep_clear_flush_young_notify ptep_clear_flush_young",
        "#define pmdp_clear_flush_young_notify pmdp_clear_flush_young",
        "#define ptep_clear_young_notify ptep_test_and_clear_young",
        "#define pmdp_clear_young_notify pmdp_test_and_clear_young",
        "#define	ptep_clear_flush_notify ptep_clear_flush",
        "#define pmdp_huge_clear_flush_notify pmdp_huge_clear_flush",
        "#define pudp_huge_clear_flush_notify pudp_huge_clear_flush",
        "",
        "static inline void mmu_notifier_synchronize(void)",
        "{",
        "}",
        "",
        "#endif /* CONFIG_MMU_NOTIFIER */",
        "",
        "#endif /* _LINUX_MMU_NOTIFIER_H */"
    ]
  },
  "kernel_bpf_disasm_c": {
    path: "kernel/bpf/disasm.c",
    covered: [189, 347],
    totalLines: 378,
    coveredCount: 2,
    coveragePct: 0.5,
    source: [
        "// SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)",
        "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com",
        " * Copyright (c) 2016 Facebook",
        " */",
        "",
        "#include <linux/bpf.h>",
        "",
        "#include \"disasm.h\"",
        "",
        "#define __BPF_FUNC_STR_FN(x) [BPF_FUNC_ ## x] = __stringify(bpf_ ## x)",
        "static const char * const func_id_str[] = {",
        "	__BPF_FUNC_MAPPER(__BPF_FUNC_STR_FN)",
        "};",
        "#undef __BPF_FUNC_STR_FN",
        "",
        "static const char *__func_get_name(const struct bpf_insn_cbs *cbs,",
        "				   const struct bpf_insn *insn,",
        "				   char *buff, size_t len)",
        "{",
        "	BUILD_BUG_ON(ARRAY_SIZE(func_id_str) != __BPF_FUNC_MAX_ID);",
        "",
        "	if (!insn->src_reg &&",
        "	    insn->imm >= 0 && insn->imm < __BPF_FUNC_MAX_ID &&",
        "	    func_id_str[insn->imm])",
        "		return func_id_str[insn->imm];",
        "",
        "	if (cbs && cbs->cb_call) {",
        "		const char *res;",
        "",
        "		res = cbs->cb_call(cbs->private_data, insn);",
        "		if (res)",
        "			return res;",
        "	}",
        "",
        "	if (insn->src_reg == BPF_PSEUDO_CALL)",
        "		snprintf(buff, len, \"%+d\", insn->imm);",
        "	else if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)",
        "		snprintf(buff, len, \"kernel-function\");",
        "",
        "	return buff;",
        "}",
        "",
        "static const char *__func_imm_name(const struct bpf_insn_cbs *cbs,",
        "				   const struct bpf_insn *insn,",
        "				   u64 full_imm, char *buff, size_t len)",
        "{",
        "	if (cbs && cbs->cb_imm)",
        "		return cbs->cb_imm(cbs->private_data, insn, full_imm);",
        "",
        "	snprintf(buff, len, \"0x%llx\", (unsigned long long)full_imm);",
        "	return buff;",
        "}",
        "",
        "const char *func_id_name(int id)",
        "{",
        "	if (id >= 0 && id < __BPF_FUNC_MAX_ID && func_id_str[id])",
        "		return func_id_str[id];",
        "	else",
        "		return \"unknown\";",
        "}",
        "",
        "const char *const bpf_class_string[8] = {",
        "	[BPF_LD]    = \"ld\",",
        "	[BPF_LDX]   = \"ldx\",",
        "	[BPF_ST]    = \"st\",",
        "	[BPF_STX]   = \"stx\",",
        "	[BPF_ALU]   = \"alu\",",
        "	[BPF_JMP]   = \"jmp\",",
        "	[BPF_JMP32] = \"jmp32\",",
        "	[BPF_ALU64] = \"alu64\",",
        "};",
        "",
        "const char *const bpf_alu_string[16] = {",
        "	[BPF_ADD >> 4]  = \"+=\",",
        "	[BPF_SUB >> 4]  = \"-=\",",
        "	[BPF_MUL >> 4]  = \"*=\",",
        "	[BPF_DIV >> 4]  = \"/=\",",
        "	[BPF_OR  >> 4]  = \"|=\",",
        "	[BPF_AND >> 4]  = \"&=\",",
        "	[BPF_LSH >> 4]  = \"<<=\",",
        "	[BPF_RSH >> 4]  = \">>=\",",
        "	[BPF_NEG >> 4]  = \"neg\",",
        "	[BPF_MOD >> 4]  = \"%=\",",
        "	[BPF_XOR >> 4]  = \"^=\",",
        "	[BPF_MOV >> 4]  = \"=\",",
        "	[BPF_ARSH >> 4] = \"s>>=\",",
        "	[BPF_END >> 4]  = \"endian\",",
        "};",
        "",
        "static const char *const bpf_alu_sign_string[16] = {",
        "	[BPF_DIV >> 4]  = \"s/=\",",
        "	[BPF_MOD >> 4]  = \"s%=\",",
        "};",
        "",
        "static const char *const bpf_movsx_string[4] = {",
        "	[0] = \"(s8)\",",
        "	[1] = \"(s16)\",",
        "	[3] = \"(s32)\",",
        "};",
        "",
        "static const char *const bpf_atomic_alu_string[16] = {",
        "	[BPF_ADD >> 4]  = \"add\",",
        "	[BPF_AND >> 4]  = \"and\",",
        "	[BPF_OR >> 4]  = \"or\",",
        "	[BPF_XOR >> 4]  = \"xor\",",
        "};",
        "",
        "static const char *const bpf_ldst_string[] = {",
        "	[BPF_W >> 3]  = \"u32\",",
        "	[BPF_H >> 3]  = \"u16\",",
        "	[BPF_B >> 3]  = \"u8\",",
        "	[BPF_DW >> 3] = \"u64\",",
        "};",
        "",
        "static const char *const bpf_ldsx_string[] = {",
        "	[BPF_W >> 3]  = \"s32\",",
        "	[BPF_H >> 3]  = \"s16\",",
        "	[BPF_B >> 3]  = \"s8\",",
        "};",
        "",
        "static const char *const bpf_jmp_string[16] = {",
        "	[BPF_JA >> 4]   = \"jmp\",",
        "	[BPF_JEQ >> 4]  = \"==\",",
        "	[BPF_JGT >> 4]  = \">\",",
        "	[BPF_JLT >> 4]  = \"<\",",
        "	[BPF_JGE >> 4]  = \">=\",",
        "	[BPF_JLE >> 4]  = \"<=\",",
        "	[BPF_JSET >> 4] = \"&\",",
        "	[BPF_JNE >> 4]  = \"!=\",",
        "	[BPF_JSGT >> 4] = \"s>\",",
        "	[BPF_JSLT >> 4] = \"s<\",",
        "	[BPF_JSGE >> 4] = \"s>=\",",
        "	[BPF_JSLE >> 4] = \"s<=\",",
        "	[BPF_CALL >> 4] = \"call\",",
        "	[BPF_EXIT >> 4] = \"exit\",",
        "};",
        "",
        "static void print_bpf_end_insn(bpf_insn_print_t verbose,",
        "			       void *private_data,",
        "			       const struct bpf_insn *insn)",
        "{",
        "	verbose(private_data, \"(%02x) r%d = %s%d r%d\\n\",",
        "		insn->code, insn->dst_reg,",
        "		BPF_SRC(insn->code) == BPF_TO_BE ? \"be\" : \"le\",",
        "		insn->imm, insn->dst_reg);",
        "}",
        "",
        "static void print_bpf_bswap_insn(bpf_insn_print_t verbose,",
        "			       void *private_data,",
        "			       const struct bpf_insn *insn)",
        "{",
        "	verbose(private_data, \"(%02x) r%d = bswap%d r%d\\n\",",
        "		insn->code, insn->dst_reg,",
        "		insn->imm, insn->dst_reg);",
        "}",
        "",
        "static bool is_sdiv_smod(const struct bpf_insn *insn)",
        "{",
        "	return (BPF_OP(insn->code)  == BPF_DIV || BPF_OP(insn->code) == BPF_MOD) &&",
        "	       insn->off == 1;",
        "}",
        "",
        "static bool is_movsx(const struct bpf_insn *insn)",
        "{",
        "	return BPF_OP(insn->code) == BPF_MOV &&",
        "	       (insn->off == 8 || insn->off == 16 || insn->off == 32);",
        "}",
        "",
        "static bool is_addr_space_cast(const struct bpf_insn *insn)",
        "{",
        "	return insn->code == (BPF_ALU64 | BPF_MOV | BPF_X) &&",
        "		insn->off == BPF_ADDR_SPACE_CAST;",
        "}",
        "",
        "/* Special (internal-only) form of mov, used to resolve per-CPU addrs:",
        " * dst_reg = src_reg + <percpu_base_off>",
        " * BPF_ADDR_PERCPU is used as a special insn->off value.",
        " */",
        "#define BPF_ADDR_PERCPU	(-1)",
        "",
        "static inline bool is_mov_percpu_addr(const struct bpf_insn *insn)",
        "{",
        "	return insn->code == (BPF_ALU64 | BPF_MOV | BPF_X) && insn->off == BPF_ADDR_PERCPU;",
        "}",
        "",
        "void print_bpf_insn(const struct bpf_insn_cbs *cbs,",
        "		    const struct bpf_insn *insn,",
        "		    bool allow_ptr_leaks)",
        "{",
        "	const bpf_insn_print_t verbose = cbs->cb_print;",
        "	u8 class = BPF_CLASS(insn->code);",
        "",
        "	if (class == BPF_ALU || class == BPF_ALU64) {",
        "		if (BPF_OP(insn->code) == BPF_END) {",
        "			if (class == BPF_ALU64)",
        "				print_bpf_bswap_insn(verbose, cbs->private_data, insn);",
        "			else",
        "				print_bpf_end_insn(verbose, cbs->private_data, insn);",
        "		} else if (BPF_OP(insn->code) == BPF_NEG) {",
        "			verbose(cbs->private_data, \"(%02x) %c%d = -%c%d\\n\",",
        "				insn->code, class == BPF_ALU ? 'w' : 'r',",
        "				insn->dst_reg, class == BPF_ALU ? 'w' : 'r',",
        "				insn->dst_reg);",
        "		} else if (is_addr_space_cast(insn)) {",
        "			verbose(cbs->private_data, \"(%02x) r%d = addr_space_cast(r%d, %d, %d)\\n\",",
        "				insn->code, insn->dst_reg,",
        "				insn->src_reg, ((u32)insn->imm) >> 16, (u16)insn->imm);",
        "		} else if (is_mov_percpu_addr(insn)) {",
        "			verbose(cbs->private_data, \"(%02x) r%d = &(void __percpu *)(r%d)\\n\",",
        "				insn->code, insn->dst_reg, insn->src_reg);",
        "		} else if (BPF_SRC(insn->code) == BPF_X) {",
        "			verbose(cbs->private_data, \"(%02x) %c%d %s %s%c%d\\n\",",
        "				insn->code, class == BPF_ALU ? 'w' : 'r',",
        "				insn->dst_reg,",
        "				is_sdiv_smod(insn) ? bpf_alu_sign_string[BPF_OP(insn->code) >> 4]",
        "						   : bpf_alu_string[BPF_OP(insn->code) >> 4],",
        "				is_movsx(insn) ? bpf_movsx_string[(insn->off >> 3) - 1] : \"\",",
        "				class == BPF_ALU ? 'w' : 'r',",
        "				insn->src_reg);",
        "		} else {",
        "			verbose(cbs->private_data, \"(%02x) %c%d %s %d\\n\",",
        "				insn->code, class == BPF_ALU ? 'w' : 'r',",
        "				insn->dst_reg,",
        "				is_sdiv_smod(insn) ? bpf_alu_sign_string[BPF_OP(insn->code) >> 4]",
        "						   : bpf_alu_string[BPF_OP(insn->code) >> 4],",
        "				insn->imm);",
        "		}",
        "	} else if (class == BPF_STX) {",
        "		if (BPF_MODE(insn->code) == BPF_MEM)",
        "			verbose(cbs->private_data, \"(%02x) *(%s *)(r%d %+d) = r%d\\n\",",
        "				insn->code,",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg,",
        "				insn->off, insn->src_reg);",
        "		else if (BPF_MODE(insn->code) == BPF_ATOMIC &&",
        "			 (insn->imm == BPF_ADD || insn->imm == BPF_AND ||",
        "			  insn->imm == BPF_OR || insn->imm == BPF_XOR)) {",
        "			verbose(cbs->private_data, \"(%02x) lock *(%s *)(r%d %+d) %s r%d\\n\",",
        "				insn->code,",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg, insn->off,",
        "				bpf_alu_string[BPF_OP(insn->imm) >> 4],",
        "				insn->src_reg);",
        "		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&",
        "			   (insn->imm == (BPF_ADD | BPF_FETCH) ||",
        "			    insn->imm == (BPF_AND | BPF_FETCH) ||",
        "			    insn->imm == (BPF_OR | BPF_FETCH) ||",
        "			    insn->imm == (BPF_XOR | BPF_FETCH))) {",
        "			verbose(cbs->private_data, \"(%02x) r%d = atomic%s_fetch_%s((%s *)(r%d %+d), r%d)\\n\",",
        "				insn->code, insn->src_reg,",
        "				BPF_SIZE(insn->code) == BPF_DW ? \"64\" : \"\",",
        "				bpf_atomic_alu_string[BPF_OP(insn->imm) >> 4],",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg, insn->off, insn->src_reg);",
        "		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&",
        "			   insn->imm == BPF_CMPXCHG) {",
        "			verbose(cbs->private_data, \"(%02x) r0 = atomic%s_cmpxchg((%s *)(r%d %+d), r0, r%d)\\n\",",
        "				insn->code,",
        "				BPF_SIZE(insn->code) == BPF_DW ? \"64\" : \"\",",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg, insn->off,",
        "				insn->src_reg);",
        "		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&",
        "			   insn->imm == BPF_XCHG) {",
        "			verbose(cbs->private_data, \"(%02x) r%d = atomic%s_xchg((%s *)(r%d %+d), r%d)\\n\",",
        "				insn->code, insn->src_reg,",
        "				BPF_SIZE(insn->code) == BPF_DW ? \"64\" : \"\",",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg, insn->off, insn->src_reg);",
        "		} else {",
        "			verbose(cbs->private_data, \"BUG_%02x\\n\", insn->code);",
        "		}",
        "	} else if (class == BPF_ST) {",
        "		if (BPF_MODE(insn->code) == BPF_MEM) {",
        "			verbose(cbs->private_data, \"(%02x) *(%s *)(r%d %+d) = %d\\n\",",
        "				insn->code,",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->dst_reg,",
        "				insn->off, insn->imm);",
        "		} else if (BPF_MODE(insn->code) == 0xc0 /* BPF_NOSPEC, no UAPI */) {",
        "			verbose(cbs->private_data, \"(%02x) nospec\\n\", insn->code);",
        "		} else {",
        "			verbose(cbs->private_data, \"BUG_st_%02x\\n\", insn->code);",
        "		}",
        "	} else if (class == BPF_LDX) {",
        "		if (BPF_MODE(insn->code) != BPF_MEM && BPF_MODE(insn->code) != BPF_MEMSX) {",
        "			verbose(cbs->private_data, \"BUG_ldx_%02x\\n\", insn->code);",
        "			return;",
        "		}",
        "		verbose(cbs->private_data, \"(%02x) r%d = *(%s *)(r%d %+d)\\n\",",
        "			insn->code, insn->dst_reg,",
        "			BPF_MODE(insn->code) == BPF_MEM ?",
        "				 bpf_ldst_string[BPF_SIZE(insn->code) >> 3] :",
        "				 bpf_ldsx_string[BPF_SIZE(insn->code) >> 3],",
        "			insn->src_reg, insn->off);",
        "	} else if (class == BPF_LD) {",
        "		if (BPF_MODE(insn->code) == BPF_ABS) {",
        "			verbose(cbs->private_data, \"(%02x) r0 = *(%s *)skb[%d]\\n\",",
        "				insn->code,",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->imm);",
        "		} else if (BPF_MODE(insn->code) == BPF_IND) {",
        "			verbose(cbs->private_data, \"(%02x) r0 = *(%s *)skb[r%d + %d]\\n\",",
        "				insn->code,",
        "				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],",
        "				insn->src_reg, insn->imm);",
        "		} else if (BPF_MODE(insn->code) == BPF_IMM &&",
        "			   BPF_SIZE(insn->code) == BPF_DW) {",
        "			/* At this point, we already made sure that the second",
        "			 * part of the ldimm64 insn is accessible.",
        "			 */",
        "			u64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;",
        "			bool is_ptr = insn->src_reg == BPF_PSEUDO_MAP_FD ||",
        "				      insn->src_reg == BPF_PSEUDO_MAP_VALUE;",
        "			char tmp[64];",
        "",
        "			if (is_ptr && !allow_ptr_leaks)",
        "				imm = 0;",
        "",
        "			verbose(cbs->private_data, \"(%02x) r%d = %s\\n\",",
        "				insn->code, insn->dst_reg,",
        "				__func_imm_name(cbs, insn, imm,",
        "						tmp, sizeof(tmp)));",
        "		} else {",
        "			verbose(cbs->private_data, \"BUG_ld_%02x\\n\", insn->code);",
        "			return;",
        "		}",
        "	} else if (class == BPF_JMP32 || class == BPF_JMP) {",
        "		u8 opcode = BPF_OP(insn->code);",
        "",
        "		if (opcode == BPF_CALL) {",
        "			char tmp[64];",
        "",
        "			if (insn->src_reg == BPF_PSEUDO_CALL) {",
        "				verbose(cbs->private_data, \"(%02x) call pc%s\\n\",",
        "					insn->code,",
        "					__func_get_name(cbs, insn,",
        "							tmp, sizeof(tmp)));",
        "			} else {",
        "				strcpy(tmp, \"unknown\");",
        "				verbose(cbs->private_data, \"(%02x) call %s#%d\\n\", insn->code,",
        "					__func_get_name(cbs, insn,",
        "							tmp, sizeof(tmp)),",
        "					insn->imm);",
        "			}",
        "		} else if (insn->code == (BPF_JMP | BPF_JA)) {",
        "			verbose(cbs->private_data, \"(%02x) goto pc%+d\\n\",",
        "				insn->code, insn->off);",
        "		} else if (insn->code == (BPF_JMP | BPF_JCOND) &&",
        "			   insn->src_reg == BPF_MAY_GOTO) {",
        "			verbose(cbs->private_data, \"(%02x) may_goto pc%+d\\n\",",
        "				insn->code, insn->off);",
        "		} else if (insn->code == (BPF_JMP32 | BPF_JA)) {",
        "			verbose(cbs->private_data, \"(%02x) gotol pc%+d\\n\",",
        "				insn->code, insn->imm);",
        "		} else if (insn->code == (BPF_JMP | BPF_EXIT)) {",
        "			verbose(cbs->private_data, \"(%02x) exit\\n\", insn->code);",
        "		} else if (BPF_SRC(insn->code) == BPF_X) {",
        "			verbose(cbs->private_data,",
        "				\"(%02x) if %c%d %s %c%d goto pc%+d\\n\",",
        "				insn->code, class == BPF_JMP32 ? 'w' : 'r',",
        "				insn->dst_reg,",
        "				bpf_jmp_string[BPF_OP(insn->code) >> 4],",
        "				class == BPF_JMP32 ? 'w' : 'r',",
        "				insn->src_reg, insn->off);",
        "		} else {",
        "			verbose(cbs->private_data,",
        "				\"(%02x) if %c%d %s 0x%x goto pc%+d\\n\",",
        "				insn->code, class == BPF_JMP32 ? 'w' : 'r',",
        "				insn->dst_reg,",
        "				bpf_jmp_string[BPF_OP(insn->code) >> 4],",
        "				insn->imm, insn->off);",
        "		}",
        "	} else {",
        "		verbose(cbs->private_data, \"(%02x) %s\\n\",",
        "			insn->code, bpf_class_string[class]);",
        "	}",
        "}"
    ]
  },
  "include_linux_vmstat_h": {
    path: "include/linux/vmstat.h",
    covered: [615],
    totalLines: 626,
    coveredCount: 1,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_VMSTAT_H",
        "#define _LINUX_VMSTAT_H",
        "",
        "#include <linux/types.h>",
        "#include <linux/percpu.h>",
        "#include <linux/mmzone.h>",
        "#include <linux/vm_event_item.h>",
        "#include <linux/atomic.h>",
        "#include <linux/static_key.h>",
        "#include <linux/mmdebug.h>",
        "",
        "extern int sysctl_stat_interval;",
        "",
        "#ifdef CONFIG_NUMA",
        "#define ENABLE_NUMA_STAT   1",
        "#define DISABLE_NUMA_STAT   0",
        "extern int sysctl_vm_numa_stat;",
        "DECLARE_STATIC_KEY_TRUE(vm_numa_stat_key);",
        "int sysctl_vm_numa_stat_handler(const struct ctl_table *table, int write,",
        "		void *buffer, size_t *length, loff_t *ppos);",
        "#endif",
        "",
        "struct reclaim_stat {",
        "	unsigned nr_dirty;",
        "	unsigned nr_unqueued_dirty;",
        "	unsigned nr_congested;",
        "	unsigned nr_writeback;",
        "	unsigned nr_immediate;",
        "	unsigned nr_pageout;",
        "	unsigned nr_activate[ANON_AND_FILE];",
        "	unsigned nr_ref_keep;",
        "	unsigned nr_unmap_fail;",
        "	unsigned nr_lazyfree_fail;",
        "	unsigned nr_demoted;",
        "};",
        "",
        "/* Stat data for system wide items */",
        "enum vm_stat_item {",
        "	NR_DIRTY_THRESHOLD,",
        "	NR_DIRTY_BG_THRESHOLD,",
        "	NR_MEMMAP_PAGES,	/* page metadata allocated through buddy allocator */",
        "	NR_MEMMAP_BOOT_PAGES,	/* page metadata allocated through boot allocator */",
        "	NR_VM_STAT_ITEMS,",
        "};",
        "",
        "#ifdef CONFIG_VM_EVENT_COUNTERS",
        "/*",
        " * Light weight per cpu counter implementation.",
        " *",
        " * Counters should only be incremented and no critical kernel component",
        " * should rely on the counter values.",
        " *",
        " * Counters are handled completely inline. On many platforms the code",
        " * generated will simply be the increment of a global address.",
        " */",
        "",
        "struct vm_event_state {",
        "	unsigned long event[NR_VM_EVENT_ITEMS];",
        "};",
        "",
        "DECLARE_PER_CPU(struct vm_event_state, vm_event_states);",
        "",
        "/*",
        " * vm counters are allowed to be racy. Use raw_cpu_ops to avoid the",
        " * local_irq_disable overhead.",
        " */",
        "static inline void __count_vm_event(enum vm_event_item item)",
        "{",
        "	raw_cpu_inc(vm_event_states.event[item]);",
        "}",
        "",
        "static inline void count_vm_event(enum vm_event_item item)",
        "{",
        "	this_cpu_inc(vm_event_states.event[item]);",
        "}",
        "",
        "static inline void __count_vm_events(enum vm_event_item item, long delta)",
        "{",
        "	raw_cpu_add(vm_event_states.event[item], delta);",
        "}",
        "",
        "static inline void count_vm_events(enum vm_event_item item, long delta)",
        "{",
        "	this_cpu_add(vm_event_states.event[item], delta);",
        "}",
        "",
        "extern void all_vm_events(unsigned long *);",
        "",
        "extern void vm_events_fold_cpu(int cpu);",
        "",
        "#else",
        "",
        "/* Disable counters */",
        "static inline void count_vm_event(enum vm_event_item item)",
        "{",
        "}",
        "static inline void count_vm_events(enum vm_event_item item, long delta)",
        "{",
        "}",
        "static inline void __count_vm_event(enum vm_event_item item)",
        "{",
        "}",
        "static inline void __count_vm_events(enum vm_event_item item, long delta)",
        "{",
        "}",
        "static inline void all_vm_events(unsigned long *ret)",
        "{",
        "}",
        "static inline void vm_events_fold_cpu(int cpu)",
        "{",
        "}",
        "",
        "#endif /* CONFIG_VM_EVENT_COUNTERS */",
        "",
        "#ifdef CONFIG_NUMA_BALANCING",
        "#define count_vm_numa_event(x)     count_vm_event(x)",
        "#define count_vm_numa_events(x, y) count_vm_events(x, y)",
        "#else",
        "#define count_vm_numa_event(x) do {} while (0)",
        "#define count_vm_numa_events(x, y) do { (void)(y); } while (0)",
        "#endif /* CONFIG_NUMA_BALANCING */",
        "",
        "#ifdef CONFIG_DEBUG_TLBFLUSH",
        "#define count_vm_tlb_event(x)	   count_vm_event(x)",
        "#define count_vm_tlb_events(x, y)  count_vm_events(x, y)",
        "#else",
        "#define count_vm_tlb_event(x)     do {} while (0)",
        "#define count_vm_tlb_events(x, y) do { (void)(y); } while (0)",
        "#endif",
        "",
        "#ifdef CONFIG_PER_VMA_LOCK_STATS",
        "#define count_vm_vma_lock_event(x) count_vm_event(x)",
        "#else",
        "#define count_vm_vma_lock_event(x) do {} while (0)",
        "#endif",
        "",
        "#define __count_zid_vm_events(item, zid, delta) \\",
        "	__count_vm_events(item##_NORMAL - ZONE_NORMAL + zid, delta)",
        "",
        "/*",
        " * Zone and node-based page accounting with per cpu differentials.",
        " */",
        "extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];",
        "extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];",
        "extern atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];",
        "",
        "#ifdef CONFIG_NUMA",
        "static inline void zone_numa_event_add(long x, struct zone *zone,",
        "				enum numa_stat_item item)",
        "{",
        "	atomic_long_add(x, &zone->vm_numa_event[item]);",
        "	atomic_long_add(x, &vm_numa_event[item]);",
        "}",
        "",
        "static inline unsigned long zone_numa_event_state(struct zone *zone,",
        "					enum numa_stat_item item)",
        "{",
        "	return atomic_long_read(&zone->vm_numa_event[item]);",
        "}",
        "",
        "static inline unsigned long",
        "global_numa_event_state(enum numa_stat_item item)",
        "{",
        "	return atomic_long_read(&vm_numa_event[item]);",
        "}",
        "#endif /* CONFIG_NUMA */",
        "",
        "static inline void zone_page_state_add(long x, struct zone *zone,",
        "				 enum zone_stat_item item)",
        "{",
        "	atomic_long_add(x, &zone->vm_stat[item]);",
        "	atomic_long_add(x, &vm_zone_stat[item]);",
        "}",
        "",
        "static inline void node_page_state_add(long x, struct pglist_data *pgdat,",
        "				 enum node_stat_item item)",
        "{",
        "	atomic_long_add(x, &pgdat->vm_stat[item]);",
        "	atomic_long_add(x, &vm_node_stat[item]);",
        "}",
        "",
        "static inline unsigned long global_zone_page_state(enum zone_stat_item item)",
        "{",
        "	long x = atomic_long_read(&vm_zone_stat[item]);",
        "#ifdef CONFIG_SMP",
        "	if (x < 0)",
        "		x = 0;",
        "#endif",
        "	return x;",
        "}",
        "",
        "static inline",
        "unsigned long global_node_page_state_pages(enum node_stat_item item)",
        "{",
        "	long x = atomic_long_read(&vm_node_stat[item]);",
        "#ifdef CONFIG_SMP",
        "	if (x < 0)",
        "		x = 0;",
        "#endif",
        "	return x;",
        "}",
        "",
        "static inline unsigned long global_node_page_state(enum node_stat_item item)",
        "{",
        "	VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));",
        "",
        "	return global_node_page_state_pages(item);",
        "}",
        "",
        "static inline unsigned long zone_page_state(struct zone *zone,",
        "					enum zone_stat_item item)",
        "{",
        "	long x = atomic_long_read(&zone->vm_stat[item]);",
        "#ifdef CONFIG_SMP",
        "	if (x < 0)",
        "		x = 0;",
        "#endif",
        "	return x;",
        "}",
        "",
        "/*",
        " * More accurate version that also considers the currently pending",
        " * deltas. For that we need to loop over all cpus to find the current",
        " * deltas. There is no synchronization so the result cannot be",
        " * exactly accurate either.",
        " */",
        "static inline unsigned long zone_page_state_snapshot(struct zone *zone,",
        "					enum zone_stat_item item)",
        "{",
        "	long x = atomic_long_read(&zone->vm_stat[item]);",
        "",
        "#ifdef CONFIG_SMP",
        "	int cpu;",
        "	for_each_online_cpu(cpu)",
        "		x += per_cpu_ptr(zone->per_cpu_zonestats, cpu)->vm_stat_diff[item];",
        "",
        "	if (x < 0)",
        "		x = 0;",
        "#endif",
        "	return x;",
        "}",
        "",
        "#ifdef CONFIG_NUMA",
        "/* See __count_vm_event comment on why raw_cpu_inc is used. */",
        "static inline void",
        "__count_numa_event(struct zone *zone, enum numa_stat_item item)",
        "{",
        "	struct per_cpu_zonestat __percpu *pzstats = zone->per_cpu_zonestats;",
        "",
        "	raw_cpu_inc(pzstats->vm_numa_event[item]);",
        "}",
        "",
        "static inline void",
        "__count_numa_events(struct zone *zone, enum numa_stat_item item, long delta)",
        "{",
        "	struct per_cpu_zonestat __percpu *pzstats = zone->per_cpu_zonestats;",
        "",
        "	raw_cpu_add(pzstats->vm_numa_event[item], delta);",
        "}",
        "",
        "extern unsigned long sum_zone_node_page_state(int node,",
        "					      enum zone_stat_item item);",
        "extern unsigned long sum_zone_numa_event_state(int node, enum numa_stat_item item);",
        "extern unsigned long node_page_state(struct pglist_data *pgdat,",
        "						enum node_stat_item item);",
        "extern unsigned long node_page_state_pages(struct pglist_data *pgdat,",
        "					   enum node_stat_item item);",
        "extern void fold_vm_numa_events(void);",
        "#else",
        "#define sum_zone_node_page_state(node, item) global_zone_page_state(item)",
        "#define node_page_state(node, item) global_node_page_state(item)",
        "#define node_page_state_pages(node, item) global_node_page_state_pages(item)",
        "static inline void fold_vm_numa_events(void)",
        "{",
        "}",
        "#endif /* CONFIG_NUMA */",
        "",
        "#ifdef CONFIG_SMP",
        "void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);",
        "void __inc_zone_page_state(struct page *, enum zone_stat_item);",
        "void __dec_zone_page_state(struct page *, enum zone_stat_item);",
        "",
        "void __mod_node_page_state(struct pglist_data *, enum node_stat_item item, long);",
        "void __inc_node_page_state(struct page *, enum node_stat_item);",
        "void __dec_node_page_state(struct page *, enum node_stat_item);",
        "",
        "void mod_zone_page_state(struct zone *, enum zone_stat_item, long);",
        "void inc_zone_page_state(struct page *, enum zone_stat_item);",
        "void dec_zone_page_state(struct page *, enum zone_stat_item);",
        "",
        "void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);",
        "void inc_node_page_state(struct page *, enum node_stat_item);",
        "void dec_node_page_state(struct page *, enum node_stat_item);",
        "",
        "extern void inc_node_state(struct pglist_data *, enum node_stat_item);",
        "extern void __inc_zone_state(struct zone *, enum zone_stat_item);",
        "extern void __inc_node_state(struct pglist_data *, enum node_stat_item);",
        "extern void dec_zone_state(struct zone *, enum zone_stat_item);",
        "extern void __dec_zone_state(struct zone *, enum zone_stat_item);",
        "extern void __dec_node_state(struct pglist_data *, enum node_stat_item);",
        "",
        "void quiet_vmstat(void);",
        "void cpu_vm_stats_fold(int cpu);",
        "void refresh_zone_stat_thresholds(void);",
        "",
        "struct ctl_table;",
        "int vmstat_refresh(const struct ctl_table *, int write, void *buffer, size_t *lenp,",
        "		loff_t *ppos);",
        "",
        "void drain_zonestat(struct zone *zone, struct per_cpu_zonestat *);",
        "",
        "int calculate_pressure_threshold(struct zone *zone);",
        "int calculate_normal_threshold(struct zone *zone);",
        "void set_pgdat_percpu_threshold(pg_data_t *pgdat,",
        "				int (*calculate_pressure)(struct zone *));",
        "#else /* CONFIG_SMP */",
        "",
        "/*",
        " * We do not maintain differentials in a single processor configuration.",
        " * The functions directly modify the zone and global counters.",
        " */",
        "static inline void __mod_zone_page_state(struct zone *zone,",
        "			enum zone_stat_item item, long delta)",
        "{",
        "	zone_page_state_add(delta, zone, item);",
        "}",
        "",
        "static inline void __mod_node_page_state(struct pglist_data *pgdat,",
        "			enum node_stat_item item, int delta)",
        "{",
        "	if (vmstat_item_in_bytes(item)) {",
        "		/*",
        "		 * Only cgroups use subpage accounting right now; at",
        "		 * the global level, these items still change in",
        "		 * multiples of whole pages. Store them as pages",
        "		 * internally to keep the per-cpu counters compact.",
        "		 */",
        "		VM_WARN_ON_ONCE(delta & (PAGE_SIZE - 1));",
        "		delta >>= PAGE_SHIFT;",
        "	}",
        "",
        "	node_page_state_add(delta, pgdat, item);",
        "}",
        "",
        "static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)",
        "{",
        "	atomic_long_inc(&zone->vm_stat[item]);",
        "	atomic_long_inc(&vm_zone_stat[item]);",
        "}",
        "",
        "static inline void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)",
        "{",
        "	atomic_long_inc(&pgdat->vm_stat[item]);",
        "	atomic_long_inc(&vm_node_stat[item]);",
        "}",
        "",
        "static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)",
        "{",
        "	atomic_long_dec(&zone->vm_stat[item]);",
        "	atomic_long_dec(&vm_zone_stat[item]);",
        "}",
        "",
        "static inline void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)",
        "{",
        "	atomic_long_dec(&pgdat->vm_stat[item]);",
        "	atomic_long_dec(&vm_node_stat[item]);",
        "}",
        "",
        "static inline void __inc_zone_page_state(struct page *page,",
        "			enum zone_stat_item item)",
        "{",
        "	__inc_zone_state(page_zone(page), item);",
        "}",
        "",
        "static inline void __inc_node_page_state(struct page *page,",
        "			enum node_stat_item item)",
        "{",
        "	__inc_node_state(page_pgdat(page), item);",
        "}",
        "",
        "",
        "static inline void __dec_zone_page_state(struct page *page,",
        "			enum zone_stat_item item)",
        "{",
        "	__dec_zone_state(page_zone(page), item);",
        "}",
        "",
        "static inline void __dec_node_page_state(struct page *page,",
        "			enum node_stat_item item)",
        "{",
        "	__dec_node_state(page_pgdat(page), item);",
        "}",
        "",
        "",
        "/*",
        " * We only use atomic operations to update counters. So there is no need to",
        " * disable interrupts.",
        " */",
        "#define inc_zone_page_state __inc_zone_page_state",
        "#define dec_zone_page_state __dec_zone_page_state",
        "#define mod_zone_page_state __mod_zone_page_state",
        "",
        "#define inc_node_page_state __inc_node_page_state",
        "#define dec_node_page_state __dec_node_page_state",
        "#define mod_node_page_state __mod_node_page_state",
        "",
        "#define inc_zone_state __inc_zone_state",
        "#define inc_node_state __inc_node_state",
        "#define dec_zone_state __dec_zone_state",
        "",
        "#define set_pgdat_percpu_threshold(pgdat, callback) { }",
        "",
        "static inline void refresh_zone_stat_thresholds(void) { }",
        "static inline void cpu_vm_stats_fold(int cpu) { }",
        "static inline void quiet_vmstat(void) { }",
        "",
        "static inline void drain_zonestat(struct zone *zone,",
        "			struct per_cpu_zonestat *pzstats) { }",
        "#endif		/* CONFIG_SMP */",
        "",
        "static inline void __zone_stat_mod_folio(struct folio *folio,",
        "		enum zone_stat_item item, long nr)",
        "{",
        "	__mod_zone_page_state(folio_zone(folio), item, nr);",
        "}",
        "",
        "static inline void __zone_stat_add_folio(struct folio *folio,",
        "		enum zone_stat_item item)",
        "{",
        "	__mod_zone_page_state(folio_zone(folio), item, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void __zone_stat_sub_folio(struct folio *folio,",
        "		enum zone_stat_item item)",
        "{",
        "	__mod_zone_page_state(folio_zone(folio), item, -folio_nr_pages(folio));",
        "}",
        "",
        "static inline void zone_stat_mod_folio(struct folio *folio,",
        "		enum zone_stat_item item, long nr)",
        "{",
        "	mod_zone_page_state(folio_zone(folio), item, nr);",
        "}",
        "",
        "static inline void zone_stat_add_folio(struct folio *folio,",
        "		enum zone_stat_item item)",
        "{",
        "	mod_zone_page_state(folio_zone(folio), item, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void zone_stat_sub_folio(struct folio *folio,",
        "		enum zone_stat_item item)",
        "{",
        "	mod_zone_page_state(folio_zone(folio), item, -folio_nr_pages(folio));",
        "}",
        "",
        "static inline void __node_stat_mod_folio(struct folio *folio,",
        "		enum node_stat_item item, long nr)",
        "{",
        "	__mod_node_page_state(folio_pgdat(folio), item, nr);",
        "}",
        "",
        "static inline void __node_stat_add_folio(struct folio *folio,",
        "		enum node_stat_item item)",
        "{",
        "	__mod_node_page_state(folio_pgdat(folio), item, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void __node_stat_sub_folio(struct folio *folio,",
        "		enum node_stat_item item)",
        "{",
        "	__mod_node_page_state(folio_pgdat(folio), item, -folio_nr_pages(folio));",
        "}",
        "",
        "static inline void node_stat_mod_folio(struct folio *folio,",
        "		enum node_stat_item item, long nr)",
        "{",
        "	mod_node_page_state(folio_pgdat(folio), item, nr);",
        "}",
        "",
        "static inline void node_stat_add_folio(struct folio *folio,",
        "		enum node_stat_item item)",
        "{",
        "	mod_node_page_state(folio_pgdat(folio), item, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void node_stat_sub_folio(struct folio *folio,",
        "		enum node_stat_item item)",
        "{",
        "	mod_node_page_state(folio_pgdat(folio), item, -folio_nr_pages(folio));",
        "}",
        "",
        "extern const char * const vmstat_text[];",
        "",
        "static inline const char *zone_stat_name(enum zone_stat_item item)",
        "{",
        "	return vmstat_text[item];",
        "}",
        "",
        "#ifdef CONFIG_NUMA",
        "static inline const char *numa_stat_name(enum numa_stat_item item)",
        "{",
        "	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +",
        "			   item];",
        "}",
        "#endif /* CONFIG_NUMA */",
        "",
        "static inline const char *node_stat_name(enum node_stat_item item)",
        "{",
        "	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +",
        "			   NR_VM_NUMA_EVENT_ITEMS +",
        "			   item];",
        "}",
        "",
        "static inline const char *lru_list_name(enum lru_list lru)",
        "{",
        "	return node_stat_name(NR_LRU_BASE + (enum node_stat_item)lru) + 3; // skip \"nr_\"",
        "}",
        "",
        "#if defined(CONFIG_VM_EVENT_COUNTERS) || defined(CONFIG_MEMCG)",
        "static inline const char *vm_event_name(enum vm_event_item item)",
        "{",
        "	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +",
        "			   NR_VM_NUMA_EVENT_ITEMS +",
        "			   NR_VM_NODE_STAT_ITEMS +",
        "			   NR_VM_STAT_ITEMS +",
        "			   item];",
        "}",
        "#endif /* CONFIG_VM_EVENT_COUNTERS || CONFIG_MEMCG */",
        "",
        "#ifdef CONFIG_MEMCG",
        "",
        "void __mod_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,",
        "			int val);",
        "",
        "static inline void mod_lruvec_state(struct lruvec *lruvec,",
        "				    enum node_stat_item idx, int val)",
        "{",
        "	unsigned long flags;",
        "",
        "	local_irq_save(flags);",
        "	__mod_lruvec_state(lruvec, idx, val);",
        "	local_irq_restore(flags);",
        "}",
        "",
        "void __lruvec_stat_mod_folio(struct folio *folio,",
        "			     enum node_stat_item idx, int val);",
        "",
        "static inline void lruvec_stat_mod_folio(struct folio *folio,",
        "					 enum node_stat_item idx, int val)",
        "{",
        "	unsigned long flags;",
        "",
        "	local_irq_save(flags);",
        "	__lruvec_stat_mod_folio(folio, idx, val);",
        "	local_irq_restore(flags);",
        "}",
        "",
        "static inline void mod_lruvec_page_state(struct page *page,",
        "					 enum node_stat_item idx, int val)",
        "{",
        "	lruvec_stat_mod_folio(page_folio(page), idx, val);",
        "}",
        "",
        "#else",
        "",
        "static inline void __mod_lruvec_state(struct lruvec *lruvec,",
        "				      enum node_stat_item idx, int val)",
        "{",
        "	__mod_node_page_state(lruvec_pgdat(lruvec), idx, val);",
        "}",
        "",
        "static inline void mod_lruvec_state(struct lruvec *lruvec,",
        "				    enum node_stat_item idx, int val)",
        "{",
        "	mod_node_page_state(lruvec_pgdat(lruvec), idx, val);",
        "}",
        "",
        "static inline void __lruvec_stat_mod_folio(struct folio *folio,",
        "					 enum node_stat_item idx, int val)",
        "{",
        "	__mod_node_page_state(folio_pgdat(folio), idx, val);",
        "}",
        "",
        "static inline void lruvec_stat_mod_folio(struct folio *folio,",
        "					 enum node_stat_item idx, int val)",
        "{",
        "	mod_node_page_state(folio_pgdat(folio), idx, val);",
        "}",
        "",
        "static inline void mod_lruvec_page_state(struct page *page,",
        "					 enum node_stat_item idx, int val)",
        "{",
        "	mod_node_page_state(page_pgdat(page), idx, val);",
        "}",
        "",
        "#endif /* CONFIG_MEMCG */",
        "",
        "static inline void __lruvec_stat_add_folio(struct folio *folio,",
        "					   enum node_stat_item idx)",
        "{",
        "	__lruvec_stat_mod_folio(folio, idx, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void __lruvec_stat_sub_folio(struct folio *folio,",
        "					   enum node_stat_item idx)",
        "{",
        "	__lruvec_stat_mod_folio(folio, idx, -folio_nr_pages(folio));",
        "}",
        "",
        "static inline void lruvec_stat_add_folio(struct folio *folio,",
        "					 enum node_stat_item idx)",
        "{",
        "	lruvec_stat_mod_folio(folio, idx, folio_nr_pages(folio));",
        "}",
        "",
        "static inline void lruvec_stat_sub_folio(struct folio *folio,",
        "					 enum node_stat_item idx)",
        "{",
        "	lruvec_stat_mod_folio(folio, idx, -folio_nr_pages(folio));",
        "}",
        "",
        "void memmap_boot_pages_add(long delta);",
        "void memmap_pages_add(long delta);",
        "#endif /* _LINUX_VMSTAT_H */"
    ]
  },
  "arch_x86_include_asm_page_64_h": {
    path: "arch/x86/include/asm/page_64.h",
    covered: [27],
    totalLines: 103,
    coveredCount: 1,
    coveragePct: 1.0,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _ASM_X86_PAGE_64_H",
        "#define _ASM_X86_PAGE_64_H",
        "",
        "#include <asm/page_64_types.h>",
        "",
        "#ifndef __ASSEMBLY__",
        "#include <asm/cpufeatures.h>",
        "#include <asm/alternative.h>",
        "",
        "#include <linux/kmsan-checks.h>",
        "",
        "/* duplicated to the one in bootmem.h */",
        "extern unsigned long max_pfn;",
        "extern unsigned long phys_base;",
        "",
        "extern unsigned long page_offset_base;",
        "extern unsigned long vmalloc_base;",
        "extern unsigned long vmemmap_base;",
        "extern unsigned long direct_map_physmem_end;",
        "",
        "static __always_inline unsigned long __phys_addr_nodebug(unsigned long x)",
        "{",
        "	unsigned long y = x - __START_KERNEL_map;",
        "",
        "	/* use the carry flag to determine if x was < __START_KERNEL_map */",
        "	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));",
        "",
        "	return x;",
        "}",
        "",
        "#ifdef CONFIG_DEBUG_VIRTUAL",
        "extern unsigned long __phys_addr(unsigned long);",
        "extern unsigned long __phys_addr_symbol(unsigned long);",
        "#else",
        "#define __phys_addr(x)		__phys_addr_nodebug(x)",
        "#define __phys_addr_symbol(x) \\",
        "	((unsigned long)(x) - __START_KERNEL_map + phys_base)",
        "#endif",
        "",
        "#define __phys_reloc_hide(x)	(x)",
        "",
        "void clear_page_orig(void *page);",
        "void clear_page_rep(void *page);",
        "void clear_page_erms(void *page);",
        "",
        "static inline void clear_page(void *page)",
        "{",
        "	/*",
        "	 * Clean up KMSAN metadata for the page being cleared. The assembly call",
        "	 * below clobbers @page, so we perform unpoisoning before it.",
        "	 */",
        "	kmsan_unpoison_memory(page, PAGE_SIZE);",
        "	alternative_call_2(clear_page_orig,",
        "			   clear_page_rep, X86_FEATURE_REP_GOOD,",
        "			   clear_page_erms, X86_FEATURE_ERMS,",
        "			   \"=D\" (page),",
        "			   \"D\" (page)",
        "			   : \"cc\", \"memory\", \"rax\", \"rcx\");",
        "}",
        "",
        "void copy_page(void *to, void *from);",
        "",
        "#ifdef CONFIG_X86_5LEVEL",
        "/*",
        " * User space process size.  This is the first address outside the user range.",
        " * There are a few constraints that determine this:",
        " *",
        " * On Intel CPUs, if a SYSCALL instruction is at the highest canonical",
        " * address, then that syscall will enter the kernel with a",
        " * non-canonical return address, and SYSRET will explode dangerously.",
        " * We avoid this particular problem by preventing anything",
        " * from being mapped at the maximum canonical address.",
        " *",
        " * On AMD CPUs in the Ryzen family, there's a nasty bug in which the",
        " * CPUs malfunction if they execute code from the highest canonical page.",
        " * They'll speculate right off the end of the canonical space, and",
        " * bad things happen.  This is worked around in the same way as the",
        " * Intel problem.",
        " *",
        " * With page table isolation enabled, we map the LDT in ... [stay tuned]",
        " */",
        "static __always_inline unsigned long task_size_max(void)",
        "{",
        "	unsigned long ret;",
        "",
        "	alternative_io(\"movq %[small],%0\",\"movq %[large],%0\",",
        "			X86_FEATURE_LA57,",
        "			\"=r\" (ret),",
        "			[small] \"i\" ((1ul << 47)-PAGE_SIZE),",
        "			[large] \"i\" ((1ul << 56)-PAGE_SIZE));",
        "",
        "	return ret;",
        "}",
        "#endif	/* CONFIG_X86_5LEVEL */",
        "",
        "#endif	/* !__ASSEMBLY__ */",
        "",
        "#ifdef CONFIG_X86_VSYSCALL_EMULATION",
        "# define __HAVE_ARCH_GATE_AREA 1",
        "#endif",
        "",
        "#endif /* _ASM_X86_PAGE_64_H */"
    ]
  },
  "kernel_bpf_hashtab_c": {
    path: "kernel/bpf/hashtab.c",
    covered: [2531, 548, 588, 569, 503, 2529, 433, 447, 437, 1509, 282, 251, 367, 331, 337, 221, 1587, 326, 496, 382, 138, 385, 444, 598, 580, 387, 609, 327, 551, 473, 353, 574, 1576, 334, 572, 322, 381, 1588, 1514, 466, 2528, 141, 453, 401, 359, 583, 519, 512, 525, 279, 441, 457, 391, 595, 415, 428, 1562, 570, 522, 370, 536, 1578],
    totalLines: 2668,
    coveredCount: 62,
    coveragePct: 2.3,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com",
        " * Copyright (c) 2016 Facebook",
        " */",
        "#include <linux/bpf.h>",
        "#include <linux/btf.h>",
        "#include <linux/jhash.h>",
        "#include <linux/filter.h>",
        "#include <linux/rculist_nulls.h>",
        "#include <linux/rcupdate_wait.h>",
        "#include <linux/random.h>",
        "#include <uapi/linux/btf.h>",
        "#include <linux/rcupdate_trace.h>",
        "#include <linux/btf_ids.h>",
        "#include \"percpu_freelist.h\"",
        "#include \"bpf_lru_list.h\"",
        "#include \"map_in_map.h\"",
        "#include <linux/bpf_mem_alloc.h>",
        "",
        "#define HTAB_CREATE_FLAG_MASK						\\",
        "	(BPF_F_NO_PREALLOC | BPF_F_NO_COMMON_LRU | BPF_F_NUMA_NODE |	\\",
        "	 BPF_F_ACCESS_MASK | BPF_F_ZERO_SEED)",
        "",
        "#define BATCH_OPS(_name)			\\",
        "	.map_lookup_batch =			\\",
        "	_name##_map_lookup_batch,		\\",
        "	.map_lookup_and_delete_batch =		\\",
        "	_name##_map_lookup_and_delete_batch,	\\",
        "	.map_update_batch =			\\",
        "	generic_map_update_batch,		\\",
        "	.map_delete_batch =			\\",
        "	generic_map_delete_batch",
        "",
        "/*",
        " * The bucket lock has two protection scopes:",
        " *",
        " * 1) Serializing concurrent operations from BPF programs on different",
        " *    CPUs",
        " *",
        " * 2) Serializing concurrent operations from BPF programs and sys_bpf()",
        " *",
        " * BPF programs can execute in any context including perf, kprobes and",
        " * tracing. As there are almost no limits where perf, kprobes and tracing",
        " * can be invoked from the lock operations need to be protected against",
        " * deadlocks. Deadlocks can be caused by recursion and by an invocation in",
        " * the lock held section when functions which acquire this lock are invoked",
        " * from sys_bpf(). BPF recursion is prevented by incrementing the per CPU",
        " * variable bpf_prog_active, which prevents BPF programs attached to perf",
        " * events, kprobes and tracing to be invoked before the prior invocation",
        " * from one of these contexts completed. sys_bpf() uses the same mechanism",
        " * by pinning the task to the current CPU and incrementing the recursion",
        " * protection across the map operation.",
        " *",
        " * This has subtle implications on PREEMPT_RT. PREEMPT_RT forbids certain",
        " * operations like memory allocations (even with GFP_ATOMIC) from atomic",
        " * contexts. This is required because even with GFP_ATOMIC the memory",
        " * allocator calls into code paths which acquire locks with long held lock",
        " * sections. To ensure the deterministic behaviour these locks are regular",
        " * spinlocks, which are converted to 'sleepable' spinlocks on RT. The only",
        " * true atomic contexts on an RT kernel are the low level hardware",
        " * handling, scheduling, low level interrupt handling, NMIs etc. None of",
        " * these contexts should ever do memory allocations.",
        " *",
        " * As regular device interrupt handlers and soft interrupts are forced into",
        " * thread context, the existing code which does",
        " *   spin_lock*(); alloc(GFP_ATOMIC); spin_unlock*();",
        " * just works.",
        " *",
        " * In theory the BPF locks could be converted to regular spinlocks as well,",
        " * but the bucket locks and percpu_freelist locks can be taken from",
        " * arbitrary contexts (perf, kprobes, tracepoints) which are required to be",
        " * atomic contexts even on RT. Before the introduction of bpf_mem_alloc,",
        " * it is only safe to use raw spinlock for preallocated hash map on a RT kernel,",
        " * because there is no memory allocation within the lock held sections. However",
        " * after hash map was fully converted to use bpf_mem_alloc, there will be",
        " * non-synchronous memory allocation for non-preallocated hash map, so it is",
        " * safe to always use raw spinlock for bucket lock.",
        " */",
        "struct bucket {",
        "	struct hlist_nulls_head head;",
        "	raw_spinlock_t raw_lock;",
        "};",
        "",
        "#define HASHTAB_MAP_LOCK_COUNT 8",
        "#define HASHTAB_MAP_LOCK_MASK (HASHTAB_MAP_LOCK_COUNT - 1)",
        "",
        "struct bpf_htab {",
        "	struct bpf_map map;",
        "	struct bpf_mem_alloc ma;",
        "	struct bpf_mem_alloc pcpu_ma;",
        "	struct bucket *buckets;",
        "	void *elems;",
        "	union {",
        "		struct pcpu_freelist freelist;",
        "		struct bpf_lru lru;",
        "	};",
        "	struct htab_elem *__percpu *extra_elems;",
        "	/* number of elements in non-preallocated hashtable are kept",
        "	 * in either pcount or count",
        "	 */",
        "	struct percpu_counter pcount;",
        "	atomic_t count;",
        "	bool use_percpu_counter;",
        "	u32 n_buckets;	/* number of hash buckets */",
        "	u32 elem_size;	/* size of each element in bytes */",
        "	u32 hashrnd;",
        "	struct lock_class_key lockdep_key;",
        "	int __percpu *map_locked[HASHTAB_MAP_LOCK_COUNT];",
        "};",
        "",
        "/* each htab element is struct htab_elem + key + value */",
        "struct htab_elem {",
        "	union {",
        "		struct hlist_nulls_node hash_node;",
        "		struct {",
        "			void *padding;",
        "			union {",
        "				struct pcpu_freelist_node fnode;",
        "				struct htab_elem *batch_flink;",
        "			};",
        "		};",
        "	};",
        "	union {",
        "		/* pointer to per-cpu pointer */",
        "		void *ptr_to_pptr;",
        "		struct bpf_lru_node lru_node;",
        "	};",
        "	u32 hash;",
        "	char key[] __aligned(8);",
        "};",
        "",
        "static inline bool htab_is_prealloc(const struct bpf_htab *htab)",
        "{",
        "	return !(htab->map.map_flags & BPF_F_NO_PREALLOC);",
        "}",
        "",
        "static void htab_init_buckets(struct bpf_htab *htab)",
        "{",
        "	unsigned int i;",
        "",
        "	for (i = 0; i < htab->n_buckets; i++) {",
        "		INIT_HLIST_NULLS_HEAD(&htab->buckets[i].head, i);",
        "		raw_spin_lock_init(&htab->buckets[i].raw_lock);",
        "		lockdep_set_class(&htab->buckets[i].raw_lock,",
        "					  &htab->lockdep_key);",
        "		cond_resched();",
        "	}",
        "}",
        "",
        "static inline int htab_lock_bucket(const struct bpf_htab *htab,",
        "				   struct bucket *b, u32 hash,",
        "				   unsigned long *pflags)",
        "{",
        "	unsigned long flags;",
        "",
        "	hash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
        "",
        "	preempt_disable();",
        "	local_irq_save(flags);",
        "	if (unlikely(__this_cpu_inc_return(*(htab->map_locked[hash])) != 1)) {",
        "		__this_cpu_dec(*(htab->map_locked[hash]));",
        "		local_irq_restore(flags);",
        "		preempt_enable();",
        "		return -EBUSY;",
        "	}",
        "",
        "	raw_spin_lock(&b->raw_lock);",
        "	*pflags = flags;",
        "",
        "	return 0;",
        "}",
        "",
        "static inline void htab_unlock_bucket(const struct bpf_htab *htab,",
        "				      struct bucket *b, u32 hash,",
        "				      unsigned long flags)",
        "{",
        "	hash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
        "	raw_spin_unlock(&b->raw_lock);",
        "	__this_cpu_dec(*(htab->map_locked[hash]));",
        "	local_irq_restore(flags);",
        "	preempt_enable();",
        "}",
        "",
        "static bool htab_lru_map_delete_node(void *arg, struct bpf_lru_node *node);",
        "",
        "static bool htab_is_lru(const struct bpf_htab *htab)",
        "{",
        "	return htab->map.map_type == BPF_MAP_TYPE_LRU_HASH ||",
        "		htab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
        "}",
        "",
        "static bool htab_is_percpu(const struct bpf_htab *htab)",
        "{",
        "	return htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
        "		htab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
        "}",
        "",
        "static inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,",
        "				     void __percpu *pptr)",
        "{",
        "	*(void __percpu **)(l->key + key_size) = pptr;",
        "}",
        "",
        "static inline void __percpu *htab_elem_get_ptr(struct htab_elem *l, u32 key_size)",
        "{",
        "	return *(void __percpu **)(l->key + key_size);",
        "}",
        "",
        "static void *fd_htab_map_get_ptr(const struct bpf_map *map, struct htab_elem *l)",
        "{",
        "	return *(void **)(l->key + roundup(map->key_size, 8));",
        "}",
        "",
        "static struct htab_elem *get_htab_elem(struct bpf_htab *htab, int i)",
        "{",
        "	return (struct htab_elem *) (htab->elems + i * (u64)htab->elem_size);",
        "}",
        "",
        "static bool htab_has_extra_elems(struct bpf_htab *htab)",
        "{",
        "	return !htab_is_percpu(htab) && !htab_is_lru(htab);",
        "}",
        "",
        "static void htab_free_prealloced_timers_and_wq(struct bpf_htab *htab)",
        "{",
        "	u32 num_entries = htab->map.max_entries;",
        "	int i;",
        "",
        "	if (htab_has_extra_elems(htab))",
        "		num_entries += num_possible_cpus();",
        "",
        "	for (i = 0; i < num_entries; i++) {",
        "		struct htab_elem *elem;",
        "",
        "		elem = get_htab_elem(htab, i);",
        "		if (btf_record_has_field(htab->map.record, BPF_TIMER))",
        "			bpf_obj_free_timer(htab->map.record,",
        "					   elem->key + round_up(htab->map.key_size, 8));",
        "		if (btf_record_has_field(htab->map.record, BPF_WORKQUEUE))",
        "			bpf_obj_free_workqueue(htab->map.record,",
        "					       elem->key + round_up(htab->map.key_size, 8));",
        "		cond_resched();",
        "	}",
        "}",
        "",
        "static void htab_free_prealloced_fields(struct bpf_htab *htab)",
        "{",
        "	u32 num_entries = htab->map.max_entries;",
        "	int i;",
        "",
        "	if (IS_ERR_OR_NULL(htab->map.record))",
        "		return;",
        "	if (htab_has_extra_elems(htab))",
        "		num_entries += num_possible_cpus();",
        "	for (i = 0; i < num_entries; i++) {",
        "		struct htab_elem *elem;",
        "",
        "		elem = get_htab_elem(htab, i);",
        "		if (htab_is_percpu(htab)) {",
        "			void __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);",
        "			int cpu;",
        "",
        "			for_each_possible_cpu(cpu) {",
        "				bpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));",
        "				cond_resched();",
        "			}",
        "		} else {",
        "			bpf_obj_free_fields(htab->map.record, elem->key + round_up(htab->map.key_size, 8));",
        "			cond_resched();",
        "		}",
        "		cond_resched();",
        "	}",
        "}",
        "",
        "static void htab_free_elems(struct bpf_htab *htab)",
        "{",
        "	int i;",
        "",
        "	if (!htab_is_percpu(htab))",
        "		goto free_elems;",
        "",
        "	for (i = 0; i < htab->map.max_entries; i++) {",
        "		void __percpu *pptr;",
        "",
        "		pptr = htab_elem_get_ptr(get_htab_elem(htab, i),",
        "					 htab->map.key_size);",
        "		free_percpu(pptr);",
        "		cond_resched();",
        "	}",
        "free_elems:",
        "	bpf_map_area_free(htab->elems);",
        "}",
        "",
        "/* The LRU list has a lock (lru_lock). Each htab bucket has a lock",
        " * (bucket_lock). If both locks need to be acquired together, the lock",
        " * order is always lru_lock -> bucket_lock and this only happens in",
        " * bpf_lru_list.c logic. For example, certain code path of",
        " * bpf_lru_pop_free(), which is called by function prealloc_lru_pop(),",
        " * will acquire lru_lock first followed by acquiring bucket_lock.",
        " *",
        " * In hashtab.c, to avoid deadlock, lock acquisition of",
        " * bucket_lock followed by lru_lock is not allowed. In such cases,",
        " * bucket_lock needs to be released first before acquiring lru_lock.",
        " */",
        "static struct htab_elem *prealloc_lru_pop(struct bpf_htab *htab, void *key,",
        "					  u32 hash)",
        "{",
        "	struct bpf_lru_node *node = bpf_lru_pop_free(&htab->lru, hash);",
        "	struct htab_elem *l;",
        "",
        "	if (node) {",
        "		bpf_map_inc_elem_count(&htab->map);",
        "		l = container_of(node, struct htab_elem, lru_node);",
        "		memcpy(l->key, key, htab->map.key_size);",
        "		return l;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "static int prealloc_init(struct bpf_htab *htab)",
        "{",
        "	u32 num_entries = htab->map.max_entries;",
        "	int err = -ENOMEM, i;",
        "",
        "	if (htab_has_extra_elems(htab))",
        "		num_entries += num_possible_cpus();",
        "",
        "	htab->elems = bpf_map_area_alloc((u64)htab->elem_size * num_entries,",
        "					 htab->map.numa_node);",
        "	if (!htab->elems)",
        "		return -ENOMEM;",
        "",
        "	if (!htab_is_percpu(htab))",
        "		goto skip_percpu_elems;",
        "",
        "	for (i = 0; i < num_entries; i++) {",
        "		u32 size = round_up(htab->map.value_size, 8);",
        "		void __percpu *pptr;",
        "",
        "		pptr = bpf_map_alloc_percpu(&htab->map, size, 8,",
        "					    GFP_USER | __GFP_NOWARN);",
        "		if (!pptr)",
        "			goto free_elems;",
        "		htab_elem_set_ptr(get_htab_elem(htab, i), htab->map.key_size,",
        "				  pptr);",
        "		cond_resched();",
        "	}",
        "",
        "skip_percpu_elems:",
        "	if (htab_is_lru(htab))",
        "		err = bpf_lru_init(&htab->lru,",
        "				   htab->map.map_flags & BPF_F_NO_COMMON_LRU,",
        "				   offsetof(struct htab_elem, hash) -",
        "				   offsetof(struct htab_elem, lru_node),",
        "				   htab_lru_map_delete_node,",
        "				   htab);",
        "	else",
        "		err = pcpu_freelist_init(&htab->freelist);",
        "",
        "	if (err)",
        "		goto free_elems;",
        "",
        "	if (htab_is_lru(htab))",
        "		bpf_lru_populate(&htab->lru, htab->elems,",
        "				 offsetof(struct htab_elem, lru_node),",
        "				 htab->elem_size, num_entries);",
        "	else",
        "		pcpu_freelist_populate(&htab->freelist,",
        "				       htab->elems + offsetof(struct htab_elem, fnode),",
        "				       htab->elem_size, num_entries);",
        "",
        "	return 0;",
        "",
        "free_elems:",
        "	htab_free_elems(htab);",
        "	return err;",
        "}",
        "",
        "static void prealloc_destroy(struct bpf_htab *htab)",
        "{",
        "	htab_free_elems(htab);",
        "",
        "	if (htab_is_lru(htab))",
        "		bpf_lru_destroy(&htab->lru);",
        "	else",
        "		pcpu_freelist_destroy(&htab->freelist);",
        "}",
        "",
        "static int alloc_extra_elems(struct bpf_htab *htab)",
        "{",
        "	struct htab_elem *__percpu *pptr, *l_new;",
        "	struct pcpu_freelist_node *l;",
        "	int cpu;",
        "",
        "	pptr = bpf_map_alloc_percpu(&htab->map, sizeof(struct htab_elem *), 8,",
        "				    GFP_USER | __GFP_NOWARN);",
        "	if (!pptr)",
        "		return -ENOMEM;",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		l = pcpu_freelist_pop(&htab->freelist);",
        "		/* pop will succeed, since prealloc_init()",
        "		 * preallocated extra num_possible_cpus elements",
        "		 */",
        "		l_new = container_of(l, struct htab_elem, fnode);",
        "		*per_cpu_ptr(pptr, cpu) = l_new;",
        "	}",
        "	htab->extra_elems = pptr;",
        "	return 0;",
        "}",
        "",
        "/* Called from syscall */",
        "static int htab_map_alloc_check(union bpf_attr *attr)",
        "{",
        "	bool percpu = (attr->map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
        "		       attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);",
        "	bool lru = (attr->map_type == BPF_MAP_TYPE_LRU_HASH ||",
        "		    attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);",
        "	/* percpu_lru means each cpu has its own LRU list.",
        "	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where",
        "	 * the map's value itself is percpu.  percpu_lru has",
        "	 * nothing to do with the map's value.",
        "	 */",
        "	bool percpu_lru = (attr->map_flags & BPF_F_NO_COMMON_LRU);",
        "	bool prealloc = !(attr->map_flags & BPF_F_NO_PREALLOC);",
        "	bool zero_seed = (attr->map_flags & BPF_F_ZERO_SEED);",
        "	int numa_node = bpf_map_attr_numa_node(attr);",
        "",
        "	BUILD_BUG_ON(offsetof(struct htab_elem, fnode.next) !=",
        "		     offsetof(struct htab_elem, hash_node.pprev));",
        "",
        "	if (zero_seed && !capable(CAP_SYS_ADMIN))",
        "		/* Guard against local DoS, and discourage production use. */",
        "		return -EPERM;",
        "",
        "	if (attr->map_flags & ~HTAB_CREATE_FLAG_MASK ||",
        "	    !bpf_map_flags_access_ok(attr->map_flags))",
        "		return -EINVAL;",
        "",
        "	if (!lru && percpu_lru)",
        "		return -EINVAL;",
        "",
        "	if (lru && !prealloc)",
        "		return -ENOTSUPP;",
        "",
        "	if (numa_node != NUMA_NO_NODE && (percpu || percpu_lru))",
        "		return -EINVAL;",
        "",
        "	/* check sanity of attributes.",
        "	 * value_size == 0 may be allowed in the future to use map as a set",
        "	 */",
        "	if (attr->max_entries == 0 || attr->key_size == 0 ||",
        "	    attr->value_size == 0)",
        "		return -EINVAL;",
        "",
        "	if ((u64)attr->key_size + attr->value_size >= KMALLOC_MAX_SIZE -",
        "	   sizeof(struct htab_elem))",
        "		/* if key_size + value_size is bigger, the user space won't be",
        "		 * able to access the elements via bpf syscall. This check",
        "		 * also makes sure that the elem_size doesn't overflow and it's",
        "		 * kmalloc-able later in htab_map_update_elem()",
        "		 */",
        "		return -E2BIG;",
        "	/* percpu map value size is bound by PCPU_MIN_UNIT_SIZE */",
        "	if (percpu && round_up(attr->value_size, 8) > PCPU_MIN_UNIT_SIZE)",
        "		return -E2BIG;",
        "",
        "	return 0;",
        "}",
        "",
        "static struct bpf_map *htab_map_alloc(union bpf_attr *attr)",
        "{",
        "	bool percpu = (attr->map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
        "		       attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);",
        "	bool lru = (attr->map_type == BPF_MAP_TYPE_LRU_HASH ||",
        "		    attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);",
        "	/* percpu_lru means each cpu has its own LRU list.",
        "	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where",
        "	 * the map's value itself is percpu.  percpu_lru has",
        "	 * nothing to do with the map's value.",
        "	 */",
        "	bool percpu_lru = (attr->map_flags & BPF_F_NO_COMMON_LRU);",
        "	bool prealloc = !(attr->map_flags & BPF_F_NO_PREALLOC);",
        "	struct bpf_htab *htab;",
        "	int err, i;",
        "",
        "	htab = bpf_map_area_alloc(sizeof(*htab), NUMA_NO_NODE);",
        "	if (!htab)",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	lockdep_register_key(&htab->lockdep_key);",
        "",
        "	bpf_map_init_from_attr(&htab->map, attr);",
        "",
        "	if (percpu_lru) {",
        "		/* ensure each CPU's lru list has >=1 elements.",
        "		 * since we are at it, make each lru list has the same",
        "		 * number of elements.",
        "		 */",
        "		htab->map.max_entries = roundup(attr->max_entries,",
        "						num_possible_cpus());",
        "		if (htab->map.max_entries < attr->max_entries)",
        "			htab->map.max_entries = rounddown(attr->max_entries,",
        "							  num_possible_cpus());",
        "	}",
        "",
        "	/* hash table size must be power of 2; roundup_pow_of_two() can overflow",
        "	 * into UB on 32-bit arches, so check that first",
        "	 */",
        "	err = -E2BIG;",
        "	if (htab->map.max_entries > 1UL << 31)",
        "		goto free_htab;",
        "",
        "	htab->n_buckets = roundup_pow_of_two(htab->map.max_entries);",
        "",
        "	htab->elem_size = sizeof(struct htab_elem) +",
        "			  round_up(htab->map.key_size, 8);",
        "	if (percpu)",
        "		htab->elem_size += sizeof(void *);",
        "	else",
        "		htab->elem_size += round_up(htab->map.value_size, 8);",
        "",
        "	/* check for u32 overflow */",
        "	if (htab->n_buckets > U32_MAX / sizeof(struct bucket))",
        "		goto free_htab;",
        "",
        "	err = bpf_map_init_elem_count(&htab->map);",
        "	if (err)",
        "		goto free_htab;",
        "",
        "	err = -ENOMEM;",
        "	htab->buckets = bpf_map_area_alloc(htab->n_buckets *",
        "					   sizeof(struct bucket),",
        "					   htab->map.numa_node);",
        "	if (!htab->buckets)",
        "		goto free_elem_count;",
        "",
        "	for (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++) {",
        "		htab->map_locked[i] = bpf_map_alloc_percpu(&htab->map,",
        "							   sizeof(int),",
        "							   sizeof(int),",
        "							   GFP_USER);",
        "		if (!htab->map_locked[i])",
        "			goto free_map_locked;",
        "	}",
        "",
        "	if (htab->map.map_flags & BPF_F_ZERO_SEED)",
        "		htab->hashrnd = 0;",
        "	else",
        "		htab->hashrnd = get_random_u32();",
        "",
        "	htab_init_buckets(htab);",
        "",
        "/* compute_batch_value() computes batch value as num_online_cpus() * 2",
        " * and __percpu_counter_compare() needs",
        " * htab->max_entries - cur_number_of_elems to be more than batch * num_online_cpus()",
        " * for percpu_counter to be faster than atomic_t. In practice the average bpf",
        " * hash map size is 10k, which means that a system with 64 cpus will fill",
        " * hashmap to 20% of 10k before percpu_counter becomes ineffective. Therefore",
        " * define our own batch count as 32 then 10k hash map can be filled up to 80%:",
        " * 10k - 8k > 32 _batch_ * 64 _cpus_",
        " * and __percpu_counter_compare() will still be fast. At that point hash map",
        " * collisions will dominate its performance anyway. Assume that hash map filled",
        " * to 50+% isn't going to be O(1) and use the following formula to choose",
        " * between percpu_counter and atomic_t.",
        " */",
        "#define PERCPU_COUNTER_BATCH 32",
        "	if (attr->max_entries / 2 > num_online_cpus() * PERCPU_COUNTER_BATCH)",
        "		htab->use_percpu_counter = true;",
        "",
        "	if (htab->use_percpu_counter) {",
        "		err = percpu_counter_init(&htab->pcount, 0, GFP_KERNEL);",
        "		if (err)",
        "			goto free_map_locked;",
        "	}",
        "",
        "	if (prealloc) {",
        "		err = prealloc_init(htab);",
        "		if (err)",
        "			goto free_map_locked;",
        "",
        "		if (!percpu && !lru) {",
        "			/* lru itself can remove the least used element, so",
        "			 * there is no need for an extra elem during map_update.",
        "			 */",
        "			err = alloc_extra_elems(htab);",
        "			if (err)",
        "				goto free_prealloc;",
        "		}",
        "	} else {",
        "		err = bpf_mem_alloc_init(&htab->ma, htab->elem_size, false);",
        "		if (err)",
        "			goto free_map_locked;",
        "		if (percpu) {",
        "			err = bpf_mem_alloc_init(&htab->pcpu_ma,",
        "						 round_up(htab->map.value_size, 8), true);",
        "			if (err)",
        "				goto free_map_locked;",
        "		}",
        "	}",
        "",
        "	return &htab->map;",
        "",
        "free_prealloc:",
        "	prealloc_destroy(htab);",
        "free_map_locked:",
        "	if (htab->use_percpu_counter)",
        "		percpu_counter_destroy(&htab->pcount);",
        "	for (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++)",
        "		free_percpu(htab->map_locked[i]);",
        "	bpf_map_area_free(htab->buckets);",
        "	bpf_mem_alloc_destroy(&htab->pcpu_ma);",
        "	bpf_mem_alloc_destroy(&htab->ma);",
        "free_elem_count:",
        "	bpf_map_free_elem_count(&htab->map);",
        "free_htab:",
        "	lockdep_unregister_key(&htab->lockdep_key);",
        "	bpf_map_area_free(htab);",
        "	return ERR_PTR(err);",
        "}",
        "",
        "static inline u32 htab_map_hash(const void *key, u32 key_len, u32 hashrnd)",
        "{",
        "	if (likely(key_len % 4 == 0))",
        "		return jhash2(key, key_len / 4, hashrnd);",
        "	return jhash(key, key_len, hashrnd);",
        "}",
        "",
        "static inline struct bucket *__select_bucket(struct bpf_htab *htab, u32 hash)",
        "{",
        "	return &htab->buckets[hash & (htab->n_buckets - 1)];",
        "}",
        "",
        "static inline struct hlist_nulls_head *select_bucket(struct bpf_htab *htab, u32 hash)",
        "{",
        "	return &__select_bucket(htab, hash)->head;",
        "}",
        "",
        "/* this lookup function can only be called with bucket lock taken */",
        "static struct htab_elem *lookup_elem_raw(struct hlist_nulls_head *head, u32 hash,",
        "					 void *key, u32 key_size)",
        "{",
        "	struct hlist_nulls_node *n;",
        "	struct htab_elem *l;",
        "",
        "	hlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
        "		if (l->hash == hash && !memcmp(&l->key, key, key_size))",
        "			return l;",
        "",
        "	return NULL;",
        "}",
        "",
        "/* can be called without bucket lock. it will repeat the loop in",
        " * the unlikely event when elements moved from one bucket into another",
        " * while link list is being walked",
        " */",
        "static struct htab_elem *lookup_nulls_elem_raw(struct hlist_nulls_head *head,",
        "					       u32 hash, void *key,",
        "					       u32 key_size, u32 n_buckets)",
        "{",
        "	struct hlist_nulls_node *n;",
        "	struct htab_elem *l;",
        "",
        "again:",
        "	hlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
        "		if (l->hash == hash && !memcmp(&l->key, key, key_size))",
        "			return l;",
        "",
        "	if (unlikely(get_nulls_value(n) != (hash & (n_buckets - 1))))",
        "		goto again;",
        "",
        "	return NULL;",
        "}",
        "",
        "/* Called from syscall or from eBPF program directly, so",
        " * arguments have to match bpf_map_lookup_elem() exactly.",
        " * The return value is adjusted by BPF instructions",
        " * in htab_map_gen_lookup().",
        " */",
        "static void *__htab_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	struct htab_elem *l;",
        "	u32 hash, key_size;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	head = select_bucket(htab, hash);",
        "",
        "	l = lookup_nulls_elem_raw(head, hash, key, key_size, htab->n_buckets);",
        "",
        "	return l;",
        "}",
        "",
        "static void *htab_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct htab_elem *l = __htab_map_lookup_elem(map, key);",
        "",
        "	if (l)",
        "		return l->key + round_up(map->key_size, 8);",
        "",
        "	return NULL;",
        "}",
        "",
        "/* inline bpf_map_lookup_elem() call.",
        " * Instead of:",
        " * bpf_prog",
        " *   bpf_map_lookup_elem",
        " *     map->ops->map_lookup_elem",
        " *       htab_map_lookup_elem",
        " *         __htab_map_lookup_elem",
        " * do:",
        " * bpf_prog",
        " *   __htab_map_lookup_elem",
        " */",
        "static int htab_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)",
        "{",
        "	struct bpf_insn *insn = insn_buf;",
        "	const int ret = BPF_REG_0;",
        "",
        "	BUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
        "		     (void *(*)(struct bpf_map *map, void *key))NULL));",
        "	*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
        "	*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 1);",
        "	*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,",
        "				offsetof(struct htab_elem, key) +",
        "				round_up(map->key_size, 8));",
        "	return insn - insn_buf;",
        "}",
        "",
        "static __always_inline void *__htab_lru_map_lookup_elem(struct bpf_map *map,",
        "							void *key, const bool mark)",
        "{",
        "	struct htab_elem *l = __htab_map_lookup_elem(map, key);",
        "",
        "	if (l) {",
        "		if (mark)",
        "			bpf_lru_node_set_ref(&l->lru_node);",
        "		return l->key + round_up(map->key_size, 8);",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "static void *htab_lru_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	return __htab_lru_map_lookup_elem(map, key, true);",
        "}",
        "",
        "static void *htab_lru_map_lookup_elem_sys(struct bpf_map *map, void *key)",
        "{",
        "	return __htab_lru_map_lookup_elem(map, key, false);",
        "}",
        "",
        "static int htab_lru_map_gen_lookup(struct bpf_map *map,",
        "				   struct bpf_insn *insn_buf)",
        "{",
        "	struct bpf_insn *insn = insn_buf;",
        "	const int ret = BPF_REG_0;",
        "	const int ref_reg = BPF_REG_1;",
        "",
        "	BUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
        "		     (void *(*)(struct bpf_map *map, void *key))NULL));",
        "	*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
        "	*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 4);",
        "	*insn++ = BPF_LDX_MEM(BPF_B, ref_reg, ret,",
        "			      offsetof(struct htab_elem, lru_node) +",
        "			      offsetof(struct bpf_lru_node, ref));",
        "	*insn++ = BPF_JMP_IMM(BPF_JNE, ref_reg, 0, 1);",
        "	*insn++ = BPF_ST_MEM(BPF_B, ret,",
        "			     offsetof(struct htab_elem, lru_node) +",
        "			     offsetof(struct bpf_lru_node, ref),",
        "			     1);",
        "	*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,",
        "				offsetof(struct htab_elem, key) +",
        "				round_up(map->key_size, 8));",
        "	return insn - insn_buf;",
        "}",
        "",
        "static void check_and_free_fields(struct bpf_htab *htab,",
        "				  struct htab_elem *elem)",
        "{",
        "	if (htab_is_percpu(htab)) {",
        "		void __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);",
        "		int cpu;",
        "",
        "		for_each_possible_cpu(cpu)",
        "			bpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));",
        "	} else {",
        "		void *map_value = elem->key + round_up(htab->map.key_size, 8);",
        "",
        "		bpf_obj_free_fields(htab->map.record, map_value);",
        "	}",
        "}",
        "",
        "/* It is called from the bpf_lru_list when the LRU needs to delete",
        " * older elements from the htab.",
        " */",
        "static bool htab_lru_map_delete_node(void *arg, struct bpf_lru_node *node)",
        "{",
        "	struct bpf_htab *htab = arg;",
        "	struct htab_elem *l = NULL, *tgt_l;",
        "	struct hlist_nulls_head *head;",
        "	struct hlist_nulls_node *n;",
        "	unsigned long flags;",
        "	struct bucket *b;",
        "	int ret;",
        "",
        "	tgt_l = container_of(node, struct htab_elem, lru_node);",
        "	b = __select_bucket(htab, tgt_l->hash);",
        "	head = &b->head;",
        "",
        "	ret = htab_lock_bucket(htab, b, tgt_l->hash, &flags);",
        "	if (ret)",
        "		return false;",
        "",
        "	hlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
        "		if (l == tgt_l) {",
        "			hlist_nulls_del_rcu(&l->hash_node);",
        "			check_and_free_fields(htab, l);",
        "			bpf_map_dec_elem_count(&htab->map);",
        "			break;",
        "		}",
        "",
        "	htab_unlock_bucket(htab, b, tgt_l->hash, flags);",
        "",
        "	return l == tgt_l;",
        "}",
        "",
        "/* Called from syscall */",
        "static int htab_map_get_next_key(struct bpf_map *map, void *key, void *next_key)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	struct htab_elem *l, *next_l;",
        "	u32 hash, key_size;",
        "	int i = 0;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	if (!key)",
        "		goto find_first_elem;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	head = select_bucket(htab, hash);",
        "",
        "	/* lookup the key */",
        "	l = lookup_nulls_elem_raw(head, hash, key, key_size, htab->n_buckets);",
        "",
        "	if (!l)",
        "		goto find_first_elem;",
        "",
        "	/* key was found, get next key in the same bucket */",
        "	next_l = hlist_nulls_entry_safe(rcu_dereference_raw(hlist_nulls_next_rcu(&l->hash_node)),",
        "				  struct htab_elem, hash_node);",
        "",
        "	if (next_l) {",
        "		/* if next elem in this hash list is non-zero, just return it */",
        "		memcpy(next_key, next_l->key, key_size);",
        "		return 0;",
        "	}",
        "",
        "	/* no more elements in this hash list, go to the next bucket */",
        "	i = hash & (htab->n_buckets - 1);",
        "	i++;",
        "",
        "find_first_elem:",
        "	/* iterate over buckets */",
        "	for (; i < htab->n_buckets; i++) {",
        "		head = select_bucket(htab, i);",
        "",
        "		/* pick first element in the bucket */",
        "		next_l = hlist_nulls_entry_safe(rcu_dereference_raw(hlist_nulls_first_rcu(head)),",
        "					  struct htab_elem, hash_node);",
        "		if (next_l) {",
        "			/* if it's not empty, just return it */",
        "			memcpy(next_key, next_l->key, key_size);",
        "			return 0;",
        "		}",
        "	}",
        "",
        "	/* iterated over all buckets and all elements */",
        "	return -ENOENT;",
        "}",
        "",
        "static void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)",
        "{",
        "	check_and_free_fields(htab, l);",
        "",
        "	migrate_disable();",
        "	if (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)",
        "		bpf_mem_cache_free(&htab->pcpu_ma, l->ptr_to_pptr);",
        "	bpf_mem_cache_free(&htab->ma, l);",
        "	migrate_enable();",
        "}",
        "",
        "static void htab_put_fd_value(struct bpf_htab *htab, struct htab_elem *l)",
        "{",
        "	struct bpf_map *map = &htab->map;",
        "	void *ptr;",
        "",
        "	if (map->ops->map_fd_put_ptr) {",
        "		ptr = fd_htab_map_get_ptr(map, l);",
        "		map->ops->map_fd_put_ptr(map, ptr, true);",
        "	}",
        "}",
        "",
        "static bool is_map_full(struct bpf_htab *htab)",
        "{",
        "	if (htab->use_percpu_counter)",
        "		return __percpu_counter_compare(&htab->pcount, htab->map.max_entries,",
        "						PERCPU_COUNTER_BATCH) >= 0;",
        "	return atomic_read(&htab->count) >= htab->map.max_entries;",
        "}",
        "",
        "static void inc_elem_count(struct bpf_htab *htab)",
        "{",
        "	bpf_map_inc_elem_count(&htab->map);",
        "",
        "	if (htab->use_percpu_counter)",
        "		percpu_counter_add_batch(&htab->pcount, 1, PERCPU_COUNTER_BATCH);",
        "	else",
        "		atomic_inc(&htab->count);",
        "}",
        "",
        "static void dec_elem_count(struct bpf_htab *htab)",
        "{",
        "	bpf_map_dec_elem_count(&htab->map);",
        "",
        "	if (htab->use_percpu_counter)",
        "		percpu_counter_add_batch(&htab->pcount, -1, PERCPU_COUNTER_BATCH);",
        "	else",
        "		atomic_dec(&htab->count);",
        "}",
        "",
        "",
        "static void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)",
        "{",
        "	htab_put_fd_value(htab, l);",
        "",
        "	if (htab_is_prealloc(htab)) {",
        "		bpf_map_dec_elem_count(&htab->map);",
        "		check_and_free_fields(htab, l);",
        "		pcpu_freelist_push(&htab->freelist, &l->fnode);",
        "	} else {",
        "		dec_elem_count(htab);",
        "		htab_elem_free(htab, l);",
        "	}",
        "}",
        "",
        "static void pcpu_copy_value(struct bpf_htab *htab, void __percpu *pptr,",
        "			    void *value, bool onallcpus)",
        "{",
        "	if (!onallcpus) {",
        "		/* copy true value_size bytes */",
        "		copy_map_value(&htab->map, this_cpu_ptr(pptr), value);",
        "	} else {",
        "		u32 size = round_up(htab->map.value_size, 8);",
        "		int off = 0, cpu;",
        "",
        "		for_each_possible_cpu(cpu) {",
        "			copy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value + off);",
        "			off += size;",
        "		}",
        "	}",
        "}",
        "",
        "static void pcpu_init_value(struct bpf_htab *htab, void __percpu *pptr,",
        "			    void *value, bool onallcpus)",
        "{",
        "	/* When not setting the initial value on all cpus, zero-fill element",
        "	 * values for other cpus. Otherwise, bpf program has no way to ensure",
        "	 * known initial values for cpus other than current one",
        "	 * (onallcpus=false always when coming from bpf prog).",
        "	 */",
        "	if (!onallcpus) {",
        "		int current_cpu = raw_smp_processor_id();",
        "		int cpu;",
        "",
        "		for_each_possible_cpu(cpu) {",
        "			if (cpu == current_cpu)",
        "				copy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value);",
        "			else /* Since elem is preallocated, we cannot touch special fields */",
        "				zero_map_value(&htab->map, per_cpu_ptr(pptr, cpu));",
        "		}",
        "	} else {",
        "		pcpu_copy_value(htab, pptr, value, onallcpus);",
        "	}",
        "}",
        "",
        "static bool fd_htab_map_needs_adjust(const struct bpf_htab *htab)",
        "{",
        "	return htab->map.map_type == BPF_MAP_TYPE_HASH_OF_MAPS &&",
        "	       BITS_PER_LONG == 64;",
        "}",
        "",
        "static struct htab_elem *alloc_htab_elem(struct bpf_htab *htab, void *key,",
        "					 void *value, u32 key_size, u32 hash,",
        "					 bool percpu, bool onallcpus,",
        "					 struct htab_elem *old_elem)",
        "{",
        "	u32 size = htab->map.value_size;",
        "	bool prealloc = htab_is_prealloc(htab);",
        "	struct htab_elem *l_new, **pl_new;",
        "	void __percpu *pptr;",
        "",
        "	if (prealloc) {",
        "		if (old_elem) {",
        "			/* if we're updating the existing element,",
        "			 * use per-cpu extra elems to avoid freelist_pop/push",
        "			 */",
        "			pl_new = this_cpu_ptr(htab->extra_elems);",
        "			l_new = *pl_new;",
        "			*pl_new = old_elem;",
        "		} else {",
        "			struct pcpu_freelist_node *l;",
        "",
        "			l = __pcpu_freelist_pop(&htab->freelist);",
        "			if (!l)",
        "				return ERR_PTR(-E2BIG);",
        "			l_new = container_of(l, struct htab_elem, fnode);",
        "			bpf_map_inc_elem_count(&htab->map);",
        "		}",
        "	} else {",
        "		if (is_map_full(htab))",
        "			if (!old_elem)",
        "				/* when map is full and update() is replacing",
        "				 * old element, it's ok to allocate, since",
        "				 * old element will be freed immediately.",
        "				 * Otherwise return an error",
        "				 */",
        "				return ERR_PTR(-E2BIG);",
        "		inc_elem_count(htab);",
        "		l_new = bpf_mem_cache_alloc(&htab->ma);",
        "		if (!l_new) {",
        "			l_new = ERR_PTR(-ENOMEM);",
        "			goto dec_count;",
        "		}",
        "	}",
        "",
        "	memcpy(l_new->key, key, key_size);",
        "	if (percpu) {",
        "		if (prealloc) {",
        "			pptr = htab_elem_get_ptr(l_new, key_size);",
        "		} else {",
        "			/* alloc_percpu zero-fills */",
        "			void *ptr = bpf_mem_cache_alloc(&htab->pcpu_ma);",
        "",
        "			if (!ptr) {",
        "				bpf_mem_cache_free(&htab->ma, l_new);",
        "				l_new = ERR_PTR(-ENOMEM);",
        "				goto dec_count;",
        "			}",
        "			l_new->ptr_to_pptr = ptr;",
        "			pptr = *(void __percpu **)ptr;",
        "		}",
        "",
        "		pcpu_init_value(htab, pptr, value, onallcpus);",
        "",
        "		if (!prealloc)",
        "			htab_elem_set_ptr(l_new, key_size, pptr);",
        "	} else if (fd_htab_map_needs_adjust(htab)) {",
        "		size = round_up(size, 8);",
        "		memcpy(l_new->key + round_up(key_size, 8), value, size);",
        "	} else {",
        "		copy_map_value(&htab->map,",
        "			       l_new->key + round_up(key_size, 8),",
        "			       value);",
        "	}",
        "",
        "	l_new->hash = hash;",
        "	return l_new;",
        "dec_count:",
        "	dec_elem_count(htab);",
        "	return l_new;",
        "}",
        "",
        "static int check_flags(struct bpf_htab *htab, struct htab_elem *l_old,",
        "		       u64 map_flags)",
        "{",
        "	if (l_old && (map_flags & ~BPF_F_LOCK) == BPF_NOEXIST)",
        "		/* elem already exists */",
        "		return -EEXIST;",
        "",
        "	if (!l_old && (map_flags & ~BPF_F_LOCK) == BPF_EXIST)",
        "		/* elem doesn't exist, cannot update it */",
        "		return -ENOENT;",
        "",
        "	return 0;",
        "}",
        "",
        "/* Called from syscall or from eBPF program */",
        "static long htab_map_update_elem(struct bpf_map *map, void *key, void *value,",
        "				 u64 map_flags)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct htab_elem *l_new = NULL, *l_old;",
        "	struct hlist_nulls_head *head;",
        "	unsigned long flags;",
        "	void *old_map_ptr;",
        "	struct bucket *b;",
        "	u32 key_size, hash;",
        "	int ret;",
        "",
        "	if (unlikely((map_flags & ~BPF_F_LOCK) > BPF_EXIST))",
        "		/* unknown flags */",
        "		return -EINVAL;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	if (unlikely(map_flags & BPF_F_LOCK)) {",
        "		if (unlikely(!btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
        "			return -EINVAL;",
        "		/* find an element without taking the bucket lock */",
        "		l_old = lookup_nulls_elem_raw(head, hash, key, key_size,",
        "					      htab->n_buckets);",
        "		ret = check_flags(htab, l_old, map_flags);",
        "		if (ret)",
        "			return ret;",
        "		if (l_old) {",
        "			/* grab the element lock and update value in place */",
        "			copy_map_value_locked(map,",
        "					      l_old->key + round_up(key_size, 8),",
        "					      value, false);",
        "			return 0;",
        "		}",
        "		/* fall through, grab the bucket lock and lookup again.",
        "		 * 99.9% chance that the element won't be found,",
        "		 * but second lookup under lock has to be done.",
        "		 */",
        "	}",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		return ret;",
        "",
        "	l_old = lookup_elem_raw(head, hash, key, key_size);",
        "",
        "	ret = check_flags(htab, l_old, map_flags);",
        "	if (ret)",
        "		goto err;",
        "",
        "	if (unlikely(l_old && (map_flags & BPF_F_LOCK))) {",
        "		/* first lookup without the bucket lock didn't find the element,",
        "		 * but second lookup with the bucket lock found it.",
        "		 * This case is highly unlikely, but has to be dealt with:",
        "		 * grab the element lock in addition to the bucket lock",
        "		 * and update element in place",
        "		 */",
        "		copy_map_value_locked(map,",
        "				      l_old->key + round_up(key_size, 8),",
        "				      value, false);",
        "		ret = 0;",
        "		goto err;",
        "	}",
        "",
        "	l_new = alloc_htab_elem(htab, key, value, key_size, hash, false, false,",
        "				l_old);",
        "	if (IS_ERR(l_new)) {",
        "		/* all pre-allocated elements are in use or memory exhausted */",
        "		ret = PTR_ERR(l_new);",
        "		goto err;",
        "	}",
        "",
        "	/* add new element to the head of the list, so that",
        "	 * concurrent search will find it before old elem",
        "	 */",
        "	hlist_nulls_add_head_rcu(&l_new->hash_node, head);",
        "	if (l_old) {",
        "		hlist_nulls_del_rcu(&l_old->hash_node);",
        "",
        "		/* l_old has already been stashed in htab->extra_elems, free",
        "		 * its special fields before it is available for reuse. Also",
        "		 * save the old map pointer in htab of maps before unlock",
        "		 * and release it after unlock.",
        "		 */",
        "		old_map_ptr = NULL;",
        "		if (htab_is_prealloc(htab)) {",
        "			if (map->ops->map_fd_put_ptr)",
        "				old_map_ptr = fd_htab_map_get_ptr(map, l_old);",
        "			check_and_free_fields(htab, l_old);",
        "		}",
        "	}",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "	if (l_old) {",
        "		if (old_map_ptr)",
        "			map->ops->map_fd_put_ptr(map, old_map_ptr, true);",
        "		if (!htab_is_prealloc(htab))",
        "			free_htab_elem(htab, l_old);",
        "	}",
        "	return 0;",
        "err:",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "	return ret;",
        "}",
        "",
        "static void htab_lru_push_free(struct bpf_htab *htab, struct htab_elem *elem)",
        "{",
        "	check_and_free_fields(htab, elem);",
        "	bpf_map_dec_elem_count(&htab->map);",
        "	bpf_lru_push_free(&htab->lru, &elem->lru_node);",
        "}",
        "",
        "static long htab_lru_map_update_elem(struct bpf_map *map, void *key, void *value,",
        "				     u64 map_flags)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct htab_elem *l_new, *l_old = NULL;",
        "	struct hlist_nulls_head *head;",
        "	unsigned long flags;",
        "	struct bucket *b;",
        "	u32 key_size, hash;",
        "	int ret;",
        "",
        "	if (unlikely(map_flags > BPF_EXIST))",
        "		/* unknown flags */",
        "		return -EINVAL;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	/* For LRU, we need to alloc before taking bucket's",
        "	 * spinlock because getting free nodes from LRU may need",
        "	 * to remove older elements from htab and this removal",
        "	 * operation will need a bucket lock.",
        "	 */",
        "	l_new = prealloc_lru_pop(htab, key, hash);",
        "	if (!l_new)",
        "		return -ENOMEM;",
        "	copy_map_value(&htab->map,",
        "		       l_new->key + round_up(map->key_size, 8), value);",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		goto err_lock_bucket;",
        "",
        "	l_old = lookup_elem_raw(head, hash, key, key_size);",
        "",
        "	ret = check_flags(htab, l_old, map_flags);",
        "	if (ret)",
        "		goto err;",
        "",
        "	/* add new element to the head of the list, so that",
        "	 * concurrent search will find it before old elem",
        "	 */",
        "	hlist_nulls_add_head_rcu(&l_new->hash_node, head);",
        "	if (l_old) {",
        "		bpf_lru_node_set_ref(&l_new->lru_node);",
        "		hlist_nulls_del_rcu(&l_old->hash_node);",
        "	}",
        "	ret = 0;",
        "",
        "err:",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "",
        "err_lock_bucket:",
        "	if (ret)",
        "		htab_lru_push_free(htab, l_new);",
        "	else if (l_old)",
        "		htab_lru_push_free(htab, l_old);",
        "",
        "	return ret;",
        "}",
        "",
        "static long __htab_percpu_map_update_elem(struct bpf_map *map, void *key,",
        "					  void *value, u64 map_flags,",
        "					  bool onallcpus)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct htab_elem *l_new = NULL, *l_old;",
        "	struct hlist_nulls_head *head;",
        "	unsigned long flags;",
        "	struct bucket *b;",
        "	u32 key_size, hash;",
        "	int ret;",
        "",
        "	if (unlikely(map_flags > BPF_EXIST))",
        "		/* unknown flags */",
        "		return -EINVAL;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		return ret;",
        "",
        "	l_old = lookup_elem_raw(head, hash, key, key_size);",
        "",
        "	ret = check_flags(htab, l_old, map_flags);",
        "	if (ret)",
        "		goto err;",
        "",
        "	if (l_old) {",
        "		/* per-cpu hash map can update value in-place */",
        "		pcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),",
        "				value, onallcpus);",
        "	} else {",
        "		l_new = alloc_htab_elem(htab, key, value, key_size,",
        "					hash, true, onallcpus, NULL);",
        "		if (IS_ERR(l_new)) {",
        "			ret = PTR_ERR(l_new);",
        "			goto err;",
        "		}",
        "		hlist_nulls_add_head_rcu(&l_new->hash_node, head);",
        "	}",
        "	ret = 0;",
        "err:",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "	return ret;",
        "}",
        "",
        "static long __htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,",
        "					      void *value, u64 map_flags,",
        "					      bool onallcpus)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct htab_elem *l_new = NULL, *l_old;",
        "	struct hlist_nulls_head *head;",
        "	unsigned long flags;",
        "	struct bucket *b;",
        "	u32 key_size, hash;",
        "	int ret;",
        "",
        "	if (unlikely(map_flags > BPF_EXIST))",
        "		/* unknown flags */",
        "		return -EINVAL;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	/* For LRU, we need to alloc before taking bucket's",
        "	 * spinlock because LRU's elem alloc may need",
        "	 * to remove older elem from htab and this removal",
        "	 * operation will need a bucket lock.",
        "	 */",
        "	if (map_flags != BPF_EXIST) {",
        "		l_new = prealloc_lru_pop(htab, key, hash);",
        "		if (!l_new)",
        "			return -ENOMEM;",
        "	}",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		goto err_lock_bucket;",
        "",
        "	l_old = lookup_elem_raw(head, hash, key, key_size);",
        "",
        "	ret = check_flags(htab, l_old, map_flags);",
        "	if (ret)",
        "		goto err;",
        "",
        "	if (l_old) {",
        "		bpf_lru_node_set_ref(&l_old->lru_node);",
        "",
        "		/* per-cpu hash map can update value in-place */",
        "		pcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),",
        "				value, onallcpus);",
        "	} else {",
        "		pcpu_init_value(htab, htab_elem_get_ptr(l_new, key_size),",
        "				value, onallcpus);",
        "		hlist_nulls_add_head_rcu(&l_new->hash_node, head);",
        "		l_new = NULL;",
        "	}",
        "	ret = 0;",
        "err:",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "err_lock_bucket:",
        "	if (l_new) {",
        "		bpf_map_dec_elem_count(&htab->map);",
        "		bpf_lru_push_free(&htab->lru, &l_new->lru_node);",
        "	}",
        "	return ret;",
        "}",
        "",
        "static long htab_percpu_map_update_elem(struct bpf_map *map, void *key,",
        "					void *value, u64 map_flags)",
        "{",
        "	return __htab_percpu_map_update_elem(map, key, value, map_flags, false);",
        "}",
        "",
        "static long htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,",
        "					    void *value, u64 map_flags)",
        "{",
        "	return __htab_lru_percpu_map_update_elem(map, key, value, map_flags,",
        "						 false);",
        "}",
        "",
        "/* Called from syscall or from eBPF program */",
        "static long htab_map_delete_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	struct bucket *b;",
        "	struct htab_elem *l;",
        "	unsigned long flags;",
        "	u32 hash, key_size;",
        "	int ret;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		return ret;",
        "",
        "	l = lookup_elem_raw(head, hash, key, key_size);",
        "	if (l)",
        "		hlist_nulls_del_rcu(&l->hash_node);",
        "	else",
        "		ret = -ENOENT;",
        "",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "",
        "	if (l)",
        "		free_htab_elem(htab, l);",
        "	return ret;",
        "}",
        "",
        "static long htab_lru_map_delete_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	struct bucket *b;",
        "	struct htab_elem *l;",
        "	unsigned long flags;",
        "	u32 hash, key_size;",
        "	int ret;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
        "		     !rcu_read_lock_bh_held());",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &flags);",
        "	if (ret)",
        "		return ret;",
        "",
        "	l = lookup_elem_raw(head, hash, key, key_size);",
        "",
        "	if (l)",
        "		hlist_nulls_del_rcu(&l->hash_node);",
        "	else",
        "		ret = -ENOENT;",
        "",
        "	htab_unlock_bucket(htab, b, hash, flags);",
        "	if (l)",
        "		htab_lru_push_free(htab, l);",
        "	return ret;",
        "}",
        "",
        "static void delete_all_elements(struct bpf_htab *htab)",
        "{",
        "	int i;",
        "",
        "	/* It's called from a worker thread, so disable migration here,",
        "	 * since bpf_mem_cache_free() relies on that.",
        "	 */",
        "	migrate_disable();",
        "	for (i = 0; i < htab->n_buckets; i++) {",
        "		struct hlist_nulls_head *head = select_bucket(htab, i);",
        "		struct hlist_nulls_node *n;",
        "		struct htab_elem *l;",
        "",
        "		hlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
        "			hlist_nulls_del_rcu(&l->hash_node);",
        "			htab_elem_free(htab, l);",
        "		}",
        "		cond_resched();",
        "	}",
        "	migrate_enable();",
        "}",
        "",
        "static void htab_free_malloced_timers_and_wq(struct bpf_htab *htab)",
        "{",
        "	int i;",
        "",
        "	rcu_read_lock();",
        "	for (i = 0; i < htab->n_buckets; i++) {",
        "		struct hlist_nulls_head *head = select_bucket(htab, i);",
        "		struct hlist_nulls_node *n;",
        "		struct htab_elem *l;",
        "",
        "		hlist_nulls_for_each_entry(l, n, head, hash_node) {",
        "			/* We only free timer on uref dropping to zero */",
        "			if (btf_record_has_field(htab->map.record, BPF_TIMER))",
        "				bpf_obj_free_timer(htab->map.record,",
        "						   l->key + round_up(htab->map.key_size, 8));",
        "			if (btf_record_has_field(htab->map.record, BPF_WORKQUEUE))",
        "				bpf_obj_free_workqueue(htab->map.record,",
        "						       l->key + round_up(htab->map.key_size, 8));",
        "		}",
        "		cond_resched_rcu();",
        "	}",
        "	rcu_read_unlock();",
        "}",
        "",
        "static void htab_map_free_timers_and_wq(struct bpf_map *map)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "",
        "	/* We only free timer and workqueue on uref dropping to zero */",
        "	if (btf_record_has_field(htab->map.record, BPF_TIMER | BPF_WORKQUEUE)) {",
        "		if (!htab_is_prealloc(htab))",
        "			htab_free_malloced_timers_and_wq(htab);",
        "		else",
        "			htab_free_prealloced_timers_and_wq(htab);",
        "	}",
        "}",
        "",
        "/* Called when map->refcnt goes to zero, either from workqueue or from syscall */",
        "static void htab_map_free(struct bpf_map *map)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	int i;",
        "",
        "	/* bpf_free_used_maps() or close(map_fd) will trigger this map_free callback.",
        "	 * bpf_free_used_maps() is called after bpf prog is no longer executing.",
        "	 * There is no need to synchronize_rcu() here to protect map elements.",
        "	 */",
        "",
        "	/* htab no longer uses call_rcu() directly. bpf_mem_alloc does it",
        "	 * underneath and is responsible for waiting for callbacks to finish",
        "	 * during bpf_mem_alloc_destroy().",
        "	 */",
        "	if (!htab_is_prealloc(htab)) {",
        "		delete_all_elements(htab);",
        "	} else {",
        "		htab_free_prealloced_fields(htab);",
        "		prealloc_destroy(htab);",
        "	}",
        "",
        "	bpf_map_free_elem_count(map);",
        "	free_percpu(htab->extra_elems);",
        "	bpf_map_area_free(htab->buckets);",
        "	bpf_mem_alloc_destroy(&htab->pcpu_ma);",
        "	bpf_mem_alloc_destroy(&htab->ma);",
        "	if (htab->use_percpu_counter)",
        "		percpu_counter_destroy(&htab->pcount);",
        "	for (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++)",
        "		free_percpu(htab->map_locked[i]);",
        "	lockdep_unregister_key(&htab->lockdep_key);",
        "	bpf_map_area_free(htab);",
        "}",
        "",
        "static void htab_map_seq_show_elem(struct bpf_map *map, void *key,",
        "				   struct seq_file *m)",
        "{",
        "	void *value;",
        "",
        "	rcu_read_lock();",
        "",
        "	value = htab_map_lookup_elem(map, key);",
        "	if (!value) {",
        "		rcu_read_unlock();",
        "		return;",
        "	}",
        "",
        "	btf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
        "	seq_puts(m, \": \");",
        "	btf_type_seq_show(map->btf, map->btf_value_type_id, value, m);",
        "	seq_putc(m, '\\n');",
        "",
        "	rcu_read_unlock();",
        "}",
        "",
        "static int __htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
        "					     void *value, bool is_lru_map,",
        "					     bool is_percpu, u64 flags)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	unsigned long bflags;",
        "	struct htab_elem *l;",
        "	u32 hash, key_size;",
        "	struct bucket *b;",
        "	int ret;",
        "",
        "	key_size = map->key_size;",
        "",
        "	hash = htab_map_hash(key, key_size, htab->hashrnd);",
        "	b = __select_bucket(htab, hash);",
        "	head = &b->head;",
        "",
        "	ret = htab_lock_bucket(htab, b, hash, &bflags);",
        "	if (ret)",
        "		return ret;",
        "",
        "	l = lookup_elem_raw(head, hash, key, key_size);",
        "	if (!l) {",
        "		ret = -ENOENT;",
        "	} else {",
        "		if (is_percpu) {",
        "			u32 roundup_value_size = round_up(map->value_size, 8);",
        "			void __percpu *pptr;",
        "			int off = 0, cpu;",
        "",
        "			pptr = htab_elem_get_ptr(l, key_size);",
        "			for_each_possible_cpu(cpu) {",
        "				copy_map_value_long(&htab->map, value + off, per_cpu_ptr(pptr, cpu));",
        "				check_and_init_map_value(&htab->map, value + off);",
        "				off += roundup_value_size;",
        "			}",
        "		} else {",
        "			u32 roundup_key_size = round_up(map->key_size, 8);",
        "",
        "			if (flags & BPF_F_LOCK)",
        "				copy_map_value_locked(map, value, l->key +",
        "						      roundup_key_size,",
        "						      true);",
        "			else",
        "				copy_map_value(map, value, l->key +",
        "					       roundup_key_size);",
        "			/* Zeroing special fields in the temp buffer */",
        "			check_and_init_map_value(map, value);",
        "		}",
        "",
        "		hlist_nulls_del_rcu(&l->hash_node);",
        "		if (!is_lru_map)",
        "			free_htab_elem(htab, l);",
        "	}",
        "",
        "	htab_unlock_bucket(htab, b, hash, bflags);",
        "",
        "	if (is_lru_map && l)",
        "		htab_lru_push_free(htab, l);",
        "",
        "	return ret;",
        "}",
        "",
        "static int htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
        "					   void *value, u64 flags)",
        "{",
        "	return __htab_map_lookup_and_delete_elem(map, key, value, false, false,",
        "						 flags);",
        "}",
        "",
        "static int htab_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
        "						  void *key, void *value,",
        "						  u64 flags)",
        "{",
        "	return __htab_map_lookup_and_delete_elem(map, key, value, false, true,",
        "						 flags);",
        "}",
        "",
        "static int htab_lru_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
        "					       void *value, u64 flags)",
        "{",
        "	return __htab_map_lookup_and_delete_elem(map, key, value, true, false,",
        "						 flags);",
        "}",
        "",
        "static int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
        "						      void *key, void *value,",
        "						      u64 flags)",
        "{",
        "	return __htab_map_lookup_and_delete_elem(map, key, value, true, true,",
        "						 flags);",
        "}",
        "",
        "static int",
        "__htab_map_lookup_and_delete_batch(struct bpf_map *map,",
        "				   const union bpf_attr *attr,",
        "				   union bpf_attr __user *uattr,",
        "				   bool do_delete, bool is_lru_map,",
        "				   bool is_percpu)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	u32 bucket_cnt, total, key_size, value_size, roundup_key_size;",
        "	void *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;",
        "	void __user *uvalues = u64_to_user_ptr(attr->batch.values);",
        "	void __user *ukeys = u64_to_user_ptr(attr->batch.keys);",
        "	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);",
        "	u32 batch, max_count, size, bucket_size, map_id;",
        "	struct htab_elem *node_to_free = NULL;",
        "	u64 elem_map_flags, map_flags;",
        "	struct hlist_nulls_head *head;",
        "	struct hlist_nulls_node *n;",
        "	unsigned long flags = 0;",
        "	bool locked = false;",
        "	struct htab_elem *l;",
        "	struct bucket *b;",
        "	int ret = 0;",
        "",
        "	elem_map_flags = attr->batch.elem_flags;",
        "	if ((elem_map_flags & ~BPF_F_LOCK) ||",
        "	    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
        "		return -EINVAL;",
        "",
        "	map_flags = attr->batch.flags;",
        "	if (map_flags)",
        "		return -EINVAL;",
        "",
        "	max_count = attr->batch.count;",
        "	if (!max_count)",
        "		return 0;",
        "",
        "	if (put_user(0, &uattr->batch.count))",
        "		return -EFAULT;",
        "",
        "	batch = 0;",
        "	if (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))",
        "		return -EFAULT;",
        "",
        "	if (batch >= htab->n_buckets)",
        "		return -ENOENT;",
        "",
        "	key_size = htab->map.key_size;",
        "	roundup_key_size = round_up(htab->map.key_size, 8);",
        "	value_size = htab->map.value_size;",
        "	size = round_up(value_size, 8);",
        "	if (is_percpu)",
        "		value_size = size * num_possible_cpus();",
        "	total = 0;",
        "	/* while experimenting with hash tables with sizes ranging from 10 to",
        "	 * 1000, it was observed that a bucket can have up to 5 entries.",
        "	 */",
        "	bucket_size = 5;",
        "",
        "alloc:",
        "	/* We cannot do copy_from_user or copy_to_user inside",
        "	 * the rcu_read_lock. Allocate enough space here.",
        "	 */",
        "	keys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
        "	values = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);",
        "	if (!keys || !values) {",
        "		ret = -ENOMEM;",
        "		goto after_loop;",
        "	}",
        "",
        "again:",
        "	bpf_disable_instrumentation();",
        "	rcu_read_lock();",
        "again_nocopy:",
        "	dst_key = keys;",
        "	dst_val = values;",
        "	b = &htab->buckets[batch];",
        "	head = &b->head;",
        "	/* do not grab the lock unless need it (bucket_cnt > 0). */",
        "	if (locked) {",
        "		ret = htab_lock_bucket(htab, b, batch, &flags);",
        "		if (ret) {",
        "			rcu_read_unlock();",
        "			bpf_enable_instrumentation();",
        "			goto after_loop;",
        "		}",
        "	}",
        "",
        "	bucket_cnt = 0;",
        "	hlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
        "		bucket_cnt++;",
        "",
        "	if (bucket_cnt && !locked) {",
        "		locked = true;",
        "		goto again_nocopy;",
        "	}",
        "",
        "	if (bucket_cnt > (max_count - total)) {",
        "		if (total == 0)",
        "			ret = -ENOSPC;",
        "		/* Note that since bucket_cnt > 0 here, it is implicit",
        "		 * that the locked was grabbed, so release it.",
        "		 */",
        "		htab_unlock_bucket(htab, b, batch, flags);",
        "		rcu_read_unlock();",
        "		bpf_enable_instrumentation();",
        "		goto after_loop;",
        "	}",
        "",
        "	if (bucket_cnt > bucket_size) {",
        "		bucket_size = bucket_cnt;",
        "		/* Note that since bucket_cnt > 0 here, it is implicit",
        "		 * that the locked was grabbed, so release it.",
        "		 */",
        "		htab_unlock_bucket(htab, b, batch, flags);",
        "		rcu_read_unlock();",
        "		bpf_enable_instrumentation();",
        "		kvfree(keys);",
        "		kvfree(values);",
        "		goto alloc;",
        "	}",
        "",
        "	/* Next block is only safe to run if you have grabbed the lock */",
        "	if (!locked)",
        "		goto next_batch;",
        "",
        "	hlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
        "		memcpy(dst_key, l->key, key_size);",
        "",
        "		if (is_percpu) {",
        "			int off = 0, cpu;",
        "			void __percpu *pptr;",
        "",
        "			pptr = htab_elem_get_ptr(l, map->key_size);",
        "			for_each_possible_cpu(cpu) {",
        "				copy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));",
        "				check_and_init_map_value(&htab->map, dst_val + off);",
        "				off += size;",
        "			}",
        "		} else {",
        "			value = l->key + roundup_key_size;",
        "			if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {",
        "				struct bpf_map **inner_map = value;",
        "",
        "				 /* Actual value is the id of the inner map */",
        "				map_id = map->ops->map_fd_sys_lookup_elem(*inner_map);",
        "				value = &map_id;",
        "			}",
        "",
        "			if (elem_map_flags & BPF_F_LOCK)",
        "				copy_map_value_locked(map, dst_val, value,",
        "						      true);",
        "			else",
        "				copy_map_value(map, dst_val, value);",
        "			/* Zeroing special fields in the temp buffer */",
        "			check_and_init_map_value(map, dst_val);",
        "		}",
        "		if (do_delete) {",
        "			hlist_nulls_del_rcu(&l->hash_node);",
        "",
        "			/* bpf_lru_push_free() will acquire lru_lock, which",
        "			 * may cause deadlock. See comments in function",
        "			 * prealloc_lru_pop(). Let us do bpf_lru_push_free()",
        "			 * after releasing the bucket lock.",
        "			 *",
        "			 * For htab of maps, htab_put_fd_value() in",
        "			 * free_htab_elem() may acquire a spinlock with bucket",
        "			 * lock being held and it violates the lock rule, so",
        "			 * invoke free_htab_elem() after unlock as well.",
        "			 */",
        "			l->batch_flink = node_to_free;",
        "			node_to_free = l;",
        "		}",
        "		dst_key += key_size;",
        "		dst_val += value_size;",
        "	}",
        "",
        "	htab_unlock_bucket(htab, b, batch, flags);",
        "	locked = false;",
        "",
        "	while (node_to_free) {",
        "		l = node_to_free;",
        "		node_to_free = node_to_free->batch_flink;",
        "		if (is_lru_map)",
        "			htab_lru_push_free(htab, l);",
        "		else",
        "			free_htab_elem(htab, l);",
        "	}",
        "",
        "next_batch:",
        "	/* If we are not copying data, we can go to next bucket and avoid",
        "	 * unlocking the rcu.",
        "	 */",
        "	if (!bucket_cnt && (batch + 1 < htab->n_buckets)) {",
        "		batch++;",
        "		goto again_nocopy;",
        "	}",
        "",
        "	rcu_read_unlock();",
        "	bpf_enable_instrumentation();",
        "	if (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,",
        "	    key_size * bucket_cnt) ||",
        "	    copy_to_user(uvalues + total * value_size, values,",
        "	    value_size * bucket_cnt))) {",
        "		ret = -EFAULT;",
        "		goto after_loop;",
        "	}",
        "",
        "	total += bucket_cnt;",
        "	batch++;",
        "	if (batch >= htab->n_buckets) {",
        "		ret = -ENOENT;",
        "		goto after_loop;",
        "	}",
        "	goto again;",
        "",
        "after_loop:",
        "	if (ret == -EFAULT)",
        "		goto out;",
        "",
        "	/* copy # of entries and next batch */",
        "	ubatch = u64_to_user_ptr(attr->batch.out_batch);",
        "	if (copy_to_user(ubatch, &batch, sizeof(batch)) ||",
        "	    put_user(total, &uattr->batch.count))",
        "		ret = -EFAULT;",
        "",
        "out:",
        "	kvfree(keys);",
        "	kvfree(values);",
        "	return ret;",
        "}",
        "",
        "static int",
        "htab_percpu_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
        "			     union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
        "						  false, true);",
        "}",
        "",
        "static int",
        "htab_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
        "					const union bpf_attr *attr,",
        "					union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
        "						  false, true);",
        "}",
        "",
        "static int",
        "htab_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
        "		      union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
        "						  false, false);",
        "}",
        "",
        "static int",
        "htab_map_lookup_and_delete_batch(struct bpf_map *map,",
        "				 const union bpf_attr *attr,",
        "				 union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
        "						  false, false);",
        "}",
        "",
        "static int",
        "htab_lru_percpu_map_lookup_batch(struct bpf_map *map,",
        "				 const union bpf_attr *attr,",
        "				 union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
        "						  true, true);",
        "}",
        "",
        "static int",
        "htab_lru_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
        "					    const union bpf_attr *attr,",
        "					    union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
        "						  true, true);",
        "}",
        "",
        "static int",
        "htab_lru_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
        "			  union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
        "						  true, false);",
        "}",
        "",
        "static int",
        "htab_lru_map_lookup_and_delete_batch(struct bpf_map *map,",
        "				     const union bpf_attr *attr,",
        "				     union bpf_attr __user *uattr)",
        "{",
        "	return __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
        "						  true, false);",
        "}",
        "",
        "struct bpf_iter_seq_hash_map_info {",
        "	struct bpf_map *map;",
        "	struct bpf_htab *htab;",
        "	void *percpu_value_buf; // non-zero means percpu hash",
        "	u32 bucket_id;",
        "	u32 skip_elems;",
        "};",
        "",
        "static struct htab_elem *",
        "bpf_hash_map_seq_find_next(struct bpf_iter_seq_hash_map_info *info,",
        "			   struct htab_elem *prev_elem)",
        "{",
        "	const struct bpf_htab *htab = info->htab;",
        "	u32 skip_elems = info->skip_elems;",
        "	u32 bucket_id = info->bucket_id;",
        "	struct hlist_nulls_head *head;",
        "	struct hlist_nulls_node *n;",
        "	struct htab_elem *elem;",
        "	struct bucket *b;",
        "	u32 i, count;",
        "",
        "	if (bucket_id >= htab->n_buckets)",
        "		return NULL;",
        "",
        "	/* try to find next elem in the same bucket */",
        "	if (prev_elem) {",
        "		/* no update/deletion on this bucket, prev_elem should be still valid",
        "		 * and we won't skip elements.",
        "		 */",
        "		n = rcu_dereference_raw(hlist_nulls_next_rcu(&prev_elem->hash_node));",
        "		elem = hlist_nulls_entry_safe(n, struct htab_elem, hash_node);",
        "		if (elem)",
        "			return elem;",
        "",
        "		/* not found, unlock and go to the next bucket */",
        "		b = &htab->buckets[bucket_id++];",
        "		rcu_read_unlock();",
        "		skip_elems = 0;",
        "	}",
        "",
        "	for (i = bucket_id; i < htab->n_buckets; i++) {",
        "		b = &htab->buckets[i];",
        "		rcu_read_lock();",
        "",
        "		count = 0;",
        "		head = &b->head;",
        "		hlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {",
        "			if (count >= skip_elems) {",
        "				info->bucket_id = i;",
        "				info->skip_elems = count;",
        "				return elem;",
        "			}",
        "			count++;",
        "		}",
        "",
        "		rcu_read_unlock();",
        "		skip_elems = 0;",
        "	}",
        "",
        "	info->bucket_id = i;",
        "	info->skip_elems = 0;",
        "	return NULL;",
        "}",
        "",
        "static void *bpf_hash_map_seq_start(struct seq_file *seq, loff_t *pos)",
        "{",
        "	struct bpf_iter_seq_hash_map_info *info = seq->private;",
        "	struct htab_elem *elem;",
        "",
        "	elem = bpf_hash_map_seq_find_next(info, NULL);",
        "	if (!elem)",
        "		return NULL;",
        "",
        "	if (*pos == 0)",
        "		++*pos;",
        "	return elem;",
        "}",
        "",
        "static void *bpf_hash_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)",
        "{",
        "	struct bpf_iter_seq_hash_map_info *info = seq->private;",
        "",
        "	++*pos;",
        "	++info->skip_elems;",
        "	return bpf_hash_map_seq_find_next(info, v);",
        "}",
        "",
        "static int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)",
        "{",
        "	struct bpf_iter_seq_hash_map_info *info = seq->private;",
        "	u32 roundup_key_size, roundup_value_size;",
        "	struct bpf_iter__bpf_map_elem ctx = {};",
        "	struct bpf_map *map = info->map;",
        "	struct bpf_iter_meta meta;",
        "	int ret = 0, off = 0, cpu;",
        "	struct bpf_prog *prog;",
        "	void __percpu *pptr;",
        "",
        "	meta.seq = seq;",
        "	prog = bpf_iter_get_info(&meta, elem == NULL);",
        "	if (prog) {",
        "		ctx.meta = &meta;",
        "		ctx.map = info->map;",
        "		if (elem) {",
        "			roundup_key_size = round_up(map->key_size, 8);",
        "			ctx.key = elem->key;",
        "			if (!info->percpu_value_buf) {",
        "				ctx.value = elem->key + roundup_key_size;",
        "			} else {",
        "				roundup_value_size = round_up(map->value_size, 8);",
        "				pptr = htab_elem_get_ptr(elem, map->key_size);",
        "				for_each_possible_cpu(cpu) {",
        "					copy_map_value_long(map, info->percpu_value_buf + off,",
        "							    per_cpu_ptr(pptr, cpu));",
        "					check_and_init_map_value(map, info->percpu_value_buf + off);",
        "					off += roundup_value_size;",
        "				}",
        "				ctx.value = info->percpu_value_buf;",
        "			}",
        "		}",
        "		ret = bpf_iter_run_prog(prog, &ctx);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "static int bpf_hash_map_seq_show(struct seq_file *seq, void *v)",
        "{",
        "	return __bpf_hash_map_seq_show(seq, v);",
        "}",
        "",
        "static void bpf_hash_map_seq_stop(struct seq_file *seq, void *v)",
        "{",
        "	if (!v)",
        "		(void)__bpf_hash_map_seq_show(seq, NULL);",
        "	else",
        "		rcu_read_unlock();",
        "}",
        "",
        "static int bpf_iter_init_hash_map(void *priv_data,",
        "				  struct bpf_iter_aux_info *aux)",
        "{",
        "	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;",
        "	struct bpf_map *map = aux->map;",
        "	void *value_buf;",
        "	u32 buf_size;",
        "",
        "	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
        "	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {",
        "		buf_size = round_up(map->value_size, 8) * num_possible_cpus();",
        "		value_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);",
        "		if (!value_buf)",
        "			return -ENOMEM;",
        "",
        "		seq_info->percpu_value_buf = value_buf;",
        "	}",
        "",
        "	bpf_map_inc_with_uref(map);",
        "	seq_info->map = map;",
        "	seq_info->htab = container_of(map, struct bpf_htab, map);",
        "	return 0;",
        "}",
        "",
        "static void bpf_iter_fini_hash_map(void *priv_data)",
        "{",
        "	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;",
        "",
        "	bpf_map_put_with_uref(seq_info->map);",
        "	kfree(seq_info->percpu_value_buf);",
        "}",
        "",
        "static const struct seq_operations bpf_hash_map_seq_ops = {",
        "	.start	= bpf_hash_map_seq_start,",
        "	.next	= bpf_hash_map_seq_next,",
        "	.stop	= bpf_hash_map_seq_stop,",
        "	.show	= bpf_hash_map_seq_show,",
        "};",
        "",
        "static const struct bpf_iter_seq_info iter_seq_info = {",
        "	.seq_ops		= &bpf_hash_map_seq_ops,",
        "	.init_seq_private	= bpf_iter_init_hash_map,",
        "	.fini_seq_private	= bpf_iter_fini_hash_map,",
        "	.seq_priv_size		= sizeof(struct bpf_iter_seq_hash_map_info),",
        "};",
        "",
        "static long bpf_for_each_hash_elem(struct bpf_map *map, bpf_callback_t callback_fn,",
        "				   void *callback_ctx, u64 flags)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_head *head;",
        "	struct hlist_nulls_node *n;",
        "	struct htab_elem *elem;",
        "	u32 roundup_key_size;",
        "	int i, num_elems = 0;",
        "	void __percpu *pptr;",
        "	struct bucket *b;",
        "	void *key, *val;",
        "	bool is_percpu;",
        "	u64 ret = 0;",
        "",
        "	if (flags != 0)",
        "		return -EINVAL;",
        "",
        "	is_percpu = htab_is_percpu(htab);",
        "",
        "	roundup_key_size = round_up(map->key_size, 8);",
        "	/* disable migration so percpu value prepared here will be the",
        "	 * same as the one seen by the bpf program with bpf_map_lookup_elem().",
        "	 */",
        "	if (is_percpu)",
        "		migrate_disable();",
        "	for (i = 0; i < htab->n_buckets; i++) {",
        "		b = &htab->buckets[i];",
        "		rcu_read_lock();",
        "		head = &b->head;",
        "		hlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {",
        "			key = elem->key;",
        "			if (is_percpu) {",
        "				/* current cpu value for percpu map */",
        "				pptr = htab_elem_get_ptr(elem, map->key_size);",
        "				val = this_cpu_ptr(pptr);",
        "			} else {",
        "				val = elem->key + roundup_key_size;",
        "			}",
        "			num_elems++;",
        "			ret = callback_fn((u64)(long)map, (u64)(long)key,",
        "					  (u64)(long)val, (u64)(long)callback_ctx, 0);",
        "			/* return value: 0 - continue, 1 - stop and return */",
        "			if (ret) {",
        "				rcu_read_unlock();",
        "				goto out;",
        "			}",
        "		}",
        "		rcu_read_unlock();",
        "	}",
        "out:",
        "	if (is_percpu)",
        "		migrate_enable();",
        "	return num_elems;",
        "}",
        "",
        "static u64 htab_map_mem_usage(const struct bpf_map *map)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	u32 value_size = round_up(htab->map.value_size, 8);",
        "	bool prealloc = htab_is_prealloc(htab);",
        "	bool percpu = htab_is_percpu(htab);",
        "	bool lru = htab_is_lru(htab);",
        "	u64 num_entries;",
        "	u64 usage = sizeof(struct bpf_htab);",
        "",
        "	usage += sizeof(struct bucket) * htab->n_buckets;",
        "	usage += sizeof(int) * num_possible_cpus() * HASHTAB_MAP_LOCK_COUNT;",
        "	if (prealloc) {",
        "		num_entries = map->max_entries;",
        "		if (htab_has_extra_elems(htab))",
        "			num_entries += num_possible_cpus();",
        "",
        "		usage += htab->elem_size * num_entries;",
        "",
        "		if (percpu)",
        "			usage += value_size * num_possible_cpus() * num_entries;",
        "		else if (!lru)",
        "			usage += sizeof(struct htab_elem *) * num_possible_cpus();",
        "	} else {",
        "#define LLIST_NODE_SZ sizeof(struct llist_node)",
        "",
        "		num_entries = htab->use_percpu_counter ?",
        "					  percpu_counter_sum(&htab->pcount) :",
        "					  atomic_read(&htab->count);",
        "		usage += (htab->elem_size + LLIST_NODE_SZ) * num_entries;",
        "		if (percpu) {",
        "			usage += (LLIST_NODE_SZ + sizeof(void *)) * num_entries;",
        "			usage += value_size * num_possible_cpus() * num_entries;",
        "		}",
        "	}",
        "	return usage;",
        "}",
        "",
        "BTF_ID_LIST_SINGLE(htab_map_btf_ids, struct, bpf_htab)",
        "const struct bpf_map_ops htab_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc_check = htab_map_alloc_check,",
        "	.map_alloc = htab_map_alloc,",
        "	.map_free = htab_map_free,",
        "	.map_get_next_key = htab_map_get_next_key,",
        "	.map_release_uref = htab_map_free_timers_and_wq,",
        "	.map_lookup_elem = htab_map_lookup_elem,",
        "	.map_lookup_and_delete_elem = htab_map_lookup_and_delete_elem,",
        "	.map_update_elem = htab_map_update_elem,",
        "	.map_delete_elem = htab_map_delete_elem,",
        "	.map_gen_lookup = htab_map_gen_lookup,",
        "	.map_seq_show_elem = htab_map_seq_show_elem,",
        "	.map_set_for_each_callback_args = map_set_for_each_callback_args,",
        "	.map_for_each_callback = bpf_for_each_hash_elem,",
        "	.map_mem_usage = htab_map_mem_usage,",
        "	BATCH_OPS(htab),",
        "	.map_btf_id = &htab_map_btf_ids[0],",
        "	.iter_seq_info = &iter_seq_info,",
        "};",
        "",
        "const struct bpf_map_ops htab_lru_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc_check = htab_map_alloc_check,",
        "	.map_alloc = htab_map_alloc,",
        "	.map_free = htab_map_free,",
        "	.map_get_next_key = htab_map_get_next_key,",
        "	.map_release_uref = htab_map_free_timers_and_wq,",
        "	.map_lookup_elem = htab_lru_map_lookup_elem,",
        "	.map_lookup_and_delete_elem = htab_lru_map_lookup_and_delete_elem,",
        "	.map_lookup_elem_sys_only = htab_lru_map_lookup_elem_sys,",
        "	.map_update_elem = htab_lru_map_update_elem,",
        "	.map_delete_elem = htab_lru_map_delete_elem,",
        "	.map_gen_lookup = htab_lru_map_gen_lookup,",
        "	.map_seq_show_elem = htab_map_seq_show_elem,",
        "	.map_set_for_each_callback_args = map_set_for_each_callback_args,",
        "	.map_for_each_callback = bpf_for_each_hash_elem,",
        "	.map_mem_usage = htab_map_mem_usage,",
        "	BATCH_OPS(htab_lru),",
        "	.map_btf_id = &htab_map_btf_ids[0],",
        "	.iter_seq_info = &iter_seq_info,",
        "};",
        "",
        "/* Called from eBPF program */",
        "static void *htab_percpu_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct htab_elem *l = __htab_map_lookup_elem(map, key);",
        "",
        "	if (l)",
        "		return this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));",
        "	else",
        "		return NULL;",
        "}",
        "",
        "/* inline bpf_map_lookup_elem() call for per-CPU hashmap */",
        "static int htab_percpu_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)",
        "{",
        "	struct bpf_insn *insn = insn_buf;",
        "",
        "	if (!bpf_jit_supports_percpu_insn())",
        "		return -EOPNOTSUPP;",
        "",
        "	BUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
        "		     (void *(*)(struct bpf_map *map, void *key))NULL));",
        "	*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
        "	*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 3);",
        "	*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_0,",
        "				offsetof(struct htab_elem, key) + map->key_size);",
        "	*insn++ = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0);",
        "	*insn++ = BPF_MOV64_PERCPU_REG(BPF_REG_0, BPF_REG_0);",
        "",
        "	return insn - insn_buf;",
        "}",
        "",
        "static void *htab_percpu_map_lookup_percpu_elem(struct bpf_map *map, void *key, u32 cpu)",
        "{",
        "	struct htab_elem *l;",
        "",
        "	if (cpu >= nr_cpu_ids)",
        "		return NULL;",
        "",
        "	l = __htab_map_lookup_elem(map, key);",
        "	if (l)",
        "		return per_cpu_ptr(htab_elem_get_ptr(l, map->key_size), cpu);",
        "	else",
        "		return NULL;",
        "}",
        "",
        "static void *htab_lru_percpu_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct htab_elem *l = __htab_map_lookup_elem(map, key);",
        "",
        "	if (l) {",
        "		bpf_lru_node_set_ref(&l->lru_node);",
        "		return this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "static void *htab_lru_percpu_map_lookup_percpu_elem(struct bpf_map *map, void *key, u32 cpu)",
        "{",
        "	struct htab_elem *l;",
        "",
        "	if (cpu >= nr_cpu_ids)",
        "		return NULL;",
        "",
        "	l = __htab_map_lookup_elem(map, key);",
        "	if (l) {",
        "		bpf_lru_node_set_ref(&l->lru_node);",
        "		return per_cpu_ptr(htab_elem_get_ptr(l, map->key_size), cpu);",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)",
        "{",
        "	struct htab_elem *l;",
        "	void __percpu *pptr;",
        "	int ret = -ENOENT;",
        "	int cpu, off = 0;",
        "	u32 size;",
        "",
        "	/* per_cpu areas are zero-filled and bpf programs can only",
        "	 * access 'value_size' of them, so copying rounded areas",
        "	 * will not leak any kernel data",
        "	 */",
        "	size = round_up(map->value_size, 8);",
        "	rcu_read_lock();",
        "	l = __htab_map_lookup_elem(map, key);",
        "	if (!l)",
        "		goto out;",
        "	/* We do not mark LRU map element here in order to not mess up",
        "	 * eviction heuristics when user space does a map walk.",
        "	 */",
        "	pptr = htab_elem_get_ptr(l, map->key_size);",
        "	for_each_possible_cpu(cpu) {",
        "		copy_map_value_long(map, value + off, per_cpu_ptr(pptr, cpu));",
        "		check_and_init_map_value(map, value + off);",
        "		off += size;",
        "	}",
        "	ret = 0;",
        "out:",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "",
        "int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,",
        "			   u64 map_flags)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	int ret;",
        "",
        "	rcu_read_lock();",
        "	if (htab_is_lru(htab))",
        "		ret = __htab_lru_percpu_map_update_elem(map, key, value,",
        "							map_flags, true);",
        "	else",
        "		ret = __htab_percpu_map_update_elem(map, key, value, map_flags,",
        "						    true);",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "static void htab_percpu_map_seq_show_elem(struct bpf_map *map, void *key,",
        "					  struct seq_file *m)",
        "{",
        "	struct htab_elem *l;",
        "	void __percpu *pptr;",
        "	int cpu;",
        "",
        "	rcu_read_lock();",
        "",
        "	l = __htab_map_lookup_elem(map, key);",
        "	if (!l) {",
        "		rcu_read_unlock();",
        "		return;",
        "	}",
        "",
        "	btf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
        "	seq_puts(m, \": {\\n\");",
        "	pptr = htab_elem_get_ptr(l, map->key_size);",
        "	for_each_possible_cpu(cpu) {",
        "		seq_printf(m, \"\\tcpu%d: \", cpu);",
        "		btf_type_seq_show(map->btf, map->btf_value_type_id,",
        "				  per_cpu_ptr(pptr, cpu), m);",
        "		seq_putc(m, '\\n');",
        "	}",
        "	seq_puts(m, \"}\\n\");",
        "",
        "	rcu_read_unlock();",
        "}",
        "",
        "const struct bpf_map_ops htab_percpu_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc_check = htab_map_alloc_check,",
        "	.map_alloc = htab_map_alloc,",
        "	.map_free = htab_map_free,",
        "	.map_get_next_key = htab_map_get_next_key,",
        "	.map_lookup_elem = htab_percpu_map_lookup_elem,",
        "	.map_gen_lookup = htab_percpu_map_gen_lookup,",
        "	.map_lookup_and_delete_elem = htab_percpu_map_lookup_and_delete_elem,",
        "	.map_update_elem = htab_percpu_map_update_elem,",
        "	.map_delete_elem = htab_map_delete_elem,",
        "	.map_lookup_percpu_elem = htab_percpu_map_lookup_percpu_elem,",
        "	.map_seq_show_elem = htab_percpu_map_seq_show_elem,",
        "	.map_set_for_each_callback_args = map_set_for_each_callback_args,",
        "	.map_for_each_callback = bpf_for_each_hash_elem,",
        "	.map_mem_usage = htab_map_mem_usage,",
        "	BATCH_OPS(htab_percpu),",
        "	.map_btf_id = &htab_map_btf_ids[0],",
        "	.iter_seq_info = &iter_seq_info,",
        "};",
        "",
        "const struct bpf_map_ops htab_lru_percpu_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc_check = htab_map_alloc_check,",
        "	.map_alloc = htab_map_alloc,",
        "	.map_free = htab_map_free,",
        "	.map_get_next_key = htab_map_get_next_key,",
        "	.map_lookup_elem = htab_lru_percpu_map_lookup_elem,",
        "	.map_lookup_and_delete_elem = htab_lru_percpu_map_lookup_and_delete_elem,",
        "	.map_update_elem = htab_lru_percpu_map_update_elem,",
        "	.map_delete_elem = htab_lru_map_delete_elem,",
        "	.map_lookup_percpu_elem = htab_lru_percpu_map_lookup_percpu_elem,",
        "	.map_seq_show_elem = htab_percpu_map_seq_show_elem,",
        "	.map_set_for_each_callback_args = map_set_for_each_callback_args,",
        "	.map_for_each_callback = bpf_for_each_hash_elem,",
        "	.map_mem_usage = htab_map_mem_usage,",
        "	BATCH_OPS(htab_lru_percpu),",
        "	.map_btf_id = &htab_map_btf_ids[0],",
        "	.iter_seq_info = &iter_seq_info,",
        "};",
        "",
        "static int fd_htab_map_alloc_check(union bpf_attr *attr)",
        "{",
        "	if (attr->value_size != sizeof(u32))",
        "		return -EINVAL;",
        "	return htab_map_alloc_check(attr);",
        "}",
        "",
        "static void fd_htab_map_free(struct bpf_map *map)",
        "{",
        "	struct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
        "	struct hlist_nulls_node *n;",
        "	struct hlist_nulls_head *head;",
        "	struct htab_elem *l;",
        "	int i;",
        "",
        "	for (i = 0; i < htab->n_buckets; i++) {",
        "		head = select_bucket(htab, i);",
        "",
        "		hlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
        "			void *ptr = fd_htab_map_get_ptr(map, l);",
        "",
        "			map->ops->map_fd_put_ptr(map, ptr, false);",
        "		}",
        "	}",
        "",
        "	htab_map_free(map);",
        "}",
        "",
        "/* only called from syscall */",
        "int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value)",
        "{",
        "	void **ptr;",
        "	int ret = 0;",
        "",
        "	if (!map->ops->map_fd_sys_lookup_elem)",
        "		return -ENOTSUPP;",
        "",
        "	rcu_read_lock();",
        "	ptr = htab_map_lookup_elem(map, key);",
        "	if (ptr)",
        "		*value = map->ops->map_fd_sys_lookup_elem(READ_ONCE(*ptr));",
        "	else",
        "		ret = -ENOENT;",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "/* only called from syscall */",
        "int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,",
        "				void *key, void *value, u64 map_flags)",
        "{",
        "	void *ptr;",
        "	int ret;",
        "	u32 ufd = *(u32 *)value;",
        "",
        "	ptr = map->ops->map_fd_get_ptr(map, map_file, ufd);",
        "	if (IS_ERR(ptr))",
        "		return PTR_ERR(ptr);",
        "",
        "	/* The htab bucket lock is always held during update operations in fd",
        "	 * htab map, and the following rcu_read_lock() is only used to avoid",
        "	 * the WARN_ON_ONCE in htab_map_update_elem().",
        "	 */",
        "	rcu_read_lock();",
        "	ret = htab_map_update_elem(map, key, &ptr, map_flags);",
        "	rcu_read_unlock();",
        "	if (ret)",
        "		map->ops->map_fd_put_ptr(map, ptr, false);",
        "",
        "	return ret;",
        "}",
        "",
        "static struct bpf_map *htab_of_map_alloc(union bpf_attr *attr)",
        "{",
        "	struct bpf_map *map, *inner_map_meta;",
        "",
        "	inner_map_meta = bpf_map_meta_alloc(attr->inner_map_fd);",
        "	if (IS_ERR(inner_map_meta))",
        "		return inner_map_meta;",
        "",
        "	map = htab_map_alloc(attr);",
        "	if (IS_ERR(map)) {",
        "		bpf_map_meta_free(inner_map_meta);",
        "		return map;",
        "	}",
        "",
        "	map->inner_map_meta = inner_map_meta;",
        "",
        "	return map;",
        "}",
        "",
        "static void *htab_of_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct bpf_map **inner_map  = htab_map_lookup_elem(map, key);",
        "",
        "	if (!inner_map)",
        "		return NULL;",
        "",
        "	return READ_ONCE(*inner_map);",
        "}",
        "",
        "static int htab_of_map_gen_lookup(struct bpf_map *map,",
        "				  struct bpf_insn *insn_buf)",
        "{",
        "	struct bpf_insn *insn = insn_buf;",
        "	const int ret = BPF_REG_0;",
        "",
        "	BUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
        "		     (void *(*)(struct bpf_map *map, void *key))NULL));",
        "	*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
        "	*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 2);",
        "	*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,",
        "				offsetof(struct htab_elem, key) +",
        "				round_up(map->key_size, 8));",
        "	*insn++ = BPF_LDX_MEM(BPF_DW, ret, ret, 0);",
        "",
        "	return insn - insn_buf;",
        "}",
        "",
        "static void htab_of_map_free(struct bpf_map *map)",
        "{",
        "	bpf_map_meta_free(map->inner_map_meta);",
        "	fd_htab_map_free(map);",
        "}",
        "",
        "const struct bpf_map_ops htab_of_maps_map_ops = {",
        "	.map_alloc_check = fd_htab_map_alloc_check,",
        "	.map_alloc = htab_of_map_alloc,",
        "	.map_free = htab_of_map_free,",
        "	.map_get_next_key = htab_map_get_next_key,",
        "	.map_lookup_elem = htab_of_map_lookup_elem,",
        "	.map_delete_elem = htab_map_delete_elem,",
        "	.map_fd_get_ptr = bpf_map_fd_get_ptr,",
        "	.map_fd_put_ptr = bpf_map_fd_put_ptr,",
        "	.map_fd_sys_lookup_elem = bpf_map_fd_sys_lookup_elem,",
        "	.map_gen_lookup = htab_of_map_gen_lookup,",
        "	.map_check_btf = map_check_no_btf,",
        "	.map_mem_usage = htab_map_mem_usage,",
        "	BATCH_OPS(htab),",
        "	.map_btf_id = &htab_map_btf_ids[0],",
        "};"
    ]
  },
  "kernel_bpf_lpm_trie_c": {
    path: "kernel/bpf/lpm_trie.c",
    covered: [588, 613, 638, 581, 593, 587, 623, 591],
    totalLines: 798,
    coveredCount: 8,
    coveragePct: 1.0,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " * Longest prefix match list implementation",
        " *",
        " * Copyright (c) 2016,2017 Daniel Mack",
        " * Copyright (c) 2016 David Herrmann",
        " */",
        "",
        "#include <linux/bpf.h>",
        "#include <linux/btf.h>",
        "#include <linux/err.h>",
        "#include <linux/slab.h>",
        "#include <linux/spinlock.h>",
        "#include <linux/vmalloc.h>",
        "#include <net/ipv6.h>",
        "#include <uapi/linux/btf.h>",
        "#include <linux/btf_ids.h>",
        "#include <linux/bpf_mem_alloc.h>",
        "",
        "/* Intermediate node */",
        "#define LPM_TREE_NODE_FLAG_IM BIT(0)",
        "",
        "struct lpm_trie_node;",
        "",
        "struct lpm_trie_node {",
        "	struct lpm_trie_node __rcu	*child[2];",
        "	u32				prefixlen;",
        "	u32				flags;",
        "	u8				data[];",
        "};",
        "",
        "struct lpm_trie {",
        "	struct bpf_map			map;",
        "	struct lpm_trie_node __rcu	*root;",
        "	struct bpf_mem_alloc		ma;",
        "	size_t				n_entries;",
        "	size_t				max_prefixlen;",
        "	size_t				data_size;",
        "	raw_spinlock_t			lock;",
        "};",
        "",
        "/* This trie implements a longest prefix match algorithm that can be used to",
        " * match IP addresses to a stored set of ranges.",
        " *",
        " * Data stored in @data of struct bpf_lpm_key and struct lpm_trie_node is",
        " * interpreted as big endian, so data[0] stores the most significant byte.",
        " *",
        " * Match ranges are internally stored in instances of struct lpm_trie_node",
        " * which each contain their prefix length as well as two pointers that may",
        " * lead to more nodes containing more specific matches. Each node also stores",
        " * a value that is defined by and returned to userspace via the update_elem",
        " * and lookup functions.",
        " *",
        " * For instance, let's start with a trie that was created with a prefix length",
        " * of 32, so it can be used for IPv4 addresses, and one single element that",
        " * matches 192.168.0.0/16. The data array would hence contain",
        " * [0xc0, 0xa8, 0x00, 0x00] in big-endian notation. This documentation will",
        " * stick to IP-address notation for readability though.",
        " *",
        " * As the trie is empty initially, the new node (1) will be places as root",
        " * node, denoted as (R) in the example below. As there are no other node, both",
        " * child pointers are %NULL.",
        " *",
        " *              +----------------+",
        " *              |       (1)  (R) |",
        " *              | 192.168.0.0/16 |",
        " *              |    value: 1    |",
        " *              |   [0]    [1]   |",
        " *              +----------------+",
        " *",
        " * Next, let's add a new node (2) matching 192.168.0.0/24. As there is already",
        " * a node with the same data and a smaller prefix (ie, a less specific one),",
        " * node (2) will become a child of (1). In child index depends on the next bit",
        " * that is outside of what (1) matches, and that bit is 0, so (2) will be",
        " * child[0] of (1):",
        " *",
        " *              +----------------+",
        " *              |       (1)  (R) |",
        " *              | 192.168.0.0/16 |",
        " *              |    value: 1    |",
        " *              |   [0]    [1]   |",
        " *              +----------------+",
        " *                   |",
        " *    +----------------+",
        " *    |       (2)      |",
        " *    | 192.168.0.0/24 |",
        " *    |    value: 2    |",
        " *    |   [0]    [1]   |",
        " *    +----------------+",
        " *",
        " * The child[1] slot of (1) could be filled with another node which has bit #17",
        " * (the next bit after the ones that (1) matches on) set to 1. For instance,",
        " * 192.168.128.0/24:",
        " *",
        " *              +----------------+",
        " *              |       (1)  (R) |",
        " *              | 192.168.0.0/16 |",
        " *              |    value: 1    |",
        " *              |   [0]    [1]   |",
        " *              +----------------+",
        " *                   |      |",
        " *    +----------------+  +------------------+",
        " *    |       (2)      |  |        (3)       |",
        " *    | 192.168.0.0/24 |  | 192.168.128.0/24 |",
        " *    |    value: 2    |  |     value: 3     |",
        " *    |   [0]    [1]   |  |    [0]    [1]    |",
        " *    +----------------+  +------------------+",
        " *",
        " * Let's add another node (4) to the game for 192.168.1.0/24. In order to place",
        " * it, node (1) is looked at first, and because (4) of the semantics laid out",
        " * above (bit #17 is 0), it would normally be attached to (1) as child[0].",
        " * However, that slot is already allocated, so a new node is needed in between.",
        " * That node does not have a value attached to it and it will never be",
        " * returned to users as result of a lookup. It is only there to differentiate",
        " * the traversal further. It will get a prefix as wide as necessary to",
        " * distinguish its two children:",
        " *",
        " *                      +----------------+",
        " *                      |       (1)  (R) |",
        " *                      | 192.168.0.0/16 |",
        " *                      |    value: 1    |",
        " *                      |   [0]    [1]   |",
        " *                      +----------------+",
        " *                           |      |",
        " *            +----------------+  +------------------+",
        " *            |       (4)  (I) |  |        (3)       |",
        " *            | 192.168.0.0/23 |  | 192.168.128.0/24 |",
        " *            |    value: ---  |  |     value: 3     |",
        " *            |   [0]    [1]   |  |    [0]    [1]    |",
        " *            +----------------+  +------------------+",
        " *                 |      |",
        " *  +----------------+  +----------------+",
        " *  |       (2)      |  |       (5)      |",
        " *  | 192.168.0.0/24 |  | 192.168.1.0/24 |",
        " *  |    value: 2    |  |     value: 5   |",
        " *  |   [0]    [1]   |  |   [0]    [1]   |",
        " *  +----------------+  +----------------+",
        " *",
        " * 192.168.1.1/32 would be a child of (5) etc.",
        " *",
        " * An intermediate node will be turned into a 'real' node on demand. In the",
        " * example above, (4) would be re-used if 192.168.0.0/23 is added to the trie.",
        " *",
        " * A fully populated trie would have a height of 32 nodes, as the trie was",
        " * created with a prefix length of 32.",
        " *",
        " * The lookup starts at the root node. If the current node matches and if there",
        " * is a child that can be used to become more specific, the trie is traversed",
        " * downwards. The last node in the traversal that is a non-intermediate one is",
        " * returned.",
        " */",
        "",
        "static inline int extract_bit(const u8 *data, size_t index)",
        "{",
        "	return !!(data[index / 8] & (1 << (7 - (index % 8))));",
        "}",
        "",
        "/**",
        " * __longest_prefix_match() - determine the longest prefix",
        " * @trie:	The trie to get internal sizes from",
        " * @node:	The node to operate on",
        " * @key:	The key to compare to @node",
        " *",
        " * Determine the longest prefix of @node that matches the bits in @key.",
        " */",
        "static __always_inline",
        "size_t __longest_prefix_match(const struct lpm_trie *trie,",
        "			      const struct lpm_trie_node *node,",
        "			      const struct bpf_lpm_trie_key_u8 *key)",
        "{",
        "	u32 limit = min(node->prefixlen, key->prefixlen);",
        "	u32 prefixlen = 0, i = 0;",
        "",
        "	BUILD_BUG_ON(offsetof(struct lpm_trie_node, data) % sizeof(u32));",
        "	BUILD_BUG_ON(offsetof(struct bpf_lpm_trie_key_u8, data) % sizeof(u32));",
        "",
        "#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && defined(CONFIG_64BIT)",
        "",
        "	/* data_size >= 16 has very small probability.",
        "	 * We do not use a loop for optimal code generation.",
        "	 */",
        "	if (trie->data_size >= 8) {",
        "		u64 diff = be64_to_cpu(*(__be64 *)node->data ^",
        "				       *(__be64 *)key->data);",
        "",
        "		prefixlen = 64 - fls64(diff);",
        "		if (prefixlen >= limit)",
        "			return limit;",
        "		if (diff)",
        "			return prefixlen;",
        "		i = 8;",
        "	}",
        "#endif",
        "",
        "	while (trie->data_size >= i + 4) {",
        "		u32 diff = be32_to_cpu(*(__be32 *)&node->data[i] ^",
        "				       *(__be32 *)&key->data[i]);",
        "",
        "		prefixlen += 32 - fls(diff);",
        "		if (prefixlen >= limit)",
        "			return limit;",
        "		if (diff)",
        "			return prefixlen;",
        "		i += 4;",
        "	}",
        "",
        "	if (trie->data_size >= i + 2) {",
        "		u16 diff = be16_to_cpu(*(__be16 *)&node->data[i] ^",
        "				       *(__be16 *)&key->data[i]);",
        "",
        "		prefixlen += 16 - fls(diff);",
        "		if (prefixlen >= limit)",
        "			return limit;",
        "		if (diff)",
        "			return prefixlen;",
        "		i += 2;",
        "	}",
        "",
        "	if (trie->data_size >= i + 1) {",
        "		prefixlen += 8 - fls(node->data[i] ^ key->data[i]);",
        "",
        "		if (prefixlen >= limit)",
        "			return limit;",
        "	}",
        "",
        "	return prefixlen;",
        "}",
        "",
        "static size_t longest_prefix_match(const struct lpm_trie *trie,",
        "				   const struct lpm_trie_node *node,",
        "				   const struct bpf_lpm_trie_key_u8 *key)",
        "{",
        "	return __longest_prefix_match(trie, node, key);",
        "}",
        "",
        "/* Called from syscall or from eBPF program */",
        "static void *trie_lookup_elem(struct bpf_map *map, void *_key)",
        "{",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	struct lpm_trie_node *node, *found = NULL;",
        "	struct bpf_lpm_trie_key_u8 *key = _key;",
        "",
        "	if (key->prefixlen > trie->max_prefixlen)",
        "		return NULL;",
        "",
        "	/* Start walking the trie from the root node ... */",
        "",
        "	for (node = rcu_dereference_check(trie->root, rcu_read_lock_bh_held());",
        "	     node;) {",
        "		unsigned int next_bit;",
        "		size_t matchlen;",
        "",
        "		/* Determine the longest prefix of @node that matches @key.",
        "		 * If it's the maximum possible prefix for this trie, we have",
        "		 * an exact match and can return it directly.",
        "		 */",
        "		matchlen = __longest_prefix_match(trie, node, key);",
        "		if (matchlen == trie->max_prefixlen) {",
        "			found = node;",
        "			break;",
        "		}",
        "",
        "		/* If the number of bits that match is smaller than the prefix",
        "		 * length of @node, bail out and return the node we have seen",
        "		 * last in the traversal (ie, the parent).",
        "		 */",
        "		if (matchlen < node->prefixlen)",
        "			break;",
        "",
        "		/* Consider this node as return candidate unless it is an",
        "		 * artificially added intermediate one.",
        "		 */",
        "		if (!(node->flags & LPM_TREE_NODE_FLAG_IM))",
        "			found = node;",
        "",
        "		/* If the node match is fully satisfied, let's see if we can",
        "		 * become more specific. Determine the next bit in the key and",
        "		 * traverse down.",
        "		 */",
        "		next_bit = extract_bit(key->data, node->prefixlen);",
        "		node = rcu_dereference_check(node->child[next_bit],",
        "					     rcu_read_lock_bh_held());",
        "	}",
        "",
        "	if (!found)",
        "		return NULL;",
        "",
        "	return found->data + trie->data_size;",
        "}",
        "",
        "static struct lpm_trie_node *lpm_trie_node_alloc(struct lpm_trie *trie,",
        "						 const void *value,",
        "						 bool disable_migration)",
        "{",
        "	struct lpm_trie_node *node;",
        "",
        "	if (disable_migration)",
        "		migrate_disable();",
        "	node = bpf_mem_cache_alloc(&trie->ma);",
        "	if (disable_migration)",
        "		migrate_enable();",
        "",
        "	if (!node)",
        "		return NULL;",
        "",
        "	node->flags = 0;",
        "",
        "	if (value)",
        "		memcpy(node->data + trie->data_size, value,",
        "		       trie->map.value_size);",
        "",
        "	return node;",
        "}",
        "",
        "static int trie_check_add_elem(struct lpm_trie *trie, u64 flags)",
        "{",
        "	if (flags == BPF_EXIST)",
        "		return -ENOENT;",
        "	if (trie->n_entries == trie->map.max_entries)",
        "		return -ENOSPC;",
        "	trie->n_entries++;",
        "	return 0;",
        "}",
        "",
        "/* Called from syscall or from eBPF program */",
        "static long trie_update_elem(struct bpf_map *map,",
        "			     void *_key, void *value, u64 flags)",
        "{",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	struct lpm_trie_node *node, *im_node, *new_node;",
        "	struct lpm_trie_node *free_node = NULL;",
        "	struct lpm_trie_node __rcu **slot;",
        "	struct bpf_lpm_trie_key_u8 *key = _key;",
        "	unsigned long irq_flags;",
        "	unsigned int next_bit;",
        "	size_t matchlen = 0;",
        "	int ret = 0;",
        "",
        "	if (unlikely(flags > BPF_EXIST))",
        "		return -EINVAL;",
        "",
        "	if (key->prefixlen > trie->max_prefixlen)",
        "		return -EINVAL;",
        "",
        "	/* Allocate and fill a new node. Need to disable migration before",
        "	 * invoking bpf_mem_cache_alloc().",
        "	 */",
        "	new_node = lpm_trie_node_alloc(trie, value, true);",
        "	if (!new_node)",
        "		return -ENOMEM;",
        "",
        "	raw_spin_lock_irqsave(&trie->lock, irq_flags);",
        "",
        "	new_node->prefixlen = key->prefixlen;",
        "	RCU_INIT_POINTER(new_node->child[0], NULL);",
        "	RCU_INIT_POINTER(new_node->child[1], NULL);",
        "	memcpy(new_node->data, key->data, trie->data_size);",
        "",
        "	/* Now find a slot to attach the new node. To do that, walk the tree",
        "	 * from the root and match as many bits as possible for each node until",
        "	 * we either find an empty slot or a slot that needs to be replaced by",
        "	 * an intermediate node.",
        "	 */",
        "	slot = &trie->root;",
        "",
        "	while ((node = rcu_dereference_protected(*slot,",
        "					lockdep_is_held(&trie->lock)))) {",
        "		matchlen = longest_prefix_match(trie, node, key);",
        "",
        "		if (node->prefixlen != matchlen ||",
        "		    node->prefixlen == key->prefixlen)",
        "			break;",
        "",
        "		next_bit = extract_bit(key->data, node->prefixlen);",
        "		slot = &node->child[next_bit];",
        "	}",
        "",
        "	/* If the slot is empty (a free child pointer or an empty root),",
        "	 * simply assign the @new_node to that slot and be done.",
        "	 */",
        "	if (!node) {",
        "		ret = trie_check_add_elem(trie, flags);",
        "		if (ret)",
        "			goto out;",
        "",
        "		rcu_assign_pointer(*slot, new_node);",
        "		goto out;",
        "	}",
        "",
        "	/* If the slot we picked already exists, replace it with @new_node",
        "	 * which already has the correct data array set.",
        "	 */",
        "	if (node->prefixlen == matchlen) {",
        "		if (!(node->flags & LPM_TREE_NODE_FLAG_IM)) {",
        "			if (flags == BPF_NOEXIST) {",
        "				ret = -EEXIST;",
        "				goto out;",
        "			}",
        "		} else {",
        "			ret = trie_check_add_elem(trie, flags);",
        "			if (ret)",
        "				goto out;",
        "		}",
        "",
        "		new_node->child[0] = node->child[0];",
        "		new_node->child[1] = node->child[1];",
        "",
        "		rcu_assign_pointer(*slot, new_node);",
        "		free_node = node;",
        "",
        "		goto out;",
        "	}",
        "",
        "	ret = trie_check_add_elem(trie, flags);",
        "	if (ret)",
        "		goto out;",
        "",
        "	/* If the new node matches the prefix completely, it must be inserted",
        "	 * as an ancestor. Simply insert it between @node and *@slot.",
        "	 */",
        "	if (matchlen == key->prefixlen) {",
        "		next_bit = extract_bit(node->data, matchlen);",
        "		rcu_assign_pointer(new_node->child[next_bit], node);",
        "		rcu_assign_pointer(*slot, new_node);",
        "		goto out;",
        "	}",
        "",
        "	/* migration is disabled within the locked scope */",
        "	im_node = lpm_trie_node_alloc(trie, NULL, false);",
        "	if (!im_node) {",
        "		trie->n_entries--;",
        "		ret = -ENOMEM;",
        "		goto out;",
        "	}",
        "",
        "	im_node->prefixlen = matchlen;",
        "	im_node->flags |= LPM_TREE_NODE_FLAG_IM;",
        "	memcpy(im_node->data, node->data, trie->data_size);",
        "",
        "	/* Now determine which child to install in which slot */",
        "	if (extract_bit(key->data, matchlen)) {",
        "		rcu_assign_pointer(im_node->child[0], node);",
        "		rcu_assign_pointer(im_node->child[1], new_node);",
        "	} else {",
        "		rcu_assign_pointer(im_node->child[0], new_node);",
        "		rcu_assign_pointer(im_node->child[1], node);",
        "	}",
        "",
        "	/* Finally, assign the intermediate node to the determined slot */",
        "	rcu_assign_pointer(*slot, im_node);",
        "",
        "out:",
        "	raw_spin_unlock_irqrestore(&trie->lock, irq_flags);",
        "",
        "	migrate_disable();",
        "	if (ret)",
        "		bpf_mem_cache_free(&trie->ma, new_node);",
        "	bpf_mem_cache_free_rcu(&trie->ma, free_node);",
        "	migrate_enable();",
        "",
        "	return ret;",
        "}",
        "",
        "/* Called from syscall or from eBPF program */",
        "static long trie_delete_elem(struct bpf_map *map, void *_key)",
        "{",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	struct lpm_trie_node *free_node = NULL, *free_parent = NULL;",
        "	struct bpf_lpm_trie_key_u8 *key = _key;",
        "	struct lpm_trie_node __rcu **trim, **trim2;",
        "	struct lpm_trie_node *node, *parent;",
        "	unsigned long irq_flags;",
        "	unsigned int next_bit;",
        "	size_t matchlen = 0;",
        "	int ret = 0;",
        "",
        "	if (key->prefixlen > trie->max_prefixlen)",
        "		return -EINVAL;",
        "",
        "	raw_spin_lock_irqsave(&trie->lock, irq_flags);",
        "",
        "	/* Walk the tree looking for an exact key/length match and keeping",
        "	 * track of the path we traverse.  We will need to know the node",
        "	 * we wish to delete, and the slot that points to the node we want",
        "	 * to delete.  We may also need to know the nodes parent and the",
        "	 * slot that contains it.",
        "	 */",
        "	trim = &trie->root;",
        "	trim2 = trim;",
        "	parent = NULL;",
        "	while ((node = rcu_dereference_protected(",
        "		       *trim, lockdep_is_held(&trie->lock)))) {",
        "		matchlen = longest_prefix_match(trie, node, key);",
        "",
        "		if (node->prefixlen != matchlen ||",
        "		    node->prefixlen == key->prefixlen)",
        "			break;",
        "",
        "		parent = node;",
        "		trim2 = trim;",
        "		next_bit = extract_bit(key->data, node->prefixlen);",
        "		trim = &node->child[next_bit];",
        "	}",
        "",
        "	if (!node || node->prefixlen != key->prefixlen ||",
        "	    node->prefixlen != matchlen ||",
        "	    (node->flags & LPM_TREE_NODE_FLAG_IM)) {",
        "		ret = -ENOENT;",
        "		goto out;",
        "	}",
        "",
        "	trie->n_entries--;",
        "",
        "	/* If the node we are removing has two children, simply mark it",
        "	 * as intermediate and we are done.",
        "	 */",
        "	if (rcu_access_pointer(node->child[0]) &&",
        "	    rcu_access_pointer(node->child[1])) {",
        "		node->flags |= LPM_TREE_NODE_FLAG_IM;",
        "		goto out;",
        "	}",
        "",
        "	/* If the parent of the node we are about to delete is an intermediate",
        "	 * node, and the deleted node doesn't have any children, we can delete",
        "	 * the intermediate parent as well and promote its other child",
        "	 * up the tree.  Doing this maintains the invariant that all",
        "	 * intermediate nodes have exactly 2 children and that there are no",
        "	 * unnecessary intermediate nodes in the tree.",
        "	 */",
        "	if (parent && (parent->flags & LPM_TREE_NODE_FLAG_IM) &&",
        "	    !node->child[0] && !node->child[1]) {",
        "		if (node == rcu_access_pointer(parent->child[0]))",
        "			rcu_assign_pointer(",
        "				*trim2, rcu_access_pointer(parent->child[1]));",
        "		else",
        "			rcu_assign_pointer(",
        "				*trim2, rcu_access_pointer(parent->child[0]));",
        "		free_parent = parent;",
        "		free_node = node;",
        "		goto out;",
        "	}",
        "",
        "	/* The node we are removing has either zero or one child. If there",
        "	 * is a child, move it into the removed node's slot then delete",
        "	 * the node.  Otherwise just clear the slot and delete the node.",
        "	 */",
        "	if (node->child[0])",
        "		rcu_assign_pointer(*trim, rcu_access_pointer(node->child[0]));",
        "	else if (node->child[1])",
        "		rcu_assign_pointer(*trim, rcu_access_pointer(node->child[1]));",
        "	else",
        "		RCU_INIT_POINTER(*trim, NULL);",
        "	free_node = node;",
        "",
        "out:",
        "	raw_spin_unlock_irqrestore(&trie->lock, irq_flags);",
        "",
        "	migrate_disable();",
        "	bpf_mem_cache_free_rcu(&trie->ma, free_parent);",
        "	bpf_mem_cache_free_rcu(&trie->ma, free_node);",
        "	migrate_enable();",
        "",
        "	return ret;",
        "}",
        "",
        "#define LPM_DATA_SIZE_MAX	256",
        "#define LPM_DATA_SIZE_MIN	1",
        "",
        "#define LPM_VAL_SIZE_MAX	(KMALLOC_MAX_SIZE - LPM_DATA_SIZE_MAX - \\",
        "				 sizeof(struct lpm_trie_node))",
        "#define LPM_VAL_SIZE_MIN	1",
        "",
        "#define LPM_KEY_SIZE(X)		(sizeof(struct bpf_lpm_trie_key_u8) + (X))",
        "#define LPM_KEY_SIZE_MAX	LPM_KEY_SIZE(LPM_DATA_SIZE_MAX)",
        "#define LPM_KEY_SIZE_MIN	LPM_KEY_SIZE(LPM_DATA_SIZE_MIN)",
        "",
        "#define LPM_CREATE_FLAG_MASK	(BPF_F_NO_PREALLOC | BPF_F_NUMA_NODE |	\\",
        "				 BPF_F_ACCESS_MASK)",
        "",
        "static struct bpf_map *trie_alloc(union bpf_attr *attr)",
        "{",
        "	struct lpm_trie *trie;",
        "	size_t leaf_size;",
        "	int err;",
        "",
        "	/* check sanity of attributes */",
        "	if (attr->max_entries == 0 ||",
        "	    !(attr->map_flags & BPF_F_NO_PREALLOC) ||",
        "	    attr->map_flags & ~LPM_CREATE_FLAG_MASK ||",
        "	    !bpf_map_flags_access_ok(attr->map_flags) ||",
        "	    attr->key_size < LPM_KEY_SIZE_MIN ||",
        "	    attr->key_size > LPM_KEY_SIZE_MAX ||",
        "	    attr->value_size < LPM_VAL_SIZE_MIN ||",
        "	    attr->value_size > LPM_VAL_SIZE_MAX)",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	trie = bpf_map_area_alloc(sizeof(*trie), NUMA_NO_NODE);",
        "	if (!trie)",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	/* copy mandatory map attributes */",
        "	bpf_map_init_from_attr(&trie->map, attr);",
        "	trie->data_size = attr->key_size -",
        "			  offsetof(struct bpf_lpm_trie_key_u8, data);",
        "	trie->max_prefixlen = trie->data_size * 8;",
        "",
        "	raw_spin_lock_init(&trie->lock);",
        "",
        "	/* Allocate intermediate and leaf nodes from the same allocator */",
        "	leaf_size = sizeof(struct lpm_trie_node) + trie->data_size +",
        "		    trie->map.value_size;",
        "	err = bpf_mem_alloc_init(&trie->ma, leaf_size, false);",
        "	if (err)",
        "		goto free_out;",
        "	return &trie->map;",
        "",
        "free_out:",
        "	bpf_map_area_free(trie);",
        "	return ERR_PTR(err);",
        "}",
        "",
        "static void trie_free(struct bpf_map *map)",
        "{",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	struct lpm_trie_node __rcu **slot;",
        "	struct lpm_trie_node *node;",
        "",
        "	/* Always start at the root and walk down to a node that has no",
        "	 * children. Then free that node, nullify its reference in the parent",
        "	 * and start over.",
        "	 */",
        "",
        "	for (;;) {",
        "		slot = &trie->root;",
        "",
        "		for (;;) {",
        "			node = rcu_dereference_protected(*slot, 1);",
        "			if (!node)",
        "				goto out;",
        "",
        "			if (rcu_access_pointer(node->child[0])) {",
        "				slot = &node->child[0];",
        "				continue;",
        "			}",
        "",
        "			if (rcu_access_pointer(node->child[1])) {",
        "				slot = &node->child[1];",
        "				continue;",
        "			}",
        "",
        "			/* No bpf program may access the map, so freeing the",
        "			 * node without waiting for the extra RCU GP.",
        "			 */",
        "			bpf_mem_cache_raw_free(node);",
        "			RCU_INIT_POINTER(*slot, NULL);",
        "			break;",
        "		}",
        "	}",
        "",
        "out:",
        "	bpf_mem_alloc_destroy(&trie->ma);",
        "	bpf_map_area_free(trie);",
        "}",
        "",
        "static int trie_get_next_key(struct bpf_map *map, void *_key, void *_next_key)",
        "{",
        "	struct lpm_trie_node *node, *next_node = NULL, *parent, *search_root;",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	struct bpf_lpm_trie_key_u8 *key = _key, *next_key = _next_key;",
        "	struct lpm_trie_node **node_stack = NULL;",
        "	int err = 0, stack_ptr = -1;",
        "	unsigned int next_bit;",
        "	size_t matchlen = 0;",
        "",
        "	/* The get_next_key follows postorder. For the 4 node example in",
        "	 * the top of this file, the trie_get_next_key() returns the following",
        "	 * one after another:",
        "	 *   192.168.0.0/24",
        "	 *   192.168.1.0/24",
        "	 *   192.168.128.0/24",
        "	 *   192.168.0.0/16",
        "	 *",
        "	 * The idea is to return more specific keys before less specific ones.",
        "	 */",
        "",
        "	/* Empty trie */",
        "	search_root = rcu_dereference(trie->root);",
        "	if (!search_root)",
        "		return -ENOENT;",
        "",
        "	/* For invalid key, find the leftmost node in the trie */",
        "	if (!key || key->prefixlen > trie->max_prefixlen)",
        "		goto find_leftmost;",
        "",
        "	node_stack = kmalloc_array(trie->max_prefixlen + 1,",
        "				   sizeof(struct lpm_trie_node *),",
        "				   GFP_ATOMIC | __GFP_NOWARN);",
        "	if (!node_stack)",
        "		return -ENOMEM;",
        "",
        "	/* Try to find the exact node for the given key */",
        "	for (node = search_root; node;) {",
        "		node_stack[++stack_ptr] = node;",
        "		matchlen = longest_prefix_match(trie, node, key);",
        "		if (node->prefixlen != matchlen ||",
        "		    node->prefixlen == key->prefixlen)",
        "			break;",
        "",
        "		next_bit = extract_bit(key->data, node->prefixlen);",
        "		node = rcu_dereference(node->child[next_bit]);",
        "	}",
        "	if (!node || node->prefixlen != matchlen ||",
        "	    (node->flags & LPM_TREE_NODE_FLAG_IM))",
        "		goto find_leftmost;",
        "",
        "	/* The node with the exactly-matching key has been found,",
        "	 * find the first node in postorder after the matched node.",
        "	 */",
        "	node = node_stack[stack_ptr];",
        "	while (stack_ptr > 0) {",
        "		parent = node_stack[stack_ptr - 1];",
        "		if (rcu_dereference(parent->child[0]) == node) {",
        "			search_root = rcu_dereference(parent->child[1]);",
        "			if (search_root)",
        "				goto find_leftmost;",
        "		}",
        "		if (!(parent->flags & LPM_TREE_NODE_FLAG_IM)) {",
        "			next_node = parent;",
        "			goto do_copy;",
        "		}",
        "",
        "		node = parent;",
        "		stack_ptr--;",
        "	}",
        "",
        "	/* did not find anything */",
        "	err = -ENOENT;",
        "	goto free_stack;",
        "",
        "find_leftmost:",
        "	/* Find the leftmost non-intermediate node, all intermediate nodes",
        "	 * have exact two children, so this function will never return NULL.",
        "	 */",
        "	for (node = search_root; node;) {",
        "		if (node->flags & LPM_TREE_NODE_FLAG_IM) {",
        "			node = rcu_dereference(node->child[0]);",
        "		} else {",
        "			next_node = node;",
        "			node = rcu_dereference(node->child[0]);",
        "			if (!node)",
        "				node = rcu_dereference(next_node->child[1]);",
        "		}",
        "	}",
        "do_copy:",
        "	next_key->prefixlen = next_node->prefixlen;",
        "	memcpy((void *)next_key + offsetof(struct bpf_lpm_trie_key_u8, data),",
        "	       next_node->data, trie->data_size);",
        "free_stack:",
        "	kfree(node_stack);",
        "	return err;",
        "}",
        "",
        "static int trie_check_btf(const struct bpf_map *map,",
        "			  const struct btf *btf,",
        "			  const struct btf_type *key_type,",
        "			  const struct btf_type *value_type)",
        "{",
        "	/* Keys must have struct bpf_lpm_trie_key_u8 embedded. */",
        "	return BTF_INFO_KIND(key_type->info) != BTF_KIND_STRUCT ?",
        "	       -EINVAL : 0;",
        "}",
        "",
        "static u64 trie_mem_usage(const struct bpf_map *map)",
        "{",
        "	struct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
        "	u64 elem_size;",
        "",
        "	elem_size = sizeof(struct lpm_trie_node) + trie->data_size +",
        "			    trie->map.value_size;",
        "	return elem_size * READ_ONCE(trie->n_entries);",
        "}",
        "",
        "BTF_ID_LIST_SINGLE(trie_map_btf_ids, struct, lpm_trie)",
        "const struct bpf_map_ops trie_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc = trie_alloc,",
        "	.map_free = trie_free,",
        "	.map_get_next_key = trie_get_next_key,",
        "	.map_lookup_elem = trie_lookup_elem,",
        "	.map_update_elem = trie_update_elem,",
        "	.map_delete_elem = trie_delete_elem,",
        "	.map_lookup_batch = generic_map_lookup_batch,",
        "	.map_update_batch = generic_map_update_batch,",
        "	.map_delete_batch = generic_map_delete_batch,",
        "	.map_check_btf = trie_check_btf,",
        "	.map_mem_usage = trie_mem_usage,",
        "	.map_btf_id = &trie_map_btf_ids[0],",
        "};"
    ]
  },
  "include_linux_cpumask_h": {
    path: "include/linux/cpumask.h",
    covered: [178, 283],
    totalLines: 1307,
    coveredCount: 2,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef __LINUX_CPUMASK_H",
        "#define __LINUX_CPUMASK_H",
        "",
        "/*",
        " * Cpumasks provide a bitmap suitable for representing the",
        " * set of CPUs in a system, one bit position per CPU number.  In general,",
        " * only nr_cpu_ids (<= NR_CPUS) bits are valid.",
        " */",
        "#include <linux/cleanup.h>",
        "#include <linux/kernel.h>",
        "#include <linux/bitmap.h>",
        "#include <linux/cpumask_types.h>",
        "#include <linux/atomic.h>",
        "#include <linux/bug.h>",
        "#include <linux/gfp_types.h>",
        "#include <linux/numa.h>",
        "",
        "/**",
        " * cpumask_pr_args - printf args to output a cpumask",
        " * @maskp: cpumask to be printed",
        " *",
        " * Can be used to provide arguments for '%*pb[l]' when printing a cpumask.",
        " */",
        "#define cpumask_pr_args(maskp)		nr_cpu_ids, cpumask_bits(maskp)",
        "",
        "#if (NR_CPUS == 1) || defined(CONFIG_FORCE_NR_CPUS)",
        "#define nr_cpu_ids ((unsigned int)NR_CPUS)",
        "#else",
        "extern unsigned int nr_cpu_ids;",
        "#endif",
        "",
        "static __always_inline void set_nr_cpu_ids(unsigned int nr)",
        "{",
        "#if (NR_CPUS == 1) || defined(CONFIG_FORCE_NR_CPUS)",
        "	WARN_ON(nr != nr_cpu_ids);",
        "#else",
        "	nr_cpu_ids = nr;",
        "#endif",
        "}",
        "",
        "/*",
        " * We have several different \"preferred sizes\" for the cpumask",
        " * operations, depending on operation.",
        " *",
        " * For example, the bitmap scanning and operating operations have",
        " * optimized routines that work for the single-word case, but only when",
        " * the size is constant. So if NR_CPUS fits in one single word, we are",
        " * better off using that small constant, in order to trigger the",
        " * optimized bit finding. That is 'small_cpumask_size'.",
        " *",
        " * The clearing and copying operations will similarly perform better",
        " * with a constant size, but we limit that size arbitrarily to four",
        " * words. We call this 'large_cpumask_size'.",
        " *",
        " * Finally, some operations just want the exact limit, either because",
        " * they set bits or just don't have any faster fixed-sized versions. We",
        " * call this just 'nr_cpumask_bits'.",
        " *",
        " * Note that these optional constants are always guaranteed to be at",
        " * least as big as 'nr_cpu_ids' itself is, and all our cpumask",
        " * allocations are at least that size (see cpumask_size()). The",
        " * optimization comes from being able to potentially use a compile-time",
        " * constant instead of a run-time generated exact number of CPUs.",
        " */",
        "#if NR_CPUS <= BITS_PER_LONG",
        "  #define small_cpumask_bits ((unsigned int)NR_CPUS)",
        "  #define large_cpumask_bits ((unsigned int)NR_CPUS)",
        "#elif NR_CPUS <= 4*BITS_PER_LONG",
        "  #define small_cpumask_bits nr_cpu_ids",
        "  #define large_cpumask_bits ((unsigned int)NR_CPUS)",
        "#else",
        "  #define small_cpumask_bits nr_cpu_ids",
        "  #define large_cpumask_bits nr_cpu_ids",
        "#endif",
        "#define nr_cpumask_bits nr_cpu_ids",
        "",
        "/*",
        " * The following particular system cpumasks and operations manage",
        " * possible, present, active and online cpus.",
        " *",
        " *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable",
        " *     cpu_present_mask - has bit 'cpu' set iff cpu is populated",
        " *     cpu_enabled_mask  - has bit 'cpu' set iff cpu can be brought online",
        " *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler",
        " *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration",
        " *",
        " *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.",
        " *",
        " *  The cpu_possible_mask is fixed at boot time, as the set of CPU IDs",
        " *  that it is possible might ever be plugged in at anytime during the",
        " *  life of that system boot.  The cpu_present_mask is dynamic(*),",
        " *  representing which CPUs are currently plugged in.  And",
        " *  cpu_online_mask is the dynamic subset of cpu_present_mask,",
        " *  indicating those CPUs available for scheduling.",
        " *",
        " *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,",
        " *  depending on what ACPI reports as currently plugged in, otherwise",
        " *  cpu_present_mask is just a copy of cpu_possible_mask.",
        " *",
        " *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not",
        " *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.",
        " *",
        " * Subtleties:",
        " * 1) UP ARCHes (NR_CPUS == 1, CONFIG_SMP not defined) hardcode",
        " *    assumption that their single CPU is online.  The UP",
        " *    cpu_{online,possible,present}_masks are placebos.  Changing them",
        " *    will have no useful affect on the following num_*_cpus()",
        " *    and cpu_*() macros in the UP case.  This ugliness is a UP",
        " *    optimization - don't waste any instructions or memory references",
        " *    asking if you're online or how many CPUs there are if there is",
        " *    only one CPU.",
        " */",
        "",
        "extern struct cpumask __cpu_possible_mask;",
        "extern struct cpumask __cpu_online_mask;",
        "extern struct cpumask __cpu_enabled_mask;",
        "extern struct cpumask __cpu_present_mask;",
        "extern struct cpumask __cpu_active_mask;",
        "extern struct cpumask __cpu_dying_mask;",
        "#define cpu_possible_mask ((const struct cpumask *)&__cpu_possible_mask)",
        "#define cpu_online_mask   ((const struct cpumask *)&__cpu_online_mask)",
        "#define cpu_enabled_mask   ((const struct cpumask *)&__cpu_enabled_mask)",
        "#define cpu_present_mask  ((const struct cpumask *)&__cpu_present_mask)",
        "#define cpu_active_mask   ((const struct cpumask *)&__cpu_active_mask)",
        "#define cpu_dying_mask    ((const struct cpumask *)&__cpu_dying_mask)",
        "",
        "extern atomic_t __num_online_cpus;",
        "",
        "extern cpumask_t cpus_booted_once_mask;",
        "",
        "static __always_inline void cpu_max_bits_warn(unsigned int cpu, unsigned int bits)",
        "{",
        "#ifdef CONFIG_DEBUG_PER_CPU_MAPS",
        "	WARN_ON_ONCE(cpu >= bits);",
        "#endif /* CONFIG_DEBUG_PER_CPU_MAPS */",
        "}",
        "",
        "/* verify cpu argument to cpumask_* operators */",
        "static __always_inline unsigned int cpumask_check(unsigned int cpu)",
        "{",
        "	cpu_max_bits_warn(cpu, small_cpumask_bits);",
        "	return cpu;",
        "}",
        "",
        "/**",
        " * cpumask_first - get the first cpu in a cpumask",
        " * @srcp: the cpumask pointer",
        " *",
        " * Return: >= nr_cpu_ids if no cpus set.",
        " */",
        "static __always_inline unsigned int cpumask_first(const struct cpumask *srcp)",
        "{",
        "	return find_first_bit(cpumask_bits(srcp), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_first_zero - get the first unset cpu in a cpumask",
        " * @srcp: the cpumask pointer",
        " *",
        " * Return: >= nr_cpu_ids if all cpus are set.",
        " */",
        "static __always_inline unsigned int cpumask_first_zero(const struct cpumask *srcp)",
        "{",
        "	return find_first_zero_bit(cpumask_bits(srcp), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_first_and - return the first cpu from *srcp1 & *srcp2",
        " * @srcp1: the first input",
        " * @srcp2: the second input",
        " *",
        " * Return: >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().",
        " */",
        "static __always_inline",
        "unsigned int cpumask_first_and(const struct cpumask *srcp1, const struct cpumask *srcp2)",
        "{",
        "	return find_first_and_bit(cpumask_bits(srcp1), cpumask_bits(srcp2), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_first_and_and - return the first cpu from *srcp1 & *srcp2 & *srcp3",
        " * @srcp1: the first input",
        " * @srcp2: the second input",
        " * @srcp3: the third input",
        " *",
        " * Return: >= nr_cpu_ids if no cpus set in all.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_first_and_and(const struct cpumask *srcp1,",
        "				   const struct cpumask *srcp2,",
        "				   const struct cpumask *srcp3)",
        "{",
        "	return find_first_and_and_bit(cpumask_bits(srcp1), cpumask_bits(srcp2),",
        "				      cpumask_bits(srcp3), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_last - get the last CPU in a cpumask",
        " * @srcp:	- the cpumask pointer",
        " *",
        " * Return:	>= nr_cpumask_bits if no CPUs set.",
        " */",
        "static __always_inline unsigned int cpumask_last(const struct cpumask *srcp)",
        "{",
        "	return find_last_bit(cpumask_bits(srcp), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_next - get the next cpu in a cpumask",
        " * @n: the cpu prior to the place to search (i.e. return will be > @n)",
        " * @srcp: the cpumask pointer",
        " *",
        " * Return: >= nr_cpu_ids if no further cpus set.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_next(int n, const struct cpumask *srcp)",
        "{",
        "	/* -1 is a legal arg here. */",
        "	if (n != -1)",
        "		cpumask_check(n);",
        "	return find_next_bit(cpumask_bits(srcp), small_cpumask_bits, n + 1);",
        "}",
        "",
        "/**",
        " * cpumask_next_zero - get the next unset cpu in a cpumask",
        " * @n: the cpu prior to the place to search (i.e. return will be > @n)",
        " * @srcp: the cpumask pointer",
        " *",
        " * Return: >= nr_cpu_ids if no further cpus unset.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)",
        "{",
        "	/* -1 is a legal arg here. */",
        "	if (n != -1)",
        "		cpumask_check(n);",
        "	return find_next_zero_bit(cpumask_bits(srcp), small_cpumask_bits, n+1);",
        "}",
        "",
        "#if NR_CPUS == 1",
        "/* Uniprocessor: there is only one valid CPU */",
        "static __always_inline",
        "unsigned int cpumask_local_spread(unsigned int i, int node)",
        "{",
        "	return 0;",
        "}",
        "",
        "static __always_inline",
        "unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,",
        "					const struct cpumask *src2p)",
        "{",
        "	return cpumask_first_and(src1p, src2p);",
        "}",
        "",
        "static __always_inline",
        "unsigned int cpumask_any_distribute(const struct cpumask *srcp)",
        "{",
        "	return cpumask_first(srcp);",
        "}",
        "#else",
        "unsigned int cpumask_local_spread(unsigned int i, int node);",
        "unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,",
        "			       const struct cpumask *src2p);",
        "unsigned int cpumask_any_distribute(const struct cpumask *srcp);",
        "#endif /* NR_CPUS */",
        "",
        "/**",
        " * cpumask_next_and - get the next cpu in *src1p & *src2p",
        " * @n: the cpu prior to the place to search (i.e. return will be > @n)",
        " * @src1p: the first cpumask pointer",
        " * @src2p: the second cpumask pointer",
        " *",
        " * Return: >= nr_cpu_ids if no further cpus set in both.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_next_and(int n, const struct cpumask *src1p,",
        "			      const struct cpumask *src2p)",
        "{",
        "	/* -1 is a legal arg here. */",
        "	if (n != -1)",
        "		cpumask_check(n);",
        "	return find_next_and_bit(cpumask_bits(src1p), cpumask_bits(src2p),",
        "		small_cpumask_bits, n + 1);",
        "}",
        "",
        "/**",
        " * for_each_cpu - iterate over every cpu in a mask",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask: the cpumask pointer",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu(cpu, mask)				\\",
        "	for_each_set_bit(cpu, cpumask_bits(mask), small_cpumask_bits)",
        "",
        "#if NR_CPUS == 1",
        "static __always_inline",
        "unsigned int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap)",
        "{",
        "	cpumask_check(start);",
        "	if (n != -1)",
        "		cpumask_check(n);",
        "",
        "	/*",
        "	 * Return the first available CPU when wrapping, or when starting before cpu0,",
        "	 * since there is only one valid option.",
        "	 */",
        "	if (wrap && n >= 0)",
        "		return nr_cpumask_bits;",
        "",
        "	return cpumask_first(mask);",
        "}",
        "#else",
        "unsigned int __pure cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);",
        "#endif",
        "",
        "/**",
        " * for_each_cpu_wrap - iterate over every cpu in a mask, starting at a specified location",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask: the cpumask pointer",
        " * @start: the start location",
        " *",
        " * The implementation does not assume any bit in @mask is set (including @start).",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu_wrap(cpu, mask, start)				\\",
        "	for_each_set_bit_wrap(cpu, cpumask_bits(mask), small_cpumask_bits, start)",
        "",
        "/**",
        " * for_each_cpu_and - iterate over every cpu in both masks",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask1: the first cpumask pointer",
        " * @mask2: the second cpumask pointer",
        " *",
        " * This saves a temporary CPU mask in many places.  It is equivalent to:",
        " *	struct cpumask tmp;",
        " *	cpumask_and(&tmp, &mask1, &mask2);",
        " *	for_each_cpu(cpu, &tmp)",
        " *		...",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu_and(cpu, mask1, mask2)				\\",
        "	for_each_and_bit(cpu, cpumask_bits(mask1), cpumask_bits(mask2), small_cpumask_bits)",
        "",
        "/**",
        " * for_each_cpu_andnot - iterate over every cpu present in one mask, excluding",
        " *			 those present in another.",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask1: the first cpumask pointer",
        " * @mask2: the second cpumask pointer",
        " *",
        " * This saves a temporary CPU mask in many places.  It is equivalent to:",
        " *	struct cpumask tmp;",
        " *	cpumask_andnot(&tmp, &mask1, &mask2);",
        " *	for_each_cpu(cpu, &tmp)",
        " *		...",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu_andnot(cpu, mask1, mask2)				\\",
        "	for_each_andnot_bit(cpu, cpumask_bits(mask1), cpumask_bits(mask2), small_cpumask_bits)",
        "",
        "/**",
        " * for_each_cpu_or - iterate over every cpu present in either mask",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask1: the first cpumask pointer",
        " * @mask2: the second cpumask pointer",
        " *",
        " * This saves a temporary CPU mask in many places.  It is equivalent to:",
        " *	struct cpumask tmp;",
        " *	cpumask_or(&tmp, &mask1, &mask2);",
        " *	for_each_cpu(cpu, &tmp)",
        " *		...",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu_or(cpu, mask1, mask2)				\\",
        "	for_each_or_bit(cpu, cpumask_bits(mask1), cpumask_bits(mask2), small_cpumask_bits)",
        "",
        "/**",
        " * for_each_cpu_from - iterate over CPUs present in @mask, from @cpu to the end of @mask.",
        " * @cpu: the (optionally unsigned) integer iterator",
        " * @mask: the cpumask pointer",
        " *",
        " * After the loop, cpu is >= nr_cpu_ids.",
        " */",
        "#define for_each_cpu_from(cpu, mask)				\\",
        "	for_each_set_bit_from(cpu, cpumask_bits(mask), small_cpumask_bits)",
        "",
        "/**",
        " * cpumask_any_but - return a \"random\" in a cpumask, but not this one.",
        " * @mask: the cpumask to search",
        " * @cpu: the cpu to ignore.",
        " *",
        " * Often used to find any cpu but smp_processor_id() in a mask.",
        " * Return: >= nr_cpu_ids if no cpus set.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)",
        "{",
        "	unsigned int i;",
        "",
        "	cpumask_check(cpu);",
        "	for_each_cpu(i, mask)",
        "		if (i != cpu)",
        "			break;",
        "	return i;",
        "}",
        "",
        "/**",
        " * cpumask_any_and_but - pick a \"random\" cpu from *mask1 & *mask2, but not this one.",
        " * @mask1: the first input cpumask",
        " * @mask2: the second input cpumask",
        " * @cpu: the cpu to ignore",
        " *",
        " * Returns >= nr_cpu_ids if no cpus set.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_any_and_but(const struct cpumask *mask1,",
        "				 const struct cpumask *mask2,",
        "				 unsigned int cpu)",
        "{",
        "	unsigned int i;",
        "",
        "	cpumask_check(cpu);",
        "	i = cpumask_first_and(mask1, mask2);",
        "	if (i != cpu)",
        "		return i;",
        "",
        "	return cpumask_next_and(cpu, mask1, mask2);",
        "}",
        "",
        "/**",
        " * cpumask_nth - get the Nth cpu in a cpumask",
        " * @srcp: the cpumask pointer",
        " * @cpu: the Nth cpu to find, starting from 0",
        " *",
        " * Return: >= nr_cpu_ids if such cpu doesn't exist.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_nth(unsigned int cpu, const struct cpumask *srcp)",
        "{",
        "	return find_nth_bit(cpumask_bits(srcp), small_cpumask_bits, cpumask_check(cpu));",
        "}",
        "",
        "/**",
        " * cpumask_nth_and - get the Nth cpu in 2 cpumasks",
        " * @srcp1: the cpumask pointer",
        " * @srcp2: the cpumask pointer",
        " * @cpu: the Nth cpu to find, starting from 0",
        " *",
        " * Return: >= nr_cpu_ids if such cpu doesn't exist.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_nth_and(unsigned int cpu, const struct cpumask *srcp1,",
        "							const struct cpumask *srcp2)",
        "{",
        "	return find_nth_and_bit(cpumask_bits(srcp1), cpumask_bits(srcp2),",
        "				small_cpumask_bits, cpumask_check(cpu));",
        "}",
        "",
        "/**",
        " * cpumask_nth_andnot - get the Nth cpu set in 1st cpumask, and clear in 2nd.",
        " * @srcp1: the cpumask pointer",
        " * @srcp2: the cpumask pointer",
        " * @cpu: the Nth cpu to find, starting from 0",
        " *",
        " * Return: >= nr_cpu_ids if such cpu doesn't exist.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_nth_andnot(unsigned int cpu, const struct cpumask *srcp1,",
        "							const struct cpumask *srcp2)",
        "{",
        "	return find_nth_andnot_bit(cpumask_bits(srcp1), cpumask_bits(srcp2),",
        "				small_cpumask_bits, cpumask_check(cpu));",
        "}",
        "",
        "/**",
        " * cpumask_nth_and_andnot - get the Nth cpu set in 1st and 2nd cpumask, and clear in 3rd.",
        " * @srcp1: the cpumask pointer",
        " * @srcp2: the cpumask pointer",
        " * @srcp3: the cpumask pointer",
        " * @cpu: the Nth cpu to find, starting from 0",
        " *",
        " * Return: >= nr_cpu_ids if such cpu doesn't exist.",
        " */",
        "static __always_inline",
        "unsigned int cpumask_nth_and_andnot(unsigned int cpu, const struct cpumask *srcp1,",
        "							const struct cpumask *srcp2,",
        "							const struct cpumask *srcp3)",
        "{",
        "	return find_nth_and_andnot_bit(cpumask_bits(srcp1),",
        "					cpumask_bits(srcp2),",
        "					cpumask_bits(srcp3),",
        "					small_cpumask_bits, cpumask_check(cpu));",
        "}",
        "",
        "#define CPU_BITS_NONE						\\",
        "{								\\",
        "	[0 ... BITS_TO_LONGS(NR_CPUS)-1] = 0UL			\\",
        "}",
        "",
        "#define CPU_BITS_CPU0						\\",
        "{								\\",
        "	[0] =  1UL						\\",
        "}",
        "",
        "/**",
        " * cpumask_set_cpu - set a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @dstp: the cpumask pointer",
        " */",
        "static __always_inline",
        "void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)",
        "{",
        "	set_bit(cpumask_check(cpu), cpumask_bits(dstp));",
        "}",
        "",
        "static __always_inline",
        "void __cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)",
        "{",
        "	__set_bit(cpumask_check(cpu), cpumask_bits(dstp));",
        "}",
        "",
        "",
        "/**",
        " * cpumask_clear_cpu - clear a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @dstp: the cpumask pointer",
        " */",
        "static __always_inline void cpumask_clear_cpu(int cpu, struct cpumask *dstp)",
        "{",
        "	clear_bit(cpumask_check(cpu), cpumask_bits(dstp));",
        "}",
        "",
        "static __always_inline void __cpumask_clear_cpu(int cpu, struct cpumask *dstp)",
        "{",
        "	__clear_bit(cpumask_check(cpu), cpumask_bits(dstp));",
        "}",
        "",
        "/**",
        " * cpumask_assign_cpu - assign a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @dstp: the cpumask pointer",
        " * @bool: the value to assign",
        " */",
        "static __always_inline void cpumask_assign_cpu(int cpu, struct cpumask *dstp, bool value)",
        "{",
        "	assign_bit(cpumask_check(cpu), cpumask_bits(dstp), value);",
        "}",
        "",
        "static __always_inline void __cpumask_assign_cpu(int cpu, struct cpumask *dstp, bool value)",
        "{",
        "	__assign_bit(cpumask_check(cpu), cpumask_bits(dstp), value);",
        "}",
        "",
        "/**",
        " * cpumask_test_cpu - test for a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @cpumask: the cpumask pointer",
        " *",
        " * Return: true if @cpu is set in @cpumask, else returns false",
        " */",
        "static __always_inline",
        "bool cpumask_test_cpu(int cpu, const struct cpumask *cpumask)",
        "{",
        "	return test_bit(cpumask_check(cpu), cpumask_bits((cpumask)));",
        "}",
        "",
        "/**",
        " * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @cpumask: the cpumask pointer",
        " *",
        " * test_and_set_bit wrapper for cpumasks.",
        " *",
        " * Return: true if @cpu is set in old bitmap of @cpumask, else returns false",
        " */",
        "static __always_inline",
        "bool cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)",
        "{",
        "	return test_and_set_bit(cpumask_check(cpu), cpumask_bits(cpumask));",
        "}",
        "",
        "/**",
        " * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask",
        " * @cpu: cpu number (< nr_cpu_ids)",
        " * @cpumask: the cpumask pointer",
        " *",
        " * test_and_clear_bit wrapper for cpumasks.",
        " *",
        " * Return: true if @cpu is set in old bitmap of @cpumask, else returns false",
        " */",
        "static __always_inline",
        "bool cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)",
        "{",
        "	return test_and_clear_bit(cpumask_check(cpu), cpumask_bits(cpumask));",
        "}",
        "",
        "/**",
        " * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask",
        " * @dstp: the cpumask pointer",
        " */",
        "static __always_inline void cpumask_setall(struct cpumask *dstp)",
        "{",
        "	if (small_const_nbits(small_cpumask_bits)) {",
        "		cpumask_bits(dstp)[0] = BITMAP_LAST_WORD_MASK(nr_cpumask_bits);",
        "		return;",
        "	}",
        "	bitmap_fill(cpumask_bits(dstp), nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask",
        " * @dstp: the cpumask pointer",
        " */",
        "static __always_inline void cpumask_clear(struct cpumask *dstp)",
        "{",
        "	bitmap_zero(cpumask_bits(dstp), large_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_and - *dstp = *src1p & *src2p",
        " * @dstp: the cpumask result",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " *",
        " * Return: false if *@dstp is empty, else returns true",
        " */",
        "static __always_inline",
        "bool cpumask_and(struct cpumask *dstp, const struct cpumask *src1p,",
        "		 const struct cpumask *src2p)",
        "{",
        "	return bitmap_and(cpumask_bits(dstp), cpumask_bits(src1p),",
        "				       cpumask_bits(src2p), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_or - *dstp = *src1p | *src2p",
        " * @dstp: the cpumask result",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " */",
        "static __always_inline",
        "void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,",
        "		const struct cpumask *src2p)",
        "{",
        "	bitmap_or(cpumask_bits(dstp), cpumask_bits(src1p),",
        "				      cpumask_bits(src2p), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_xor - *dstp = *src1p ^ *src2p",
        " * @dstp: the cpumask result",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " */",
        "static __always_inline",
        "void cpumask_xor(struct cpumask *dstp, const struct cpumask *src1p,",
        "		 const struct cpumask *src2p)",
        "{",
        "	bitmap_xor(cpumask_bits(dstp), cpumask_bits(src1p),",
        "				       cpumask_bits(src2p), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_andnot - *dstp = *src1p & ~*src2p",
        " * @dstp: the cpumask result",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " *",
        " * Return: false if *@dstp is empty, else returns true",
        " */",
        "static __always_inline",
        "bool cpumask_andnot(struct cpumask *dstp, const struct cpumask *src1p,",
        "		    const struct cpumask *src2p)",
        "{",
        "	return bitmap_andnot(cpumask_bits(dstp), cpumask_bits(src1p),",
        "					  cpumask_bits(src2p), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_equal - *src1p == *src2p",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " *",
        " * Return: true if the cpumasks are equal, false if not",
        " */",
        "static __always_inline",
        "bool cpumask_equal(const struct cpumask *src1p, const struct cpumask *src2p)",
        "{",
        "	return bitmap_equal(cpumask_bits(src1p), cpumask_bits(src2p),",
        "						 small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_or_equal - *src1p | *src2p == *src3p",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " * @src3p: the third input",
        " *",
        " * Return: true if first cpumask ORed with second cpumask == third cpumask,",
        " *	   otherwise false",
        " */",
        "static __always_inline",
        "bool cpumask_or_equal(const struct cpumask *src1p, const struct cpumask *src2p,",
        "		      const struct cpumask *src3p)",
        "{",
        "	return bitmap_or_equal(cpumask_bits(src1p), cpumask_bits(src2p),",
        "			       cpumask_bits(src3p), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_intersects - (*src1p & *src2p) != 0",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " *",
        " * Return: true if first cpumask ANDed with second cpumask is non-empty,",
        " *	   otherwise false",
        " */",
        "static __always_inline",
        "bool cpumask_intersects(const struct cpumask *src1p, const struct cpumask *src2p)",
        "{",
        "	return bitmap_intersects(cpumask_bits(src1p), cpumask_bits(src2p),",
        "						      small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_subset - (*src1p & ~*src2p) == 0",
        " * @src1p: the first input",
        " * @src2p: the second input",
        " *",
        " * Return: true if *@src1p is a subset of *@src2p, else returns false",
        " */",
        "static __always_inline",
        "bool cpumask_subset(const struct cpumask *src1p, const struct cpumask *src2p)",
        "{",
        "	return bitmap_subset(cpumask_bits(src1p), cpumask_bits(src2p),",
        "						  small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_empty - *srcp == 0",
        " * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.",
        " *",
        " * Return: true if srcp is empty (has no bits set), else false",
        " */",
        "static __always_inline bool cpumask_empty(const struct cpumask *srcp)",
        "{",
        "	return bitmap_empty(cpumask_bits(srcp), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_full - *srcp == 0xFFFFFFFF...",
        " * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.",
        " *",
        " * Return: true if srcp is full (has all bits set), else false",
        " */",
        "static __always_inline bool cpumask_full(const struct cpumask *srcp)",
        "{",
        "	return bitmap_full(cpumask_bits(srcp), nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_weight - Count of bits in *srcp",
        " * @srcp: the cpumask to count bits (< nr_cpu_ids) in.",
        " *",
        " * Return: count of bits set in *srcp",
        " */",
        "static __always_inline unsigned int cpumask_weight(const struct cpumask *srcp)",
        "{",
        "	return bitmap_weight(cpumask_bits(srcp), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_weight_and - Count of bits in (*srcp1 & *srcp2)",
        " * @srcp1: the cpumask to count bits (< nr_cpu_ids) in.",
        " * @srcp2: the cpumask to count bits (< nr_cpu_ids) in.",
        " *",
        " * Return: count of bits set in both *srcp1 and *srcp2",
        " */",
        "static __always_inline",
        "unsigned int cpumask_weight_and(const struct cpumask *srcp1, const struct cpumask *srcp2)",
        "{",
        "	return bitmap_weight_and(cpumask_bits(srcp1), cpumask_bits(srcp2), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_weight_andnot - Count of bits in (*srcp1 & ~*srcp2)",
        " * @srcp1: the cpumask to count bits (< nr_cpu_ids) in.",
        " * @srcp2: the cpumask to count bits (< nr_cpu_ids) in.",
        " *",
        " * Return: count of bits set in both *srcp1 and *srcp2",
        " */",
        "static __always_inline",
        "unsigned int cpumask_weight_andnot(const struct cpumask *srcp1,",
        "				   const struct cpumask *srcp2)",
        "{",
        "	return bitmap_weight_andnot(cpumask_bits(srcp1), cpumask_bits(srcp2), small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_shift_right - *dstp = *srcp >> n",
        " * @dstp: the cpumask result",
        " * @srcp: the input to shift",
        " * @n: the number of bits to shift by",
        " */",
        "static __always_inline",
        "void cpumask_shift_right(struct cpumask *dstp, const struct cpumask *srcp, int n)",
        "{",
        "	bitmap_shift_right(cpumask_bits(dstp), cpumask_bits(srcp), n,",
        "					       small_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_shift_left - *dstp = *srcp << n",
        " * @dstp: the cpumask result",
        " * @srcp: the input to shift",
        " * @n: the number of bits to shift by",
        " */",
        "static __always_inline",
        "void cpumask_shift_left(struct cpumask *dstp, const struct cpumask *srcp, int n)",
        "{",
        "	bitmap_shift_left(cpumask_bits(dstp), cpumask_bits(srcp), n,",
        "					      nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_copy - *dstp = *srcp",
        " * @dstp: the result",
        " * @srcp: the input cpumask",
        " */",
        "static __always_inline",
        "void cpumask_copy(struct cpumask *dstp, const struct cpumask *srcp)",
        "{",
        "	bitmap_copy(cpumask_bits(dstp), cpumask_bits(srcp), large_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_any - pick a \"random\" cpu from *srcp",
        " * @srcp: the input cpumask",
        " *",
        " * Return: >= nr_cpu_ids if no cpus set.",
        " */",
        "#define cpumask_any(srcp) cpumask_first(srcp)",
        "",
        "/**",
        " * cpumask_any_and - pick a \"random\" cpu from *mask1 & *mask2",
        " * @mask1: the first input cpumask",
        " * @mask2: the second input cpumask",
        " *",
        " * Return: >= nr_cpu_ids if no cpus set.",
        " */",
        "#define cpumask_any_and(mask1, mask2) cpumask_first_and((mask1), (mask2))",
        "",
        "/**",
        " * cpumask_of - the cpumask containing just a given cpu",
        " * @cpu: the cpu (<= nr_cpu_ids)",
        " */",
        "#define cpumask_of(cpu) (get_cpu_mask(cpu))",
        "",
        "/**",
        " * cpumask_parse_user - extract a cpumask from a user string",
        " * @buf: the buffer to extract from",
        " * @len: the length of the buffer",
        " * @dstp: the cpumask to set.",
        " *",
        " * Return: -errno, or 0 for success.",
        " */",
        "static __always_inline",
        "int cpumask_parse_user(const char __user *buf, int len, struct cpumask *dstp)",
        "{",
        "	return bitmap_parse_user(buf, len, cpumask_bits(dstp), nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_parselist_user - extract a cpumask from a user string",
        " * @buf: the buffer to extract from",
        " * @len: the length of the buffer",
        " * @dstp: the cpumask to set.",
        " *",
        " * Return: -errno, or 0 for success.",
        " */",
        "static __always_inline",
        "int cpumask_parselist_user(const char __user *buf, int len, struct cpumask *dstp)",
        "{",
        "	return bitmap_parselist_user(buf, len, cpumask_bits(dstp),",
        "				     nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_parse - extract a cpumask from a string",
        " * @buf: the buffer to extract from",
        " * @dstp: the cpumask to set.",
        " *",
        " * Return: -errno, or 0 for success.",
        " */",
        "static __always_inline int cpumask_parse(const char *buf, struct cpumask *dstp)",
        "{",
        "	return bitmap_parse(buf, UINT_MAX, cpumask_bits(dstp), nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpulist_parse - extract a cpumask from a user string of ranges",
        " * @buf: the buffer to extract from",
        " * @dstp: the cpumask to set.",
        " *",
        " * Return: -errno, or 0 for success.",
        " */",
        "static __always_inline int cpulist_parse(const char *buf, struct cpumask *dstp)",
        "{",
        "	return bitmap_parselist(buf, cpumask_bits(dstp), nr_cpumask_bits);",
        "}",
        "",
        "/**",
        " * cpumask_size - calculate size to allocate for a 'struct cpumask' in bytes",
        " *",
        " * Return: size to allocate for a &struct cpumask in bytes",
        " */",
        "static __always_inline unsigned int cpumask_size(void)",
        "{",
        "	return bitmap_size(large_cpumask_bits);",
        "}",
        "",
        "#ifdef CONFIG_CPUMASK_OFFSTACK",
        "",
        "#define this_cpu_cpumask_var_ptr(x)	this_cpu_read(x)",
        "#define __cpumask_var_read_mostly	__read_mostly",
        "",
        "bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);",
        "",
        "static __always_inline",
        "bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)",
        "{",
        "	return alloc_cpumask_var_node(mask, flags | __GFP_ZERO, node);",
        "}",
        "",
        "/**",
        " * alloc_cpumask_var - allocate a struct cpumask",
        " * @mask: pointer to cpumask_var_t where the cpumask is returned",
        " * @flags: GFP_ flags",
        " *",
        " * Only defined when CONFIG_CPUMASK_OFFSTACK=y, otherwise is",
        " * a nop returning a constant 1 (in <linux/cpumask.h>).",
        " *",
        " * See alloc_cpumask_var_node.",
        " *",
        " * Return: %true if allocation succeeded, %false if not",
        " */",
        "static __always_inline",
        "bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)",
        "{",
        "	return alloc_cpumask_var_node(mask, flags, NUMA_NO_NODE);",
        "}",
        "",
        "static __always_inline",
        "bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)",
        "{",
        "	return alloc_cpumask_var(mask, flags | __GFP_ZERO);",
        "}",
        "",
        "void alloc_bootmem_cpumask_var(cpumask_var_t *mask);",
        "void free_cpumask_var(cpumask_var_t mask);",
        "void free_bootmem_cpumask_var(cpumask_var_t mask);",
        "",
        "static __always_inline bool cpumask_available(cpumask_var_t mask)",
        "{",
        "	return mask != NULL;",
        "}",
        "",
        "#else",
        "",
        "#define this_cpu_cpumask_var_ptr(x) this_cpu_ptr(x)",
        "#define __cpumask_var_read_mostly",
        "",
        "static __always_inline bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)",
        "{",
        "	return true;",
        "}",
        "",
        "static __always_inline bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,",
        "					  int node)",
        "{",
        "	return true;",
        "}",
        "",
        "static __always_inline bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)",
        "{",
        "	cpumask_clear(*mask);",
        "	return true;",
        "}",
        "",
        "static __always_inline bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,",
        "					  int node)",
        "{",
        "	cpumask_clear(*mask);",
        "	return true;",
        "}",
        "",
        "static __always_inline void alloc_bootmem_cpumask_var(cpumask_var_t *mask)",
        "{",
        "}",
        "",
        "static __always_inline void free_cpumask_var(cpumask_var_t mask)",
        "{",
        "}",
        "",
        "static __always_inline void free_bootmem_cpumask_var(cpumask_var_t mask)",
        "{",
        "}",
        "",
        "static __always_inline bool cpumask_available(cpumask_var_t mask)",
        "{",
        "	return true;",
        "}",
        "#endif /* CONFIG_CPUMASK_OFFSTACK */",
        "",
        "DEFINE_FREE(free_cpumask_var, struct cpumask *, if (_T) free_cpumask_var(_T));",
        "",
        "/* It's common to want to use cpu_all_mask in struct member initializers,",
        " * so it has to refer to an address rather than a pointer. */",
        "extern const DECLARE_BITMAP(cpu_all_bits, NR_CPUS);",
        "#define cpu_all_mask to_cpumask(cpu_all_bits)",
        "",
        "/* First bits of cpu_bit_bitmap are in fact unset. */",
        "#define cpu_none_mask to_cpumask(cpu_bit_bitmap[0])",
        "",
        "#if NR_CPUS == 1",
        "/* Uniprocessor: the possible/online/present masks are always \"1\" */",
        "#define for_each_possible_cpu(cpu)	for ((cpu) = 0; (cpu) < 1; (cpu)++)",
        "#define for_each_online_cpu(cpu)	for ((cpu) = 0; (cpu) < 1; (cpu)++)",
        "#define for_each_present_cpu(cpu)	for ((cpu) = 0; (cpu) < 1; (cpu)++)",
        "#else",
        "#define for_each_possible_cpu(cpu) for_each_cpu((cpu), cpu_possible_mask)",
        "#define for_each_online_cpu(cpu)   for_each_cpu((cpu), cpu_online_mask)",
        "#define for_each_enabled_cpu(cpu)   for_each_cpu((cpu), cpu_enabled_mask)",
        "#define for_each_present_cpu(cpu)  for_each_cpu((cpu), cpu_present_mask)",
        "#endif",
        "",
        "/* Wrappers for arch boot code to manipulate normally-constant masks */",
        "void init_cpu_present(const struct cpumask *src);",
        "void init_cpu_possible(const struct cpumask *src);",
        "void init_cpu_online(const struct cpumask *src);",
        "",
        "#define assign_cpu(cpu, mask, val)	\\",
        "	assign_bit(cpumask_check(cpu), cpumask_bits(mask), (val))",
        "",
        "#define set_cpu_possible(cpu, possible)	assign_cpu((cpu), &__cpu_possible_mask, (possible))",
        "#define set_cpu_enabled(cpu, enabled)	assign_cpu((cpu), &__cpu_enabled_mask, (enabled))",
        "#define set_cpu_present(cpu, present)	assign_cpu((cpu), &__cpu_present_mask, (present))",
        "#define set_cpu_active(cpu, active)	assign_cpu((cpu), &__cpu_active_mask, (active))",
        "#define set_cpu_dying(cpu, dying)	assign_cpu((cpu), &__cpu_dying_mask, (dying))",
        "",
        "void set_cpu_online(unsigned int cpu, bool online);",
        "",
        "/**",
        " * to_cpumask - convert a NR_CPUS bitmap to a struct cpumask *",
        " * @bitmap: the bitmap",
        " *",
        " * There are a few places where cpumask_var_t isn't appropriate and",
        " * static cpumasks must be used (eg. very early boot), yet we don't",
        " * expose the definition of 'struct cpumask'.",
        " *",
        " * This does the conversion, and can be used as a constant initializer.",
        " */",
        "#define to_cpumask(bitmap)						\\",
        "	((struct cpumask *)(1 ? (bitmap)				\\",
        "			    : (void *)sizeof(__check_is_bitmap(bitmap))))",
        "",
        "static __always_inline int __check_is_bitmap(const unsigned long *bitmap)",
        "{",
        "	return 1;",
        "}",
        "",
        "/*",
        " * Special-case data structure for \"single bit set only\" constant CPU masks.",
        " *",
        " * We pre-generate all the 64 (or 32) possible bit positions, with enough",
        " * padding to the left and the right, and return the constant pointer",
        " * appropriately offset.",
        " */",
        "extern const unsigned long",
        "	cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)];",
        "",
        "static __always_inline const struct cpumask *get_cpu_mask(unsigned int cpu)",
        "{",
        "	const unsigned long *p = cpu_bit_bitmap[1 + cpu % BITS_PER_LONG];",
        "	p -= cpu / BITS_PER_LONG;",
        "	return to_cpumask(p);",
        "}",
        "",
        "#if NR_CPUS > 1",
        "/**",
        " * num_online_cpus() - Read the number of online CPUs",
        " *",
        " * Despite the fact that __num_online_cpus is of type atomic_t, this",
        " * interface gives only a momentary snapshot and is not protected against",
        " * concurrent CPU hotplug operations unless invoked from a cpuhp_lock held",
        " * region.",
        " *",
        " * Return: momentary snapshot of the number of online CPUs",
        " */",
        "static __always_inline unsigned int num_online_cpus(void)",
        "{",
        "	return raw_atomic_read(&__num_online_cpus);",
        "}",
        "#define num_possible_cpus()	cpumask_weight(cpu_possible_mask)",
        "#define num_enabled_cpus()	cpumask_weight(cpu_enabled_mask)",
        "#define num_present_cpus()	cpumask_weight(cpu_present_mask)",
        "#define num_active_cpus()	cpumask_weight(cpu_active_mask)",
        "",
        "static __always_inline bool cpu_online(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_online_mask);",
        "}",
        "",
        "static __always_inline bool cpu_enabled(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_enabled_mask);",
        "}",
        "",
        "static __always_inline bool cpu_possible(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_possible_mask);",
        "}",
        "",
        "static __always_inline bool cpu_present(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_present_mask);",
        "}",
        "",
        "static __always_inline bool cpu_active(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_active_mask);",
        "}",
        "",
        "static __always_inline bool cpu_dying(unsigned int cpu)",
        "{",
        "	return cpumask_test_cpu(cpu, cpu_dying_mask);",
        "}",
        "",
        "#else",
        "",
        "#define num_online_cpus()	1U",
        "#define num_possible_cpus()	1U",
        "#define num_enabled_cpus()	1U",
        "#define num_present_cpus()	1U",
        "#define num_active_cpus()	1U",
        "",
        "static __always_inline bool cpu_online(unsigned int cpu)",
        "{",
        "	return cpu == 0;",
        "}",
        "",
        "static __always_inline bool cpu_possible(unsigned int cpu)",
        "{",
        "	return cpu == 0;",
        "}",
        "",
        "static __always_inline bool cpu_enabled(unsigned int cpu)",
        "{",
        "	return cpu == 0;",
        "}",
        "",
        "static __always_inline bool cpu_present(unsigned int cpu)",
        "{",
        "	return cpu == 0;",
        "}",
        "",
        "static __always_inline bool cpu_active(unsigned int cpu)",
        "{",
        "	return cpu == 0;",
        "}",
        "",
        "static __always_inline bool cpu_dying(unsigned int cpu)",
        "{",
        "	return false;",
        "}",
        "",
        "#endif /* NR_CPUS > 1 */",
        "",
        "#define cpu_is_offline(cpu)	unlikely(!cpu_online(cpu))",
        "",
        "#if NR_CPUS <= BITS_PER_LONG",
        "#define CPU_BITS_ALL						\\",
        "{								\\",
        "	[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)	\\",
        "}",
        "",
        "#else /* NR_CPUS > BITS_PER_LONG */",
        "",
        "#define CPU_BITS_ALL						\\",
        "{								\\",
        "	[0 ... BITS_TO_LONGS(NR_CPUS)-2] = ~0UL,		\\",
        "	[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)	\\",
        "}",
        "#endif /* NR_CPUS > BITS_PER_LONG */",
        "",
        "/**",
        " * cpumap_print_to_pagebuf  - copies the cpumask into the buffer either",
        " *	as comma-separated list of cpus or hex values of cpumask",
        " * @list: indicates whether the cpumap must be list",
        " * @mask: the cpumask to copy",
        " * @buf: the buffer to copy into",
        " *",
        " * Return: the length of the (null-terminated) @buf string, zero if",
        " * nothing is copied.",
        " */",
        "static __always_inline ssize_t",
        "cpumap_print_to_pagebuf(bool list, char *buf, const struct cpumask *mask)",
        "{",
        "	return bitmap_print_to_pagebuf(list, buf, cpumask_bits(mask),",
        "				      nr_cpu_ids);",
        "}",
        "",
        "/**",
        " * cpumap_print_bitmask_to_buf  - copies the cpumask into the buffer as",
        " *	hex values of cpumask",
        " *",
        " * @buf: the buffer to copy into",
        " * @mask: the cpumask to copy",
        " * @off: in the string from which we are copying, we copy to @buf",
        " * @count: the maximum number of bytes to print",
        " *",
        " * The function prints the cpumask into the buffer as hex values of",
        " * cpumask; Typically used by bin_attribute to export cpumask bitmask",
        " * ABI.",
        " *",
        " * Return: the length of how many bytes have been copied, excluding",
        " * terminating '\\0'.",
        " */",
        "static __always_inline",
        "ssize_t cpumap_print_bitmask_to_buf(char *buf, const struct cpumask *mask,",
        "				    loff_t off, size_t count)",
        "{",
        "	return bitmap_print_bitmask_to_buf(buf, cpumask_bits(mask),",
        "				   nr_cpu_ids, off, count) - 1;",
        "}",
        "",
        "/**",
        " * cpumap_print_list_to_buf  - copies the cpumask into the buffer as",
        " *	comma-separated list of cpus",
        " * @buf: the buffer to copy into",
        " * @mask: the cpumask to copy",
        " * @off: in the string from which we are copying, we copy to @buf",
        " * @count: the maximum number of bytes to print",
        " *",
        " * Everything is same with the above cpumap_print_bitmask_to_buf()",
        " * except the print format.",
        " *",
        " * Return: the length of how many bytes have been copied, excluding",
        " * terminating '\\0'.",
        " */",
        "static __always_inline",
        "ssize_t cpumap_print_list_to_buf(char *buf, const struct cpumask *mask,",
        "				 loff_t off, size_t count)",
        "{",
        "	return bitmap_print_list_to_buf(buf, cpumask_bits(mask),",
        "				   nr_cpu_ids, off, count) - 1;",
        "}",
        "",
        "#if NR_CPUS <= BITS_PER_LONG",
        "#define CPU_MASK_ALL							\\",
        "(cpumask_t) { {								\\",
        "	[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)	\\",
        "} }",
        "#else",
        "#define CPU_MASK_ALL							\\",
        "(cpumask_t) { {								\\",
        "	[0 ... BITS_TO_LONGS(NR_CPUS)-2] = ~0UL,			\\",
        "	[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)	\\",
        "} }",
        "#endif /* NR_CPUS > BITS_PER_LONG */",
        "",
        "#define CPU_MASK_NONE							\\",
        "(cpumask_t) { {								\\",
        "	[0 ... BITS_TO_LONGS(NR_CPUS)-1] =  0UL				\\",
        "} }",
        "",
        "#define CPU_MASK_CPU0							\\",
        "(cpumask_t) { {								\\",
        "	[0] =  1UL							\\",
        "} }",
        "",
        "/*",
        " * Provide a valid theoretical max size for cpumap and cpulist sysfs files",
        " * to avoid breaking userspace which may allocate a buffer based on the size",
        " * reported by e.g. fstat.",
        " *",
        " * for cpumap NR_CPUS * 9/32 - 1 should be an exact length.",
        " *",
        " * For cpulist 7 is (ceil(log10(NR_CPUS)) + 1) allowing for NR_CPUS to be up",
        " * to 2 orders of magnitude larger than 8192. And then we divide by 2 to",
        " * cover a worst-case of every other cpu being on one of two nodes for a",
        " * very large NR_CPUS.",
        " *",
        " *  Use PAGE_SIZE as a minimum for smaller configurations while avoiding",
        " *  unsigned comparison to -1.",
        " */",
        "#define CPUMAP_FILE_MAX_BYTES  (((NR_CPUS * 9)/32 > PAGE_SIZE) \\",
        "					? (NR_CPUS * 9)/32 - 1 : PAGE_SIZE)",
        "#define CPULIST_FILE_MAX_BYTES  (((NR_CPUS * 7)/2 > PAGE_SIZE) ? (NR_CPUS * 7)/2 : PAGE_SIZE)",
        "",
        "#endif /* __LINUX_CPUMASK_H */"
    ]
  },
  "kernel_bpf_mprog_c": {
    path: "kernel/bpf/mprog.c",
    covered: [397, 410, 424, 408],
    totalLines: 452,
    coveredCount: 4,
    coveragePct: 0.9,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/* Copyright (c) 2023 Isovalent */",
        "",
        "#include <linux/bpf.h>",
        "#include <linux/bpf_mprog.h>",
        "",
        "static int bpf_mprog_link(struct bpf_tuple *tuple,",
        "			  u32 id_or_fd, u32 flags,",
        "			  enum bpf_prog_type type)",
        "{",
        "	struct bpf_link *link = ERR_PTR(-EINVAL);",
        "	bool id = flags & BPF_F_ID;",
        "",
        "	if (id)",
        "		link = bpf_link_by_id(id_or_fd);",
        "	else if (id_or_fd)",
        "		link = bpf_link_get_from_fd(id_or_fd);",
        "	if (IS_ERR(link))",
        "		return PTR_ERR(link);",
        "	if (type && link->prog->type != type) {",
        "		bpf_link_put(link);",
        "		return -EINVAL;",
        "	}",
        "",
        "	tuple->link = link;",
        "	tuple->prog = link->prog;",
        "	return 0;",
        "}",
        "",
        "static int bpf_mprog_prog(struct bpf_tuple *tuple,",
        "			  u32 id_or_fd, u32 flags,",
        "			  enum bpf_prog_type type)",
        "{",
        "	struct bpf_prog *prog = ERR_PTR(-EINVAL);",
        "	bool id = flags & BPF_F_ID;",
        "",
        "	if (id)",
        "		prog = bpf_prog_by_id(id_or_fd);",
        "	else if (id_or_fd)",
        "		prog = bpf_prog_get(id_or_fd);",
        "	if (IS_ERR(prog))",
        "		return PTR_ERR(prog);",
        "	if (type && prog->type != type) {",
        "		bpf_prog_put(prog);",
        "		return -EINVAL;",
        "	}",
        "",
        "	tuple->link = NULL;",
        "	tuple->prog = prog;",
        "	return 0;",
        "}",
        "",
        "static int bpf_mprog_tuple_relative(struct bpf_tuple *tuple,",
        "				    u32 id_or_fd, u32 flags,",
        "				    enum bpf_prog_type type)",
        "{",
        "	bool link = flags & BPF_F_LINK;",
        "	bool id = flags & BPF_F_ID;",
        "",
        "	memset(tuple, 0, sizeof(*tuple));",
        "	if (link)",
        "		return bpf_mprog_link(tuple, id_or_fd, flags, type);",
        "	/* If no relevant flag is set and no id_or_fd was passed, then",
        "	 * tuple link/prog is just NULLed. This is the case when before/",
        "	 * after selects first/last position without passing fd.",
        "	 */",
        "	if (!id && !id_or_fd)",
        "		return 0;",
        "	return bpf_mprog_prog(tuple, id_or_fd, flags, type);",
        "}",
        "",
        "static void bpf_mprog_tuple_put(struct bpf_tuple *tuple)",
        "{",
        "	if (tuple->link)",
        "		bpf_link_put(tuple->link);",
        "	else if (tuple->prog)",
        "		bpf_prog_put(tuple->prog);",
        "}",
        "",
        "/* The bpf_mprog_{replace,delete}() operate on exact idx position with the",
        " * one exception that for deletion we support delete from front/back. In",
        " * case of front idx is -1, in case of back idx is bpf_mprog_total(entry).",
        " * Adjustment to first and last entry is trivial. The bpf_mprog_insert()",
        " * we have to deal with the following cases:",
        " *",
        " * idx + before:",
        " *",
        " * Insert P4 before P3: idx for old array is 1, idx for new array is 2,",
        " * hence we adjust target idx for the new array, so that memmove copies",
        " * P1 and P2 to the new entry, and we insert P4 into idx 2. Inserting",
        " * before P1 would have old idx -1 and new idx 0.",
        " *",
        " * +--+--+--+     +--+--+--+--+     +--+--+--+--+",
        " * |P1|P2|P3| ==> |P1|P2|  |P3| ==> |P1|P2|P4|P3|",
        " * +--+--+--+     +--+--+--+--+     +--+--+--+--+",
        " *",
        " * idx + after:",
        " *",
        " * Insert P4 after P2: idx for old array is 2, idx for new array is 2.",
        " * Again, memmove copies P1 and P2 to the new entry, and we insert P4",
        " * into idx 2. Inserting after P3 would have both old/new idx at 4 aka",
        " * bpf_mprog_total(entry).",
        " *",
        " * +--+--+--+     +--+--+--+--+     +--+--+--+--+",
        " * |P1|P2|P3| ==> |P1|P2|  |P3| ==> |P1|P2|P4|P3|",
        " * +--+--+--+     +--+--+--+--+     +--+--+--+--+",
        " */",
        "static int bpf_mprog_replace(struct bpf_mprog_entry *entry,",
        "			     struct bpf_mprog_entry **entry_new,",
        "			     struct bpf_tuple *ntuple, int idx)",
        "{",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	struct bpf_prog *oprog;",
        "",
        "	bpf_mprog_read(entry, idx, &fp, &cp);",
        "	oprog = READ_ONCE(fp->prog);",
        "	bpf_mprog_write(fp, cp, ntuple);",
        "	if (!ntuple->link) {",
        "		WARN_ON_ONCE(cp->link);",
        "		bpf_prog_put(oprog);",
        "	}",
        "	*entry_new = entry;",
        "	return 0;",
        "}",
        "",
        "static int bpf_mprog_insert(struct bpf_mprog_entry *entry,",
        "			    struct bpf_mprog_entry **entry_new,",
        "			    struct bpf_tuple *ntuple, int idx, u32 flags)",
        "{",
        "	int total = bpf_mprog_total(entry);",
        "	struct bpf_mprog_entry *peer;",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "",
        "	peer = bpf_mprog_peer(entry);",
        "	bpf_mprog_entry_copy(peer, entry);",
        "	if (idx == total)",
        "		goto insert;",
        "	else if (flags & BPF_F_BEFORE)",
        "		idx += 1;",
        "	bpf_mprog_entry_grow(peer, idx);",
        "insert:",
        "	bpf_mprog_read(peer, idx, &fp, &cp);",
        "	bpf_mprog_write(fp, cp, ntuple);",
        "	bpf_mprog_inc(peer);",
        "	*entry_new = peer;",
        "	return 0;",
        "}",
        "",
        "static int bpf_mprog_delete(struct bpf_mprog_entry *entry,",
        "			    struct bpf_mprog_entry **entry_new,",
        "			    struct bpf_tuple *dtuple, int idx)",
        "{",
        "	int total = bpf_mprog_total(entry);",
        "	struct bpf_mprog_entry *peer;",
        "",
        "	peer = bpf_mprog_peer(entry);",
        "	bpf_mprog_entry_copy(peer, entry);",
        "	if (idx == -1)",
        "		idx = 0;",
        "	else if (idx == total)",
        "		idx = total - 1;",
        "	bpf_mprog_entry_shrink(peer, idx);",
        "	bpf_mprog_dec(peer);",
        "	bpf_mprog_mark_for_release(peer, dtuple);",
        "	*entry_new = peer;",
        "	return 0;",
        "}",
        "",
        "/* In bpf_mprog_pos_*() we evaluate the target position for the BPF",
        " * program/link that needs to be replaced, inserted or deleted for",
        " * each \"rule\" independently. If all rules agree on that position",
        " * or existing element, then enact replacement, addition or deletion.",
        " * If this is not the case, then the request cannot be satisfied and",
        " * we bail out with an error.",
        " */",
        "static int bpf_mprog_pos_exact(struct bpf_mprog_entry *entry,",
        "			       struct bpf_tuple *tuple)",
        "{",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	int i;",
        "",
        "	for (i = 0; i < bpf_mprog_total(entry); i++) {",
        "		bpf_mprog_read(entry, i, &fp, &cp);",
        "		if (tuple->prog == READ_ONCE(fp->prog))",
        "			return tuple->link == cp->link ? i : -EBUSY;",
        "	}",
        "	return -ENOENT;",
        "}",
        "",
        "static int bpf_mprog_pos_before(struct bpf_mprog_entry *entry,",
        "				struct bpf_tuple *tuple)",
        "{",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	int i;",
        "",
        "	for (i = 0; i < bpf_mprog_total(entry); i++) {",
        "		bpf_mprog_read(entry, i, &fp, &cp);",
        "		if (tuple->prog == READ_ONCE(fp->prog) &&",
        "		    (!tuple->link || tuple->link == cp->link))",
        "			return i - 1;",
        "	}",
        "	return tuple->prog ? -ENOENT : -1;",
        "}",
        "",
        "static int bpf_mprog_pos_after(struct bpf_mprog_entry *entry,",
        "			       struct bpf_tuple *tuple)",
        "{",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	int i;",
        "",
        "	for (i = 0; i < bpf_mprog_total(entry); i++) {",
        "		bpf_mprog_read(entry, i, &fp, &cp);",
        "		if (tuple->prog == READ_ONCE(fp->prog) &&",
        "		    (!tuple->link || tuple->link == cp->link))",
        "			return i + 1;",
        "	}",
        "	return tuple->prog ? -ENOENT : bpf_mprog_total(entry);",
        "}",
        "",
        "int bpf_mprog_attach(struct bpf_mprog_entry *entry,",
        "		     struct bpf_mprog_entry **entry_new,",
        "		     struct bpf_prog *prog_new, struct bpf_link *link,",
        "		     struct bpf_prog *prog_old,",
        "		     u32 flags, u32 id_or_fd, u64 revision)",
        "{",
        "	struct bpf_tuple rtuple, ntuple = {",
        "		.prog = prog_new,",
        "		.link = link,",
        "	}, otuple = {",
        "		.prog = prog_old,",
        "		.link = link,",
        "	};",
        "	int ret, idx = -ERANGE, tidx;",
        "",
        "	if (revision && revision != bpf_mprog_revision(entry))",
        "		return -ESTALE;",
        "	if (bpf_mprog_exists(entry, prog_new))",
        "		return -EEXIST;",
        "	ret = bpf_mprog_tuple_relative(&rtuple, id_or_fd,",
        "				       flags & ~BPF_F_REPLACE,",
        "				       prog_new->type);",
        "	if (ret)",
        "		return ret;",
        "	if (flags & BPF_F_REPLACE) {",
        "		tidx = bpf_mprog_pos_exact(entry, &otuple);",
        "		if (tidx < 0) {",
        "			ret = tidx;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	} else if (bpf_mprog_total(entry) == bpf_mprog_max()) {",
        "		ret = -ERANGE;",
        "		goto out;",
        "	}",
        "	if (flags & BPF_F_BEFORE) {",
        "		tidx = bpf_mprog_pos_before(entry, &rtuple);",
        "		if (tidx < -1 || (idx >= -1 && tidx != idx)) {",
        "			ret = tidx < -1 ? tidx : -ERANGE;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	}",
        "	if (flags & BPF_F_AFTER) {",
        "		tidx = bpf_mprog_pos_after(entry, &rtuple);",
        "		if (tidx < -1 || (idx >= -1 && tidx != idx)) {",
        "			ret = tidx < 0 ? tidx : -ERANGE;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	}",
        "	if (idx < -1) {",
        "		if (rtuple.prog || flags) {",
        "			ret = -EINVAL;",
        "			goto out;",
        "		}",
        "		idx = bpf_mprog_total(entry);",
        "		flags = BPF_F_AFTER;",
        "	}",
        "	if (idx >= bpf_mprog_max()) {",
        "		ret = -ERANGE;",
        "		goto out;",
        "	}",
        "	if (flags & BPF_F_REPLACE)",
        "		ret = bpf_mprog_replace(entry, entry_new, &ntuple, idx);",
        "	else",
        "		ret = bpf_mprog_insert(entry, entry_new, &ntuple, idx, flags);",
        "out:",
        "	bpf_mprog_tuple_put(&rtuple);",
        "	return ret;",
        "}",
        "",
        "static int bpf_mprog_fetch(struct bpf_mprog_entry *entry,",
        "			   struct bpf_tuple *tuple, int idx)",
        "{",
        "	int total = bpf_mprog_total(entry);",
        "	struct bpf_mprog_cp *cp;",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_prog *prog;",
        "	struct bpf_link *link;",
        "",
        "	if (idx == -1)",
        "		idx = 0;",
        "	else if (idx == total)",
        "		idx = total - 1;",
        "	bpf_mprog_read(entry, idx, &fp, &cp);",
        "	prog = READ_ONCE(fp->prog);",
        "	link = cp->link;",
        "	/* The deletion request can either be without filled tuple in which",
        "	 * case it gets populated here based on idx, or with filled tuple",
        "	 * where the only thing we end up doing is the WARN_ON_ONCE() assert.",
        "	 * If we hit a BPF link at the given index, it must not be removed",
        "	 * from opts path.",
        "	 */",
        "	if (link && !tuple->link)",
        "		return -EBUSY;",
        "	WARN_ON_ONCE(tuple->prog && tuple->prog != prog);",
        "	WARN_ON_ONCE(tuple->link && tuple->link != link);",
        "	tuple->prog = prog;",
        "	tuple->link = link;",
        "	return 0;",
        "}",
        "",
        "int bpf_mprog_detach(struct bpf_mprog_entry *entry,",
        "		     struct bpf_mprog_entry **entry_new,",
        "		     struct bpf_prog *prog, struct bpf_link *link,",
        "		     u32 flags, u32 id_or_fd, u64 revision)",
        "{",
        "	struct bpf_tuple rtuple, dtuple = {",
        "		.prog = prog,",
        "		.link = link,",
        "	};",
        "	int ret, idx = -ERANGE, tidx;",
        "",
        "	if (flags & BPF_F_REPLACE)",
        "		return -EINVAL;",
        "	if (revision && revision != bpf_mprog_revision(entry))",
        "		return -ESTALE;",
        "	if (!bpf_mprog_total(entry))",
        "		return -ENOENT;",
        "	ret = bpf_mprog_tuple_relative(&rtuple, id_or_fd, flags,",
        "				       prog ? prog->type :",
        "				       BPF_PROG_TYPE_UNSPEC);",
        "	if (ret)",
        "		return ret;",
        "	if (dtuple.prog) {",
        "		tidx = bpf_mprog_pos_exact(entry, &dtuple);",
        "		if (tidx < 0) {",
        "			ret = tidx;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	}",
        "	if (flags & BPF_F_BEFORE) {",
        "		tidx = bpf_mprog_pos_before(entry, &rtuple);",
        "		if (tidx < -1 || (idx >= -1 && tidx != idx)) {",
        "			ret = tidx < -1 ? tidx : -ERANGE;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	}",
        "	if (flags & BPF_F_AFTER) {",
        "		tidx = bpf_mprog_pos_after(entry, &rtuple);",
        "		if (tidx < -1 || (idx >= -1 && tidx != idx)) {",
        "			ret = tidx < 0 ? tidx : -ERANGE;",
        "			goto out;",
        "		}",
        "		idx = tidx;",
        "	}",
        "	if (idx < -1) {",
        "		if (rtuple.prog || flags) {",
        "			ret = -EINVAL;",
        "			goto out;",
        "		}",
        "		idx = bpf_mprog_total(entry);",
        "		flags = BPF_F_AFTER;",
        "	}",
        "	if (idx >= bpf_mprog_max()) {",
        "		ret = -ERANGE;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_fetch(entry, &dtuple, idx);",
        "	if (ret)",
        "		goto out;",
        "	ret = bpf_mprog_delete(entry, entry_new, &dtuple, idx);",
        "out:",
        "	bpf_mprog_tuple_put(&rtuple);",
        "	return ret;",
        "}",
        "",
        "int bpf_mprog_query(const union bpf_attr *attr, union bpf_attr __user *uattr,",
        "		    struct bpf_mprog_entry *entry)",
        "{",
        "	u32 __user *uprog_flags, *ulink_flags;",
        "	u32 __user *uprog_id, *ulink_id;",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	struct bpf_prog *prog;",
        "	const u32 flags = 0;",
        "	u32 id, count = 0;",
        "	u64 revision = 1;",
        "	int i, ret = 0;",
        "",
        "	if (attr->query.query_flags || attr->query.attach_flags)",
        "		return -EINVAL;",
        "	if (entry) {",
        "		revision = bpf_mprog_revision(entry);",
        "		count = bpf_mprog_total(entry);",
        "	}",
        "	if (copy_to_user(&uattr->query.attach_flags, &flags, sizeof(flags)))",
        "		return -EFAULT;",
        "	if (copy_to_user(&uattr->query.revision, &revision, sizeof(revision)))",
        "		return -EFAULT;",
        "	if (copy_to_user(&uattr->query.count, &count, sizeof(count)))",
        "		return -EFAULT;",
        "	uprog_id = u64_to_user_ptr(attr->query.prog_ids);",
        "	uprog_flags = u64_to_user_ptr(attr->query.prog_attach_flags);",
        "	ulink_id = u64_to_user_ptr(attr->query.link_ids);",
        "	ulink_flags = u64_to_user_ptr(attr->query.link_attach_flags);",
        "	if (attr->query.count == 0 || !uprog_id || !count)",
        "		return 0;",
        "	if (attr->query.count < count) {",
        "		count = attr->query.count;",
        "		ret = -ENOSPC;",
        "	}",
        "	for (i = 0; i < bpf_mprog_max(); i++) {",
        "		bpf_mprog_read(entry, i, &fp, &cp);",
        "		prog = READ_ONCE(fp->prog);",
        "		if (!prog)",
        "			break;",
        "		id = prog->aux->id;",
        "		if (copy_to_user(uprog_id + i, &id, sizeof(id)))",
        "			return -EFAULT;",
        "		if (uprog_flags &&",
        "		    copy_to_user(uprog_flags + i, &flags, sizeof(flags)))",
        "			return -EFAULT;",
        "		id = cp->link ? cp->link->id : 0;",
        "		if (ulink_id &&",
        "		    copy_to_user(ulink_id + i, &id, sizeof(id)))",
        "			return -EFAULT;",
        "		if (ulink_flags &&",
        "		    copy_to_user(ulink_flags + i, &flags, sizeof(flags)))",
        "			return -EFAULT;",
        "		if (i + 1 == count)",
        "			break;",
        "	}",
        "	return ret;",
        "}"
    ]
  },
  "include_linux_sched_h": {
    path: "include/linux/sched.h",
    covered: [2059],
    totalLines: 2244,
    coveredCount: 1,
    coveragePct: 0.0,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_SCHED_H",
        "#define _LINUX_SCHED_H",
        "",
        "/*",
        " * Define 'struct task_struct' and provide the main scheduler",
        " * APIs (schedule(), wakeup variants, etc.)",
        " */",
        "",
        "#include <uapi/linux/sched.h>",
        "",
        "#include <asm/current.h>",
        "#include <asm/processor.h>",
        "#include <linux/thread_info.h>",
        "#include <linux/preempt.h>",
        "#include <linux/cpumask_types.h>",
        "",
        "#include <linux/cache.h>",
        "#include <linux/irqflags_types.h>",
        "#include <linux/smp_types.h>",
        "#include <linux/pid_types.h>",
        "#include <linux/sem_types.h>",
        "#include <linux/shm.h>",
        "#include <linux/kmsan_types.h>",
        "#include <linux/mutex_types.h>",
        "#include <linux/plist_types.h>",
        "#include <linux/hrtimer_types.h>",
        "#include <linux/timer_types.h>",
        "#include <linux/seccomp_types.h>",
        "#include <linux/nodemask_types.h>",
        "#include <linux/refcount_types.h>",
        "#include <linux/resource.h>",
        "#include <linux/latencytop.h>",
        "#include <linux/sched/prio.h>",
        "#include <linux/sched/types.h>",
        "#include <linux/signal_types.h>",
        "#include <linux/syscall_user_dispatch_types.h>",
        "#include <linux/mm_types_task.h>",
        "#include <linux/netdevice_xmit.h>",
        "#include <linux/task_io_accounting.h>",
        "#include <linux/posix-timers_types.h>",
        "#include <linux/restart_block.h>",
        "#include <uapi/linux/rseq.h>",
        "#include <linux/seqlock_types.h>",
        "#include <linux/kcsan.h>",
        "#include <linux/rv.h>",
        "#include <linux/livepatch_sched.h>",
        "#include <linux/uidgid_types.h>",
        "#include <asm/kmap_size.h>",
        "",
        "/* task_struct member predeclarations (sorted alphabetically): */",
        "struct audit_context;",
        "struct bio_list;",
        "struct blk_plug;",
        "struct bpf_local_storage;",
        "struct bpf_run_ctx;",
        "struct bpf_net_context;",
        "struct capture_control;",
        "struct cfs_rq;",
        "struct fs_struct;",
        "struct futex_pi_state;",
        "struct io_context;",
        "struct io_uring_task;",
        "struct mempolicy;",
        "struct nameidata;",
        "struct nsproxy;",
        "struct perf_event_context;",
        "struct pid_namespace;",
        "struct pipe_inode_info;",
        "struct rcu_node;",
        "struct reclaim_state;",
        "struct robust_list_head;",
        "struct root_domain;",
        "struct rq;",
        "struct sched_attr;",
        "struct sched_dl_entity;",
        "struct seq_file;",
        "struct sighand_struct;",
        "struct signal_struct;",
        "struct task_delay_info;",
        "struct task_group;",
        "struct task_struct;",
        "struct user_event_mm;",
        "",
        "#include <linux/sched/ext.h>",
        "",
        "/*",
        " * Task state bitmask. NOTE! These bits are also",
        " * encoded in fs/proc/array.c: get_task_state().",
        " *",
        " * We have two separate sets of flags: task->__state",
        " * is about runnability, while task->exit_state are",
        " * about the task exiting. Confusing, but this way",
        " * modifying one set can't modify the other one by",
        " * mistake.",
        " */",
        "",
        "/* Used in tsk->__state: */",
        "#define TASK_RUNNING			0x00000000",
        "#define TASK_INTERRUPTIBLE		0x00000001",
        "#define TASK_UNINTERRUPTIBLE		0x00000002",
        "#define __TASK_STOPPED			0x00000004",
        "#define __TASK_TRACED			0x00000008",
        "/* Used in tsk->exit_state: */",
        "#define EXIT_DEAD			0x00000010",
        "#define EXIT_ZOMBIE			0x00000020",
        "#define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)",
        "/* Used in tsk->__state again: */",
        "#define TASK_PARKED			0x00000040",
        "#define TASK_DEAD			0x00000080",
        "#define TASK_WAKEKILL			0x00000100",
        "#define TASK_WAKING			0x00000200",
        "#define TASK_NOLOAD			0x00000400",
        "#define TASK_NEW			0x00000800",
        "#define TASK_RTLOCK_WAIT		0x00001000",
        "#define TASK_FREEZABLE			0x00002000",
        "#define __TASK_FREEZABLE_UNSAFE	       (0x00004000 * IS_ENABLED(CONFIG_LOCKDEP))",
        "#define TASK_FROZEN			0x00008000",
        "#define TASK_STATE_MAX			0x00010000",
        "",
        "#define TASK_ANY			(TASK_STATE_MAX-1)",
        "",
        "/*",
        " * DO NOT ADD ANY NEW USERS !",
        " */",
        "#define TASK_FREEZABLE_UNSAFE		(TASK_FREEZABLE | __TASK_FREEZABLE_UNSAFE)",
        "",
        "/* Convenience macros for the sake of set_current_state: */",
        "#define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)",
        "#define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)",
        "#define TASK_TRACED			__TASK_TRACED",
        "",
        "#define TASK_IDLE			(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)",
        "",
        "/* Convenience macros for the sake of wake_up(): */",
        "#define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)",
        "",
        "/* get_task_state(): */",
        "#define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \\",
        "					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\",
        "					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\",
        "					 TASK_PARKED)",
        "",
        "#define task_is_running(task)		(READ_ONCE((task)->__state) == TASK_RUNNING)",
        "",
        "#define task_is_traced(task)		((READ_ONCE(task->jobctl) & JOBCTL_TRACED) != 0)",
        "#define task_is_stopped(task)		((READ_ONCE(task->jobctl) & JOBCTL_STOPPED) != 0)",
        "#define task_is_stopped_or_traced(task)	((READ_ONCE(task->jobctl) & (JOBCTL_STOPPED | JOBCTL_TRACED)) != 0)",
        "",
        "/*",
        " * Special states are those that do not use the normal wait-loop pattern. See",
        " * the comment with set_special_state().",
        " */",
        "#define is_special_task_state(state)					\\",
        "	((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_PARKED |	\\",
        "		    TASK_DEAD | TASK_FROZEN))",
        "",
        "#ifdef CONFIG_DEBUG_ATOMIC_SLEEP",
        "# define debug_normal_state_change(state_value)				\\",
        "	do {								\\",
        "		WARN_ON_ONCE(is_special_task_state(state_value));	\\",
        "		current->task_state_change = _THIS_IP_;			\\",
        "	} while (0)",
        "",
        "# define debug_special_state_change(state_value)			\\",
        "	do {								\\",
        "		WARN_ON_ONCE(!is_special_task_state(state_value));	\\",
        "		current->task_state_change = _THIS_IP_;			\\",
        "	} while (0)",
        "",
        "# define debug_rtlock_wait_set_state()					\\",
        "	do {								 \\",
        "		current->saved_state_change = current->task_state_change;\\",
        "		current->task_state_change = _THIS_IP_;			 \\",
        "	} while (0)",
        "",
        "# define debug_rtlock_wait_restore_state()				\\",
        "	do {								 \\",
        "		current->task_state_change = current->saved_state_change;\\",
        "	} while (0)",
        "",
        "#else",
        "# define debug_normal_state_change(cond)	do { } while (0)",
        "# define debug_special_state_change(cond)	do { } while (0)",
        "# define debug_rtlock_wait_set_state()		do { } while (0)",
        "# define debug_rtlock_wait_restore_state()	do { } while (0)",
        "#endif",
        "",
        "/*",
        " * set_current_state() includes a barrier so that the write of current->__state",
        " * is correctly serialised wrt the caller's subsequent test of whether to",
        " * actually sleep:",
        " *",
        " *   for (;;) {",
        " *	set_current_state(TASK_UNINTERRUPTIBLE);",
        " *	if (CONDITION)",
        " *	   break;",
        " *",
        " *	schedule();",
        " *   }",
        " *   __set_current_state(TASK_RUNNING);",
        " *",
        " * If the caller does not need such serialisation (because, for instance, the",
        " * CONDITION test and condition change and wakeup are under the same lock) then",
        " * use __set_current_state().",
        " *",
        " * The above is typically ordered against the wakeup, which does:",
        " *",
        " *   CONDITION = 1;",
        " *   wake_up_state(p, TASK_UNINTERRUPTIBLE);",
        " *",
        " * where wake_up_state()/try_to_wake_up() executes a full memory barrier before",
        " * accessing p->__state.",
        " *",
        " * Wakeup will do: if (@state & p->__state) p->__state = TASK_RUNNING, that is,",
        " * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a",
        " * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).",
        " *",
        " * However, with slightly different timing the wakeup TASK_RUNNING store can",
        " * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not",
        " * a problem either because that will result in one extra go around the loop",
        " * and our @cond test will save the day.",
        " *",
        " * Also see the comments of try_to_wake_up().",
        " */",
        "#define __set_current_state(state_value)				\\",
        "	do {								\\",
        "		debug_normal_state_change((state_value));		\\",
        "		WRITE_ONCE(current->__state, (state_value));		\\",
        "	} while (0)",
        "",
        "#define set_current_state(state_value)					\\",
        "	do {								\\",
        "		debug_normal_state_change((state_value));		\\",
        "		smp_store_mb(current->__state, (state_value));		\\",
        "	} while (0)",
        "",
        "/*",
        " * set_special_state() should be used for those states when the blocking task",
        " * can not use the regular condition based wait-loop. In that case we must",
        " * serialize against wakeups such that any possible in-flight TASK_RUNNING",
        " * stores will not collide with our state change.",
        " */",
        "#define set_special_state(state_value)					\\",
        "	do {								\\",
        "		unsigned long flags; /* may shadow */			\\",
        "									\\",
        "		raw_spin_lock_irqsave(&current->pi_lock, flags);	\\",
        "		debug_special_state_change((state_value));		\\",
        "		WRITE_ONCE(current->__state, (state_value));		\\",
        "		raw_spin_unlock_irqrestore(&current->pi_lock, flags);	\\",
        "	} while (0)",
        "",
        "/*",
        " * PREEMPT_RT specific variants for \"sleeping\" spin/rwlocks",
        " *",
        " * RT's spin/rwlock substitutions are state preserving. The state of the",
        " * task when blocking on the lock is saved in task_struct::saved_state and",
        " * restored after the lock has been acquired.  These operations are",
        " * serialized by task_struct::pi_lock against try_to_wake_up(). Any non RT",
        " * lock related wakeups while the task is blocked on the lock are",
        " * redirected to operate on task_struct::saved_state to ensure that these",
        " * are not dropped. On restore task_struct::saved_state is set to",
        " * TASK_RUNNING so any wakeup attempt redirected to saved_state will fail.",
        " *",
        " * The lock operation looks like this:",
        " *",
        " *	current_save_and_set_rtlock_wait_state();",
        " *	for (;;) {",
        " *		if (try_lock())",
        " *			break;",
        " *		raw_spin_unlock_irq(&lock->wait_lock);",
        " *		schedule_rtlock();",
        " *		raw_spin_lock_irq(&lock->wait_lock);",
        " *		set_current_state(TASK_RTLOCK_WAIT);",
        " *	}",
        " *	current_restore_rtlock_saved_state();",
        " */",
        "#define current_save_and_set_rtlock_wait_state()			\\",
        "	do {								\\",
        "		lockdep_assert_irqs_disabled();				\\",
        "		raw_spin_lock(&current->pi_lock);			\\",
        "		current->saved_state = current->__state;		\\",
        "		debug_rtlock_wait_set_state();				\\",
        "		WRITE_ONCE(current->__state, TASK_RTLOCK_WAIT);		\\",
        "		raw_spin_unlock(&current->pi_lock);			\\",
        "	} while (0);",
        "",
        "#define current_restore_rtlock_saved_state()				\\",
        "	do {								\\",
        "		lockdep_assert_irqs_disabled();				\\",
        "		raw_spin_lock(&current->pi_lock);			\\",
        "		debug_rtlock_wait_restore_state();			\\",
        "		WRITE_ONCE(current->__state, current->saved_state);	\\",
        "		current->saved_state = TASK_RUNNING;			\\",
        "		raw_spin_unlock(&current->pi_lock);			\\",
        "	} while (0);",
        "",
        "#define get_current_state()	READ_ONCE(current->__state)",
        "",
        "/*",
        " * Define the task command name length as enum, then it can be visible to",
        " * BPF programs.",
        " */",
        "enum {",
        "	TASK_COMM_LEN = 16,",
        "};",
        "",
        "extern void sched_tick(void);",
        "",
        "#define	MAX_SCHEDULE_TIMEOUT		LONG_MAX",
        "",
        "extern long schedule_timeout(long timeout);",
        "extern long schedule_timeout_interruptible(long timeout);",
        "extern long schedule_timeout_killable(long timeout);",
        "extern long schedule_timeout_uninterruptible(long timeout);",
        "extern long schedule_timeout_idle(long timeout);",
        "asmlinkage void schedule(void);",
        "extern void schedule_preempt_disabled(void);",
        "asmlinkage void preempt_schedule_irq(void);",
        "#ifdef CONFIG_PREEMPT_RT",
        " extern void schedule_rtlock(void);",
        "#endif",
        "",
        "extern int __must_check io_schedule_prepare(void);",
        "extern void io_schedule_finish(int token);",
        "extern long io_schedule_timeout(long timeout);",
        "extern void io_schedule(void);",
        "",
        "/**",
        " * struct prev_cputime - snapshot of system and user cputime",
        " * @utime: time spent in user mode",
        " * @stime: time spent in system mode",
        " * @lock: protects the above two fields",
        " *",
        " * Stores previous user/system time values such that we can guarantee",
        " * monotonicity.",
        " */",
        "struct prev_cputime {",
        "#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE",
        "	u64				utime;",
        "	u64				stime;",
        "	raw_spinlock_t			lock;",
        "#endif",
        "};",
        "",
        "enum vtime_state {",
        "	/* Task is sleeping or running in a CPU with VTIME inactive: */",
        "	VTIME_INACTIVE = 0,",
        "	/* Task is idle */",
        "	VTIME_IDLE,",
        "	/* Task runs in kernelspace in a CPU with VTIME active: */",
        "	VTIME_SYS,",
        "	/* Task runs in userspace in a CPU with VTIME active: */",
        "	VTIME_USER,",
        "	/* Task runs as guests in a CPU with VTIME active: */",
        "	VTIME_GUEST,",
        "};",
        "",
        "struct vtime {",
        "	seqcount_t		seqcount;",
        "	unsigned long long	starttime;",
        "	enum vtime_state	state;",
        "	unsigned int		cpu;",
        "	u64			utime;",
        "	u64			stime;",
        "	u64			gtime;",
        "};",
        "",
        "/*",
        " * Utilization clamp constraints.",
        " * @UCLAMP_MIN:	Minimum utilization",
        " * @UCLAMP_MAX:	Maximum utilization",
        " * @UCLAMP_CNT:	Utilization clamp constraints count",
        " */",
        "enum uclamp_id {",
        "	UCLAMP_MIN = 0,",
        "	UCLAMP_MAX,",
        "	UCLAMP_CNT",
        "};",
        "",
        "#ifdef CONFIG_SMP",
        "extern struct root_domain def_root_domain;",
        "extern struct mutex sched_domains_mutex;",
        "#endif",
        "",
        "struct sched_param {",
        "	int sched_priority;",
        "};",
        "",
        "struct sched_info {",
        "#ifdef CONFIG_SCHED_INFO",
        "	/* Cumulative counters: */",
        "",
        "	/* # of times we have run on this CPU: */",
        "	unsigned long			pcount;",
        "",
        "	/* Time spent waiting on a runqueue: */",
        "	unsigned long long		run_delay;",
        "",
        "	/* Timestamps: */",
        "",
        "	/* When did we last run on a CPU? */",
        "	unsigned long long		last_arrival;",
        "",
        "	/* When were we last queued to run? */",
        "	unsigned long long		last_queued;",
        "",
        "#endif /* CONFIG_SCHED_INFO */",
        "};",
        "",
        "/*",
        " * Integer metrics need fixed point arithmetic, e.g., sched/fair",
        " * has a few: load, load_avg, util_avg, freq, and capacity.",
        " *",
        " * We define a basic fixed point arithmetic range, and then formalize",
        " * all these metrics based on that basic range.",
        " */",
        "# define SCHED_FIXEDPOINT_SHIFT		10",
        "# define SCHED_FIXEDPOINT_SCALE		(1L << SCHED_FIXEDPOINT_SHIFT)",
        "",
        "/* Increase resolution of cpu_capacity calculations */",
        "# define SCHED_CAPACITY_SHIFT		SCHED_FIXEDPOINT_SHIFT",
        "# define SCHED_CAPACITY_SCALE		(1L << SCHED_CAPACITY_SHIFT)",
        "",
        "struct load_weight {",
        "	unsigned long			weight;",
        "	u32				inv_weight;",
        "};",
        "",
        "/*",
        " * The load/runnable/util_avg accumulates an infinite geometric series",
        " * (see __update_load_avg_cfs_rq() in kernel/sched/pelt.c).",
        " *",
        " * [load_avg definition]",
        " *",
        " *   load_avg = runnable% * scale_load_down(load)",
        " *",
        " * [runnable_avg definition]",
        " *",
        " *   runnable_avg = runnable% * SCHED_CAPACITY_SCALE",
        " *",
        " * [util_avg definition]",
        " *",
        " *   util_avg = running% * SCHED_CAPACITY_SCALE",
        " *",
        " * where runnable% is the time ratio that a sched_entity is runnable and",
        " * running% the time ratio that a sched_entity is running.",
        " *",
        " * For cfs_rq, they are the aggregated values of all runnable and blocked",
        " * sched_entities.",
        " *",
        " * The load/runnable/util_avg doesn't directly factor frequency scaling and CPU",
        " * capacity scaling. The scaling is done through the rq_clock_pelt that is used",
        " * for computing those signals (see update_rq_clock_pelt())",
        " *",
        " * N.B., the above ratios (runnable% and running%) themselves are in the",
        " * range of [0, 1]. To do fixed point arithmetics, we therefore scale them",
        " * to as large a range as necessary. This is for example reflected by",
        " * util_avg's SCHED_CAPACITY_SCALE.",
        " *",
        " * [Overflow issue]",
        " *",
        " * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities",
        " * with the highest load (=88761), always runnable on a single cfs_rq,",
        " * and should not overflow as the number already hits PID_MAX_LIMIT.",
        " *",
        " * For all other cases (including 32-bit kernels), struct load_weight's",
        " * weight will overflow first before we do, because:",
        " *",
        " *    Max(load_avg) <= Max(load.weight)",
        " *",
        " * Then it is the load_weight's responsibility to consider overflow",
        " * issues.",
        " */",
        "struct sched_avg {",
        "	u64				last_update_time;",
        "	u64				load_sum;",
        "	u64				runnable_sum;",
        "	u32				util_sum;",
        "	u32				period_contrib;",
        "	unsigned long			load_avg;",
        "	unsigned long			runnable_avg;",
        "	unsigned long			util_avg;",
        "	unsigned int			util_est;",
        "} ____cacheline_aligned;",
        "",
        "/*",
        " * The UTIL_AVG_UNCHANGED flag is used to synchronize util_est with util_avg",
        " * updates. When a task is dequeued, its util_est should not be updated if its",
        " * util_avg has not been updated in the meantime.",
        " * This information is mapped into the MSB bit of util_est at dequeue time.",
        " * Since max value of util_est for a task is 1024 (PELT util_avg for a task)",
        " * it is safe to use MSB.",
        " */",
        "#define UTIL_EST_WEIGHT_SHIFT		2",
        "#define UTIL_AVG_UNCHANGED		0x80000000",
        "",
        "struct sched_statistics {",
        "#ifdef CONFIG_SCHEDSTATS",
        "	u64				wait_start;",
        "	u64				wait_max;",
        "	u64				wait_count;",
        "	u64				wait_sum;",
        "	u64				iowait_count;",
        "	u64				iowait_sum;",
        "",
        "	u64				sleep_start;",
        "	u64				sleep_max;",
        "	s64				sum_sleep_runtime;",
        "",
        "	u64				block_start;",
        "	u64				block_max;",
        "	s64				sum_block_runtime;",
        "",
        "	s64				exec_max;",
        "	u64				slice_max;",
        "",
        "	u64				nr_migrations_cold;",
        "	u64				nr_failed_migrations_affine;",
        "	u64				nr_failed_migrations_running;",
        "	u64				nr_failed_migrations_hot;",
        "	u64				nr_forced_migrations;",
        "",
        "	u64				nr_wakeups;",
        "	u64				nr_wakeups_sync;",
        "	u64				nr_wakeups_migrate;",
        "	u64				nr_wakeups_local;",
        "	u64				nr_wakeups_remote;",
        "	u64				nr_wakeups_affine;",
        "	u64				nr_wakeups_affine_attempts;",
        "	u64				nr_wakeups_passive;",
        "	u64				nr_wakeups_idle;",
        "",
        "#ifdef CONFIG_SCHED_CORE",
        "	u64				core_forceidle_sum;",
        "#endif",
        "#endif /* CONFIG_SCHEDSTATS */",
        "} ____cacheline_aligned;",
        "",
        "struct sched_entity {",
        "	/* For load-balancing: */",
        "	struct load_weight		load;",
        "	struct rb_node			run_node;",
        "	u64				deadline;",
        "	u64				min_vruntime;",
        "	u64				min_slice;",
        "",
        "	struct list_head		group_node;",
        "	unsigned char			on_rq;",
        "	unsigned char			sched_delayed;",
        "	unsigned char			rel_deadline;",
        "	unsigned char			custom_slice;",
        "					/* hole */",
        "",
        "	u64				exec_start;",
        "	u64				sum_exec_runtime;",
        "	u64				prev_sum_exec_runtime;",
        "	u64				vruntime;",
        "	s64				vlag;",
        "	u64				slice;",
        "",
        "	u64				nr_migrations;",
        "",
        "#ifdef CONFIG_FAIR_GROUP_SCHED",
        "	int				depth;",
        "	struct sched_entity		*parent;",
        "	/* rq on which this entity is (to be) queued: */",
        "	struct cfs_rq			*cfs_rq;",
        "	/* rq \"owned\" by this entity/group: */",
        "	struct cfs_rq			*my_q;",
        "	/* cached value of my_q->h_nr_running */",
        "	unsigned long			runnable_weight;",
        "#endif",
        "",
        "#ifdef CONFIG_SMP",
        "	/*",
        "	 * Per entity load average tracking.",
        "	 *",
        "	 * Put into separate cache line so it does not",
        "	 * collide with read-mostly values above.",
        "	 */",
        "	struct sched_avg		avg;",
        "#endif",
        "};",
        "",
        "struct sched_rt_entity {",
        "	struct list_head		run_list;",
        "	unsigned long			timeout;",
        "	unsigned long			watchdog_stamp;",
        "	unsigned int			time_slice;",
        "	unsigned short			on_rq;",
        "	unsigned short			on_list;",
        "",
        "	struct sched_rt_entity		*back;",
        "#ifdef CONFIG_RT_GROUP_SCHED",
        "	struct sched_rt_entity		*parent;",
        "	/* rq on which this entity is (to be) queued: */",
        "	struct rt_rq			*rt_rq;",
        "	/* rq \"owned\" by this entity/group: */",
        "	struct rt_rq			*my_q;",
        "#endif",
        "} __randomize_layout;",
        "",
        "typedef bool (*dl_server_has_tasks_f)(struct sched_dl_entity *);",
        "typedef struct task_struct *(*dl_server_pick_f)(struct sched_dl_entity *);",
        "",
        "struct sched_dl_entity {",
        "	struct rb_node			rb_node;",
        "",
        "	/*",
        "	 * Original scheduling parameters. Copied here from sched_attr",
        "	 * during sched_setattr(), they will remain the same until",
        "	 * the next sched_setattr().",
        "	 */",
        "	u64				dl_runtime;	/* Maximum runtime for each instance	*/",
        "	u64				dl_deadline;	/* Relative deadline of each instance	*/",
        "	u64				dl_period;	/* Separation of two instances (period) */",
        "	u64				dl_bw;		/* dl_runtime / dl_period		*/",
        "	u64				dl_density;	/* dl_runtime / dl_deadline		*/",
        "",
        "	/*",
        "	 * Actual scheduling parameters. Initialized with the values above,",
        "	 * they are continuously updated during task execution. Note that",
        "	 * the remaining runtime could be < 0 in case we are in overrun.",
        "	 */",
        "	s64				runtime;	/* Remaining runtime for this instance	*/",
        "	u64				deadline;	/* Absolute deadline for this instance	*/",
        "	unsigned int			flags;		/* Specifying the scheduler behaviour	*/",
        "",
        "	/*",
        "	 * Some bool flags:",
        "	 *",
        "	 * @dl_throttled tells if we exhausted the runtime. If so, the",
        "	 * task has to wait for a replenishment to be performed at the",
        "	 * next firing of dl_timer.",
        "	 *",
        "	 * @dl_yielded tells if task gave up the CPU before consuming",
        "	 * all its available runtime during the last job.",
        "	 *",
        "	 * @dl_non_contending tells if the task is inactive while still",
        "	 * contributing to the active utilization. In other words, it",
        "	 * indicates if the inactive timer has been armed and its handler",
        "	 * has not been executed yet. This flag is useful to avoid race",
        "	 * conditions between the inactive timer handler and the wakeup",
        "	 * code.",
        "	 *",
        "	 * @dl_overrun tells if the task asked to be informed about runtime",
        "	 * overruns.",
        "	 *",
        "	 * @dl_server tells if this is a server entity.",
        "	 *",
        "	 * @dl_defer tells if this is a deferred or regular server. For",
        "	 * now only defer server exists.",
        "	 *",
        "	 * @dl_defer_armed tells if the deferrable server is waiting",
        "	 * for the replenishment timer to activate it.",
        "	 *",
        "	 * @dl_server_active tells if the dlserver is active(started).",
        "	 * dlserver is started on first cfs enqueue on an idle runqueue",
        "	 * and is stopped when a dequeue results in 0 cfs tasks on the",
        "	 * runqueue. In other words, dlserver is active only when cpu's",
        "	 * runqueue has atleast one cfs task.",
        "	 *",
        "	 * @dl_defer_running tells if the deferrable server is actually",
        "	 * running, skipping the defer phase.",
        "	 */",
        "	unsigned int			dl_throttled      : 1;",
        "	unsigned int			dl_yielded        : 1;",
        "	unsigned int			dl_non_contending : 1;",
        "	unsigned int			dl_overrun	  : 1;",
        "	unsigned int			dl_server         : 1;",
        "	unsigned int			dl_server_active  : 1;",
        "	unsigned int			dl_defer	  : 1;",
        "	unsigned int			dl_defer_armed	  : 1;",
        "	unsigned int			dl_defer_running  : 1;",
        "",
        "	/*",
        "	 * Bandwidth enforcement timer. Each -deadline task has its",
        "	 * own bandwidth to be enforced, thus we need one timer per task.",
        "	 */",
        "	struct hrtimer			dl_timer;",
        "",
        "	/*",
        "	 * Inactive timer, responsible for decreasing the active utilization",
        "	 * at the \"0-lag time\". When a -deadline task blocks, it contributes",
        "	 * to GRUB's active utilization until the \"0-lag time\", hence a",
        "	 * timer is needed to decrease the active utilization at the correct",
        "	 * time.",
        "	 */",
        "	struct hrtimer			inactive_timer;",
        "",
        "	/*",
        "	 * Bits for DL-server functionality. Also see the comment near",
        "	 * dl_server_update().",
        "	 *",
        "	 * @rq the runqueue this server is for",
        "	 *",
        "	 * @server_has_tasks() returns true if @server_pick return a",
        "	 * runnable task.",
        "	 */",
        "	struct rq			*rq;",
        "	dl_server_has_tasks_f		server_has_tasks;",
        "	dl_server_pick_f		server_pick_task;",
        "",
        "#ifdef CONFIG_RT_MUTEXES",
        "	/*",
        "	 * Priority Inheritance. When a DEADLINE scheduling entity is boosted",
        "	 * pi_se points to the donor, otherwise points to the dl_se it belongs",
        "	 * to (the original one/itself).",
        "	 */",
        "	struct sched_dl_entity *pi_se;",
        "#endif",
        "};",
        "",
        "#ifdef CONFIG_UCLAMP_TASK",
        "/* Number of utilization clamp buckets (shorter alias) */",
        "#define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT",
        "",
        "/*",
        " * Utilization clamp for a scheduling entity",
        " * @value:		clamp value \"assigned\" to a se",
        " * @bucket_id:		bucket index corresponding to the \"assigned\" value",
        " * @active:		the se is currently refcounted in a rq's bucket",
        " * @user_defined:	the requested clamp value comes from user-space",
        " *",
        " * The bucket_id is the index of the clamp bucket matching the clamp value",
        " * which is pre-computed and stored to avoid expensive integer divisions from",
        " * the fast path.",
        " *",
        " * The active bit is set whenever a task has got an \"effective\" value assigned,",
        " * which can be different from the clamp value \"requested\" from user-space.",
        " * This allows to know a task is refcounted in the rq's bucket corresponding",
        " * to the \"effective\" bucket_id.",
        " *",
        " * The user_defined bit is set whenever a task has got a task-specific clamp",
        " * value requested from userspace, i.e. the system defaults apply to this task",
        " * just as a restriction. This allows to relax default clamps when a less",
        " * restrictive task-specific value has been requested, thus allowing to",
        " * implement a \"nice\" semantic. For example, a task running with a 20%",
        " * default boost can still drop its own boosting to 0%.",
        " */",
        "struct uclamp_se {",
        "	unsigned int value		: bits_per(SCHED_CAPACITY_SCALE);",
        "	unsigned int bucket_id		: bits_per(UCLAMP_BUCKETS);",
        "	unsigned int active		: 1;",
        "	unsigned int user_defined	: 1;",
        "};",
        "#endif /* CONFIG_UCLAMP_TASK */",
        "",
        "union rcu_special {",
        "	struct {",
        "		u8			blocked;",
        "		u8			need_qs;",
        "		u8			exp_hint; /* Hint for performance. */",
        "		u8			need_mb; /* Readers need smp_mb(). */",
        "	} b; /* Bits. */",
        "	u32 s; /* Set of bits. */",
        "};",
        "",
        "enum perf_event_task_context {",
        "	perf_invalid_context = -1,",
        "	perf_hw_context = 0,",
        "	perf_sw_context,",
        "	perf_nr_task_contexts,",
        "};",
        "",
        "/*",
        " * Number of contexts where an event can trigger:",
        " *      task, softirq, hardirq, nmi.",
        " */",
        "#define PERF_NR_CONTEXTS	4",
        "",
        "struct wake_q_node {",
        "	struct wake_q_node *next;",
        "};",
        "",
        "struct kmap_ctrl {",
        "#ifdef CONFIG_KMAP_LOCAL",
        "	int				idx;",
        "	pte_t				pteval[KM_MAX_IDX];",
        "#endif",
        "};",
        "",
        "struct task_struct {",
        "#ifdef CONFIG_THREAD_INFO_IN_TASK",
        "	/*",
        "	 * For reasons of header soup (see current_thread_info()), this",
        "	 * must be the first element of task_struct.",
        "	 */",
        "	struct thread_info		thread_info;",
        "#endif",
        "	unsigned int			__state;",
        "",
        "	/* saved state for \"spinlock sleepers\" */",
        "	unsigned int			saved_state;",
        "",
        "	/*",
        "	 * This begins the randomizable portion of task_struct. Only",
        "	 * scheduling-critical items should be added above here.",
        "	 */",
        "	randomized_struct_fields_start",
        "",
        "	void				*stack;",
        "	refcount_t			usage;",
        "	/* Per task flags (PF_*), defined further below: */",
        "	unsigned int			flags;",
        "	unsigned int			ptrace;",
        "",
        "#ifdef CONFIG_MEM_ALLOC_PROFILING",
        "	struct alloc_tag		*alloc_tag;",
        "#endif",
        "",
        "#ifdef CONFIG_SMP",
        "	int				on_cpu;",
        "	struct __call_single_node	wake_entry;",
        "	unsigned int			wakee_flips;",
        "	unsigned long			wakee_flip_decay_ts;",
        "	struct task_struct		*last_wakee;",
        "",
        "	/*",
        "	 * recent_used_cpu is initially set as the last CPU used by a task",
        "	 * that wakes affine another task. Waker/wakee relationships can",
        "	 * push tasks around a CPU where each wakeup moves to the next one.",
        "	 * Tracking a recently used CPU allows a quick search for a recently",
        "	 * used CPU that may be idle.",
        "	 */",
        "	int				recent_used_cpu;",
        "	int				wake_cpu;",
        "#endif",
        "	int				on_rq;",
        "",
        "	int				prio;",
        "	int				static_prio;",
        "	int				normal_prio;",
        "	unsigned int			rt_priority;",
        "",
        "	struct sched_entity		se;",
        "	struct sched_rt_entity		rt;",
        "	struct sched_dl_entity		dl;",
        "	struct sched_dl_entity		*dl_server;",
        "#ifdef CONFIG_SCHED_CLASS_EXT",
        "	struct sched_ext_entity		scx;",
        "#endif",
        "	const struct sched_class	*sched_class;",
        "",
        "#ifdef CONFIG_SCHED_CORE",
        "	struct rb_node			core_node;",
        "	unsigned long			core_cookie;",
        "	unsigned int			core_occupation;",
        "#endif",
        "",
        "#ifdef CONFIG_CGROUP_SCHED",
        "	struct task_group		*sched_task_group;",
        "#endif",
        "",
        "",
        "#ifdef CONFIG_UCLAMP_TASK",
        "	/*",
        "	 * Clamp values requested for a scheduling entity.",
        "	 * Must be updated with task_rq_lock() held.",
        "	 */",
        "	struct uclamp_se		uclamp_req[UCLAMP_CNT];",
        "	/*",
        "	 * Effective clamp values used for a scheduling entity.",
        "	 * Must be updated with task_rq_lock() held.",
        "	 */",
        "	struct uclamp_se		uclamp[UCLAMP_CNT];",
        "#endif",
        "",
        "	struct sched_statistics         stats;",
        "",
        "#ifdef CONFIG_PREEMPT_NOTIFIERS",
        "	/* List of struct preempt_notifier: */",
        "	struct hlist_head		preempt_notifiers;",
        "#endif",
        "",
        "#ifdef CONFIG_BLK_DEV_IO_TRACE",
        "	unsigned int			btrace_seq;",
        "#endif",
        "",
        "	unsigned int			policy;",
        "	unsigned long			max_allowed_capacity;",
        "	int				nr_cpus_allowed;",
        "	const cpumask_t			*cpus_ptr;",
        "	cpumask_t			*user_cpus_ptr;",
        "	cpumask_t			cpus_mask;",
        "	void				*migration_pending;",
        "#ifdef CONFIG_SMP",
        "	unsigned short			migration_disabled;",
        "#endif",
        "	unsigned short			migration_flags;",
        "",
        "#ifdef CONFIG_PREEMPT_RCU",
        "	int				rcu_read_lock_nesting;",
        "	union rcu_special		rcu_read_unlock_special;",
        "	struct list_head		rcu_node_entry;",
        "	struct rcu_node			*rcu_blocked_node;",
        "#endif /* #ifdef CONFIG_PREEMPT_RCU */",
        "",
        "#ifdef CONFIG_TASKS_RCU",
        "	unsigned long			rcu_tasks_nvcsw;",
        "	u8				rcu_tasks_holdout;",
        "	u8				rcu_tasks_idx;",
        "	int				rcu_tasks_idle_cpu;",
        "	struct list_head		rcu_tasks_holdout_list;",
        "	int				rcu_tasks_exit_cpu;",
        "	struct list_head		rcu_tasks_exit_list;",
        "#endif /* #ifdef CONFIG_TASKS_RCU */",
        "",
        "#ifdef CONFIG_TASKS_TRACE_RCU",
        "	int				trc_reader_nesting;",
        "	int				trc_ipi_to_cpu;",
        "	union rcu_special		trc_reader_special;",
        "	struct list_head		trc_holdout_list;",
        "	struct list_head		trc_blkd_node;",
        "	int				trc_blkd_cpu;",
        "#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */",
        "",
        "	struct sched_info		sched_info;",
        "",
        "	struct list_head		tasks;",
        "#ifdef CONFIG_SMP",
        "	struct plist_node		pushable_tasks;",
        "	struct rb_node			pushable_dl_tasks;",
        "#endif",
        "",
        "	struct mm_struct		*mm;",
        "	struct mm_struct		*active_mm;",
        "	struct address_space		*faults_disabled_mapping;",
        "",
        "	int				exit_state;",
        "	int				exit_code;",
        "	int				exit_signal;",
        "	/* The signal sent when the parent dies: */",
        "	int				pdeath_signal;",
        "	/* JOBCTL_*, siglock protected: */",
        "	unsigned long			jobctl;",
        "",
        "	/* Used for emulating ABI behavior of previous Linux versions: */",
        "	unsigned int			personality;",
        "",
        "	/* Scheduler bits, serialized by scheduler locks: */",
        "	unsigned			sched_reset_on_fork:1;",
        "	unsigned			sched_contributes_to_load:1;",
        "	unsigned			sched_migrated:1;",
        "	unsigned			sched_task_hot:1;",
        "",
        "	/* Force alignment to the next boundary: */",
        "	unsigned			:0;",
        "",
        "	/* Unserialized, strictly 'current' */",
        "",
        "	/*",
        "	 * This field must not be in the scheduler word above due to wakelist",
        "	 * queueing no longer being serialized by p->on_cpu. However:",
        "	 *",
        "	 * p->XXX = X;			ttwu()",
        "	 * schedule()			  if (p->on_rq && ..) // false",
        "	 *   smp_mb__after_spinlock();	  if (smp_load_acquire(&p->on_cpu) && //true",
        "	 *   deactivate_task()		      ttwu_queue_wakelist())",
        "	 *     p->on_rq = 0;			p->sched_remote_wakeup = Y;",
        "	 *",
        "	 * guarantees all stores of 'current' are visible before",
        "	 * ->sched_remote_wakeup gets used, so it can be in this word.",
        "	 */",
        "	unsigned			sched_remote_wakeup:1;",
        "#ifdef CONFIG_RT_MUTEXES",
        "	unsigned			sched_rt_mutex:1;",
        "#endif",
        "",
        "	/* Bit to tell TOMOYO we're in execve(): */",
        "	unsigned			in_execve:1;",
        "	unsigned			in_iowait:1;",
        "#ifndef TIF_RESTORE_SIGMASK",
        "	unsigned			restore_sigmask:1;",
        "#endif",
        "#ifdef CONFIG_MEMCG_V1",
        "	unsigned			in_user_fault:1;",
        "#endif",
        "#ifdef CONFIG_LRU_GEN",
        "	/* whether the LRU algorithm may apply to this access */",
        "	unsigned			in_lru_fault:1;",
        "#endif",
        "#ifdef CONFIG_COMPAT_BRK",
        "	unsigned			brk_randomized:1;",
        "#endif",
        "#ifdef CONFIG_CGROUPS",
        "	/* disallow userland-initiated cgroup migration */",
        "	unsigned			no_cgroup_migration:1;",
        "	/* task is frozen/stopped (used by the cgroup freezer) */",
        "	unsigned			frozen:1;",
        "#endif",
        "#ifdef CONFIG_BLK_CGROUP",
        "	unsigned			use_memdelay:1;",
        "#endif",
        "#ifdef CONFIG_PSI",
        "	/* Stalled due to lack of memory */",
        "	unsigned			in_memstall:1;",
        "#endif",
        "#ifdef CONFIG_PAGE_OWNER",
        "	/* Used by page_owner=on to detect recursion in page tracking. */",
        "	unsigned			in_page_owner:1;",
        "#endif",
        "#ifdef CONFIG_EVENTFD",
        "	/* Recursion prevention for eventfd_signal() */",
        "	unsigned			in_eventfd:1;",
        "#endif",
        "#ifdef CONFIG_ARCH_HAS_CPU_PASID",
        "	unsigned			pasid_activated:1;",
        "#endif",
        "#ifdef CONFIG_X86_BUS_LOCK_DETECT",
        "	unsigned			reported_split_lock:1;",
        "#endif",
        "#ifdef CONFIG_TASK_DELAY_ACCT",
        "	/* delay due to memory thrashing */",
        "	unsigned                        in_thrashing:1;",
        "#endif",
        "#ifdef CONFIG_PREEMPT_RT",
        "	struct netdev_xmit		net_xmit;",
        "#endif",
        "	unsigned long			atomic_flags; /* Flags requiring atomic access. */",
        "",
        "	struct restart_block		restart_block;",
        "",
        "	pid_t				pid;",
        "	pid_t				tgid;",
        "",
        "#ifdef CONFIG_STACKPROTECTOR",
        "	/* Canary value for the -fstack-protector GCC feature: */",
        "	unsigned long			stack_canary;",
        "#endif",
        "	/*",
        "	 * Pointers to the (original) parent process, youngest child, younger sibling,",
        "	 * older sibling, respectively.  (p->father can be replaced with",
        "	 * p->real_parent->pid)",
        "	 */",
        "",
        "	/* Real parent process: */",
        "	struct task_struct __rcu	*real_parent;",
        "",
        "	/* Recipient of SIGCHLD, wait4() reports: */",
        "	struct task_struct __rcu	*parent;",
        "",
        "	/*",
        "	 * Children/sibling form the list of natural children:",
        "	 */",
        "	struct list_head		children;",
        "	struct list_head		sibling;",
        "	struct task_struct		*group_leader;",
        "",
        "	/*",
        "	 * 'ptraced' is the list of tasks this task is using ptrace() on.",
        "	 *",
        "	 * This includes both natural children and PTRACE_ATTACH targets.",
        "	 * 'ptrace_entry' is this task's link on the p->parent->ptraced list.",
        "	 */",
        "	struct list_head		ptraced;",
        "	struct list_head		ptrace_entry;",
        "",
        "	/* PID/PID hash table linkage. */",
        "	struct pid			*thread_pid;",
        "	struct hlist_node		pid_links[PIDTYPE_MAX];",
        "	struct list_head		thread_node;",
        "",
        "	struct completion		*vfork_done;",
        "",
        "	/* CLONE_CHILD_SETTID: */",
        "	int __user			*set_child_tid;",
        "",
        "	/* CLONE_CHILD_CLEARTID: */",
        "	int __user			*clear_child_tid;",
        "",
        "	/* PF_KTHREAD | PF_IO_WORKER */",
        "	void				*worker_private;",
        "",
        "	u64				utime;",
        "	u64				stime;",
        "#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME",
        "	u64				utimescaled;",
        "	u64				stimescaled;",
        "#endif",
        "	u64				gtime;",
        "	struct prev_cputime		prev_cputime;",
        "#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN",
        "	struct vtime			vtime;",
        "#endif",
        "",
        "#ifdef CONFIG_NO_HZ_FULL",
        "	atomic_t			tick_dep_mask;",
        "#endif",
        "	/* Context switch counts: */",
        "	unsigned long			nvcsw;",
        "	unsigned long			nivcsw;",
        "",
        "	/* Monotonic time in nsecs: */",
        "	u64				start_time;",
        "",
        "	/* Boot based time in nsecs: */",
        "	u64				start_boottime;",
        "",
        "	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */",
        "	unsigned long			min_flt;",
        "	unsigned long			maj_flt;",
        "",
        "	/* Empty if CONFIG_POSIX_CPUTIMERS=n */",
        "	struct posix_cputimers		posix_cputimers;",
        "",
        "#ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK",
        "	struct posix_cputimers_work	posix_cputimers_work;",
        "#endif",
        "",
        "	/* Process credentials: */",
        "",
        "	/* Tracer's credentials at attach: */",
        "	const struct cred __rcu		*ptracer_cred;",
        "",
        "	/* Objective and real subjective task credentials (COW): */",
        "	const struct cred __rcu		*real_cred;",
        "",
        "	/* Effective (overridable) subjective task credentials (COW): */",
        "	const struct cred __rcu		*cred;",
        "",
        "#ifdef CONFIG_KEYS",
        "	/* Cached requested key. */",
        "	struct key			*cached_requested_key;",
        "#endif",
        "",
        "	/*",
        "	 * executable name, excluding path.",
        "	 *",
        "	 * - normally initialized begin_new_exec()",
        "	 * - set it with set_task_comm()",
        "	 *   - strscpy_pad() to ensure it is always NUL-terminated and",
        "	 *     zero-padded",
        "	 *   - task_lock() to ensure the operation is atomic and the name is",
        "	 *     fully updated.",
        "	 */",
        "	char				comm[TASK_COMM_LEN];",
        "",
        "	struct nameidata		*nameidata;",
        "",
        "#ifdef CONFIG_SYSVIPC",
        "	struct sysv_sem			sysvsem;",
        "	struct sysv_shm			sysvshm;",
        "#endif",
        "#ifdef CONFIG_DETECT_HUNG_TASK",
        "	unsigned long			last_switch_count;",
        "	unsigned long			last_switch_time;",
        "#endif",
        "	/* Filesystem information: */",
        "	struct fs_struct		*fs;",
        "",
        "	/* Open file information: */",
        "	struct files_struct		*files;",
        "",
        "#ifdef CONFIG_IO_URING",
        "	struct io_uring_task		*io_uring;",
        "#endif",
        "",
        "	/* Namespaces: */",
        "	struct nsproxy			*nsproxy;",
        "",
        "	/* Signal handlers: */",
        "	struct signal_struct		*signal;",
        "	struct sighand_struct __rcu		*sighand;",
        "	sigset_t			blocked;",
        "	sigset_t			real_blocked;",
        "	/* Restored if set_restore_sigmask() was used: */",
        "	sigset_t			saved_sigmask;",
        "	struct sigpending		pending;",
        "	unsigned long			sas_ss_sp;",
        "	size_t				sas_ss_size;",
        "	unsigned int			sas_ss_flags;",
        "",
        "	struct callback_head		*task_works;",
        "",
        "#ifdef CONFIG_AUDIT",
        "#ifdef CONFIG_AUDITSYSCALL",
        "	struct audit_context		*audit_context;",
        "#endif",
        "	kuid_t				loginuid;",
        "	unsigned int			sessionid;",
        "#endif",
        "	struct seccomp			seccomp;",
        "	struct syscall_user_dispatch	syscall_dispatch;",
        "",
        "	/* Thread group tracking: */",
        "	u64				parent_exec_id;",
        "	u64				self_exec_id;",
        "",
        "	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */",
        "	spinlock_t			alloc_lock;",
        "",
        "	/* Protection of the PI data structures: */",
        "	raw_spinlock_t			pi_lock;",
        "",
        "	struct wake_q_node		wake_q;",
        "",
        "#ifdef CONFIG_RT_MUTEXES",
        "	/* PI waiters blocked on a rt_mutex held by this task: */",
        "	struct rb_root_cached		pi_waiters;",
        "	/* Updated under owner's pi_lock and rq lock */",
        "	struct task_struct		*pi_top_task;",
        "	/* Deadlock detection and priority inheritance handling: */",
        "	struct rt_mutex_waiter		*pi_blocked_on;",
        "#endif",
        "",
        "#ifdef CONFIG_DEBUG_MUTEXES",
        "	/* Mutex deadlock detection: */",
        "	struct mutex_waiter		*blocked_on;",
        "#endif",
        "",
        "#ifdef CONFIG_DEBUG_ATOMIC_SLEEP",
        "	int				non_block_count;",
        "#endif",
        "",
        "#ifdef CONFIG_TRACE_IRQFLAGS",
        "	struct irqtrace_events		irqtrace;",
        "	unsigned int			hardirq_threaded;",
        "	u64				hardirq_chain_key;",
        "	int				softirqs_enabled;",
        "	int				softirq_context;",
        "	int				irq_config;",
        "#endif",
        "#ifdef CONFIG_PREEMPT_RT",
        "	int				softirq_disable_cnt;",
        "#endif",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "# define MAX_LOCK_DEPTH			48UL",
        "	u64				curr_chain_key;",
        "	int				lockdep_depth;",
        "	unsigned int			lockdep_recursion;",
        "	struct held_lock		held_locks[MAX_LOCK_DEPTH];",
        "#endif",
        "",
        "#if defined(CONFIG_UBSAN) && !defined(CONFIG_UBSAN_TRAP)",
        "	unsigned int			in_ubsan;",
        "#endif",
        "",
        "	/* Journalling filesystem info: */",
        "	void				*journal_info;",
        "",
        "	/* Stacked block device info: */",
        "	struct bio_list			*bio_list;",
        "",
        "	/* Stack plugging: */",
        "	struct blk_plug			*plug;",
        "",
        "	/* VM state: */",
        "	struct reclaim_state		*reclaim_state;",
        "",
        "	struct io_context		*io_context;",
        "",
        "#ifdef CONFIG_COMPACTION",
        "	struct capture_control		*capture_control;",
        "#endif",
        "	/* Ptrace state: */",
        "	unsigned long			ptrace_message;",
        "	kernel_siginfo_t		*last_siginfo;",
        "",
        "	struct task_io_accounting	ioac;",
        "#ifdef CONFIG_PSI",
        "	/* Pressure stall state */",
        "	unsigned int			psi_flags;",
        "#endif",
        "#ifdef CONFIG_TASK_XACCT",
        "	/* Accumulated RSS usage: */",
        "	u64				acct_rss_mem1;",
        "	/* Accumulated virtual memory usage: */",
        "	u64				acct_vm_mem1;",
        "	/* stime + utime since last update: */",
        "	u64				acct_timexpd;",
        "#endif",
        "#ifdef CONFIG_CPUSETS",
        "	/* Protected by ->alloc_lock: */",
        "	nodemask_t			mems_allowed;",
        "	/* Sequence number to catch updates: */",
        "	seqcount_spinlock_t		mems_allowed_seq;",
        "	int				cpuset_mem_spread_rotor;",
        "#endif",
        "#ifdef CONFIG_CGROUPS",
        "	/* Control Group info protected by css_set_lock: */",
        "	struct css_set __rcu		*cgroups;",
        "	/* cg_list protected by css_set_lock and tsk->alloc_lock: */",
        "	struct list_head		cg_list;",
        "#endif",
        "#ifdef CONFIG_X86_CPU_RESCTRL",
        "	u32				closid;",
        "	u32				rmid;",
        "#endif",
        "#ifdef CONFIG_FUTEX",
        "	struct robust_list_head __user	*robust_list;",
        "#ifdef CONFIG_COMPAT",
        "	struct compat_robust_list_head __user *compat_robust_list;",
        "#endif",
        "	struct list_head		pi_state_list;",
        "	struct futex_pi_state		*pi_state_cache;",
        "	struct mutex			futex_exit_mutex;",
        "	unsigned int			futex_state;",
        "#endif",
        "#ifdef CONFIG_PERF_EVENTS",
        "	u8				perf_recursion[PERF_NR_CONTEXTS];",
        "	struct perf_event_context	*perf_event_ctxp;",
        "	struct mutex			perf_event_mutex;",
        "	struct list_head		perf_event_list;",
        "#endif",
        "#ifdef CONFIG_DEBUG_PREEMPT",
        "	unsigned long			preempt_disable_ip;",
        "#endif",
        "#ifdef CONFIG_NUMA",
        "	/* Protected by alloc_lock: */",
        "	struct mempolicy		*mempolicy;",
        "	short				il_prev;",
        "	u8				il_weight;",
        "	short				pref_node_fork;",
        "#endif",
        "#ifdef CONFIG_NUMA_BALANCING",
        "	int				numa_scan_seq;",
        "	unsigned int			numa_scan_period;",
        "	unsigned int			numa_scan_period_max;",
        "	int				numa_preferred_nid;",
        "	unsigned long			numa_migrate_retry;",
        "	/* Migration stamp: */",
        "	u64				node_stamp;",
        "	u64				last_task_numa_placement;",
        "	u64				last_sum_exec_runtime;",
        "	struct callback_head		numa_work;",
        "",
        "	/*",
        "	 * This pointer is only modified for current in syscall and",
        "	 * pagefault context (and for tasks being destroyed), so it can be read",
        "	 * from any of the following contexts:",
        "	 *  - RCU read-side critical section",
        "	 *  - current->numa_group from everywhere",
        "	 *  - task's runqueue locked, task not running",
        "	 */",
        "	struct numa_group __rcu		*numa_group;",
        "",
        "	/*",
        "	 * numa_faults is an array split into four regions:",
        "	 * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer",
        "	 * in this precise order.",
        "	 *",
        "	 * faults_memory: Exponential decaying average of faults on a per-node",
        "	 * basis. Scheduling placement decisions are made based on these",
        "	 * counts. The values remain static for the duration of a PTE scan.",
        "	 * faults_cpu: Track the nodes the process was running on when a NUMA",
        "	 * hinting fault was incurred.",
        "	 * faults_memory_buffer and faults_cpu_buffer: Record faults per node",
        "	 * during the current scan window. When the scan completes, the counts",
        "	 * in faults_memory and faults_cpu decay and these values are copied.",
        "	 */",
        "	unsigned long			*numa_faults;",
        "	unsigned long			total_numa_faults;",
        "",
        "	/*",
        "	 * numa_faults_locality tracks if faults recorded during the last",
        "	 * scan window were remote/local or failed to migrate. The task scan",
        "	 * period is adapted based on the locality of the faults with different",
        "	 * weights depending on whether they were shared or private faults",
        "	 */",
        "	unsigned long			numa_faults_locality[3];",
        "",
        "	unsigned long			numa_pages_migrated;",
        "#endif /* CONFIG_NUMA_BALANCING */",
        "",
        "#ifdef CONFIG_RSEQ",
        "	struct rseq __user *rseq;",
        "	u32 rseq_len;",
        "	u32 rseq_sig;",
        "	/*",
        "	 * RmW on rseq_event_mask must be performed atomically",
        "	 * with respect to preemption.",
        "	 */",
        "	unsigned long rseq_event_mask;",
        "#endif",
        "",
        "#ifdef CONFIG_SCHED_MM_CID",
        "	int				mm_cid;		/* Current cid in mm */",
        "	int				last_mm_cid;	/* Most recent cid in mm */",
        "	int				migrate_from_cpu;",
        "	int				mm_cid_active;	/* Whether cid bitmap is active */",
        "	struct callback_head		cid_work;",
        "#endif",
        "",
        "	struct tlbflush_unmap_batch	tlb_ubc;",
        "",
        "	/* Cache last used pipe for splice(): */",
        "	struct pipe_inode_info		*splice_pipe;",
        "",
        "	struct page_frag		task_frag;",
        "",
        "#ifdef CONFIG_TASK_DELAY_ACCT",
        "	struct task_delay_info		*delays;",
        "#endif",
        "",
        "#ifdef CONFIG_FAULT_INJECTION",
        "	int				make_it_fail;",
        "	unsigned int			fail_nth;",
        "#endif",
        "	/*",
        "	 * When (nr_dirtied >= nr_dirtied_pause), it's time to call",
        "	 * balance_dirty_pages() for a dirty throttling pause:",
        "	 */",
        "	int				nr_dirtied;",
        "	int				nr_dirtied_pause;",
        "	/* Start of a write-and-pause period: */",
        "	unsigned long			dirty_paused_when;",
        "",
        "#ifdef CONFIG_LATENCYTOP",
        "	int				latency_record_count;",
        "	struct latency_record		latency_record[LT_SAVECOUNT];",
        "#endif",
        "	/*",
        "	 * Time slack values; these are used to round up poll() and",
        "	 * select() etc timeout values. These are in nanoseconds.",
        "	 */",
        "	u64				timer_slack_ns;",
        "	u64				default_timer_slack_ns;",
        "",
        "#if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)",
        "	unsigned int			kasan_depth;",
        "#endif",
        "",
        "#ifdef CONFIG_KCSAN",
        "	struct kcsan_ctx		kcsan_ctx;",
        "#ifdef CONFIG_TRACE_IRQFLAGS",
        "	struct irqtrace_events		kcsan_save_irqtrace;",
        "#endif",
        "#ifdef CONFIG_KCSAN_WEAK_MEMORY",
        "	int				kcsan_stack_depth;",
        "#endif",
        "#endif",
        "",
        "#ifdef CONFIG_KMSAN",
        "	struct kmsan_ctx		kmsan_ctx;",
        "#endif",
        "",
        "#if IS_ENABLED(CONFIG_KUNIT)",
        "	struct kunit			*kunit_test;",
        "#endif",
        "",
        "#ifdef CONFIG_FUNCTION_GRAPH_TRACER",
        "	/* Index of current stored address in ret_stack: */",
        "	int				curr_ret_stack;",
        "	int				curr_ret_depth;",
        "",
        "	/* Stack of return addresses for return function tracing: */",
        "	unsigned long			*ret_stack;",
        "",
        "	/* Timestamp for last schedule: */",
        "	unsigned long long		ftrace_timestamp;",
        "	unsigned long long		ftrace_sleeptime;",
        "",
        "	/*",
        "	 * Number of functions that haven't been traced",
        "	 * because of depth overrun:",
        "	 */",
        "	atomic_t			trace_overrun;",
        "",
        "	/* Pause tracing: */",
        "	atomic_t			tracing_graph_pause;",
        "#endif",
        "",
        "#ifdef CONFIG_TRACING",
        "	/* Bitmask and counter of trace recursion: */",
        "	unsigned long			trace_recursion;",
        "#endif /* CONFIG_TRACING */",
        "",
        "#ifdef CONFIG_KCOV",
        "	/* See kernel/kcov.c for more details. */",
        "",
        "	/* Coverage collection mode enabled for this task (0 if disabled): */",
        "	unsigned int			kcov_mode;",
        "",
        "	/* Size of the kcov_area: */",
        "	unsigned int			kcov_size;",
        "",
        "	/* Buffer for coverage collection: */",
        "	void				*kcov_area;",
        "",
        "	/* KCOV descriptor wired with this task or NULL: */",
        "	struct kcov			*kcov;",
        "",
        "	/* KCOV common handle for remote coverage collection: */",
        "	u64				kcov_handle;",
        "",
        "	/* KCOV sequence number: */",
        "	int				kcov_sequence;",
        "",
        "	/* Collect coverage from softirq context: */",
        "	unsigned int			kcov_softirq;",
        "#endif",
        "	int fuzz_enabled;//# ç»™task_structæ·»åŠ ï¼Œç®¡ç†æ¯ä¸€ä¸ªçº¿ç¨‹",
        "	void* fuzz_dev;",
        "#ifdef CONFIG_MEMCG_V1",
        "	struct mem_cgroup		*memcg_in_oom;",
        "#endif",
        "",
        "#ifdef CONFIG_MEMCG",
        "	/* Number of pages to reclaim on returning to userland: */",
        "	unsigned int			memcg_nr_pages_over_high;",
        "",
        "	/* Used by memcontrol for targeted memcg charge: */",
        "	struct mem_cgroup		*active_memcg;",
        "",
        "	/* Cache for current->cgroups->memcg->objcg lookups: */",
        "	struct obj_cgroup		*objcg;",
        "#endif",
        "",
        "#ifdef CONFIG_BLK_CGROUP",
        "	struct gendisk			*throttle_disk;",
        "#endif",
        "",
        "#ifdef CONFIG_UPROBES",
        "	struct uprobe_task		*utask;",
        "#endif",
        "#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)",
        "	unsigned int			sequential_io;",
        "	unsigned int			sequential_io_avg;",
        "#endif",
        "	struct kmap_ctrl		kmap_ctrl;",
        "#ifdef CONFIG_DEBUG_ATOMIC_SLEEP",
        "	unsigned long			task_state_change;",
        "# ifdef CONFIG_PREEMPT_RT",
        "	unsigned long			saved_state_change;",
        "# endif",
        "#endif",
        "	struct rcu_head			rcu;",
        "	refcount_t			rcu_users;",
        "	int				pagefault_disabled;",
        "#ifdef CONFIG_MMU",
        "	struct task_struct		*oom_reaper_list;",
        "	struct timer_list		oom_reaper_timer;",
        "#endif",
        "#ifdef CONFIG_VMAP_STACK",
        "	struct vm_struct		*stack_vm_area;",
        "#endif",
        "#ifdef CONFIG_THREAD_INFO_IN_TASK",
        "	/* A live task holds one reference: */",
        "	refcount_t			stack_refcount;",
        "#endif",
        "#ifdef CONFIG_LIVEPATCH",
        "	int patch_state;",
        "#endif",
        "#ifdef CONFIG_SECURITY",
        "	/* Used by LSM modules for access restriction: */",
        "	void				*security;",
        "#endif",
        "#ifdef CONFIG_BPF_SYSCALL",
        "	/* Used by BPF task local storage */",
        "	struct bpf_local_storage __rcu	*bpf_storage;",
        "	/* Used for BPF run context */",
        "	struct bpf_run_ctx		*bpf_ctx;",
        "#endif",
        "	/* Used by BPF for per-TASK xdp storage */",
        "	struct bpf_net_context		*bpf_net_context;",
        "",
        "#ifdef CONFIG_GCC_PLUGIN_STACKLEAK",
        "	unsigned long			lowest_stack;",
        "	unsigned long			prev_lowest_stack;",
        "#endif",
        "",
        "#ifdef CONFIG_X86_MCE",
        "	void __user			*mce_vaddr;",
        "	__u64				mce_kflags;",
        "	u64				mce_addr;",
        "	__u64				mce_ripv : 1,",
        "					mce_whole_page : 1,",
        "					__mce_reserved : 62;",
        "	struct callback_head		mce_kill_me;",
        "	int				mce_count;",
        "#endif",
        "",
        "#ifdef CONFIG_KRETPROBES",
        "	struct llist_head               kretprobe_instances;",
        "#endif",
        "#ifdef CONFIG_RETHOOK",
        "	struct llist_head               rethooks;",
        "#endif",
        "",
        "#ifdef CONFIG_ARCH_HAS_PARANOID_L1D_FLUSH",
        "	/*",
        "	 * If L1D flush is supported on mm context switch",
        "	 * then we use this callback head to queue kill work",
        "	 * to kill tasks that are not running on SMT disabled",
        "	 * cores",
        "	 */",
        "	struct callback_head		l1d_flush_kill;",
        "#endif",
        "",
        "#ifdef CONFIG_RV",
        "	/*",
        "	 * Per-task RV monitor. Nowadays fixed in RV_PER_TASK_MONITORS.",
        "	 * If we find justification for more monitors, we can think",
        "	 * about adding more or developing a dynamic method. So far,",
        "	 * none of these are justified.",
        "	 */",
        "	union rv_task_monitor		rv[RV_PER_TASK_MONITORS];",
        "#endif",
        "",
        "#ifdef CONFIG_USER_EVENTS",
        "	struct user_event_mm		*user_event_mm;",
        "#endif",
        "",
        "	/*",
        "	 * New fields for task_struct should be added above here, so that",
        "	 * they are included in the randomized portion of task_struct.",
        "	 */",
        "	randomized_struct_fields_end",
        "",
        "	/* CPU-specific state of this task: */",
        "	struct thread_struct		thread;",
        "",
        "	/*",
        "	 * WARNING: on x86, 'thread_struct' contains a variable-sized",
        "	 * structure.  It *MUST* be at the end of 'task_struct'.",
        "	 *",
        "	 * Do not put anything below here!",
        "	 */",
        "};",
        "",
        "#define TASK_REPORT_IDLE	(TASK_REPORT + 1)",
        "#define TASK_REPORT_MAX		(TASK_REPORT_IDLE << 1)",
        "",
        "static inline unsigned int __task_state_index(unsigned int tsk_state,",
        "					      unsigned int tsk_exit_state)",
        "{",
        "	unsigned int state = (tsk_state | tsk_exit_state) & TASK_REPORT;",
        "",
        "	BUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);",
        "",
        "	if ((tsk_state & TASK_IDLE) == TASK_IDLE)",
        "		state = TASK_REPORT_IDLE;",
        "",
        "	/*",
        "	 * We're lying here, but rather than expose a completely new task state",
        "	 * to userspace, we can make this appear as if the task has gone through",
        "	 * a regular rt_mutex_lock() call.",
        "	 * Report frozen tasks as uninterruptible.",
        "	 */",
        "	if ((tsk_state & TASK_RTLOCK_WAIT) || (tsk_state & TASK_FROZEN))",
        "		state = TASK_UNINTERRUPTIBLE;",
        "",
        "	return fls(state);",
        "}",
        "",
        "static inline unsigned int task_state_index(struct task_struct *tsk)",
        "{",
        "	return __task_state_index(READ_ONCE(tsk->__state), tsk->exit_state);",
        "}",
        "",
        "static inline char task_index_to_char(unsigned int state)",
        "{",
        "	static const char state_char[] = \"RSDTtXZPI\";",
        "",
        "	BUILD_BUG_ON(TASK_REPORT_MAX * 2 != 1 << (sizeof(state_char) - 1));",
        "",
        "	return state_char[state];",
        "}",
        "",
        "static inline char task_state_to_char(struct task_struct *tsk)",
        "{",
        "	return task_index_to_char(task_state_index(tsk));",
        "}",
        "",
        "extern struct pid *cad_pid;",
        "",
        "/*",
        " * Per process flags",
        " */",
        "#define PF_VCPU			0x00000001	/* I'm a virtual CPU */",
        "#define PF_IDLE			0x00000002	/* I am an IDLE thread */",
        "#define PF_EXITING		0x00000004	/* Getting shut down */",
        "#define PF_POSTCOREDUMP		0x00000008	/* Coredumps should ignore this task */",
        "#define PF_IO_WORKER		0x00000010	/* Task is an IO worker */",
        "#define PF_WQ_WORKER		0x00000020	/* I'm a workqueue worker */",
        "#define PF_FORKNOEXEC		0x00000040	/* Forked but didn't exec */",
        "#define PF_MCE_PROCESS		0x00000080      /* Process policy on mce errors */",
        "#define PF_SUPERPRIV		0x00000100	/* Used super-user privileges */",
        "#define PF_DUMPCORE		0x00000200	/* Dumped core */",
        "#define PF_SIGNALED		0x00000400	/* Killed by a signal */",
        "#define PF_MEMALLOC		0x00000800	/* Allocating memory to free memory. See memalloc_noreclaim_save() */",
        "#define PF_NPROC_EXCEEDED	0x00001000	/* set_user() noticed that RLIMIT_NPROC was exceeded */",
        "#define PF_USED_MATH		0x00002000	/* If unset the fpu must be initialized before use */",
        "#define PF_USER_WORKER		0x00004000	/* Kernel thread cloned from userspace thread */",
        "#define PF_NOFREEZE		0x00008000	/* This thread should not be frozen */",
        "#define PF_KCOMPACTD		0x00010000	/* I am kcompactd */",
        "#define PF_KSWAPD		0x00020000	/* I am kswapd */",
        "#define PF_MEMALLOC_NOFS	0x00040000	/* All allocations inherit GFP_NOFS. See memalloc_nfs_save() */",
        "#define PF_MEMALLOC_NOIO	0x00080000	/* All allocations inherit GFP_NOIO. See memalloc_noio_save() */",
        "#define PF_LOCAL_THROTTLE	0x00100000	/* Throttle writes only against the bdi I write to,",
        "						 * I am cleaning dirty pages from some other bdi. */",
        "#define PF_KTHREAD		0x00200000	/* I am a kernel thread */",
        "#define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */",
        "#define PF__HOLE__00800000	0x00800000",
        "#define PF__HOLE__01000000	0x01000000",
        "#define PF__HOLE__02000000	0x02000000",
        "#define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_mask */",
        "#define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */",
        "#define PF_MEMALLOC_PIN		0x10000000	/* Allocations constrained to zones which allow long term pinning.",
        "						 * See memalloc_pin_save() */",
        "#define PF_BLOCK_TS		0x20000000	/* plug has ts that needs updating */",
        "#define PF__HOLE__40000000	0x40000000",
        "#define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */",
        "",
        "/*",
        " * Only the _current_ task can read/write to tsk->flags, but other",
        " * tasks can access tsk->flags in readonly mode for example",
        " * with tsk_used_math (like during threaded core dumping).",
        " * There is however an exception to this rule during ptrace",
        " * or during fork: the ptracer task is allowed to write to the",
        " * child->flags of its traced child (same goes for fork, the parent",
        " * can write to the child->flags), because we're guaranteed the",
        " * child is not running and in turn not changing child->flags",
        " * at the same time the parent does it.",
        " */",
        "#define clear_stopped_child_used_math(child)	do { (child)->flags &= ~PF_USED_MATH; } while (0)",
        "#define set_stopped_child_used_math(child)	do { (child)->flags |= PF_USED_MATH; } while (0)",
        "#define clear_used_math()			clear_stopped_child_used_math(current)",
        "#define set_used_math()				set_stopped_child_used_math(current)",
        "",
        "#define conditional_stopped_child_used_math(condition, child) \\",
        "	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)",
        "",
        "#define conditional_used_math(condition)	conditional_stopped_child_used_math(condition, current)",
        "",
        "#define copy_to_stopped_child_used_math(child) \\",
        "	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)",
        "",
        "/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */",
        "#define tsk_used_math(p)			((p)->flags & PF_USED_MATH)",
        "#define used_math()				tsk_used_math(current)",
        "",
        "static __always_inline bool is_percpu_thread(void)",
        "{",
        "#ifdef CONFIG_SMP",
        "	return (current->flags & PF_NO_SETAFFINITY) &&",
        "		(current->nr_cpus_allowed  == 1);",
        "#else",
        "	return true;",
        "#endif",
        "}",
        "",
        "/* Per-process atomic flags. */",
        "#define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */",
        "#define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */",
        "#define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */",
        "#define PFA_SPEC_SSB_DISABLE		3	/* Speculative Store Bypass disabled */",
        "#define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/",
        "#define PFA_SPEC_IB_DISABLE		5	/* Indirect branch speculation restricted */",
        "#define PFA_SPEC_IB_FORCE_DISABLE	6	/* Indirect branch speculation permanently restricted */",
        "#define PFA_SPEC_SSB_NOEXEC		7	/* Speculative Store Bypass clear on execve() */",
        "",
        "#define TASK_PFA_TEST(name, func)					\\",
        "	static inline bool task_##func(struct task_struct *p)		\\",
        "	{ return test_bit(PFA_##name, &p->atomic_flags); }",
        "",
        "#define TASK_PFA_SET(name, func)					\\",
        "	static inline void task_set_##func(struct task_struct *p)	\\",
        "	{ set_bit(PFA_##name, &p->atomic_flags); }",
        "",
        "#define TASK_PFA_CLEAR(name, func)					\\",
        "	static inline void task_clear_##func(struct task_struct *p)	\\",
        "	{ clear_bit(PFA_##name, &p->atomic_flags); }",
        "",
        "TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)",
        "TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)",
        "",
        "TASK_PFA_TEST(SPREAD_PAGE, spread_page)",
        "TASK_PFA_SET(SPREAD_PAGE, spread_page)",
        "TASK_PFA_CLEAR(SPREAD_PAGE, spread_page)",
        "",
        "TASK_PFA_TEST(SPREAD_SLAB, spread_slab)",
        "TASK_PFA_SET(SPREAD_SLAB, spread_slab)",
        "TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)",
        "",
        "TASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)",
        "TASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)",
        "TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)",
        "",
        "TASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)",
        "TASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)",
        "TASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)",
        "",
        "TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)",
        "TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)",
        "",
        "TASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)",
        "TASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)",
        "TASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)",
        "",
        "TASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)",
        "TASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)",
        "",
        "static inline void",
        "current_restore_flags(unsigned long orig_flags, unsigned long flags)",
        "{",
        "	current->flags &= ~flags;",
        "	current->flags |= orig_flags & flags;",
        "}",
        "",
        "extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);",
        "extern int task_can_attach(struct task_struct *p);",
        "extern int dl_bw_alloc(int cpu, u64 dl_bw);",
        "extern void dl_bw_free(int cpu, u64 dl_bw);",
        "#ifdef CONFIG_SMP",
        "",
        "/* do_set_cpus_allowed() - consider using set_cpus_allowed_ptr() instead */",
        "extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);",
        "",
        "/**",
        " * set_cpus_allowed_ptr - set CPU affinity mask of a task",
        " * @p: the task",
        " * @new_mask: CPU affinity mask",
        " *",
        " * Return: zero if successful, or a negative error code",
        " */",
        "extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);",
        "extern int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src, int node);",
        "extern void release_user_cpus_ptr(struct task_struct *p);",
        "extern int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask);",
        "extern void force_compatible_cpus_allowed_ptr(struct task_struct *p);",
        "extern void relax_compatible_cpus_allowed_ptr(struct task_struct *p);",
        "#else",
        "static inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)",
        "{",
        "}",
        "static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)",
        "{",
        "	/* Opencoded cpumask_test_cpu(0, new_mask) to avoid dependency on cpumask.h */",
        "	if ((*cpumask_bits(new_mask) & 1) == 0)",
        "		return -EINVAL;",
        "	return 0;",
        "}",
        "static inline int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src, int node)",
        "{",
        "	if (src->user_cpus_ptr)",
        "		return -EINVAL;",
        "	return 0;",
        "}",
        "static inline void release_user_cpus_ptr(struct task_struct *p)",
        "{",
        "	WARN_ON(p->user_cpus_ptr);",
        "}",
        "",
        "static inline int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)",
        "{",
        "	return 0;",
        "}",
        "#endif",
        "",
        "extern int yield_to(struct task_struct *p, bool preempt);",
        "extern void set_user_nice(struct task_struct *p, long nice);",
        "extern int task_prio(const struct task_struct *p);",
        "",
        "/**",
        " * task_nice - return the nice value of a given task.",
        " * @p: the task in question.",
        " *",
        " * Return: The nice value [ -20 ... 0 ... 19 ].",
        " */",
        "static inline int task_nice(const struct task_struct *p)",
        "{",
        "	return PRIO_TO_NICE((p)->static_prio);",
        "}",
        "",
        "extern int can_nice(const struct task_struct *p, const int nice);",
        "extern int task_curr(const struct task_struct *p);",
        "extern int idle_cpu(int cpu);",
        "extern int available_idle_cpu(int cpu);",
        "extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);",
        "extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);",
        "extern void sched_set_fifo(struct task_struct *p);",
        "extern void sched_set_fifo_low(struct task_struct *p);",
        "extern void sched_set_normal(struct task_struct *p, int nice);",
        "extern int sched_setattr(struct task_struct *, const struct sched_attr *);",
        "extern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);",
        "extern struct task_struct *idle_task(int cpu);",
        "",
        "/**",
        " * is_idle_task - is the specified task an idle task?",
        " * @p: the task in question.",
        " *",
        " * Return: 1 if @p is an idle task. 0 otherwise.",
        " */",
        "static __always_inline bool is_idle_task(const struct task_struct *p)",
        "{",
        "	return !!(p->flags & PF_IDLE);",
        "}",
        "",
        "extern struct task_struct *curr_task(int cpu);",
        "extern void ia64_set_curr_task(int cpu, struct task_struct *p);",
        "",
        "void yield(void);",
        "",
        "union thread_union {",
        "	struct task_struct task;",
        "#ifndef CONFIG_THREAD_INFO_IN_TASK",
        "	struct thread_info thread_info;",
        "#endif",
        "	unsigned long stack[THREAD_SIZE/sizeof(long)];",
        "};",
        "",
        "#ifndef CONFIG_THREAD_INFO_IN_TASK",
        "extern struct thread_info init_thread_info;",
        "#endif",
        "",
        "extern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];",
        "",
        "#ifdef CONFIG_THREAD_INFO_IN_TASK",
        "# define task_thread_info(task)	(&(task)->thread_info)",
        "#else",
        "# define task_thread_info(task)	((struct thread_info *)(task)->stack)",
        "#endif",
        "",
        "/*",
        " * find a task by one of its numerical ids",
        " *",
        " * find_task_by_pid_ns():",
        " *      finds a task by its pid in the specified namespace",
        " * find_task_by_vpid():",
        " *      finds a task by its virtual pid",
        " *",
        " * see also find_vpid() etc in include/linux/pid.h",
        " */",
        "",
        "extern struct task_struct *find_task_by_vpid(pid_t nr);",
        "extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);",
        "",
        "/*",
        " * find a task by its virtual pid and get the task struct",
        " */",
        "extern struct task_struct *find_get_task_by_vpid(pid_t nr);",
        "",
        "extern int wake_up_state(struct task_struct *tsk, unsigned int state);",
        "extern int wake_up_process(struct task_struct *tsk);",
        "extern void wake_up_new_task(struct task_struct *tsk);",
        "",
        "#ifdef CONFIG_SMP",
        "extern void kick_process(struct task_struct *tsk);",
        "#else",
        "static inline void kick_process(struct task_struct *tsk) { }",
        "#endif",
        "",
        "extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);",
        "",
        "static inline void set_task_comm(struct task_struct *tsk, const char *from)",
        "{",
        "	__set_task_comm(tsk, from, false);",
        "}",
        "",
        "/*",
        " * - Why not use task_lock()?",
        " *   User space can randomly change their names anyway, so locking for readers",
        " *   doesn't make sense. For writers, locking is probably necessary, as a race",
        " *   condition could lead to long-term mixed results.",
        " *   The strscpy_pad() in __set_task_comm() can ensure that the task comm is",
        " *   always NUL-terminated and zero-padded. Therefore the race condition between",
        " *   reader and writer is not an issue.",
        " *",
        " * - BUILD_BUG_ON() can help prevent the buf from being truncated.",
        " *   Since the callers don't perform any return value checks, this safeguard is",
        " *   necessary.",
        " */",
        "#define get_task_comm(buf, tsk) ({			\\",
        "	BUILD_BUG_ON(sizeof(buf) < TASK_COMM_LEN);	\\",
        "	strscpy_pad(buf, (tsk)->comm);			\\",
        "	buf;						\\",
        "})",
        "",
        "#ifdef CONFIG_SMP",
        "static __always_inline void scheduler_ipi(void)",
        "{",
        "	/*",
        "	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting",
        "	 * TIF_NEED_RESCHED remotely (for the first time) will also send",
        "	 * this IPI.",
        "	 */",
        "	preempt_fold_need_resched();",
        "}",
        "#else",
        "static inline void scheduler_ipi(void) { }",
        "#endif",
        "",
        "extern unsigned long wait_task_inactive(struct task_struct *, unsigned int match_state);",
        "",
        "/*",
        " * Set thread flags in other task's structures.",
        " * See asm/thread_info.h for TIF_xxxx flags available:",
        " */",
        "static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)",
        "{",
        "	set_ti_thread_flag(task_thread_info(tsk), flag);",
        "}",
        "",
        "static inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)",
        "{",
        "	clear_ti_thread_flag(task_thread_info(tsk), flag);",
        "}",
        "",
        "static inline void update_tsk_thread_flag(struct task_struct *tsk, int flag,",
        "					  bool value)",
        "{",
        "	update_ti_thread_flag(task_thread_info(tsk), flag, value);",
        "}",
        "",
        "static inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)",
        "{",
        "	return test_and_set_ti_thread_flag(task_thread_info(tsk), flag);",
        "}",
        "",
        "static inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)",
        "{",
        "	return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);",
        "}",
        "",
        "static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)",
        "{",
        "	return test_ti_thread_flag(task_thread_info(tsk), flag);",
        "}",
        "",
        "static inline void set_tsk_need_resched(struct task_struct *tsk)",
        "{",
        "	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);",
        "}",
        "",
        "static inline void clear_tsk_need_resched(struct task_struct *tsk)",
        "{",
        "	atomic_long_andnot(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY,",
        "			   (atomic_long_t *)&task_thread_info(tsk)->flags);",
        "}",
        "",
        "static inline int test_tsk_need_resched(struct task_struct *tsk)",
        "{",
        "	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));",
        "}",
        "",
        "/*",
        " * cond_resched() and cond_resched_lock(): latency reduction via",
        " * explicit rescheduling in places that are safe. The return",
        " * value indicates whether a reschedule was done in fact.",
        " * cond_resched_lock() will drop the spinlock before scheduling,",
        " */",
        "#if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)",
        "extern int __cond_resched(void);",
        "",
        "#if defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)",
        "",
        "void sched_dynamic_klp_enable(void);",
        "void sched_dynamic_klp_disable(void);",
        "",
        "DECLARE_STATIC_CALL(cond_resched, __cond_resched);",
        "",
        "static __always_inline int _cond_resched(void)",
        "{",
        "	return static_call_mod(cond_resched)();",
        "}",
        "",
        "#elif defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)",
        "",
        "extern int dynamic_cond_resched(void);",
        "",
        "static __always_inline int _cond_resched(void)",
        "{",
        "	return dynamic_cond_resched();",
        "}",
        "",
        "#else /* !CONFIG_PREEMPTION */",
        "",
        "static inline int _cond_resched(void)",
        "{",
        "	klp_sched_try_switch();",
        "	return __cond_resched();",
        "}",
        "",
        "#endif /* PREEMPT_DYNAMIC && CONFIG_HAVE_PREEMPT_DYNAMIC_CALL */",
        "",
        "#else /* CONFIG_PREEMPTION && !CONFIG_PREEMPT_DYNAMIC */",
        "",
        "static inline int _cond_resched(void)",
        "{",
        "	klp_sched_try_switch();",
        "	return 0;",
        "}",
        "",
        "#endif /* !CONFIG_PREEMPTION || CONFIG_PREEMPT_DYNAMIC */",
        "",
        "#define cond_resched() ({			\\",
        "	__might_resched(__FILE__, __LINE__, 0);	\\",
        "	_cond_resched();			\\",
        "})",
        "",
        "extern int __cond_resched_lock(spinlock_t *lock);",
        "extern int __cond_resched_rwlock_read(rwlock_t *lock);",
        "extern int __cond_resched_rwlock_write(rwlock_t *lock);",
        "",
        "#define MIGHT_RESCHED_RCU_SHIFT		8",
        "#define MIGHT_RESCHED_PREEMPT_MASK	((1U << MIGHT_RESCHED_RCU_SHIFT) - 1)",
        "",
        "#ifndef CONFIG_PREEMPT_RT",
        "/*",
        " * Non RT kernels have an elevated preempt count due to the held lock,",
        " * but are not allowed to be inside a RCU read side critical section",
        " */",
        "# define PREEMPT_LOCK_RESCHED_OFFSETS	PREEMPT_LOCK_OFFSET",
        "#else",
        "/*",
        " * spin/rw_lock() on RT implies rcu_read_lock(). The might_sleep() check in",
        " * cond_resched*lock() has to take that into account because it checks for",
        " * preempt_count() and rcu_preempt_depth().",
        " */",
        "# define PREEMPT_LOCK_RESCHED_OFFSETS	\\",
        "	(PREEMPT_LOCK_OFFSET + (1U << MIGHT_RESCHED_RCU_SHIFT))",
        "#endif",
        "",
        "#define cond_resched_lock(lock) ({						\\",
        "	__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);	\\",
        "	__cond_resched_lock(lock);						\\",
        "})",
        "",
        "#define cond_resched_rwlock_read(lock) ({					\\",
        "	__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);	\\",
        "	__cond_resched_rwlock_read(lock);					\\",
        "})",
        "",
        "#define cond_resched_rwlock_write(lock) ({					\\",
        "	__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);	\\",
        "	__cond_resched_rwlock_write(lock);					\\",
        "})",
        "",
        "static __always_inline bool need_resched(void)",
        "{",
        "	return unlikely(tif_need_resched());",
        "}",
        "",
        "/*",
        " * Wrappers for p->thread_info->cpu access. No-op on UP.",
        " */",
        "#ifdef CONFIG_SMP",
        "",
        "static inline unsigned int task_cpu(const struct task_struct *p)",
        "{",
        "	return READ_ONCE(task_thread_info(p)->cpu);",
        "}",
        "",
        "extern void set_task_cpu(struct task_struct *p, unsigned int cpu);",
        "",
        "#else",
        "",
        "static inline unsigned int task_cpu(const struct task_struct *p)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)",
        "{",
        "}",
        "",
        "#endif /* CONFIG_SMP */",
        "",
        "static inline bool task_is_runnable(struct task_struct *p)",
        "{",
        "	return p->on_rq && !p->se.sched_delayed;",
        "}",
        "",
        "extern bool sched_task_on_rq(struct task_struct *p);",
        "extern unsigned long get_wchan(struct task_struct *p);",
        "extern struct task_struct *cpu_curr_snapshot(int cpu);",
        "",
        "#include <linux/spinlock.h>",
        "",
        "/*",
        " * In order to reduce various lock holder preemption latencies provide an",
        " * interface to see if a vCPU is currently running or not.",
        " *",
        " * This allows us to terminate optimistic spin loops and block, analogous to",
        " * the native optimistic spin heuristic of testing if the lock owner task is",
        " * running or not.",
        " */",
        "#ifndef vcpu_is_preempted",
        "static inline bool vcpu_is_preempted(int cpu)",
        "{",
        "	return false;",
        "}",
        "#endif",
        "",
        "extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);",
        "extern long sched_getaffinity(pid_t pid, struct cpumask *mask);",
        "",
        "#ifndef TASK_SIZE_OF",
        "#define TASK_SIZE_OF(tsk)	TASK_SIZE",
        "#endif",
        "",
        "#ifdef CONFIG_SMP",
        "static inline bool owner_on_cpu(struct task_struct *owner)",
        "{",
        "	/*",
        "	 * As lock holder preemption issue, we both skip spinning if",
        "	 * task is not on cpu or its cpu is preempted",
        "	 */",
        "	return READ_ONCE(owner->on_cpu) && !vcpu_is_preempted(task_cpu(owner));",
        "}",
        "",
        "/* Returns effective CPU energy utilization, as seen by the scheduler */",
        "unsigned long sched_cpu_util(int cpu);",
        "#endif /* CONFIG_SMP */",
        "",
        "#ifdef CONFIG_SCHED_CORE",
        "extern void sched_core_free(struct task_struct *tsk);",
        "extern void sched_core_fork(struct task_struct *p);",
        "extern int sched_core_share_pid(unsigned int cmd, pid_t pid, enum pid_type type,",
        "				unsigned long uaddr);",
        "extern int sched_core_idle_cpu(int cpu);",
        "#else",
        "static inline void sched_core_free(struct task_struct *tsk) { }",
        "static inline void sched_core_fork(struct task_struct *p) { }",
        "static inline int sched_core_idle_cpu(int cpu) { return idle_cpu(cpu); }",
        "#endif",
        "",
        "extern void sched_set_stop_task(int cpu, struct task_struct *stop);",
        "",
        "#ifdef CONFIG_MEM_ALLOC_PROFILING",
        "static __always_inline struct alloc_tag *alloc_tag_save(struct alloc_tag *tag)",
        "{",
        "	swap(current->alloc_tag, tag);",
        "	return tag;",
        "}",
        "",
        "static __always_inline void alloc_tag_restore(struct alloc_tag *tag, struct alloc_tag *old)",
        "{",
        "#ifdef CONFIG_MEM_ALLOC_PROFILING_DEBUG",
        "	WARN(current->alloc_tag != tag, \"current->alloc_tag was changed:\\n\");",
        "#endif",
        "	current->alloc_tag = old;",
        "}",
        "#else",
        "#define alloc_tag_save(_tag)			NULL",
        "#define alloc_tag_restore(_tag, _old)		do {} while (0)",
        "#endif",
        "",
        "#endif"
    ]
  },
  "arch_x86_kernel_process_c": {
    path: "arch/x86/kernel/process.c",
    covered: [483, 470, 451],
    totalLines: 1061,
    coveredCount: 3,
    coveragePct: 0.3,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
        "",
        "#include <linux/errno.h>",
        "#include <linux/kernel.h>",
        "#include <linux/mm.h>",
        "#include <linux/smp.h>",
        "#include <linux/cpu.h>",
        "#include <linux/prctl.h>",
        "#include <linux/slab.h>",
        "#include <linux/sched.h>",
        "#include <linux/sched/idle.h>",
        "#include <linux/sched/debug.h>",
        "#include <linux/sched/task.h>",
        "#include <linux/sched/task_stack.h>",
        "#include <linux/init.h>",
        "#include <linux/export.h>",
        "#include <linux/pm.h>",
        "#include <linux/tick.h>",
        "#include <linux/random.h>",
        "#include <linux/user-return-notifier.h>",
        "#include <linux/dmi.h>",
        "#include <linux/utsname.h>",
        "#include <linux/stackprotector.h>",
        "#include <linux/cpuidle.h>",
        "#include <linux/acpi.h>",
        "#include <linux/elf-randomize.h>",
        "#include <linux/static_call.h>",
        "#include <trace/events/power.h>",
        "#include <linux/hw_breakpoint.h>",
        "#include <linux/entry-common.h>",
        "#include <asm/cpu.h>",
        "#include <asm/apic.h>",
        "#include <linux/uaccess.h>",
        "#include <asm/mwait.h>",
        "#include <asm/fpu/api.h>",
        "#include <asm/fpu/sched.h>",
        "#include <asm/fpu/xstate.h>",
        "#include <asm/debugreg.h>",
        "#include <asm/nmi.h>",
        "#include <asm/tlbflush.h>",
        "#include <asm/mce.h>",
        "#include <asm/vm86.h>",
        "#include <asm/switch_to.h>",
        "#include <asm/desc.h>",
        "#include <asm/prctl.h>",
        "#include <asm/spec-ctrl.h>",
        "#include <asm/io_bitmap.h>",
        "#include <asm/proto.h>",
        "#include <asm/frame.h>",
        "#include <asm/unwind.h>",
        "#include <asm/tdx.h>",
        "#include <asm/mmu_context.h>",
        "#include <asm/shstk.h>",
        "",
        "#include \"process.h\"",
        "",
        "/*",
        " * per-CPU TSS segments. Threads are completely 'soft' on Linux,",
        " * no more per-task TSS's. The TSS size is kept cacheline-aligned",
        " * so they are allowed to end up in the .data..cacheline_aligned",
        " * section. Since TSS's are completely CPU-local, we want them",
        " * on exact cacheline boundaries, to eliminate cacheline ping-pong.",
        " */",
        "__visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {",
        "	.x86_tss = {",
        "		/*",
        "		 * .sp0 is only used when entering ring 0 from a lower",
        "		 * privilege level.  Since the init task never runs anything",
        "		 * but ring 0 code, there is no need for a valid value here.",
        "		 * Poison it.",
        "		 */",
        "		.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,",
        "",
        "#ifdef CONFIG_X86_32",
        "		.sp1 = TOP_OF_INIT_STACK,",
        "",
        "		.ss0 = __KERNEL_DS,",
        "		.ss1 = __KERNEL_CS,",
        "#endif",
        "		.io_bitmap_base	= IO_BITMAP_OFFSET_INVALID,",
        "	 },",
        "};",
        "EXPORT_PER_CPU_SYMBOL(cpu_tss_rw);",
        "",
        "DEFINE_PER_CPU(bool, __tss_limit_invalid);",
        "EXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);",
        "",
        "/*",
        " * this gets called so that we can store lazy state into memory and copy the",
        " * current task into the new thread.",
        " */",
        "int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)",
        "{",
        "	memcpy(dst, src, arch_task_struct_size);",
        "#ifdef CONFIG_VM86",
        "	dst->thread.vm86 = NULL;",
        "#endif",
        "	/* Drop the copied pointer to current's fpstate */",
        "	dst->thread.fpu.fpstate = NULL;",
        "",
        "	return 0;",
        "}",
        "",
        "#ifdef CONFIG_X86_64",
        "void arch_release_task_struct(struct task_struct *tsk)",
        "{",
        "	if (fpu_state_size_dynamic())",
        "		fpstate_free(&tsk->thread.fpu);",
        "}",
        "#endif",
        "",
        "/*",
        " * Free thread data structures etc..",
        " */",
        "void exit_thread(struct task_struct *tsk)",
        "{",
        "	struct thread_struct *t = &tsk->thread;",
        "	struct fpu *fpu = &t->fpu;",
        "",
        "	if (test_thread_flag(TIF_IO_BITMAP))",
        "		io_bitmap_exit(tsk);",
        "",
        "	free_vm86(t);",
        "",
        "	shstk_free(tsk);",
        "	fpu__drop(fpu);",
        "}",
        "",
        "static int set_new_tls(struct task_struct *p, unsigned long tls)",
        "{",
        "	struct user_desc __user *utls = (struct user_desc __user *)tls;",
        "",
        "	if (in_ia32_syscall())",
        "		return do_set_thread_area(p, -1, utls, 0);",
        "	else",
        "		return do_set_thread_area_64(p, ARCH_SET_FS, tls);",
        "}",
        "",
        "__visible void ret_from_fork(struct task_struct *prev, struct pt_regs *regs,",
        "				     int (*fn)(void *), void *fn_arg)",
        "{",
        "	schedule_tail(prev);",
        "",
        "	/* Is this a kernel thread? */",
        "	if (unlikely(fn)) {",
        "		fn(fn_arg);",
        "		/*",
        "		 * A kernel thread is allowed to return here after successfully",
        "		 * calling kernel_execve().  Exit to userspace to complete the",
        "		 * execve() syscall.",
        "		 */",
        "		regs->ax = 0;",
        "	}",
        "",
        "	syscall_exit_to_user_mode(regs);",
        "}",
        "",
        "int copy_thread(struct task_struct *p, const struct kernel_clone_args *args)",
        "{",
        "	unsigned long clone_flags = args->flags;",
        "	unsigned long sp = args->stack;",
        "	unsigned long tls = args->tls;",
        "	struct inactive_task_frame *frame;",
        "	struct fork_frame *fork_frame;",
        "	struct pt_regs *childregs;",
        "	unsigned long new_ssp;",
        "	int ret = 0;",
        "",
        "	childregs = task_pt_regs(p);",
        "	fork_frame = container_of(childregs, struct fork_frame, regs);",
        "	frame = &fork_frame->frame;",
        "",
        "	frame->bp = encode_frame_pointer(childregs);",
        "	frame->ret_addr = (unsigned long) ret_from_fork_asm;",
        "	p->thread.sp = (unsigned long) fork_frame;",
        "	p->thread.io_bitmap = NULL;",
        "	p->thread.iopl_warn = 0;",
        "	memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));",
        "",
        "#ifdef CONFIG_X86_64",
        "	current_save_fsgs();",
        "	p->thread.fsindex = current->thread.fsindex;",
        "	p->thread.fsbase = current->thread.fsbase;",
        "	p->thread.gsindex = current->thread.gsindex;",
        "	p->thread.gsbase = current->thread.gsbase;",
        "",
        "	savesegment(es, p->thread.es);",
        "	savesegment(ds, p->thread.ds);",
        "",
        "	if (p->mm && (clone_flags & (CLONE_VM | CLONE_VFORK)) == CLONE_VM)",
        "		set_bit(MM_CONTEXT_LOCK_LAM, &p->mm->context.flags);",
        "#else",
        "	p->thread.sp0 = (unsigned long) (childregs + 1);",
        "	savesegment(gs, p->thread.gs);",
        "	/*",
        "	 * Clear all status flags including IF and set fixed bit. 64bit",
        "	 * does not have this initialization as the frame does not contain",
        "	 * flags. The flags consistency (especially vs. AC) is there",
        "	 * ensured via objtool, which lacks 32bit support.",
        "	 */",
        "	frame->flags = X86_EFLAGS_FIXED;",
        "#endif",
        "",
        "	/*",
        "	 * Allocate a new shadow stack for thread if needed. If shadow stack,",
        "	 * is disabled, new_ssp will remain 0, and fpu_clone() will know not to",
        "	 * update it.",
        "	 */",
        "	new_ssp = shstk_alloc_thread_stack(p, clone_flags, args->stack_size);",
        "	if (IS_ERR_VALUE(new_ssp))",
        "		return PTR_ERR((void *)new_ssp);",
        "",
        "	fpu_clone(p, clone_flags, args->fn, new_ssp);",
        "",
        "	/* Kernel thread ? */",
        "	if (unlikely(p->flags & PF_KTHREAD)) {",
        "		p->thread.pkru = pkru_get_init_value();",
        "		memset(childregs, 0, sizeof(struct pt_regs));",
        "		kthread_frame_init(frame, args->fn, args->fn_arg);",
        "		return 0;",
        "	}",
        "",
        "	/*",
        "	 * Clone current's PKRU value from hardware. tsk->thread.pkru",
        "	 * is only valid when scheduled out.",
        "	 */",
        "	p->thread.pkru = read_pkru();",
        "",
        "	frame->bx = 0;",
        "	*childregs = *current_pt_regs();",
        "	childregs->ax = 0;",
        "	if (sp)",
        "		childregs->sp = sp;",
        "",
        "	if (unlikely(args->fn)) {",
        "		/*",
        "		 * A user space thread, but it doesn't return to",
        "		 * ret_after_fork().",
        "		 *",
        "		 * In order to indicate that to tools like gdb,",
        "		 * we reset the stack and instruction pointers.",
        "		 *",
        "		 * It does the same kernel frame setup to return to a kernel",
        "		 * function that a kernel thread does.",
        "		 */",
        "		childregs->sp = 0;",
        "		childregs->ip = 0;",
        "		kthread_frame_init(frame, args->fn, args->fn_arg);",
        "		return 0;",
        "	}",
        "",
        "	/* Set a new TLS for the child thread? */",
        "	if (clone_flags & CLONE_SETTLS)",
        "		ret = set_new_tls(p, tls);",
        "",
        "	if (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))",
        "		io_bitmap_share(p);",
        "",
        "	return ret;",
        "}",
        "",
        "static void pkru_flush_thread(void)",
        "{",
        "	/*",
        "	 * If PKRU is enabled the default PKRU value has to be loaded into",
        "	 * the hardware right here (similar to context switch).",
        "	 */",
        "	pkru_write_default();",
        "}",
        "",
        "void flush_thread(void)",
        "{",
        "	struct task_struct *tsk = current;",
        "",
        "	flush_ptrace_hw_breakpoint(tsk);",
        "	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));",
        "",
        "	fpu_flush_thread();",
        "	pkru_flush_thread();",
        "}",
        "",
        "void disable_TSC(void)",
        "{",
        "	preempt_disable();",
        "	if (!test_and_set_thread_flag(TIF_NOTSC))",
        "		/*",
        "		 * Must flip the CPU state synchronously with",
        "		 * TIF_NOTSC in the current running context.",
        "		 */",
        "		cr4_set_bits(X86_CR4_TSD);",
        "	preempt_enable();",
        "}",
        "",
        "static void enable_TSC(void)",
        "{",
        "	preempt_disable();",
        "	if (test_and_clear_thread_flag(TIF_NOTSC))",
        "		/*",
        "		 * Must flip the CPU state synchronously with",
        "		 * TIF_NOTSC in the current running context.",
        "		 */",
        "		cr4_clear_bits(X86_CR4_TSD);",
        "	preempt_enable();",
        "}",
        "",
        "int get_tsc_mode(unsigned long adr)",
        "{",
        "	unsigned int val;",
        "",
        "	if (test_thread_flag(TIF_NOTSC))",
        "		val = PR_TSC_SIGSEGV;",
        "	else",
        "		val = PR_TSC_ENABLE;",
        "",
        "	return put_user(val, (unsigned int __user *)adr);",
        "}",
        "",
        "int set_tsc_mode(unsigned int val)",
        "{",
        "	if (val == PR_TSC_SIGSEGV)",
        "		disable_TSC();",
        "	else if (val == PR_TSC_ENABLE)",
        "		enable_TSC();",
        "	else",
        "		return -EINVAL;",
        "",
        "	return 0;",
        "}",
        "",
        "DEFINE_PER_CPU(u64, msr_misc_features_shadow);",
        "",
        "static void set_cpuid_faulting(bool on)",
        "{",
        "	u64 msrval;",
        "",
        "	msrval = this_cpu_read(msr_misc_features_shadow);",
        "	msrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;",
        "	msrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);",
        "	this_cpu_write(msr_misc_features_shadow, msrval);",
        "	wrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);",
        "}",
        "",
        "static void disable_cpuid(void)",
        "{",
        "	preempt_disable();",
        "	if (!test_and_set_thread_flag(TIF_NOCPUID)) {",
        "		/*",
        "		 * Must flip the CPU state synchronously with",
        "		 * TIF_NOCPUID in the current running context.",
        "		 */",
        "		set_cpuid_faulting(true);",
        "	}",
        "	preempt_enable();",
        "}",
        "",
        "static void enable_cpuid(void)",
        "{",
        "	preempt_disable();",
        "	if (test_and_clear_thread_flag(TIF_NOCPUID)) {",
        "		/*",
        "		 * Must flip the CPU state synchronously with",
        "		 * TIF_NOCPUID in the current running context.",
        "		 */",
        "		set_cpuid_faulting(false);",
        "	}",
        "	preempt_enable();",
        "}",
        "",
        "static int get_cpuid_mode(void)",
        "{",
        "	return !test_thread_flag(TIF_NOCPUID);",
        "}",
        "",
        "static int set_cpuid_mode(unsigned long cpuid_enabled)",
        "{",
        "	if (!boot_cpu_has(X86_FEATURE_CPUID_FAULT))",
        "		return -ENODEV;",
        "",
        "	if (cpuid_enabled)",
        "		enable_cpuid();",
        "	else",
        "		disable_cpuid();",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Called immediately after a successful exec.",
        " */",
        "void arch_setup_new_exec(void)",
        "{",
        "	/* If cpuid was previously disabled for this task, re-enable it. */",
        "	if (test_thread_flag(TIF_NOCPUID))",
        "		enable_cpuid();",
        "",
        "	/*",
        "	 * Don't inherit TIF_SSBD across exec boundary when",
        "	 * PR_SPEC_DISABLE_NOEXEC is used.",
        "	 */",
        "	if (test_thread_flag(TIF_SSBD) &&",
        "	    task_spec_ssb_noexec(current)) {",
        "		clear_thread_flag(TIF_SSBD);",
        "		task_clear_spec_ssb_disable(current);",
        "		task_clear_spec_ssb_noexec(current);",
        "		speculation_ctrl_update(read_thread_flags());",
        "	}",
        "",
        "	mm_reset_untag_mask(current->mm);",
        "}",
        "",
        "#ifdef CONFIG_X86_IOPL_IOPERM",
        "static inline void switch_to_bitmap(unsigned long tifp)",
        "{",
        "	/*",
        "	 * Invalidate I/O bitmap if the previous task used it. This prevents",
        "	 * any possible leakage of an active I/O bitmap.",
        "	 *",
        "	 * If the next task has an I/O bitmap it will handle it on exit to",
        "	 * user mode.",
        "	 */",
        "	if (tifp & _TIF_IO_BITMAP)",
        "		tss_invalidate_io_bitmap();",
        "}",
        "",
        "static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)",
        "{",
        "	/*",
        "	 * Copy at least the byte range of the incoming tasks bitmap which",
        "	 * covers the permitted I/O ports.",
        "	 *",
        "	 * If the previous task which used an I/O bitmap had more bits",
        "	 * permitted, then the copy needs to cover those as well so they",
        "	 * get turned off.",
        "	 */",
        "	memcpy(tss->io_bitmap.bitmap, iobm->bitmap,",
        "	       max(tss->io_bitmap.prev_max, iobm->max));",
        "",
        "	/*",
        "	 * Store the new max and the sequence number of this bitmap",
        "	 * and a pointer to the bitmap itself.",
        "	 */",
        "	tss->io_bitmap.prev_max = iobm->max;",
        "	tss->io_bitmap.prev_sequence = iobm->sequence;",
        "}",
        "",
        "/**",
        " * native_tss_update_io_bitmap - Update I/O bitmap before exiting to user mode",
        " */",
        "void native_tss_update_io_bitmap(void)",
        "{",
        "	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);",
        "	struct thread_struct *t = &current->thread;",
        "	u16 *base = &tss->x86_tss.io_bitmap_base;",
        "",
        "	if (!test_thread_flag(TIF_IO_BITMAP)) {",
        "		native_tss_invalidate_io_bitmap();",
        "		return;",
        "	}",
        "",
        "	if (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {",
        "		*base = IO_BITMAP_OFFSET_VALID_ALL;",
        "	} else {",
        "		struct io_bitmap *iobm = t->io_bitmap;",
        "",
        "		/*",
        "		 * Only copy bitmap data when the sequence number differs. The",
        "		 * update time is accounted to the incoming task.",
        "		 */",
        "		if (tss->io_bitmap.prev_sequence != iobm->sequence)",
        "			tss_copy_io_bitmap(tss, iobm);",
        "",
        "		/* Enable the bitmap */",
        "		*base = IO_BITMAP_OFFSET_VALID_MAP;",
        "	}",
        "",
        "	/*",
        "	 * Make sure that the TSS limit is covering the IO bitmap. It might have",
        "	 * been cut down by a VMEXIT to 0x67 which would cause a subsequent I/O",
        "	 * access from user space to trigger a #GP because the bitmap is outside",
        "	 * the TSS limit.",
        "	 */",
        "	refresh_tss_limit();",
        "}",
        "#else /* CONFIG_X86_IOPL_IOPERM */",
        "static inline void switch_to_bitmap(unsigned long tifp) { }",
        "#endif",
        "",
        "#ifdef CONFIG_SMP",
        "",
        "struct ssb_state {",
        "	struct ssb_state	*shared_state;",
        "	raw_spinlock_t		lock;",
        "	unsigned int		disable_state;",
        "	unsigned long		local_state;",
        "};",
        "",
        "#define LSTATE_SSB	0",
        "",
        "static DEFINE_PER_CPU(struct ssb_state, ssb_state);",
        "",
        "void speculative_store_bypass_ht_init(void)",
        "{",
        "	struct ssb_state *st = this_cpu_ptr(&ssb_state);",
        "	unsigned int this_cpu = smp_processor_id();",
        "	unsigned int cpu;",
        "",
        "	st->local_state = 0;",
        "",
        "	/*",
        "	 * Shared state setup happens once on the first bringup",
        "	 * of the CPU. It's not destroyed on CPU hotunplug.",
        "	 */",
        "	if (st->shared_state)",
        "		return;",
        "",
        "	raw_spin_lock_init(&st->lock);",
        "",
        "	/*",
        "	 * Go over HT siblings and check whether one of them has set up the",
        "	 * shared state pointer already.",
        "	 */",
        "	for_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {",
        "		if (cpu == this_cpu)",
        "			continue;",
        "",
        "		if (!per_cpu(ssb_state, cpu).shared_state)",
        "			continue;",
        "",
        "		/* Link it to the state of the sibling: */",
        "		st->shared_state = per_cpu(ssb_state, cpu).shared_state;",
        "		return;",
        "	}",
        "",
        "	/*",
        "	 * First HT sibling to come up on the core.  Link shared state of",
        "	 * the first HT sibling to itself. The siblings on the same core",
        "	 * which come up later will see the shared state pointer and link",
        "	 * themselves to the state of this CPU.",
        "	 */",
        "	st->shared_state = st;",
        "}",
        "",
        "/*",
        " * Logic is: First HT sibling enables SSBD for both siblings in the core",
        " * and last sibling to disable it, disables it for the whole core. This how",
        " * MSR_SPEC_CTRL works in \"hardware\":",
        " *",
        " *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL",
        " */",
        "static __always_inline void amd_set_core_ssb_state(unsigned long tifn)",
        "{",
        "	struct ssb_state *st = this_cpu_ptr(&ssb_state);",
        "	u64 msr = x86_amd_ls_cfg_base;",
        "",
        "	if (!static_cpu_has(X86_FEATURE_ZEN)) {",
        "		msr |= ssbd_tif_to_amd_ls_cfg(tifn);",
        "		wrmsrl(MSR_AMD64_LS_CFG, msr);",
        "		return;",
        "	}",
        "",
        "	if (tifn & _TIF_SSBD) {",
        "		/*",
        "		 * Since this can race with prctl(), block reentry on the",
        "		 * same CPU.",
        "		 */",
        "		if (__test_and_set_bit(LSTATE_SSB, &st->local_state))",
        "			return;",
        "",
        "		msr |= x86_amd_ls_cfg_ssbd_mask;",
        "",
        "		raw_spin_lock(&st->shared_state->lock);",
        "		/* First sibling enables SSBD: */",
        "		if (!st->shared_state->disable_state)",
        "			wrmsrl(MSR_AMD64_LS_CFG, msr);",
        "		st->shared_state->disable_state++;",
        "		raw_spin_unlock(&st->shared_state->lock);",
        "	} else {",
        "		if (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))",
        "			return;",
        "",
        "		raw_spin_lock(&st->shared_state->lock);",
        "		st->shared_state->disable_state--;",
        "		if (!st->shared_state->disable_state)",
        "			wrmsrl(MSR_AMD64_LS_CFG, msr);",
        "		raw_spin_unlock(&st->shared_state->lock);",
        "	}",
        "}",
        "#else",
        "static __always_inline void amd_set_core_ssb_state(unsigned long tifn)",
        "{",
        "	u64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);",
        "",
        "	wrmsrl(MSR_AMD64_LS_CFG, msr);",
        "}",
        "#endif",
        "",
        "static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)",
        "{",
        "	/*",
        "	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,",
        "	 * so ssbd_tif_to_spec_ctrl() just works.",
        "	 */",
        "	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));",
        "}",
        "",
        "/*",
        " * Update the MSRs managing speculation control, during context switch.",
        " *",
        " * tifp: Previous task's thread flags",
        " * tifn: Next task's thread flags",
        " */",
        "static __always_inline void __speculation_ctrl_update(unsigned long tifp,",
        "						      unsigned long tifn)",
        "{",
        "	unsigned long tif_diff = tifp ^ tifn;",
        "	u64 msr = x86_spec_ctrl_base;",
        "	bool updmsr = false;",
        "",
        "	lockdep_assert_irqs_disabled();",
        "",
        "	/* Handle change of TIF_SSBD depending on the mitigation method. */",
        "	if (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {",
        "		if (tif_diff & _TIF_SSBD)",
        "			amd_set_ssb_virt_state(tifn);",
        "	} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {",
        "		if (tif_diff & _TIF_SSBD)",
        "			amd_set_core_ssb_state(tifn);",
        "	} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||",
        "		   static_cpu_has(X86_FEATURE_AMD_SSBD)) {",
        "		updmsr |= !!(tif_diff & _TIF_SSBD);",
        "		msr |= ssbd_tif_to_spec_ctrl(tifn);",
        "	}",
        "",
        "	/* Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */",
        "	if (IS_ENABLED(CONFIG_SMP) &&",
        "	    static_branch_unlikely(&switch_to_cond_stibp)) {",
        "		updmsr |= !!(tif_diff & _TIF_SPEC_IB);",
        "		msr |= stibp_tif_to_spec_ctrl(tifn);",
        "	}",
        "",
        "	if (updmsr)",
        "		update_spec_ctrl_cond(msr);",
        "}",
        "",
        "static unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)",
        "{",
        "	if (test_and_clear_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE)) {",
        "		if (task_spec_ssb_disable(tsk))",
        "			set_tsk_thread_flag(tsk, TIF_SSBD);",
        "		else",
        "			clear_tsk_thread_flag(tsk, TIF_SSBD);",
        "",
        "		if (task_spec_ib_disable(tsk))",
        "			set_tsk_thread_flag(tsk, TIF_SPEC_IB);",
        "		else",
        "			clear_tsk_thread_flag(tsk, TIF_SPEC_IB);",
        "	}",
        "	/* Return the updated threadinfo flags*/",
        "	return read_task_thread_flags(tsk);",
        "}",
        "",
        "void speculation_ctrl_update(unsigned long tif)",
        "{",
        "	unsigned long flags;",
        "",
        "	/* Forced update. Make sure all relevant TIF flags are different */",
        "	local_irq_save(flags);",
        "	__speculation_ctrl_update(~tif, tif);",
        "	local_irq_restore(flags);",
        "}",
        "",
        "/* Called from seccomp/prctl update */",
        "void speculation_ctrl_update_current(void)",
        "{",
        "	preempt_disable();",
        "	speculation_ctrl_update(speculation_ctrl_update_tif(current));",
        "	preempt_enable();",
        "}",
        "",
        "static inline void cr4_toggle_bits_irqsoff(unsigned long mask)",
        "{",
        "	unsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);",
        "",
        "	newval = cr4 ^ mask;",
        "	if (newval != cr4) {",
        "		this_cpu_write(cpu_tlbstate.cr4, newval);",
        "		__write_cr4(newval);",
        "	}",
        "}",
        "",
        "void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)",
        "{",
        "	unsigned long tifp, tifn;",
        "",
        "	tifn = read_task_thread_flags(next_p);",
        "	tifp = read_task_thread_flags(prev_p);",
        "",
        "	switch_to_bitmap(tifp);",
        "",
        "	propagate_user_return_notify(prev_p, next_p);",
        "",
        "	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&",
        "	    arch_has_block_step()) {",
        "		unsigned long debugctl, msk;",
        "",
        "		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);",
        "		debugctl &= ~DEBUGCTLMSR_BTF;",
        "		msk = tifn & _TIF_BLOCKSTEP;",
        "		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;",
        "		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);",
        "	}",
        "",
        "	if ((tifp ^ tifn) & _TIF_NOTSC)",
        "		cr4_toggle_bits_irqsoff(X86_CR4_TSD);",
        "",
        "	if ((tifp ^ tifn) & _TIF_NOCPUID)",
        "		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));",
        "",
        "	if (likely(!((tifp | tifn) & _TIF_SPEC_FORCE_UPDATE))) {",
        "		__speculation_ctrl_update(tifp, tifn);",
        "	} else {",
        "		speculation_ctrl_update_tif(prev_p);",
        "		tifn = speculation_ctrl_update_tif(next_p);",
        "",
        "		/* Enforce MSR update to ensure consistent state */",
        "		__speculation_ctrl_update(~tifn, tifn);",
        "	}",
        "}",
        "",
        "/*",
        " * Idle related variables and functions",
        " */",
        "unsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;",
        "EXPORT_SYMBOL(boot_option_idle_override);",
        "",
        "/*",
        " * We use this if we don't have any better idle routine..",
        " */",
        "void __cpuidle default_idle(void)",
        "{",
        "	raw_safe_halt();",
        "	raw_local_irq_disable();",
        "}",
        "#if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)",
        "EXPORT_SYMBOL(default_idle);",
        "#endif",
        "",
        "DEFINE_STATIC_CALL_NULL(x86_idle, default_idle);",
        "",
        "static bool x86_idle_set(void)",
        "{",
        "	return !!static_call_query(x86_idle);",
        "}",
        "",
        "#ifndef CONFIG_SMP",
        "static inline void __noreturn play_dead(void)",
        "{",
        "	BUG();",
        "}",
        "#endif",
        "",
        "void arch_cpu_idle_enter(void)",
        "{",
        "	tsc_verify_tsc_adjust(false);",
        "	local_touch_nmi();",
        "}",
        "",
        "void __noreturn arch_cpu_idle_dead(void)",
        "{",
        "	play_dead();",
        "}",
        "",
        "/*",
        " * Called from the generic idle code.",
        " */",
        "void __cpuidle arch_cpu_idle(void)",
        "{",
        "	static_call(x86_idle)();",
        "}",
        "EXPORT_SYMBOL_GPL(arch_cpu_idle);",
        "",
        "#ifdef CONFIG_XEN",
        "bool xen_set_default_idle(void)",
        "{",
        "	bool ret = x86_idle_set();",
        "",
        "	static_call_update(x86_idle, default_idle);",
        "",
        "	return ret;",
        "}",
        "#endif",
        "",
        "struct cpumask cpus_stop_mask;",
        "",
        "void __noreturn stop_this_cpu(void *dummy)",
        "{",
        "	struct cpuinfo_x86 *c = this_cpu_ptr(&cpu_info);",
        "	unsigned int cpu = smp_processor_id();",
        "",
        "	local_irq_disable();",
        "",
        "	/*",
        "	 * Remove this CPU from the online mask and disable it",
        "	 * unconditionally. This might be redundant in case that the reboot",
        "	 * vector was handled late and stop_other_cpus() sent an NMI.",
        "	 *",
        "	 * According to SDM and APM NMIs can be accepted even after soft",
        "	 * disabling the local APIC.",
        "	 */",
        "	set_cpu_online(cpu, false);",
        "	disable_local_APIC();",
        "	mcheck_cpu_clear(c);",
        "",
        "	/*",
        "	 * Use wbinvd on processors that support SME. This provides support",
        "	 * for performing a successful kexec when going from SME inactive",
        "	 * to SME active (or vice-versa). The cache must be cleared so that",
        "	 * if there are entries with the same physical address, both with and",
        "	 * without the encryption bit, they don't race each other when flushed",
        "	 * and potentially end up with the wrong entry being committed to",
        "	 * memory.",
        "	 *",
        "	 * Test the CPUID bit directly because the machine might've cleared",
        "	 * X86_FEATURE_SME due to cmdline options.",
        "	 */",
        "	if (c->extended_cpuid_level >= 0x8000001f && (cpuid_eax(0x8000001f) & BIT(0)))",
        "		native_wbinvd();",
        "",
        "	/*",
        "	 * This brings a cache line back and dirties it, but",
        "	 * native_stop_other_cpus() will overwrite cpus_stop_mask after it",
        "	 * observed that all CPUs reported stop. This write will invalidate",
        "	 * the related cache line on this CPU.",
        "	 */",
        "	cpumask_clear_cpu(cpu, &cpus_stop_mask);",
        "",
        "#ifdef CONFIG_SMP",
        "	if (smp_ops.stop_this_cpu) {",
        "		smp_ops.stop_this_cpu();",
        "		BUG();",
        "	}",
        "#endif",
        "",
        "	for (;;) {",
        "		/*",
        "		 * Use native_halt() so that memory contents don't change",
        "		 * (stack usage and variables) after possibly issuing the",
        "		 * native_wbinvd() above.",
        "		 */",
        "		native_halt();",
        "	}",
        "}",
        "",
        "/*",
        " * Prefer MWAIT over HALT if MWAIT is supported, MWAIT_CPUID leaf",
        " * exists and whenever MONITOR/MWAIT extensions are present there is at",
        " * least one C1 substate.",
        " *",
        " * Do not prefer MWAIT if MONITOR instruction has a bug or idle=nomwait",
        " * is passed to kernel commandline parameter.",
        " */",
        "static __init bool prefer_mwait_c1_over_halt(void)",
        "{",
        "	const struct cpuinfo_x86 *c = &boot_cpu_data;",
        "	u32 eax, ebx, ecx, edx;",
        "",
        "	/* If override is enforced on the command line, fall back to HALT. */",
        "	if (boot_option_idle_override != IDLE_NO_OVERRIDE)",
        "		return false;",
        "",
        "	/* MWAIT is not supported on this platform. Fallback to HALT */",
        "	if (!cpu_has(c, X86_FEATURE_MWAIT))",
        "		return false;",
        "",
        "	/* Monitor has a bug or APIC stops in C1E. Fallback to HALT */",
        "	if (boot_cpu_has_bug(X86_BUG_MONITOR) || boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E))",
        "		return false;",
        "",
        "	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);",
        "",
        "	/*",
        "	 * If MWAIT extensions are not available, it is safe to use MWAIT",
        "	 * with EAX=0, ECX=0.",
        "	 */",
        "	if (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED))",
        "		return true;",
        "",
        "	/*",
        "	 * If MWAIT extensions are available, there should be at least one",
        "	 * MWAIT C1 substate present.",
        "	 */",
        "	return !!(edx & MWAIT_C1_SUBSTATE_MASK);",
        "}",
        "",
        "/*",
        " * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT",
        " * with interrupts enabled and no flags, which is backwards compatible with the",
        " * original MWAIT implementation.",
        " */",
        "static __cpuidle void mwait_idle(void)",
        "{",
        "	if (!current_set_polling_and_test()) {",
        "		if (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {",
        "			mb(); /* quirk */",
        "			clflush((void *)&current_thread_info()->flags);",
        "			mb(); /* quirk */",
        "		}",
        "",
        "		__monitor((void *)&current_thread_info()->flags, 0, 0);",
        "		if (!need_resched()) {",
        "			__sti_mwait(0, 0);",
        "			raw_local_irq_disable();",
        "		}",
        "	}",
        "	__current_clr_polling();",
        "}",
        "",
        "void __init select_idle_routine(void)",
        "{",
        "	if (boot_option_idle_override == IDLE_POLL) {",
        "		if (IS_ENABLED(CONFIG_SMP) && __max_threads_per_core > 1)",
        "			pr_warn_once(\"WARNING: polling idle and HT enabled, performance may degrade\\n\");",
        "		return;",
        "	}",
        "",
        "	/* Required to guard against xen_set_default_idle() */",
        "	if (x86_idle_set())",
        "		return;",
        "",
        "	if (prefer_mwait_c1_over_halt()) {",
        "		pr_info(\"using mwait in idle threads\\n\");",
        "		static_call_update(x86_idle, mwait_idle);",
        "	} else if (cpu_feature_enabled(X86_FEATURE_TDX_GUEST)) {",
        "		pr_info(\"using TDX aware idle routine\\n\");",
        "		static_call_update(x86_idle, tdx_safe_halt);",
        "	} else {",
        "		static_call_update(x86_idle, default_idle);",
        "	}",
        "}",
        "",
        "void amd_e400_c1e_apic_setup(void)",
        "{",
        "	if (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {",
        "		pr_info(\"Switch to broadcast mode on CPU%d\\n\", smp_processor_id());",
        "		local_irq_disable();",
        "		tick_broadcast_force();",
        "		local_irq_enable();",
        "	}",
        "}",
        "",
        "void __init arch_post_acpi_subsys_init(void)",
        "{",
        "	u32 lo, hi;",
        "",
        "	if (!boot_cpu_has_bug(X86_BUG_AMD_E400))",
        "		return;",
        "",
        "	/*",
        "	 * AMD E400 detection needs to happen after ACPI has been enabled. If",
        "	 * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in",
        "	 * MSR_K8_INT_PENDING_MSG.",
        "	 */",
        "	rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);",
        "	if (!(lo & K8_INTP_C1E_ACTIVE_MASK))",
        "		return;",
        "",
        "	boot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);",
        "",
        "	if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))",
        "		mark_tsc_unstable(\"TSC halt in AMD C1E\");",
        "",
        "	if (IS_ENABLED(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST_IDLE))",
        "		static_branch_enable(&arch_needs_tick_broadcast);",
        "	pr_info(\"System has AMD C1E erratum E400. Workaround enabled.\\n\");",
        "}",
        "",
        "static int __init idle_setup(char *str)",
        "{",
        "	if (!str)",
        "		return -EINVAL;",
        "",
        "	if (!strcmp(str, \"poll\")) {",
        "		pr_info(\"using polling idle threads\\n\");",
        "		boot_option_idle_override = IDLE_POLL;",
        "		cpu_idle_poll_ctrl(true);",
        "	} else if (!strcmp(str, \"halt\")) {",
        "		/* 'idle=halt' HALT for idle. C-states are disabled. */",
        "		boot_option_idle_override = IDLE_HALT;",
        "	} else if (!strcmp(str, \"nomwait\")) {",
        "		/* 'idle=nomwait' disables MWAIT for idle */",
        "		boot_option_idle_override = IDLE_NOMWAIT;",
        "	} else {",
        "		return -EINVAL;",
        "	}",
        "",
        "	return 0;",
        "}",
        "early_param(\"idle\", idle_setup);",
        "",
        "unsigned long arch_align_stack(unsigned long sp)",
        "{",
        "	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)",
        "		sp -= get_random_u32_below(8192);",
        "	return sp & ~0xf;",
        "}",
        "",
        "unsigned long arch_randomize_brk(struct mm_struct *mm)",
        "{",
        "	if (mmap_is_ia32())",
        "		return randomize_page(mm->brk, SZ_32M);",
        "",
        "	return randomize_page(mm->brk, SZ_1G);",
        "}",
        "",
        "/*",
        " * Called from fs/proc with a reference on @p to find the function",
        " * which called into schedule(). This needs to be done carefully",
        " * because the task might wake up and we might look at a stack",
        " * changing under us.",
        " */",
        "unsigned long __get_wchan(struct task_struct *p)",
        "{",
        "	struct unwind_state state;",
        "	unsigned long addr = 0;",
        "",
        "	if (!try_get_task_stack(p))",
        "		return 0;",
        "",
        "	for (unwind_start(&state, p, NULL, NULL); !unwind_done(&state);",
        "	     unwind_next_frame(&state)) {",
        "		addr = unwind_get_return_address(&state);",
        "		if (!addr)",
        "			break;",
        "		if (in_sched_functions(addr))",
        "			continue;",
        "		break;",
        "	}",
        "",
        "	put_task_stack(p);",
        "",
        "	return addr;",
        "}",
        "",
        "long do_arch_prctl_common(int option, unsigned long arg2)",
        "{",
        "	switch (option) {",
        "	case ARCH_GET_CPUID:",
        "		return get_cpuid_mode();",
        "	case ARCH_SET_CPUID:",
        "		return set_cpuid_mode(arg2);",
        "	case ARCH_GET_XCOMP_SUPP:",
        "	case ARCH_GET_XCOMP_PERM:",
        "	case ARCH_REQ_XCOMP_PERM:",
        "	case ARCH_GET_XCOMP_GUEST_PERM:",
        "	case ARCH_REQ_XCOMP_GUEST_PERM:",
        "		return fpu_xstate_prctl(option, arg2);",
        "	}",
        "",
        "	return -EINVAL;",
        "}"
    ]
  },
  "kernel_bpf_offload_c": {
    path: "kernel/bpf/offload.c",
    covered: [63, 229, 252, 237, 514, 247, 535, 233, 256, 522],
    totalLines: 873,
    coveredCount: 10,
    coveragePct: 1.1,
    source: [
        "/*",
        " * Copyright (C) 2017-2018 Netronome Systems, Inc.",
        " *",
        " * This software is licensed under the GNU General License Version 2,",
        " * June 1991 as shown in the file COPYING in the top-level directory of this",
        " * source tree.",
        " *",
        " * THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\"",
        " * WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING,",
        " * BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS",
        " * FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE",
        " * OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME",
        " * THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.",
        " */",
        "",
        "#include <linux/bpf.h>",
        "#include <linux/bpf_verifier.h>",
        "#include <linux/bug.h>",
        "#include <linux/kdev_t.h>",
        "#include <linux/list.h>",
        "#include <linux/lockdep.h>",
        "#include <linux/netdevice.h>",
        "#include <linux/printk.h>",
        "#include <linux/proc_ns.h>",
        "#include <linux/rhashtable.h>",
        "#include <linux/rtnetlink.h>",
        "#include <linux/rwsem.h>",
        "#include <net/xdp.h>",
        "",
        "/* Protects offdevs, members of bpf_offload_netdev and offload members",
        " * of all progs.",
        " * RTNL lock cannot be taken when holding this lock.",
        " */",
        "static DECLARE_RWSEM(bpf_devs_lock);",
        "",
        "struct bpf_offload_dev {",
        "	const struct bpf_prog_offload_ops *ops;",
        "	struct list_head netdevs;",
        "	void *priv;",
        "};",
        "",
        "struct bpf_offload_netdev {",
        "	struct rhash_head l;",
        "	struct net_device *netdev;",
        "	struct bpf_offload_dev *offdev; /* NULL when bound-only */",
        "	struct list_head progs;",
        "	struct list_head maps;",
        "	struct list_head offdev_netdevs;",
        "};",
        "",
        "static const struct rhashtable_params offdevs_params = {",
        "	.nelem_hint		= 4,",
        "	.key_len		= sizeof(struct net_device *),",
        "	.key_offset		= offsetof(struct bpf_offload_netdev, netdev),",
        "	.head_offset		= offsetof(struct bpf_offload_netdev, l),",
        "	.automatic_shrinking	= true,",
        "};",
        "",
        "static struct rhashtable offdevs;",
        "",
        "static int bpf_dev_offload_check(struct net_device *netdev)",
        "{",
        "	if (!netdev)",
        "		return -EINVAL;",
        "	if (!netdev->netdev_ops->ndo_bpf)",
        "		return -EOPNOTSUPP;",
        "	return 0;",
        "}",
        "",
        "static struct bpf_offload_netdev *",
        "bpf_offload_find_netdev(struct net_device *netdev)",
        "{",
        "	lockdep_assert_held(&bpf_devs_lock);",
        "",
        "	return rhashtable_lookup_fast(&offdevs, &netdev, offdevs_params);",
        "}",
        "",
        "static int __bpf_offload_dev_netdev_register(struct bpf_offload_dev *offdev,",
        "					     struct net_device *netdev)",
        "{",
        "	struct bpf_offload_netdev *ondev;",
        "	int err;",
        "",
        "	ondev = kzalloc(sizeof(*ondev), GFP_KERNEL);",
        "	if (!ondev)",
        "		return -ENOMEM;",
        "",
        "	ondev->netdev = netdev;",
        "	ondev->offdev = offdev;",
        "	INIT_LIST_HEAD(&ondev->progs);",
        "	INIT_LIST_HEAD(&ondev->maps);",
        "",
        "	err = rhashtable_insert_fast(&offdevs, &ondev->l, offdevs_params);",
        "	if (err) {",
        "		netdev_warn(netdev, \"failed to register for BPF offload\\n\");",
        "		goto err_free;",
        "	}",
        "",
        "	if (offdev)",
        "		list_add(&ondev->offdev_netdevs, &offdev->netdevs);",
        "	return 0;",
        "",
        "err_free:",
        "	kfree(ondev);",
        "	return err;",
        "}",
        "",
        "static void __bpf_prog_offload_destroy(struct bpf_prog *prog)",
        "{",
        "	struct bpf_prog_offload *offload = prog->aux->offload;",
        "",
        "	if (offload->dev_state)",
        "		offload->offdev->ops->destroy(prog);",
        "",
        "	list_del_init(&offload->offloads);",
        "	kfree(offload);",
        "	prog->aux->offload = NULL;",
        "}",
        "",
        "static int bpf_map_offload_ndo(struct bpf_offloaded_map *offmap,",
        "			       enum bpf_netdev_command cmd)",
        "{",
        "	struct netdev_bpf data = {};",
        "	struct net_device *netdev;",
        "",
        "	ASSERT_RTNL();",
        "",
        "	data.command = cmd;",
        "	data.offmap = offmap;",
        "	/* Caller must make sure netdev is valid */",
        "	netdev = offmap->netdev;",
        "",
        "	return netdev->netdev_ops->ndo_bpf(netdev, &data);",
        "}",
        "",
        "static void __bpf_map_offload_destroy(struct bpf_offloaded_map *offmap)",
        "{",
        "	WARN_ON(bpf_map_offload_ndo(offmap, BPF_OFFLOAD_MAP_FREE));",
        "	/* Make sure BPF_MAP_GET_NEXT_ID can't find this dead map */",
        "	bpf_map_free_id(&offmap->map);",
        "	list_del_init(&offmap->offloads);",
        "	offmap->netdev = NULL;",
        "}",
        "",
        "static void __bpf_offload_dev_netdev_unregister(struct bpf_offload_dev *offdev,",
        "						struct net_device *netdev)",
        "{",
        "	struct bpf_offload_netdev *ondev, *altdev = NULL;",
        "	struct bpf_offloaded_map *offmap, *mtmp;",
        "	struct bpf_prog_offload *offload, *ptmp;",
        "",
        "	ASSERT_RTNL();",
        "",
        "	ondev = rhashtable_lookup_fast(&offdevs, &netdev, offdevs_params);",
        "	if (WARN_ON(!ondev))",
        "		return;",
        "",
        "	WARN_ON(rhashtable_remove_fast(&offdevs, &ondev->l, offdevs_params));",
        "",
        "	/* Try to move the objects to another netdev of the device */",
        "	if (offdev) {",
        "		list_del(&ondev->offdev_netdevs);",
        "		altdev = list_first_entry_or_null(&offdev->netdevs,",
        "						  struct bpf_offload_netdev,",
        "						  offdev_netdevs);",
        "	}",
        "",
        "	if (altdev) {",
        "		list_for_each_entry(offload, &ondev->progs, offloads)",
        "			offload->netdev = altdev->netdev;",
        "		list_splice_init(&ondev->progs, &altdev->progs);",
        "",
        "		list_for_each_entry(offmap, &ondev->maps, offloads)",
        "			offmap->netdev = altdev->netdev;",
        "		list_splice_init(&ondev->maps, &altdev->maps);",
        "	} else {",
        "		list_for_each_entry_safe(offload, ptmp, &ondev->progs, offloads)",
        "			__bpf_prog_offload_destroy(offload->prog);",
        "		list_for_each_entry_safe(offmap, mtmp, &ondev->maps, offloads)",
        "			__bpf_map_offload_destroy(offmap);",
        "	}",
        "",
        "	WARN_ON(!list_empty(&ondev->progs));",
        "	WARN_ON(!list_empty(&ondev->maps));",
        "	kfree(ondev);",
        "}",
        "",
        "static int __bpf_prog_dev_bound_init(struct bpf_prog *prog, struct net_device *netdev)",
        "{",
        "	struct bpf_offload_netdev *ondev;",
        "	struct bpf_prog_offload *offload;",
        "	int err;",
        "",
        "	offload = kzalloc(sizeof(*offload), GFP_USER);",
        "	if (!offload)",
        "		return -ENOMEM;",
        "",
        "	offload->prog = prog;",
        "	offload->netdev = netdev;",
        "",
        "	ondev = bpf_offload_find_netdev(offload->netdev);",
        "	/* When program is offloaded require presence of \"true\"",
        "	 * bpf_offload_netdev, avoid the one created for !ondev case below.",
        "	 */",
        "	if (bpf_prog_is_offloaded(prog->aux) && (!ondev || !ondev->offdev)) {",
        "		err = -EINVAL;",
        "		goto err_free;",
        "	}",
        "	if (!ondev) {",
        "		/* When only binding to the device, explicitly",
        "		 * create an entry in the hashtable.",
        "		 */",
        "		err = __bpf_offload_dev_netdev_register(NULL, offload->netdev);",
        "		if (err)",
        "			goto err_free;",
        "		ondev = bpf_offload_find_netdev(offload->netdev);",
        "	}",
        "	offload->offdev = ondev->offdev;",
        "	prog->aux->offload = offload;",
        "	list_add_tail(&offload->offloads, &ondev->progs);",
        "",
        "	return 0;",
        "err_free:",
        "	kfree(offload);",
        "	return err;",
        "}",
        "",
        "int bpf_prog_dev_bound_init(struct bpf_prog *prog, union bpf_attr *attr)",
        "{",
        "	struct net_device *netdev;",
        "	int err;",
        "",
        "	if (attr->prog_type != BPF_PROG_TYPE_SCHED_CLS &&",
        "	    attr->prog_type != BPF_PROG_TYPE_XDP)",
        "		return -EINVAL;",
        "",
        "	if (attr->prog_flags & ~(BPF_F_XDP_DEV_BOUND_ONLY | BPF_F_XDP_HAS_FRAGS))",
        "		return -EINVAL;",
        "",
        "	/* Frags are allowed only if program is dev-bound-only, but not",
        "	 * if it is requesting bpf offload.",
        "	 */",
        "	if (attr->prog_flags & BPF_F_XDP_HAS_FRAGS &&",
        "	    !(attr->prog_flags & BPF_F_XDP_DEV_BOUND_ONLY))",
        "		return -EINVAL;",
        "",
        "	if (attr->prog_type == BPF_PROG_TYPE_SCHED_CLS &&",
        "	    attr->prog_flags & BPF_F_XDP_DEV_BOUND_ONLY)",
        "		return -EINVAL;",
        "",
        "	netdev = dev_get_by_index(current->nsproxy->net_ns, attr->prog_ifindex);",
        "	if (!netdev)",
        "		return -EINVAL;",
        "",
        "	err = bpf_dev_offload_check(netdev);",
        "	if (err)",
        "		goto out;",
        "",
        "	prog->aux->offload_requested = !(attr->prog_flags & BPF_F_XDP_DEV_BOUND_ONLY);",
        "",
        "	down_write(&bpf_devs_lock);",
        "	err = __bpf_prog_dev_bound_init(prog, netdev);",
        "	up_write(&bpf_devs_lock);",
        "",
        "out:",
        "	dev_put(netdev);",
        "	return err;",
        "}",
        "",
        "int bpf_prog_dev_bound_inherit(struct bpf_prog *new_prog, struct bpf_prog *old_prog)",
        "{",
        "	int err;",
        "",
        "	if (!bpf_prog_is_dev_bound(old_prog->aux))",
        "		return 0;",
        "",
        "	if (bpf_prog_is_offloaded(old_prog->aux))",
        "		return -EINVAL;",
        "",
        "	new_prog->aux->dev_bound = old_prog->aux->dev_bound;",
        "	new_prog->aux->offload_requested = old_prog->aux->offload_requested;",
        "",
        "	down_write(&bpf_devs_lock);",
        "	if (!old_prog->aux->offload) {",
        "		err = -EINVAL;",
        "		goto out;",
        "	}",
        "",
        "	err = __bpf_prog_dev_bound_init(new_prog, old_prog->aux->offload->netdev);",
        "",
        "out:",
        "	up_write(&bpf_devs_lock);",
        "	return err;",
        "}",
        "",
        "int bpf_prog_offload_verifier_prep(struct bpf_prog *prog)",
        "{",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = prog->aux->offload;",
        "	if (offload) {",
        "		ret = offload->offdev->ops->prepare(prog);",
        "		offload->dev_state = !ret;",
        "	}",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,",
        "				 int insn_idx, int prev_insn_idx)",
        "{",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = env->prog->aux->offload;",
        "	if (offload)",
        "		ret = offload->offdev->ops->insn_hook(env, insn_idx,",
        "						      prev_insn_idx);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_prog_offload_finalize(struct bpf_verifier_env *env)",
        "{",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = env->prog->aux->offload;",
        "	if (offload) {",
        "		if (offload->offdev->ops->finalize)",
        "			ret = offload->offdev->ops->finalize(env);",
        "		else",
        "			ret = 0;",
        "	}",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "void",
        "bpf_prog_offload_replace_insn(struct bpf_verifier_env *env, u32 off,",
        "			      struct bpf_insn *insn)",
        "{",
        "	const struct bpf_prog_offload_ops *ops;",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -EOPNOTSUPP;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = env->prog->aux->offload;",
        "	if (offload) {",
        "		ops = offload->offdev->ops;",
        "		if (!offload->opt_failed && ops->replace_insn)",
        "			ret = ops->replace_insn(env, off, insn);",
        "		offload->opt_failed |= ret;",
        "	}",
        "	up_read(&bpf_devs_lock);",
        "}",
        "",
        "void",
        "bpf_prog_offload_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)",
        "{",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -EOPNOTSUPP;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = env->prog->aux->offload;",
        "	if (offload) {",
        "		if (!offload->opt_failed && offload->offdev->ops->remove_insns)",
        "			ret = offload->offdev->ops->remove_insns(env, off, cnt);",
        "		offload->opt_failed |= ret;",
        "	}",
        "	up_read(&bpf_devs_lock);",
        "}",
        "",
        "void bpf_prog_dev_bound_destroy(struct bpf_prog *prog)",
        "{",
        "	struct bpf_offload_netdev *ondev;",
        "	struct net_device *netdev;",
        "",
        "	rtnl_lock();",
        "	down_write(&bpf_devs_lock);",
        "	if (prog->aux->offload) {",
        "		list_del_init(&prog->aux->offload->offloads);",
        "",
        "		netdev = prog->aux->offload->netdev;",
        "		__bpf_prog_offload_destroy(prog);",
        "",
        "		ondev = bpf_offload_find_netdev(netdev);",
        "		if (!ondev->offdev && list_empty(&ondev->progs))",
        "			__bpf_offload_dev_netdev_unregister(NULL, netdev);",
        "	}",
        "	up_write(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "}",
        "",
        "static int bpf_prog_offload_translate(struct bpf_prog *prog)",
        "{",
        "	struct bpf_prog_offload *offload;",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	offload = prog->aux->offload;",
        "	if (offload)",
        "		ret = offload->offdev->ops->translate(prog);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "static unsigned int bpf_prog_warn_on_exec(const void *ctx,",
        "					  const struct bpf_insn *insn)",
        "{",
        "	WARN(1, \"attempt to execute device eBPF program on the host!\");",
        "	return 0;",
        "}",
        "",
        "int bpf_prog_offload_compile(struct bpf_prog *prog)",
        "{",
        "	prog->bpf_func = bpf_prog_warn_on_exec;",
        "",
        "	return bpf_prog_offload_translate(prog);",
        "}",
        "",
        "struct ns_get_path_bpf_prog_args {",
        "	struct bpf_prog *prog;",
        "	struct bpf_prog_info *info;",
        "};",
        "",
        "static struct ns_common *bpf_prog_offload_info_fill_ns(void *private_data)",
        "{",
        "	struct ns_get_path_bpf_prog_args *args = private_data;",
        "	struct bpf_prog_aux *aux = args->prog->aux;",
        "	struct ns_common *ns;",
        "	struct net *net;",
        "",
        "	rtnl_lock();",
        "	down_read(&bpf_devs_lock);",
        "",
        "	if (aux->offload) {",
        "		args->info->ifindex = aux->offload->netdev->ifindex;",
        "		net = dev_net(aux->offload->netdev);",
        "		get_net(net);",
        "		ns = &net->ns;",
        "	} else {",
        "		args->info->ifindex = 0;",
        "		ns = NULL;",
        "	}",
        "",
        "	up_read(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "",
        "	return ns;",
        "}",
        "",
        "int bpf_prog_offload_info_fill(struct bpf_prog_info *info,",
        "			       struct bpf_prog *prog)",
        "{",
        "	struct ns_get_path_bpf_prog_args args = {",
        "		.prog	= prog,",
        "		.info	= info,",
        "	};",
        "	struct bpf_prog_aux *aux = prog->aux;",
        "	struct inode *ns_inode;",
        "	struct path ns_path;",
        "	char __user *uinsns;",
        "	int res;",
        "	u32 ulen;",
        "",
        "	res = ns_get_path_cb(&ns_path, bpf_prog_offload_info_fill_ns, &args);",
        "	if (res) {",
        "		if (!info->ifindex)",
        "			return -ENODEV;",
        "		return res;",
        "	}",
        "",
        "	down_read(&bpf_devs_lock);",
        "",
        "	if (!aux->offload) {",
        "		up_read(&bpf_devs_lock);",
        "		return -ENODEV;",
        "	}",
        "",
        "	ulen = info->jited_prog_len;",
        "	info->jited_prog_len = aux->offload->jited_len;",
        "	if (info->jited_prog_len && ulen) {",
        "		uinsns = u64_to_user_ptr(info->jited_prog_insns);",
        "		ulen = min_t(u32, info->jited_prog_len, ulen);",
        "		if (copy_to_user(uinsns, aux->offload->jited_image, ulen)) {",
        "			up_read(&bpf_devs_lock);",
        "			return -EFAULT;",
        "		}",
        "	}",
        "",
        "	up_read(&bpf_devs_lock);",
        "",
        "	ns_inode = ns_path.dentry->d_inode;",
        "	info->netns_dev = new_encode_dev(ns_inode->i_sb->s_dev);",
        "	info->netns_ino = ns_inode->i_ino;",
        "	path_put(&ns_path);",
        "",
        "	return 0;",
        "}",
        "",
        "const struct bpf_prog_ops bpf_offload_prog_ops = {",
        "};",
        "",
        "struct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr)",
        "{",
        "	struct net *net = current->nsproxy->net_ns;",
        "	struct bpf_offload_netdev *ondev;",
        "	struct bpf_offloaded_map *offmap;",
        "	int err;",
        "",
        "	if (!capable(CAP_SYS_ADMIN))",
        "		return ERR_PTR(-EPERM);",
        "	if (attr->map_type != BPF_MAP_TYPE_ARRAY &&",
        "	    attr->map_type != BPF_MAP_TYPE_HASH)",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	offmap = bpf_map_area_alloc(sizeof(*offmap), NUMA_NO_NODE);",
        "	if (!offmap)",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	bpf_map_init_from_attr(&offmap->map, attr);",
        "",
        "	rtnl_lock();",
        "	down_write(&bpf_devs_lock);",
        "	offmap->netdev = __dev_get_by_index(net, attr->map_ifindex);",
        "	err = bpf_dev_offload_check(offmap->netdev);",
        "	if (err)",
        "		goto err_unlock;",
        "",
        "	ondev = bpf_offload_find_netdev(offmap->netdev);",
        "	if (!ondev) {",
        "		err = -EINVAL;",
        "		goto err_unlock;",
        "	}",
        "",
        "	err = bpf_map_offload_ndo(offmap, BPF_OFFLOAD_MAP_ALLOC);",
        "	if (err)",
        "		goto err_unlock;",
        "",
        "	list_add_tail(&offmap->offloads, &ondev->maps);",
        "	up_write(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "",
        "	return &offmap->map;",
        "",
        "err_unlock:",
        "	up_write(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "	bpf_map_area_free(offmap);",
        "	return ERR_PTR(err);",
        "}",
        "",
        "void bpf_map_offload_map_free(struct bpf_map *map)",
        "{",
        "	struct bpf_offloaded_map *offmap = map_to_offmap(map);",
        "",
        "	rtnl_lock();",
        "	down_write(&bpf_devs_lock);",
        "	if (offmap->netdev)",
        "		__bpf_map_offload_destroy(offmap);",
        "	up_write(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "",
        "	bpf_map_area_free(offmap);",
        "}",
        "",
        "u64 bpf_map_offload_map_mem_usage(const struct bpf_map *map)",
        "{",
        "	/* The memory dynamically allocated in netdev dev_ops is not counted */",
        "	return sizeof(struct bpf_offloaded_map);",
        "}",
        "",
        "int bpf_map_offload_lookup_elem(struct bpf_map *map, void *key, void *value)",
        "{",
        "	struct bpf_offloaded_map *offmap = map_to_offmap(map);",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	if (offmap->netdev)",
        "		ret = offmap->dev_ops->map_lookup_elem(offmap, key, value);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_map_offload_update_elem(struct bpf_map *map,",
        "				void *key, void *value, u64 flags)",
        "{",
        "	struct bpf_offloaded_map *offmap = map_to_offmap(map);",
        "	int ret = -ENODEV;",
        "",
        "	if (unlikely(flags > BPF_EXIST))",
        "		return -EINVAL;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	if (offmap->netdev)",
        "		ret = offmap->dev_ops->map_update_elem(offmap, key, value,",
        "						       flags);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_map_offload_delete_elem(struct bpf_map *map, void *key)",
        "{",
        "	struct bpf_offloaded_map *offmap = map_to_offmap(map);",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	if (offmap->netdev)",
        "		ret = offmap->dev_ops->map_delete_elem(offmap, key);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_map_offload_get_next_key(struct bpf_map *map, void *key, void *next_key)",
        "{",
        "	struct bpf_offloaded_map *offmap = map_to_offmap(map);",
        "	int ret = -ENODEV;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	if (offmap->netdev)",
        "		ret = offmap->dev_ops->map_get_next_key(offmap, key, next_key);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "struct ns_get_path_bpf_map_args {",
        "	struct bpf_offloaded_map *offmap;",
        "	struct bpf_map_info *info;",
        "};",
        "",
        "static struct ns_common *bpf_map_offload_info_fill_ns(void *private_data)",
        "{",
        "	struct ns_get_path_bpf_map_args *args = private_data;",
        "	struct ns_common *ns;",
        "	struct net *net;",
        "",
        "	rtnl_lock();",
        "	down_read(&bpf_devs_lock);",
        "",
        "	if (args->offmap->netdev) {",
        "		args->info->ifindex = args->offmap->netdev->ifindex;",
        "		net = dev_net(args->offmap->netdev);",
        "		get_net(net);",
        "		ns = &net->ns;",
        "	} else {",
        "		args->info->ifindex = 0;",
        "		ns = NULL;",
        "	}",
        "",
        "	up_read(&bpf_devs_lock);",
        "	rtnl_unlock();",
        "",
        "	return ns;",
        "}",
        "",
        "int bpf_map_offload_info_fill(struct bpf_map_info *info, struct bpf_map *map)",
        "{",
        "	struct ns_get_path_bpf_map_args args = {",
        "		.offmap	= map_to_offmap(map),",
        "		.info	= info,",
        "	};",
        "	struct inode *ns_inode;",
        "	struct path ns_path;",
        "	int res;",
        "",
        "	res = ns_get_path_cb(&ns_path, bpf_map_offload_info_fill_ns, &args);",
        "	if (res) {",
        "		if (!info->ifindex)",
        "			return -ENODEV;",
        "		return res;",
        "	}",
        "",
        "	ns_inode = ns_path.dentry->d_inode;",
        "	info->netns_dev = new_encode_dev(ns_inode->i_sb->s_dev);",
        "	info->netns_ino = ns_inode->i_ino;",
        "	path_put(&ns_path);",
        "",
        "	return 0;",
        "}",
        "",
        "static bool __bpf_offload_dev_match(struct bpf_prog *prog,",
        "				    struct net_device *netdev)",
        "{",
        "	struct bpf_offload_netdev *ondev1, *ondev2;",
        "	struct bpf_prog_offload *offload;",
        "",
        "	if (!bpf_prog_is_dev_bound(prog->aux))",
        "		return false;",
        "",
        "	offload = prog->aux->offload;",
        "	if (!offload)",
        "		return false;",
        "	if (offload->netdev == netdev)",
        "		return true;",
        "",
        "	ondev1 = bpf_offload_find_netdev(offload->netdev);",
        "	ondev2 = bpf_offload_find_netdev(netdev);",
        "",
        "	return ondev1 && ondev2 && ondev1->offdev == ondev2->offdev;",
        "}",
        "",
        "bool bpf_offload_dev_match(struct bpf_prog *prog, struct net_device *netdev)",
        "{",
        "	bool ret;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	ret = __bpf_offload_dev_match(prog, netdev);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_match);",
        "",
        "bool bpf_prog_dev_bound_match(const struct bpf_prog *lhs, const struct bpf_prog *rhs)",
        "{",
        "	bool ret;",
        "",
        "	if (bpf_prog_is_offloaded(lhs->aux) != bpf_prog_is_offloaded(rhs->aux))",
        "		return false;",
        "",
        "	down_read(&bpf_devs_lock);",
        "	ret = lhs->aux->offload && rhs->aux->offload &&",
        "	      lhs->aux->offload->netdev &&",
        "	      lhs->aux->offload->netdev == rhs->aux->offload->netdev;",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "bool bpf_offload_prog_map_match(struct bpf_prog *prog, struct bpf_map *map)",
        "{",
        "	struct bpf_offloaded_map *offmap;",
        "	bool ret;",
        "",
        "	if (!bpf_map_is_offloaded(map))",
        "		return bpf_map_offload_neutral(map);",
        "	offmap = map_to_offmap(map);",
        "",
        "	down_read(&bpf_devs_lock);",
        "	ret = __bpf_offload_dev_match(prog, offmap->netdev);",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return ret;",
        "}",
        "",
        "int bpf_offload_dev_netdev_register(struct bpf_offload_dev *offdev,",
        "				    struct net_device *netdev)",
        "{",
        "	int err;",
        "",
        "	down_write(&bpf_devs_lock);",
        "	err = __bpf_offload_dev_netdev_register(offdev, netdev);",
        "	up_write(&bpf_devs_lock);",
        "	return err;",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_netdev_register);",
        "",
        "void bpf_offload_dev_netdev_unregister(struct bpf_offload_dev *offdev,",
        "				       struct net_device *netdev)",
        "{",
        "	down_write(&bpf_devs_lock);",
        "	__bpf_offload_dev_netdev_unregister(offdev, netdev);",
        "	up_write(&bpf_devs_lock);",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_netdev_unregister);",
        "",
        "struct bpf_offload_dev *",
        "bpf_offload_dev_create(const struct bpf_prog_offload_ops *ops, void *priv)",
        "{",
        "	struct bpf_offload_dev *offdev;",
        "",
        "	offdev = kzalloc(sizeof(*offdev), GFP_KERNEL);",
        "	if (!offdev)",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	offdev->ops = ops;",
        "	offdev->priv = priv;",
        "	INIT_LIST_HEAD(&offdev->netdevs);",
        "",
        "	return offdev;",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_create);",
        "",
        "void bpf_offload_dev_destroy(struct bpf_offload_dev *offdev)",
        "{",
        "	WARN_ON(!list_empty(&offdev->netdevs));",
        "	kfree(offdev);",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_destroy);",
        "",
        "void *bpf_offload_dev_priv(struct bpf_offload_dev *offdev)",
        "{",
        "	return offdev->priv;",
        "}",
        "EXPORT_SYMBOL_GPL(bpf_offload_dev_priv);",
        "",
        "void bpf_dev_bound_netdev_unregister(struct net_device *dev)",
        "{",
        "	struct bpf_offload_netdev *ondev;",
        "",
        "	ASSERT_RTNL();",
        "",
        "	down_write(&bpf_devs_lock);",
        "	ondev = bpf_offload_find_netdev(dev);",
        "	if (ondev && !ondev->offdev)",
        "		__bpf_offload_dev_netdev_unregister(NULL, ondev->netdev);",
        "	up_write(&bpf_devs_lock);",
        "}",
        "",
        "int bpf_dev_bound_kfunc_check(struct bpf_verifier_log *log,",
        "			      struct bpf_prog_aux *prog_aux)",
        "{",
        "	if (!bpf_prog_is_dev_bound(prog_aux)) {",
        "		bpf_log(log, \"metadata kfuncs require device-bound program\\n\");",
        "		return -EINVAL;",
        "	}",
        "",
        "	if (bpf_prog_is_offloaded(prog_aux)) {",
        "		bpf_log(log, \"metadata kfuncs can't be offloaded\\n\");",
        "		return -EINVAL;",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "void *bpf_dev_bound_resolve_kfunc(struct bpf_prog *prog, u32 func_id)",
        "{",
        "	const struct xdp_metadata_ops *ops;",
        "	void *p = NULL;",
        "",
        "	/* We don't hold bpf_devs_lock while resolving several",
        "	 * kfuncs and can race with the unregister_netdevice().",
        "	 * We rely on bpf_dev_bound_match() check at attach",
        "	 * to render this program unusable.",
        "	 */",
        "	down_read(&bpf_devs_lock);",
        "	if (!prog->aux->offload)",
        "		goto out;",
        "",
        "	ops = prog->aux->offload->netdev->xdp_metadata_ops;",
        "	if (!ops)",
        "		goto out;",
        "",
        "#define XDP_METADATA_KFUNC(name, _, __, xmo) \\",
        "	if (func_id == bpf_xdp_metadata_kfunc_id(name)) p = ops->xmo;",
        "	XDP_METADATA_KFUNC_xxx",
        "#undef XDP_METADATA_KFUNC",
        "",
        "out:",
        "	up_read(&bpf_devs_lock);",
        "",
        "	return p;",
        "}",
        "",
        "static int __init bpf_offload_init(void)",
        "{",
        "	return rhashtable_init(&offdevs, &offdevs_params);",
        "}",
        "",
        "core_initcall(bpf_offload_init);"
    ]
  },
  "include_trace_events_pagemap_h": {
    path: "include/trace/events/pagemap.h",
    covered: [28],
    totalLines: 83,
    coveredCount: 1,
    coveragePct: 1.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#undef TRACE_SYSTEM",
        "#define TRACE_SYSTEM pagemap",
        "",
        "#if !defined(_TRACE_PAGEMAP_H) || defined(TRACE_HEADER_MULTI_READ)",
        "#define _TRACE_PAGEMAP_H",
        "",
        "#include <linux/tracepoint.h>",
        "#include <linux/mm.h>",
        "",
        "#define	PAGEMAP_MAPPED		0x0001u",
        "#define PAGEMAP_ANONYMOUS	0x0002u",
        "#define PAGEMAP_FILE		0x0004u",
        "#define PAGEMAP_SWAPCACHE	0x0008u",
        "#define PAGEMAP_SWAPBACKED	0x0010u",
        "#define PAGEMAP_MAPPEDDISK	0x0020u",
        "#define PAGEMAP_BUFFERS		0x0040u",
        "",
        "#define trace_pagemap_flags(folio) ( \\",
        "	(folio_test_anon(folio)		? PAGEMAP_ANONYMOUS  : PAGEMAP_FILE) | \\",
        "	(folio_mapped(folio)		? PAGEMAP_MAPPED     : 0) | \\",
        "	(folio_test_swapcache(folio)	? PAGEMAP_SWAPCACHE  : 0) | \\",
        "	(folio_test_swapbacked(folio)	? PAGEMAP_SWAPBACKED : 0) | \\",
        "	(folio_test_mappedtodisk(folio)	? PAGEMAP_MAPPEDDISK : 0) | \\",
        "	(folio_test_private(folio)	? PAGEMAP_BUFFERS    : 0) \\",
        "	)",
        "",
        "TRACE_EVENT(mm_lru_insertion,",
        "",
        "	TP_PROTO(struct folio *folio),",
        "",
        "	TP_ARGS(folio),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(struct folio *,	folio	)",
        "		__field(unsigned long,	pfn	)",
        "		__field(enum lru_list,	lru	)",
        "		__field(unsigned long,	flags	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->folio	= folio;",
        "		__entry->pfn	= folio_pfn(folio);",
        "		__entry->lru	= folio_lru_list(folio);",
        "		__entry->flags	= trace_pagemap_flags(folio);",
        "	),",
        "",
        "	/* Flag format is based on page-types.c formatting for pagemap */",
        "	TP_printk(\"folio=%p pfn=0x%lx lru=%d flags=%s%s%s%s%s%s\",",
        "			__entry->folio,",
        "			__entry->pfn,",
        "			__entry->lru,",
        "			__entry->flags & PAGEMAP_MAPPED		? \"M\" : \" \",",
        "			__entry->flags & PAGEMAP_ANONYMOUS	? \"a\" : \"f\",",
        "			__entry->flags & PAGEMAP_SWAPCACHE	? \"s\" : \" \",",
        "			__entry->flags & PAGEMAP_SWAPBACKED	? \"b\" : \" \",",
        "			__entry->flags & PAGEMAP_MAPPEDDISK	? \"d\" : \" \",",
        "			__entry->flags & PAGEMAP_BUFFERS	? \"B\" : \" \")",
        ");",
        "",
        "TRACE_EVENT(mm_lru_activate,",
        "",
        "	TP_PROTO(struct folio *folio),",
        "",
        "	TP_ARGS(folio),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(struct folio *,	folio	)",
        "		__field(unsigned long,	pfn	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->folio	= folio;",
        "		__entry->pfn	= folio_pfn(folio);",
        "	),",
        "",
        "	TP_printk(\"folio=%p pfn=0x%lx\", __entry->folio, __entry->pfn)",
        ");",
        "",
        "#endif /* _TRACE_PAGEMAP_H */",
        "",
        "/* This part must be outside protection */",
        "#include <trace/define_trace.h>"
    ]
  },
  "kernel_bpf_arena_c": {
    path: "kernel/bpf/arena.c",
    covered: [98, 103, 96],
    totalLines: 591,
    coveredCount: 3,
    coveragePct: 0.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/* Copyright (c) 2024 Meta Platforms, Inc. and affiliates. */",
        "#include <linux/bpf.h>",
        "#include <linux/btf.h>",
        "#include <linux/err.h>",
        "#include \"linux/filter.h\"",
        "#include <linux/btf_ids.h>",
        "#include <linux/vmalloc.h>",
        "#include <linux/pagemap.h>",
        "#include \"range_tree.h\"",
        "",
        "/*",
        " * bpf_arena is a sparsely populated shared memory region between bpf program and",
        " * user space process.",
        " *",
        " * For example on x86-64 the values could be:",
        " * user_vm_start 7f7d26200000     // picked by mmap()",
        " * kern_vm_start ffffc90001e69000 // picked by get_vm_area()",
        " * For user space all pointers within the arena are normal 8-byte addresses.",
        " * In this example 7f7d26200000 is the address of the first page (pgoff=0).",
        " * The bpf program will access it as: kern_vm_start + lower_32bit_of_user_ptr",
        " * (u32)7f7d26200000 -> 26200000",
        " * hence",
        " * ffffc90001e69000 + 26200000 == ffffc90028069000 is \"pgoff=0\" within 4Gb",
        " * kernel memory region.",
        " *",
        " * BPF JITs generate the following code to access arena:",
        " *   mov eax, eax  // eax has lower 32-bit of user pointer",
        " *   mov word ptr [rax + r12 + off], bx",
        " * where r12 == kern_vm_start and off is s16.",
        " * Hence allocate 4Gb + GUARD_SZ/2 on each side.",
        " *",
        " * Initially kernel vm_area and user vma are not populated.",
        " * User space can fault-in any address which will insert the page",
        " * into kernel and user vma.",
        " * bpf program can allocate a page via bpf_arena_alloc_pages() kfunc",
        " * which will insert it into kernel vm_area.",
        " * The later fault-in from user space will populate that page into user vma.",
        " */",
        "",
        "/* number of bytes addressable by LDX/STX insn with 16-bit 'off' field */",
        "#define GUARD_SZ round_up(1ull << sizeof_field(struct bpf_insn, off) * 8, PAGE_SIZE << 1)",
        "#define KERN_VM_SZ (SZ_4G + GUARD_SZ)",
        "",
        "struct bpf_arena {",
        "	struct bpf_map map;",
        "	u64 user_vm_start;",
        "	u64 user_vm_end;",
        "	struct vm_struct *kern_vm;",
        "	struct range_tree rt;",
        "	struct list_head vma_list;",
        "	struct mutex lock;",
        "};",
        "",
        "u64 bpf_arena_get_kern_vm_start(struct bpf_arena *arena)",
        "{",
        "	return arena ? (u64) (long) arena->kern_vm->addr + GUARD_SZ / 2 : 0;",
        "}",
        "",
        "u64 bpf_arena_get_user_vm_start(struct bpf_arena *arena)",
        "{",
        "	return arena ? arena->user_vm_start : 0;",
        "}",
        "",
        "static long arena_map_peek_elem(struct bpf_map *map, void *value)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static long arena_map_push_elem(struct bpf_map *map, void *value, u64 flags)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static long arena_map_pop_elem(struct bpf_map *map, void *value)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static long arena_map_delete_elem(struct bpf_map *map, void *value)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static int arena_map_get_next_key(struct bpf_map *map, void *key, void *next_key)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static long compute_pgoff(struct bpf_arena *arena, long uaddr)",
        "{",
        "	return (u32)(uaddr - (u32)arena->user_vm_start) >> PAGE_SHIFT;",
        "}",
        "",
        "static struct bpf_map *arena_map_alloc(union bpf_attr *attr)",
        "{",
        "	struct vm_struct *kern_vm;",
        "	int numa_node = bpf_map_attr_numa_node(attr);",
        "	struct bpf_arena *arena;",
        "	u64 vm_range;",
        "	int err = -ENOMEM;",
        "",
        "	if (!bpf_jit_supports_arena())",
        "		return ERR_PTR(-EOPNOTSUPP);",
        "",
        "	if (attr->key_size || attr->value_size || attr->max_entries == 0 ||",
        "	    /* BPF_F_MMAPABLE must be set */",
        "	    !(attr->map_flags & BPF_F_MMAPABLE) ||",
        "	    /* No unsupported flags present */",
        "	    (attr->map_flags & ~(BPF_F_SEGV_ON_FAULT | BPF_F_MMAPABLE | BPF_F_NO_USER_CONV)))",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	if (attr->map_extra & ~PAGE_MASK)",
        "		/* If non-zero the map_extra is an expected user VMA start address */",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	vm_range = (u64)attr->max_entries * PAGE_SIZE;",
        "	if (vm_range > SZ_4G)",
        "		return ERR_PTR(-E2BIG);",
        "",
        "	if ((attr->map_extra >> 32) != ((attr->map_extra + vm_range - 1) >> 32))",
        "		/* user vma must not cross 32-bit boundary */",
        "		return ERR_PTR(-ERANGE);",
        "",
        "	kern_vm = get_vm_area(KERN_VM_SZ, VM_SPARSE | VM_USERMAP);",
        "	if (!kern_vm)",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	arena = bpf_map_area_alloc(sizeof(*arena), numa_node);",
        "	if (!arena)",
        "		goto err;",
        "",
        "	arena->kern_vm = kern_vm;",
        "	arena->user_vm_start = attr->map_extra;",
        "	if (arena->user_vm_start)",
        "		arena->user_vm_end = arena->user_vm_start + vm_range;",
        "",
        "	INIT_LIST_HEAD(&arena->vma_list);",
        "	bpf_map_init_from_attr(&arena->map, attr);",
        "	range_tree_init(&arena->rt);",
        "	range_tree_set(&arena->rt, 0, attr->max_entries);",
        "	mutex_init(&arena->lock);",
        "",
        "	return &arena->map;",
        "err:",
        "	free_vm_area(kern_vm);",
        "	return ERR_PTR(err);",
        "}",
        "",
        "static int existing_page_cb(pte_t *ptep, unsigned long addr, void *data)",
        "{",
        "	struct page *page;",
        "	pte_t pte;",
        "",
        "	pte = ptep_get(ptep);",
        "	if (!pte_present(pte)) /* sanity check */",
        "		return 0;",
        "	page = pte_page(pte);",
        "	/*",
        "	 * We do not update pte here:",
        "	 * 1. Nobody should be accessing bpf_arena's range outside of a kernel bug",
        "	 * 2. TLB flushing is batched or deferred. Even if we clear pte,",
        "	 * the TLB entries can stick around and continue to permit access to",
        "	 * the freed page. So it all relies on 1.",
        "	 */",
        "	__free_page(page);",
        "	return 0;",
        "}",
        "",
        "static void arena_map_free(struct bpf_map *map)",
        "{",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "",
        "	/*",
        "	 * Check that user vma-s are not around when bpf map is freed.",
        "	 * mmap() holds vm_file which holds bpf_map refcnt.",
        "	 * munmap() must have happened on vma followed by arena_vm_close()",
        "	 * which would clear arena->vma_list.",
        "	 */",
        "	if (WARN_ON_ONCE(!list_empty(&arena->vma_list)))",
        "		return;",
        "",
        "	/*",
        "	 * free_vm_area() calls remove_vm_area() that calls free_unmap_vmap_area().",
        "	 * It unmaps everything from vmalloc area and clears pgtables.",
        "	 * Call apply_to_existing_page_range() first to find populated ptes and",
        "	 * free those pages.",
        "	 */",
        "	apply_to_existing_page_range(&init_mm, bpf_arena_get_kern_vm_start(arena),",
        "				     KERN_VM_SZ - GUARD_SZ, existing_page_cb, NULL);",
        "	free_vm_area(arena->kern_vm);",
        "	range_tree_destroy(&arena->rt);",
        "	bpf_map_area_free(arena);",
        "}",
        "",
        "static void *arena_map_lookup_elem(struct bpf_map *map, void *key)",
        "{",
        "	return ERR_PTR(-EINVAL);",
        "}",
        "",
        "static long arena_map_update_elem(struct bpf_map *map, void *key,",
        "				  void *value, u64 flags)",
        "{",
        "	return -EOPNOTSUPP;",
        "}",
        "",
        "static int arena_map_check_btf(const struct bpf_map *map, const struct btf *btf,",
        "			       const struct btf_type *key_type, const struct btf_type *value_type)",
        "{",
        "	return 0;",
        "}",
        "",
        "static u64 arena_map_mem_usage(const struct bpf_map *map)",
        "{",
        "	return 0;",
        "}",
        "",
        "struct vma_list {",
        "	struct vm_area_struct *vma;",
        "	struct list_head head;",
        "	refcount_t mmap_count;",
        "};",
        "",
        "static int remember_vma(struct bpf_arena *arena, struct vm_area_struct *vma)",
        "{",
        "	struct vma_list *vml;",
        "",
        "	vml = kmalloc(sizeof(*vml), GFP_KERNEL);",
        "	if (!vml)",
        "		return -ENOMEM;",
        "	refcount_set(&vml->mmap_count, 1);",
        "	vma->vm_private_data = vml;",
        "	vml->vma = vma;",
        "	list_add(&vml->head, &arena->vma_list);",
        "	return 0;",
        "}",
        "",
        "static void arena_vm_open(struct vm_area_struct *vma)",
        "{",
        "	struct vma_list *vml = vma->vm_private_data;",
        "",
        "	refcount_inc(&vml->mmap_count);",
        "}",
        "",
        "static void arena_vm_close(struct vm_area_struct *vma)",
        "{",
        "	struct bpf_map *map = vma->vm_file->private_data;",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "	struct vma_list *vml = vma->vm_private_data;",
        "",
        "	if (!refcount_dec_and_test(&vml->mmap_count))",
        "		return;",
        "	guard(mutex)(&arena->lock);",
        "	/* update link list under lock */",
        "	list_del(&vml->head);",
        "	vma->vm_private_data = NULL;",
        "	kfree(vml);",
        "}",
        "",
        "#define MT_ENTRY ((void *)&arena_map_ops) /* unused. has to be valid pointer */",
        "",
        "static vm_fault_t arena_vm_fault(struct vm_fault *vmf)",
        "{",
        "	struct bpf_map *map = vmf->vma->vm_file->private_data;",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "	struct page *page;",
        "	long kbase, kaddr;",
        "	int ret;",
        "",
        "	kbase = bpf_arena_get_kern_vm_start(arena);",
        "	kaddr = kbase + (u32)(vmf->address);",
        "",
        "	guard(mutex)(&arena->lock);",
        "	page = vmalloc_to_page((void *)kaddr);",
        "	if (page)",
        "		/* already have a page vmap-ed */",
        "		goto out;",
        "",
        "	if (arena->map.map_flags & BPF_F_SEGV_ON_FAULT)",
        "		/* User space requested to segfault when page is not allocated by bpf prog */",
        "		return VM_FAULT_SIGSEGV;",
        "",
        "	ret = range_tree_clear(&arena->rt, vmf->pgoff, 1);",
        "	if (ret)",
        "		return VM_FAULT_SIGSEGV;",
        "",
        "	/* Account into memcg of the process that created bpf_arena */",
        "	ret = bpf_map_alloc_pages(map, GFP_KERNEL | __GFP_ZERO, NUMA_NO_NODE, 1, &page);",
        "	if (ret) {",
        "		range_tree_set(&arena->rt, vmf->pgoff, 1);",
        "		return VM_FAULT_SIGSEGV;",
        "	}",
        "",
        "	ret = vm_area_map_pages(arena->kern_vm, kaddr, kaddr + PAGE_SIZE, &page);",
        "	if (ret) {",
        "		range_tree_set(&arena->rt, vmf->pgoff, 1);",
        "		__free_page(page);",
        "		return VM_FAULT_SIGSEGV;",
        "	}",
        "out:",
        "	page_ref_add(page, 1);",
        "	vmf->page = page;",
        "	return 0;",
        "}",
        "",
        "static const struct vm_operations_struct arena_vm_ops = {",
        "	.open		= arena_vm_open,",
        "	.close		= arena_vm_close,",
        "	.fault          = arena_vm_fault,",
        "};",
        "",
        "static unsigned long arena_get_unmapped_area(struct file *filp, unsigned long addr,",
        "					     unsigned long len, unsigned long pgoff,",
        "					     unsigned long flags)",
        "{",
        "	struct bpf_map *map = filp->private_data;",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "	long ret;",
        "",
        "	if (pgoff)",
        "		return -EINVAL;",
        "	if (len > SZ_4G)",
        "		return -E2BIG;",
        "",
        "	/* if user_vm_start was specified at arena creation time */",
        "	if (arena->user_vm_start) {",
        "		if (len > arena->user_vm_end - arena->user_vm_start)",
        "			return -E2BIG;",
        "		if (len != arena->user_vm_end - arena->user_vm_start)",
        "			return -EINVAL;",
        "		if (addr != arena->user_vm_start)",
        "			return -EINVAL;",
        "	}",
        "",
        "	ret = mm_get_unmapped_area(current->mm, filp, addr, len * 2, 0, flags);",
        "	if (IS_ERR_VALUE(ret))",
        "		return ret;",
        "	if ((ret >> 32) == ((ret + len - 1) >> 32))",
        "		return ret;",
        "	if (WARN_ON_ONCE(arena->user_vm_start))",
        "		/* checks at map creation time should prevent this */",
        "		return -EFAULT;",
        "	return round_up(ret, SZ_4G);",
        "}",
        "",
        "static int arena_map_mmap(struct bpf_map *map, struct vm_area_struct *vma)",
        "{",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "",
        "	guard(mutex)(&arena->lock);",
        "	if (arena->user_vm_start && arena->user_vm_start != vma->vm_start)",
        "		/*",
        "		 * If map_extra was not specified at arena creation time then",
        "		 * 1st user process can do mmap(NULL, ...) to pick user_vm_start",
        "		 * 2nd user process must pass the same addr to mmap(addr, MAP_FIXED..);",
        "		 *   or",
        "		 * specify addr in map_extra and",
        "		 * use the same addr later with mmap(addr, MAP_FIXED..);",
        "		 */",
        "		return -EBUSY;",
        "",
        "	if (arena->user_vm_end && arena->user_vm_end != vma->vm_end)",
        "		/* all user processes must have the same size of mmap-ed region */",
        "		return -EBUSY;",
        "",
        "	/* Earlier checks should prevent this */",
        "	if (WARN_ON_ONCE(vma->vm_end - vma->vm_start > SZ_4G || vma->vm_pgoff))",
        "		return -EFAULT;",
        "",
        "	if (remember_vma(arena, vma))",
        "		return -ENOMEM;",
        "",
        "	arena->user_vm_start = vma->vm_start;",
        "	arena->user_vm_end = vma->vm_end;",
        "	/*",
        "	 * bpf_map_mmap() checks that it's being mmaped as VM_SHARED and",
        "	 * clears VM_MAYEXEC. Set VM_DONTEXPAND as well to avoid",
        "	 * potential change of user_vm_start.",
        "	 */",
        "	vm_flags_set(vma, VM_DONTEXPAND);",
        "	vma->vm_ops = &arena_vm_ops;",
        "	return 0;",
        "}",
        "",
        "static int arena_map_direct_value_addr(const struct bpf_map *map, u64 *imm, u32 off)",
        "{",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "",
        "	if ((u64)off > arena->user_vm_end - arena->user_vm_start)",
        "		return -ERANGE;",
        "	*imm = (unsigned long)arena->user_vm_start;",
        "	return 0;",
        "}",
        "",
        "BTF_ID_LIST_SINGLE(bpf_arena_map_btf_ids, struct, bpf_arena)",
        "const struct bpf_map_ops arena_map_ops = {",
        "	.map_meta_equal = bpf_map_meta_equal,",
        "	.map_alloc = arena_map_alloc,",
        "	.map_free = arena_map_free,",
        "	.map_direct_value_addr = arena_map_direct_value_addr,",
        "	.map_mmap = arena_map_mmap,",
        "	.map_get_unmapped_area = arena_get_unmapped_area,",
        "	.map_get_next_key = arena_map_get_next_key,",
        "	.map_push_elem = arena_map_push_elem,",
        "	.map_peek_elem = arena_map_peek_elem,",
        "	.map_pop_elem = arena_map_pop_elem,",
        "	.map_lookup_elem = arena_map_lookup_elem,",
        "	.map_update_elem = arena_map_update_elem,",
        "	.map_delete_elem = arena_map_delete_elem,",
        "	.map_check_btf = arena_map_check_btf,",
        "	.map_mem_usage = arena_map_mem_usage,",
        "	.map_btf_id = &bpf_arena_map_btf_ids[0],",
        "};",
        "",
        "static u64 clear_lo32(u64 val)",
        "{",
        "	return val & ~(u64)~0U;",
        "}",
        "",
        "/*",
        " * Allocate pages and vmap them into kernel vmalloc area.",
        " * Later the pages will be mmaped into user space vma.",
        " */",
        "static long arena_alloc_pages(struct bpf_arena *arena, long uaddr, long page_cnt, int node_id)",
        "{",
        "	/* user_vm_end/start are fixed before bpf prog runs */",
        "	long page_cnt_max = (arena->user_vm_end - arena->user_vm_start) >> PAGE_SHIFT;",
        "	u64 kern_vm_start = bpf_arena_get_kern_vm_start(arena);",
        "	struct page **pages;",
        "	long pgoff = 0;",
        "	u32 uaddr32;",
        "	int ret, i;",
        "",
        "	if (page_cnt > page_cnt_max)",
        "		return 0;",
        "",
        "	if (uaddr) {",
        "		if (uaddr & ~PAGE_MASK)",
        "			return 0;",
        "		pgoff = compute_pgoff(arena, uaddr);",
        "		if (pgoff > page_cnt_max - page_cnt)",
        "			/* requested address will be outside of user VMA */",
        "			return 0;",
        "	}",
        "",
        "	/* zeroing is needed, since alloc_pages_bulk_array() only fills in non-zero entries */",
        "	pages = kvcalloc(page_cnt, sizeof(struct page *), GFP_KERNEL);",
        "	if (!pages)",
        "		return 0;",
        "",
        "	guard(mutex)(&arena->lock);",
        "",
        "	if (uaddr) {",
        "		ret = is_range_tree_set(&arena->rt, pgoff, page_cnt);",
        "		if (ret)",
        "			goto out_free_pages;",
        "		ret = range_tree_clear(&arena->rt, pgoff, page_cnt);",
        "	} else {",
        "		ret = pgoff = range_tree_find(&arena->rt, page_cnt);",
        "		if (pgoff >= 0)",
        "			ret = range_tree_clear(&arena->rt, pgoff, page_cnt);",
        "	}",
        "	if (ret)",
        "		goto out_free_pages;",
        "",
        "	ret = bpf_map_alloc_pages(&arena->map, GFP_KERNEL | __GFP_ZERO,",
        "				  node_id, page_cnt, pages);",
        "	if (ret)",
        "		goto out;",
        "",
        "	uaddr32 = (u32)(arena->user_vm_start + pgoff * PAGE_SIZE);",
        "	/* Earlier checks made sure that uaddr32 + page_cnt * PAGE_SIZE - 1",
        "	 * will not overflow 32-bit. Lower 32-bit need to represent",
        "	 * contiguous user address range.",
        "	 * Map these pages at kern_vm_start base.",
        "	 * kern_vm_start + uaddr32 + page_cnt * PAGE_SIZE - 1 can overflow",
        "	 * lower 32-bit and it's ok.",
        "	 */",
        "	ret = vm_area_map_pages(arena->kern_vm, kern_vm_start + uaddr32,",
        "				kern_vm_start + uaddr32 + page_cnt * PAGE_SIZE, pages);",
        "	if (ret) {",
        "		for (i = 0; i < page_cnt; i++)",
        "			__free_page(pages[i]);",
        "		goto out;",
        "	}",
        "	kvfree(pages);",
        "	return clear_lo32(arena->user_vm_start) + uaddr32;",
        "out:",
        "	range_tree_set(&arena->rt, pgoff, page_cnt);",
        "out_free_pages:",
        "	kvfree(pages);",
        "	return 0;",
        "}",
        "",
        "/*",
        " * If page is present in vmalloc area, unmap it from vmalloc area,",
        " * unmap it from all user space vma-s,",
        " * and free it.",
        " */",
        "static void zap_pages(struct bpf_arena *arena, long uaddr, long page_cnt)",
        "{",
        "	struct vma_list *vml;",
        "",
        "	list_for_each_entry(vml, &arena->vma_list, head)",
        "		zap_page_range_single(vml->vma, uaddr,",
        "				      PAGE_SIZE * page_cnt, NULL);",
        "}",
        "",
        "static void arena_free_pages(struct bpf_arena *arena, long uaddr, long page_cnt)",
        "{",
        "	u64 full_uaddr, uaddr_end;",
        "	long kaddr, pgoff, i;",
        "	struct page *page;",
        "",
        "	/* only aligned lower 32-bit are relevant */",
        "	uaddr = (u32)uaddr;",
        "	uaddr &= PAGE_MASK;",
        "	full_uaddr = clear_lo32(arena->user_vm_start) + uaddr;",
        "	uaddr_end = min(arena->user_vm_end, full_uaddr + (page_cnt << PAGE_SHIFT));",
        "	if (full_uaddr >= uaddr_end)",
        "		return;",
        "",
        "	page_cnt = (uaddr_end - full_uaddr) >> PAGE_SHIFT;",
        "",
        "	guard(mutex)(&arena->lock);",
        "",
        "	pgoff = compute_pgoff(arena, uaddr);",
        "	/* clear range */",
        "	range_tree_set(&arena->rt, pgoff, page_cnt);",
        "",
        "	if (page_cnt > 1)",
        "		/* bulk zap if multiple pages being freed */",
        "		zap_pages(arena, full_uaddr, page_cnt);",
        "",
        "	kaddr = bpf_arena_get_kern_vm_start(arena) + uaddr;",
        "	for (i = 0; i < page_cnt; i++, kaddr += PAGE_SIZE, full_uaddr += PAGE_SIZE) {",
        "		page = vmalloc_to_page((void *)kaddr);",
        "		if (!page)",
        "			continue;",
        "		if (page_cnt == 1 && page_mapped(page)) /* mapped by some user process */",
        "			/* Optimization for the common case of page_cnt==1:",
        "			 * If page wasn't mapped into some user vma there",
        "			 * is no need to call zap_pages which is slow. When",
        "			 * page_cnt is big it's faster to do the batched zap.",
        "			 */",
        "			zap_pages(arena, full_uaddr, 1);",
        "		vm_area_unmap_pages(arena->kern_vm, kaddr, kaddr + PAGE_SIZE);",
        "		__free_page(page);",
        "	}",
        "}",
        "",
        "__bpf_kfunc_start_defs();",
        "",
        "__bpf_kfunc void *bpf_arena_alloc_pages(void *p__map, void *addr__ign, u32 page_cnt,",
        "					int node_id, u64 flags)",
        "{",
        "	struct bpf_map *map = p__map;",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "",
        "	if (map->map_type != BPF_MAP_TYPE_ARENA || flags || !page_cnt)",
        "		return NULL;",
        "",
        "	return (void *)arena_alloc_pages(arena, (long)addr__ign, page_cnt, node_id);",
        "}",
        "",
        "__bpf_kfunc void bpf_arena_free_pages(void *p__map, void *ptr__ign, u32 page_cnt)",
        "{",
        "	struct bpf_map *map = p__map;",
        "	struct bpf_arena *arena = container_of(map, struct bpf_arena, map);",
        "",
        "	if (map->map_type != BPF_MAP_TYPE_ARENA || !page_cnt || !ptr__ign)",
        "		return;",
        "	arena_free_pages(arena, (long)ptr__ign, page_cnt);",
        "}",
        "__bpf_kfunc_end_defs();",
        "",
        "BTF_KFUNCS_START(arena_kfuncs)",
        "BTF_ID_FLAGS(func, bpf_arena_alloc_pages, KF_TRUSTED_ARGS | KF_SLEEPABLE)",
        "BTF_ID_FLAGS(func, bpf_arena_free_pages, KF_TRUSTED_ARGS | KF_SLEEPABLE)",
        "BTF_KFUNCS_END(arena_kfuncs)",
        "",
        "static const struct btf_kfunc_id_set common_kfunc_set = {",
        "	.owner = THIS_MODULE,",
        "	.set   = &arena_kfuncs,",
        "};",
        "",
        "static int __init kfunc_init(void)",
        "{",
        "	return register_btf_kfunc_id_set(BPF_PROG_TYPE_UNSPEC, &common_kfunc_set);",
        "}",
        "late_initcall(kfunc_init);"
    ]
  },
  "kernel_bpf_memalloc_c": {
    path: "kernel/bpf/memalloc.c",
    covered: [184, 141, 240, 545, 725, 640, 652, 628, 638, 510, 206, 198, 471, 642, 258, 474, 151, 665, 651, 750, 539, 650, 257, 268, 683, 641, 654, 639, 761, 682, 648, 249, 155, 546, 748, 643, 718, 655, 482, 649, 743, 653, 644, 497, 670, 269],
    totalLines: 1016,
    coveredCount: 46,
    coveragePct: 4.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/* Copyright (c) 2022 Meta Platforms, Inc. and affiliates. */",
        "#include <linux/mm.h>",
        "#include <linux/llist.h>",
        "#include <linux/bpf.h>",
        "#include <linux/irq_work.h>",
        "#include <linux/bpf_mem_alloc.h>",
        "#include <linux/memcontrol.h>",
        "#include <asm/local.h>",
        "",
        "/* Any context (including NMI) BPF specific memory allocator.",
        " *",
        " * Tracing BPF programs can attach to kprobe and fentry. Hence they",
        " * run in unknown context where calling plain kmalloc() might not be safe.",
        " *",
        " * Front-end kmalloc() with per-cpu per-bucket cache of free elements.",
        " * Refill this cache asynchronously from irq_work.",
        " *",
        " * CPU_0 buckets",
        " * 16 32 64 96 128 196 256 512 1024 2048 4096",
        " * ...",
        " * CPU_N buckets",
        " * 16 32 64 96 128 196 256 512 1024 2048 4096",
        " *",
        " * The buckets are prefilled at the start.",
        " * BPF programs always run with migration disabled.",
        " * It's safe to allocate from cache of the current cpu with irqs disabled.",
        " * Free-ing is always done into bucket of the current cpu as well.",
        " * irq_work trims extra free elements from buckets with kfree",
        " * and refills them with kmalloc, so global kmalloc logic takes care",
        " * of freeing objects allocated by one cpu and freed on another.",
        " *",
        " * Every allocated objected is padded with extra 8 bytes that contains",
        " * struct llist_node.",
        " */",
        "#define LLIST_NODE_SZ sizeof(struct llist_node)",
        "",
        "#define BPF_MEM_ALLOC_SIZE_MAX 4096",
        "",
        "/* similar to kmalloc, but sizeof == 8 bucket is gone */",
        "static u8 size_index[24] __ro_after_init = {",
        "	3,	/* 8 */",
        "	3,	/* 16 */",
        "	4,	/* 24 */",
        "	4,	/* 32 */",
        "	5,	/* 40 */",
        "	5,	/* 48 */",
        "	5,	/* 56 */",
        "	5,	/* 64 */",
        "	1,	/* 72 */",
        "	1,	/* 80 */",
        "	1,	/* 88 */",
        "	1,	/* 96 */",
        "	6,	/* 104 */",
        "	6,	/* 112 */",
        "	6,	/* 120 */",
        "	6,	/* 128 */",
        "	2,	/* 136 */",
        "	2,	/* 144 */",
        "	2,	/* 152 */",
        "	2,	/* 160 */",
        "	2,	/* 168 */",
        "	2,	/* 176 */",
        "	2,	/* 184 */",
        "	2	/* 192 */",
        "};",
        "",
        "static int bpf_mem_cache_idx(size_t size)",
        "{",
        "	if (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
        "		return -1;",
        "",
        "	if (size <= 192)",
        "		return size_index[(size - 1) / 8] - 1;",
        "",
        "	return fls(size - 1) - 2;",
        "}",
        "",
        "#define NUM_CACHES 11",
        "",
        "struct bpf_mem_cache {",
        "	/* per-cpu list of free objects of size 'unit_size'.",
        "	 * All accesses are done with interrupts disabled and 'active' counter",
        "	 * protection with __llist_add() and __llist_del_first().",
        "	 */",
        "	struct llist_head free_llist;",
        "	local_t active;",
        "",
        "	/* Operations on the free_list from unit_alloc/unit_free/bpf_mem_refill",
        "	 * are sequenced by per-cpu 'active' counter. But unit_free() cannot",
        "	 * fail. When 'active' is busy the unit_free() will add an object to",
        "	 * free_llist_extra.",
        "	 */",
        "	struct llist_head free_llist_extra;",
        "",
        "	struct irq_work refill_work;",
        "	struct obj_cgroup *objcg;",
        "	int unit_size;",
        "	/* count of objects in free_llist */",
        "	int free_cnt;",
        "	int low_watermark, high_watermark, batch;",
        "	int percpu_size;",
        "	bool draining;",
        "	struct bpf_mem_cache *tgt;",
        "",
        "	/* list of objects to be freed after RCU GP */",
        "	struct llist_head free_by_rcu;",
        "	struct llist_node *free_by_rcu_tail;",
        "	struct llist_head waiting_for_gp;",
        "	struct llist_node *waiting_for_gp_tail;",
        "	struct rcu_head rcu;",
        "	atomic_t call_rcu_in_progress;",
        "	struct llist_head free_llist_extra_rcu;",
        "",
        "	/* list of objects to be freed after RCU tasks trace GP */",
        "	struct llist_head free_by_rcu_ttrace;",
        "	struct llist_head waiting_for_gp_ttrace;",
        "	struct rcu_head rcu_ttrace;",
        "	atomic_t call_rcu_ttrace_in_progress;",
        "};",
        "",
        "struct bpf_mem_caches {",
        "	struct bpf_mem_cache cache[NUM_CACHES];",
        "};",
        "",
        "static const u16 sizes[NUM_CACHES] = {96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096};",
        "",
        "static struct llist_node notrace *__llist_del_first(struct llist_head *head)",
        "{",
        "	struct llist_node *entry, *next;",
        "",
        "	entry = head->first;",
        "	if (!entry)",
        "		return NULL;",
        "	next = entry->next;",
        "	head->first = next;",
        "	return entry;",
        "}",
        "",
        "static void *__alloc(struct bpf_mem_cache *c, int node, gfp_t flags)",
        "{",
        "	if (c->percpu_size) {",
        "		void __percpu **obj = kmalloc_node(c->percpu_size, flags, node);",
        "		void __percpu *pptr = __alloc_percpu_gfp(c->unit_size, 8, flags);",
        "",
        "		if (!obj || !pptr) {",
        "			free_percpu(pptr);",
        "			kfree(obj);",
        "			return NULL;",
        "		}",
        "		obj[1] = pptr;",
        "		return obj;",
        "	}",
        "",
        "	return kmalloc_node(c->unit_size, flags | __GFP_ZERO, node);",
        "}",
        "",
        "static struct mem_cgroup *get_memcg(const struct bpf_mem_cache *c)",
        "{",
        "#ifdef CONFIG_MEMCG",
        "	if (c->objcg)",
        "		return get_mem_cgroup_from_objcg(c->objcg);",
        "	return root_mem_cgroup;",
        "#else",
        "	return NULL;",
        "#endif",
        "}",
        "",
        "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
        "{",
        "	if (IS_ENABLED(CONFIG_PREEMPT_RT))",
        "		/* In RT irq_work runs in per-cpu kthread, so disable",
        "		 * interrupts to avoid preemption and interrupts and",
        "		 * reduce the chance of bpf prog executing on this cpu",
        "		 * when active counter is busy.",
        "		 */",
        "		local_irq_save(*flags);",
        "	/* alloc_bulk runs from irq_work which will not preempt a bpf",
        "	 * program that does unit_alloc/unit_free since IRQs are",
        "	 * disabled there. There is no race to increment 'active'",
        "	 * counter. It protects free_llist from corruption in case NMI",
        "	 * bpf prog preempted this loop.",
        "	 */",
        "	WARN_ON_ONCE(local_inc_return(&c->active) != 1);",
        "}",
        "",
        "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
        "{",
        "	local_dec(&c->active);",
        "	if (IS_ENABLED(CONFIG_PREEMPT_RT))",
        "		local_irq_restore(*flags);",
        "}",
        "",
        "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
        "{",
        "	unsigned long flags;",
        "",
        "	inc_active(c, &flags);",
        "	__llist_add(obj, &c->free_llist);",
        "	c->free_cnt++;",
        "	dec_active(c, &flags);",
        "}",
        "",
        "/* Mostly runs from irq_work except __init phase. */",
        "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
        "{",
        "	struct mem_cgroup *memcg = NULL, *old_memcg;",
        "	gfp_t gfp;",
        "	void *obj;",
        "	int i;",
        "",
        "	gfp = __GFP_NOWARN | __GFP_ACCOUNT;",
        "	gfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
        "",
        "	for (i = 0; i < cnt; i++) {",
        "		/*",
        "		 * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
        "		 * done only by one CPU == current CPU. Other CPUs might",
        "		 * llist_add() and llist_del_all() in parallel.",
        "		 */",
        "		obj = llist_del_first(&c->free_by_rcu_ttrace);",
        "		if (!obj)",
        "			break;",
        "		add_obj_to_free_list(c, obj);",
        "	}",
        "	if (i >= cnt)",
        "		return;",
        "",
        "	for (; i < cnt; i++) {",
        "		obj = llist_del_first(&c->waiting_for_gp_ttrace);",
        "		if (!obj)",
        "			break;",
        "		add_obj_to_free_list(c, obj);",
        "	}",
        "	if (i >= cnt)",
        "		return;",
        "",
        "	memcg = get_memcg(c);",
        "	old_memcg = set_active_memcg(memcg);",
        "	for (; i < cnt; i++) {",
        "		/* Allocate, but don't deplete atomic reserves that typical",
        "		 * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
        "		 * will allocate from the current numa node which is what we",
        "		 * want here.",
        "		 */",
        "		obj = __alloc(c, node, gfp);",
        "		if (!obj)",
        "			break;",
        "		add_obj_to_free_list(c, obj);",
        "	}",
        "	set_active_memcg(old_memcg);",
        "	mem_cgroup_put(memcg);",
        "}",
        "",
        "static void free_one(void *obj, bool percpu)",
        "{",
        "	if (percpu)",
        "		free_percpu(((void __percpu **)obj)[1]);",
        "",
        "	kfree(obj);",
        "}",
        "",
        "static int free_all(struct llist_node *llnode, bool percpu)",
        "{",
        "	struct llist_node *pos, *t;",
        "	int cnt = 0;",
        "",
        "	llist_for_each_safe(pos, t, llnode) {",
        "		free_one(pos, percpu);",
        "		cnt++;",
        "	}",
        "	return cnt;",
        "}",
        "",
        "static void __free_rcu(struct rcu_head *head)",
        "{",
        "	struct bpf_mem_cache *c = container_of(head, struct bpf_mem_cache, rcu_ttrace);",
        "",
        "	free_all(llist_del_all(&c->waiting_for_gp_ttrace), !!c->percpu_size);",
        "	atomic_set(&c->call_rcu_ttrace_in_progress, 0);",
        "}",
        "",
        "static void __free_rcu_tasks_trace(struct rcu_head *head)",
        "{",
        "	/* If RCU Tasks Trace grace period implies RCU grace period,",
        "	 * there is no need to invoke call_rcu().",
        "	 */",
        "	if (rcu_trace_implies_rcu_gp())",
        "		__free_rcu(head);",
        "	else",
        "		call_rcu(head, __free_rcu);",
        "}",
        "",
        "static void enque_to_free(struct bpf_mem_cache *c, void *obj)",
        "{",
        "	struct llist_node *llnode = obj;",
        "",
        "	/* bpf_mem_cache is a per-cpu object. Freeing happens in irq_work.",
        "	 * Nothing races to add to free_by_rcu_ttrace list.",
        "	 */",
        "	llist_add(llnode, &c->free_by_rcu_ttrace);",
        "}",
        "",
        "static void do_call_rcu_ttrace(struct bpf_mem_cache *c)",
        "{",
        "	struct llist_node *llnode, *t;",
        "",
        "	if (atomic_xchg(&c->call_rcu_ttrace_in_progress, 1)) {",
        "		if (unlikely(READ_ONCE(c->draining))) {",
        "			llnode = llist_del_all(&c->free_by_rcu_ttrace);",
        "			free_all(llnode, !!c->percpu_size);",
        "		}",
        "		return;",
        "	}",
        "",
        "	WARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));",
        "	llist_for_each_safe(llnode, t, llist_del_all(&c->free_by_rcu_ttrace))",
        "		llist_add(llnode, &c->waiting_for_gp_ttrace);",
        "",
        "	if (unlikely(READ_ONCE(c->draining))) {",
        "		__free_rcu(&c->rcu_ttrace);",
        "		return;",
        "	}",
        "",
        "	/* Use call_rcu_tasks_trace() to wait for sleepable progs to finish.",
        "	 * If RCU Tasks Trace grace period implies RCU grace period, free",
        "	 * these elements directly, else use call_rcu() to wait for normal",
        "	 * progs to finish and finally do free_one() on each element.",
        "	 */",
        "	call_rcu_tasks_trace(&c->rcu_ttrace, __free_rcu_tasks_trace);",
        "}",
        "",
        "static void free_bulk(struct bpf_mem_cache *c)",
        "{",
        "	struct bpf_mem_cache *tgt = c->tgt;",
        "	struct llist_node *llnode, *t;",
        "	unsigned long flags;",
        "	int cnt;",
        "",
        "	WARN_ON_ONCE(tgt->unit_size != c->unit_size);",
        "	WARN_ON_ONCE(tgt->percpu_size != c->percpu_size);",
        "",
        "	do {",
        "		inc_active(c, &flags);",
        "		llnode = __llist_del_first(&c->free_llist);",
        "		if (llnode)",
        "			cnt = --c->free_cnt;",
        "		else",
        "			cnt = 0;",
        "		dec_active(c, &flags);",
        "		if (llnode)",
        "			enque_to_free(tgt, llnode);",
        "	} while (cnt > (c->high_watermark + c->low_watermark) / 2);",
        "",
        "	/* and drain free_llist_extra */",
        "	llist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra))",
        "		enque_to_free(tgt, llnode);",
        "	do_call_rcu_ttrace(tgt);",
        "}",
        "",
        "static void __free_by_rcu(struct rcu_head *head)",
        "{",
        "	struct bpf_mem_cache *c = container_of(head, struct bpf_mem_cache, rcu);",
        "	struct bpf_mem_cache *tgt = c->tgt;",
        "	struct llist_node *llnode;",
        "",
        "	WARN_ON_ONCE(tgt->unit_size != c->unit_size);",
        "	WARN_ON_ONCE(tgt->percpu_size != c->percpu_size);",
        "",
        "	llnode = llist_del_all(&c->waiting_for_gp);",
        "	if (!llnode)",
        "		goto out;",
        "",
        "	llist_add_batch(llnode, c->waiting_for_gp_tail, &tgt->free_by_rcu_ttrace);",
        "",
        "	/* Objects went through regular RCU GP. Send them to RCU tasks trace */",
        "	do_call_rcu_ttrace(tgt);",
        "out:",
        "	atomic_set(&c->call_rcu_in_progress, 0);",
        "}",
        "",
        "static void check_free_by_rcu(struct bpf_mem_cache *c)",
        "{",
        "	struct llist_node *llnode, *t;",
        "	unsigned long flags;",
        "",
        "	/* drain free_llist_extra_rcu */",
        "	if (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {",
        "		inc_active(c, &flags);",
        "		llist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))",
        "			if (__llist_add(llnode, &c->free_by_rcu))",
        "				c->free_by_rcu_tail = llnode;",
        "		dec_active(c, &flags);",
        "	}",
        "",
        "	if (llist_empty(&c->free_by_rcu))",
        "		return;",
        "",
        "	if (atomic_xchg(&c->call_rcu_in_progress, 1)) {",
        "		/*",
        "		 * Instead of kmalloc-ing new rcu_head and triggering 10k",
        "		 * call_rcu() to hit rcutree.qhimark and force RCU to notice",
        "		 * the overload just ask RCU to hurry up. There could be many",
        "		 * objects in free_by_rcu list.",
        "		 * This hint reduces memory consumption for an artificial",
        "		 * benchmark from 2 Gbyte to 150 Mbyte.",
        "		 */",
        "		rcu_request_urgent_qs_task(current);",
        "		return;",
        "	}",
        "",
        "	WARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
        "",
        "	inc_active(c, &flags);",
        "	WRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));",
        "	c->waiting_for_gp_tail = c->free_by_rcu_tail;",
        "	dec_active(c, &flags);",
        "",
        "	if (unlikely(READ_ONCE(c->draining))) {",
        "		free_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);",
        "		atomic_set(&c->call_rcu_in_progress, 0);",
        "	} else {",
        "		call_rcu_hurry(&c->rcu, __free_by_rcu);",
        "	}",
        "}",
        "",
        "static void bpf_mem_refill(struct irq_work *work)",
        "{",
        "	struct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);",
        "	int cnt;",
        "",
        "	/* Racy access to free_cnt. It doesn't need to be 100% accurate */",
        "	cnt = c->free_cnt;",
        "	if (cnt < c->low_watermark)",
        "		/* irq_work runs on this cpu and kmalloc will allocate",
        "		 * from the current numa node which is what we want here.",
        "		 */",
        "		alloc_bulk(c, c->batch, NUMA_NO_NODE, true);",
        "	else if (cnt > c->high_watermark)",
        "		free_bulk(c);",
        "",
        "	check_free_by_rcu(c);",
        "}",
        "",
        "static void notrace irq_work_raise(struct bpf_mem_cache *c)",
        "{",
        "	irq_work_queue(&c->refill_work);",
        "}",
        "",
        "/* For typical bpf map case that uses bpf_mem_cache_alloc and single bucket",
        " * the freelist cache will be elem_size * 64 (or less) on each cpu.",
        " *",
        " * For bpf programs that don't have statically known allocation sizes and",
        " * assuming (low_mark + high_mark) / 2 as an average number of elements per",
        " * bucket and all buckets are used the total amount of memory in freelists",
        " * on each cpu will be:",
        " * 64*16 + 64*32 + 64*64 + 64*96 + 64*128 + 64*196 + 64*256 + 32*512 + 16*1024 + 8*2048 + 4*4096",
        " * == ~ 116 Kbyte using below heuristic.",
        " * Initialized, but unused bpf allocator (not bpf map specific one) will",
        " * consume ~ 11 Kbyte per cpu.",
        " * Typical case will be between 11K and 116K closer to 11K.",
        " * bpf progs can and should share bpf_mem_cache when possible.",
        " *",
        " * Percpu allocation is typically rare. To avoid potential unnecessary large",
        " * memory consumption, set low_mark = 1 and high_mark = 3, resulting in c->batch = 1.",
        " */",
        "static void init_refill_work(struct bpf_mem_cache *c)",
        "{",
        "	init_irq_work(&c->refill_work, bpf_mem_refill);",
        "	if (c->percpu_size) {",
        "		c->low_watermark = 1;",
        "		c->high_watermark = 3;",
        "	} else if (c->unit_size <= 256) {",
        "		c->low_watermark = 32;",
        "		c->high_watermark = 96;",
        "	} else {",
        "		/* When page_size == 4k, order-0 cache will have low_mark == 2",
        "		 * and high_mark == 6 with batch alloc of 3 individual pages at",
        "		 * a time.",
        "		 * 8k allocs and above low == 1, high == 3, batch == 1.",
        "		 */",
        "		c->low_watermark = max(32 * 256 / c->unit_size, 1);",
        "		c->high_watermark = max(96 * 256 / c->unit_size, 3);",
        "	}",
        "	c->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);",
        "}",
        "",
        "static void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)",
        "{",
        "	int cnt = 1;",
        "",
        "	/* To avoid consuming memory, for non-percpu allocation, assume that",
        "	 * 1st run of bpf prog won't be doing more than 4 map_update_elem from",
        "	 * irq disabled region if unit size is less than or equal to 256.",
        "	 * For all other cases, let us just do one allocation.",
        "	 */",
        "	if (!c->percpu_size && c->unit_size <= 256)",
        "		cnt = 4;",
        "	alloc_bulk(c, cnt, cpu_to_node(cpu), false);",
        "}",
        "",
        "/* When size != 0 bpf_mem_cache for each cpu.",
        " * This is typical bpf hash map use case when all elements have equal size.",
        " *",
        " * When size == 0 allocate 11 bpf_mem_cache-s for each cpu, then rely on",
        " * kmalloc/kfree. Max allocation size is 4096 in this case.",
        " * This is bpf_dynptr and bpf_kptr use case.",
        " */",
        "int bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)",
        "{",
        "	struct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
        "	struct bpf_mem_cache *c; struct bpf_mem_cache __percpu *pc;",
        "	struct obj_cgroup *objcg = NULL;",
        "	int cpu, i, unit_size, percpu_size = 0;",
        "",
        "	if (percpu && size == 0)",
        "		return -EINVAL;",
        "",
        "	/* room for llist_node and per-cpu pointer */",
        "	if (percpu)",
        "		percpu_size = LLIST_NODE_SZ + sizeof(void *);",
        "	ma->percpu = percpu;",
        "",
        "	if (size) {",
        "		pc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);",
        "		if (!pc)",
        "			return -ENOMEM;",
        "",
        "		if (!percpu)",
        "			size += LLIST_NODE_SZ; /* room for llist_node */",
        "		unit_size = size;",
        "",
        "#ifdef CONFIG_MEMCG",
        "		if (memcg_bpf_enabled())",
        "			objcg = get_obj_cgroup_from_current();",
        "#endif",
        "		ma->objcg = objcg;",
        "",
        "		for_each_possible_cpu(cpu) {",
        "			c = per_cpu_ptr(pc, cpu);",
        "			c->unit_size = unit_size;",
        "			c->objcg = objcg;",
        "			c->percpu_size = percpu_size;",
        "			c->tgt = c;",
        "			init_refill_work(c);",
        "			prefill_mem_cache(c, cpu);",
        "		}",
        "		ma->cache = pc;",
        "		return 0;",
        "	}",
        "",
        "	pcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);",
        "	if (!pcc)",
        "		return -ENOMEM;",
        "#ifdef CONFIG_MEMCG",
        "	objcg = get_obj_cgroup_from_current();",
        "#endif",
        "	ma->objcg = objcg;",
        "	for_each_possible_cpu(cpu) {",
        "		cc = per_cpu_ptr(pcc, cpu);",
        "		for (i = 0; i < NUM_CACHES; i++) {",
        "			c = &cc->cache[i];",
        "			c->unit_size = sizes[i];",
        "			c->objcg = objcg;",
        "			c->percpu_size = percpu_size;",
        "			c->tgt = c;",
        "",
        "			init_refill_work(c);",
        "			prefill_mem_cache(c, cpu);",
        "		}",
        "	}",
        "",
        "	ma->caches = pcc;",
        "	return 0;",
        "}",
        "",
        "int bpf_mem_alloc_percpu_init(struct bpf_mem_alloc *ma, struct obj_cgroup *objcg)",
        "{",
        "	struct bpf_mem_caches __percpu *pcc;",
        "",
        "	pcc = __alloc_percpu_gfp(sizeof(struct bpf_mem_caches), 8, GFP_KERNEL);",
        "	if (!pcc)",
        "		return -ENOMEM;",
        "",
        "	ma->caches = pcc;",
        "	ma->objcg = objcg;",
        "	ma->percpu = true;",
        "	return 0;",
        "}",
        "",
        "int bpf_mem_alloc_percpu_unit_init(struct bpf_mem_alloc *ma, int size)",
        "{",
        "	struct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
        "	int cpu, i, unit_size, percpu_size;",
        "	struct obj_cgroup *objcg;",
        "	struct bpf_mem_cache *c;",
        "",
        "	i = bpf_mem_cache_idx(size);",
        "	if (i < 0)",
        "		return -EINVAL;",
        "",
        "	/* room for llist_node and per-cpu pointer */",
        "	percpu_size = LLIST_NODE_SZ + sizeof(void *);",
        "",
        "	unit_size = sizes[i];",
        "	objcg = ma->objcg;",
        "	pcc = ma->caches;",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		cc = per_cpu_ptr(pcc, cpu);",
        "		c = &cc->cache[i];",
        "		if (c->unit_size)",
        "			break;",
        "",
        "		c->unit_size = unit_size;",
        "		c->objcg = objcg;",
        "		c->percpu_size = percpu_size;",
        "		c->tgt = c;",
        "",
        "		init_refill_work(c);",
        "		prefill_mem_cache(c, cpu);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static void drain_mem_cache(struct bpf_mem_cache *c)",
        "{",
        "	bool percpu = !!c->percpu_size;",
        "",
        "	/* No progs are using this bpf_mem_cache, but htab_map_free() called",
        "	 * bpf_mem_cache_free() for all remaining elements and they can be in",
        "	 * free_by_rcu_ttrace or in waiting_for_gp_ttrace lists, so drain those lists now.",
        "	 *",
        "	 * Except for waiting_for_gp_ttrace list, there are no concurrent operations",
        "	 * on these lists, so it is safe to use __llist_del_all().",
        "	 */",
        "	free_all(llist_del_all(&c->free_by_rcu_ttrace), percpu);",
        "	free_all(llist_del_all(&c->waiting_for_gp_ttrace), percpu);",
        "	free_all(__llist_del_all(&c->free_llist), percpu);",
        "	free_all(__llist_del_all(&c->free_llist_extra), percpu);",
        "	free_all(__llist_del_all(&c->free_by_rcu), percpu);",
        "	free_all(__llist_del_all(&c->free_llist_extra_rcu), percpu);",
        "	free_all(llist_del_all(&c->waiting_for_gp), percpu);",
        "}",
        "",
        "static void check_mem_cache(struct bpf_mem_cache *c)",
        "{",
        "	WARN_ON_ONCE(!llist_empty(&c->free_by_rcu_ttrace));",
        "	WARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));",
        "	WARN_ON_ONCE(!llist_empty(&c->free_llist));",
        "	WARN_ON_ONCE(!llist_empty(&c->free_llist_extra));",
        "	WARN_ON_ONCE(!llist_empty(&c->free_by_rcu));",
        "	WARN_ON_ONCE(!llist_empty(&c->free_llist_extra_rcu));",
        "	WARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
        "}",
        "",
        "static void check_leaked_objs(struct bpf_mem_alloc *ma)",
        "{",
        "	struct bpf_mem_caches *cc;",
        "	struct bpf_mem_cache *c;",
        "	int cpu, i;",
        "",
        "	if (ma->cache) {",
        "		for_each_possible_cpu(cpu) {",
        "			c = per_cpu_ptr(ma->cache, cpu);",
        "			check_mem_cache(c);",
        "		}",
        "	}",
        "	if (ma->caches) {",
        "		for_each_possible_cpu(cpu) {",
        "			cc = per_cpu_ptr(ma->caches, cpu);",
        "			for (i = 0; i < NUM_CACHES; i++) {",
        "				c = &cc->cache[i];",
        "				check_mem_cache(c);",
        "			}",
        "		}",
        "	}",
        "}",
        "",
        "static void free_mem_alloc_no_barrier(struct bpf_mem_alloc *ma)",
        "{",
        "	check_leaked_objs(ma);",
        "	free_percpu(ma->cache);",
        "	free_percpu(ma->caches);",
        "	ma->cache = NULL;",
        "	ma->caches = NULL;",
        "}",
        "",
        "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
        "{",
        "	/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
        "	 * might still execute. Wait for them.",
        "	 *",
        "	 * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
        "	 * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
        "	 * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
        "	 * so if call_rcu(head, __free_rcu) is skipped due to",
        "	 * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
        "	 * using rcu_trace_implies_rcu_gp() as well.",
        "	 */",
        "	rcu_barrier(); /* wait for __free_by_rcu */",
        "	rcu_barrier_tasks_trace(); /* wait for __free_rcu */",
        "	if (!rcu_trace_implies_rcu_gp())",
        "		rcu_barrier();",
        "	free_mem_alloc_no_barrier(ma);",
        "}",
        "",
        "static void free_mem_alloc_deferred(struct work_struct *work)",
        "{",
        "	struct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
        "",
        "	free_mem_alloc(ma);",
        "	kfree(ma);",
        "}",
        "",
        "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
        "{",
        "	struct bpf_mem_alloc *copy;",
        "",
        "	if (!rcu_in_progress) {",
        "		/* Fast path. No callbacks are pending, hence no need to do",
        "		 * rcu_barrier-s.",
        "		 */",
        "		free_mem_alloc_no_barrier(ma);",
        "		return;",
        "	}",
        "",
        "	copy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
        "	if (!copy) {",
        "		/* Slow path with inline barrier-s */",
        "		free_mem_alloc(ma);",
        "		return;",
        "	}",
        "",
        "	/* Defer barriers into worker to let the rest of map memory to be freed */",
        "	memset(ma, 0, sizeof(*ma));",
        "	INIT_WORK(&copy->work, free_mem_alloc_deferred);",
        "	queue_work(system_unbound_wq, &copy->work);",
        "}",
        "",
        "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
        "{",
        "	struct bpf_mem_caches *cc;",
        "	struct bpf_mem_cache *c;",
        "	int cpu, i, rcu_in_progress;",
        "",
        "	if (ma->cache) {",
        "		rcu_in_progress = 0;",
        "		for_each_possible_cpu(cpu) {",
        "			c = per_cpu_ptr(ma->cache, cpu);",
        "			WRITE_ONCE(c->draining, true);",
        "			irq_work_sync(&c->refill_work);",
        "			drain_mem_cache(c);",
        "			rcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
        "			rcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
        "		}",
        "		obj_cgroup_put(ma->objcg);",
        "		destroy_mem_alloc(ma, rcu_in_progress);",
        "	}",
        "	if (ma->caches) {",
        "		rcu_in_progress = 0;",
        "		for_each_possible_cpu(cpu) {",
        "			cc = per_cpu_ptr(ma->caches, cpu);",
        "			for (i = 0; i < NUM_CACHES; i++) {",
        "				c = &cc->cache[i];",
        "				WRITE_ONCE(c->draining, true);",
        "				irq_work_sync(&c->refill_work);",
        "				drain_mem_cache(c);",
        "				rcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
        "				rcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
        "			}",
        "		}",
        "		obj_cgroup_put(ma->objcg);",
        "		destroy_mem_alloc(ma, rcu_in_progress);",
        "	}",
        "}",
        "",
        "/* notrace is necessary here and in other functions to make sure",
        " * bpf programs cannot attach to them and cause llist corruptions.",
        " */",
        "static void notrace *unit_alloc(struct bpf_mem_cache *c)",
        "{",
        "	struct llist_node *llnode = NULL;",
        "	unsigned long flags;",
        "	int cnt = 0;",
        "",
        "	/* Disable irqs to prevent the following race for majority of prog types:",
        "	 * prog_A",
        "	 *   bpf_mem_alloc",
        "	 *      preemption or irq -> prog_B",
        "	 *        bpf_mem_alloc",
        "	 *",
        "	 * but prog_B could be a perf_event NMI prog.",
        "	 * Use per-cpu 'active' counter to order free_list access between",
        "	 * unit_alloc/unit_free/bpf_mem_refill.",
        "	 */",
        "	local_irq_save(flags);",
        "	if (local_inc_return(&c->active) == 1) {",
        "		llnode = __llist_del_first(&c->free_llist);",
        "		if (llnode) {",
        "			cnt = --c->free_cnt;",
        "			*(struct bpf_mem_cache **)llnode = c;",
        "		}",
        "	}",
        "	local_dec(&c->active);",
        "",
        "	WARN_ON(cnt < 0);",
        "",
        "	if (cnt < c->low_watermark)",
        "		irq_work_raise(c);",
        "	/* Enable IRQ after the enqueue of irq work completes, so irq work",
        "	 * will run after IRQ is enabled and free_llist may be refilled by",
        "	 * irq work before other task preempts current task.",
        "	 */",
        "	local_irq_restore(flags);",
        "",
        "	return llnode;",
        "}",
        "",
        "/* Though 'ptr' object could have been allocated on a different cpu",
        " * add it to the free_llist of the current cpu.",
        " * Let kfree() logic deal with it when it's later called from irq_work.",
        " */",
        "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
        "{",
        "	struct llist_node *llnode = ptr - LLIST_NODE_SZ;",
        "	unsigned long flags;",
        "	int cnt = 0;",
        "",
        "	BUILD_BUG_ON(LLIST_NODE_SZ > 8);",
        "",
        "	/*",
        "	 * Remember bpf_mem_cache that allocated this object.",
        "	 * The hint is not accurate.",
        "	 */",
        "	c->tgt = *(struct bpf_mem_cache **)llnode;",
        "",
        "	local_irq_save(flags);",
        "	if (local_inc_return(&c->active) == 1) {",
        "		__llist_add(llnode, &c->free_llist);",
        "		cnt = ++c->free_cnt;",
        "	} else {",
        "		/* unit_free() cannot fail. Therefore add an object to atomic",
        "		 * llist. free_bulk() will drain it. Though free_llist_extra is",
        "		 * a per-cpu list we have to use atomic llist_add here, since",
        "		 * it also can be interrupted by bpf nmi prog that does another",
        "		 * unit_free() into the same free_llist_extra.",
        "		 */",
        "		llist_add(llnode, &c->free_llist_extra);",
        "	}",
        "	local_dec(&c->active);",
        "",
        "	if (cnt > c->high_watermark)",
        "		/* free few objects from current cpu into global kmalloc pool */",
        "		irq_work_raise(c);",
        "	/* Enable IRQ after irq_work_raise() completes, otherwise when current",
        "	 * task is preempted by task which does unit_alloc(), unit_alloc() may",
        "	 * return NULL unexpectedly because irq work is already pending but can",
        "	 * not been triggered and free_llist can not be refilled timely.",
        "	 */",
        "	local_irq_restore(flags);",
        "}",
        "",
        "static void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)",
        "{",
        "	struct llist_node *llnode = ptr - LLIST_NODE_SZ;",
        "	unsigned long flags;",
        "",
        "	c->tgt = *(struct bpf_mem_cache **)llnode;",
        "",
        "	local_irq_save(flags);",
        "	if (local_inc_return(&c->active) == 1) {",
        "		if (__llist_add(llnode, &c->free_by_rcu))",
        "			c->free_by_rcu_tail = llnode;",
        "	} else {",
        "		llist_add(llnode, &c->free_llist_extra_rcu);",
        "	}",
        "	local_dec(&c->active);",
        "",
        "	if (!atomic_read(&c->call_rcu_in_progress))",
        "		irq_work_raise(c);",
        "	local_irq_restore(flags);",
        "}",
        "",
        "/* Called from BPF program or from sys_bpf syscall.",
        " * In both cases migration is disabled.",
        " */",
        "void notrace *bpf_mem_alloc(struct bpf_mem_alloc *ma, size_t size)",
        "{",
        "	int idx;",
        "	void *ret;",
        "",
        "	if (!size)",
        "		return NULL;",
        "",
        "	if (!ma->percpu)",
        "		size += LLIST_NODE_SZ;",
        "	idx = bpf_mem_cache_idx(size);",
        "	if (idx < 0)",
        "		return NULL;",
        "",
        "	ret = unit_alloc(this_cpu_ptr(ma->caches)->cache + idx);",
        "	return !ret ? NULL : ret + LLIST_NODE_SZ;",
        "}",
        "",
        "void notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)",
        "{",
        "	struct bpf_mem_cache *c;",
        "	int idx;",
        "",
        "	if (!ptr)",
        "		return;",
        "",
        "	c = *(void **)(ptr - LLIST_NODE_SZ);",
        "	idx = bpf_mem_cache_idx(c->unit_size);",
        "	if (WARN_ON_ONCE(idx < 0))",
        "		return;",
        "",
        "	unit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
        "}",
        "",
        "void notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
        "{",
        "	struct bpf_mem_cache *c;",
        "	int idx;",
        "",
        "	if (!ptr)",
        "		return;",
        "",
        "	c = *(void **)(ptr - LLIST_NODE_SZ);",
        "	idx = bpf_mem_cache_idx(c->unit_size);",
        "	if (WARN_ON_ONCE(idx < 0))",
        "		return;",
        "",
        "	unit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
        "}",
        "",
        "void notrace *bpf_mem_cache_alloc(struct bpf_mem_alloc *ma)",
        "{",
        "	void *ret;",
        "",
        "	ret = unit_alloc(this_cpu_ptr(ma->cache));",
        "	return !ret ? NULL : ret + LLIST_NODE_SZ;",
        "}",
        "",
        "void notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)",
        "{",
        "	if (!ptr)",
        "		return;",
        "",
        "	unit_free(this_cpu_ptr(ma->cache), ptr);",
        "}",
        "",
        "void notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
        "{",
        "	if (!ptr)",
        "		return;",
        "",
        "	unit_free_rcu(this_cpu_ptr(ma->cache), ptr);",
        "}",
        "",
        "/* Directly does a kfree() without putting 'ptr' back to the free_llist",
        " * for reuse and without waiting for a rcu_tasks_trace gp.",
        " * The caller must first go through the rcu_tasks_trace gp for 'ptr'",
        " * before calling bpf_mem_cache_raw_free().",
        " * It could be used when the rcu_tasks_trace callback does not have",
        " * a hold on the original bpf_mem_alloc object that allocated the",
        " * 'ptr'. This should only be used in the uncommon code path.",
        " * Otherwise, the bpf_mem_alloc's free_llist cannot be refilled",
        " * and may affect performance.",
        " */",
        "void bpf_mem_cache_raw_free(void *ptr)",
        "{",
        "	if (!ptr)",
        "		return;",
        "",
        "	kfree(ptr - LLIST_NODE_SZ);",
        "}",
        "",
        "/* When flags == GFP_KERNEL, it signals that the caller will not cause",
        " * deadlock when using kmalloc. bpf_mem_cache_alloc_flags() will use",
        " * kmalloc if the free_llist is empty.",
        " */",
        "void notrace *bpf_mem_cache_alloc_flags(struct bpf_mem_alloc *ma, gfp_t flags)",
        "{",
        "	struct bpf_mem_cache *c;",
        "	void *ret;",
        "",
        "	c = this_cpu_ptr(ma->cache);",
        "",
        "	ret = unit_alloc(c);",
        "	if (!ret && flags == GFP_KERNEL) {",
        "		struct mem_cgroup *memcg, *old_memcg;",
        "",
        "		memcg = get_memcg(c);",
        "		old_memcg = set_active_memcg(memcg);",
        "		ret = __alloc(c, NUMA_NO_NODE, GFP_KERNEL | __GFP_NOWARN | __GFP_ACCOUNT);",
        "		if (ret)",
        "			*(struct bpf_mem_cache **)ret = c;",
        "		set_active_memcg(old_memcg);",
        "		mem_cgroup_put(memcg);",
        "	}",
        "",
        "	return !ret ? NULL : ret + LLIST_NODE_SZ;",
        "}",
        "",
        "int bpf_mem_alloc_check_size(bool percpu, size_t size)",
        "{",
        "	/* The size of percpu allocation doesn't have LLIST_NODE_SZ overhead */",
        "	if ((percpu && size > BPF_MEM_ALLOC_SIZE_MAX) ||",
        "	    (!percpu && size > BPF_MEM_ALLOC_SIZE_MAX - LLIST_NODE_SZ))",
        "		return -E2BIG;",
        "",
        "	return 0;",
        "}"
    ]
  },
  "include_linux_find_h": {
    path: "include/linux/find.h",
    covered: [438, 457, 343, 66, 62, 96, 455, 207],
    totalLines: 697,
    coveredCount: 8,
    coveragePct: 1.1,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef __LINUX_FIND_H_",
        "#define __LINUX_FIND_H_",
        "",
        "#ifndef __LINUX_BITMAP_H",
        "#error only <linux/bitmap.h> can be included directly",
        "#endif",
        "",
        "#include <linux/bitops.h>",
        "",
        "unsigned long _find_next_bit(const unsigned long *addr1, unsigned long nbits,",
        "				unsigned long start);",
        "unsigned long _find_next_and_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "					unsigned long nbits, unsigned long start);",
        "unsigned long _find_next_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "					unsigned long nbits, unsigned long start);",
        "unsigned long _find_next_or_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "					unsigned long nbits, unsigned long start);",
        "unsigned long _find_next_zero_bit(const unsigned long *addr, unsigned long nbits,",
        "					 unsigned long start);",
        "extern unsigned long _find_first_bit(const unsigned long *addr, unsigned long size);",
        "unsigned long __find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n);",
        "unsigned long __find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "				unsigned long size, unsigned long n);",
        "unsigned long __find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "					unsigned long size, unsigned long n);",
        "unsigned long __find_nth_and_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "					const unsigned long *addr3, unsigned long size,",
        "					unsigned long n);",
        "extern unsigned long _find_first_and_bit(const unsigned long *addr1,",
        "					 const unsigned long *addr2, unsigned long size);",
        "unsigned long _find_first_and_and_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "				      const unsigned long *addr3, unsigned long size);",
        "extern unsigned long _find_first_zero_bit(const unsigned long *addr, unsigned long size);",
        "extern unsigned long _find_last_bit(const unsigned long *addr, unsigned long size);",
        "",
        "#ifdef __BIG_ENDIAN",
        "unsigned long _find_first_zero_bit_le(const unsigned long *addr, unsigned long size);",
        "unsigned long _find_next_zero_bit_le(const  unsigned long *addr, unsigned",
        "					long size, unsigned long offset);",
        "unsigned long _find_next_bit_le(const unsigned long *addr, unsigned",
        "				long size, unsigned long offset);",
        "#endif",
        "",
        "#ifndef find_next_bit",
        "/**",
        " * find_next_bit - find the next set bit in a memory region",
        " * @addr: The address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_bit(const unsigned long *addr, unsigned long size,",
        "			    unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = *addr & GENMASK(size - 1, offset);",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_next_bit(addr, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_next_and_bit",
        "/**",
        " * find_next_and_bit - find the next set bit in both memory regions",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_and_bit(const unsigned long *addr1,",
        "		const unsigned long *addr2, unsigned long size,",
        "		unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = *addr1 & *addr2 & GENMASK(size - 1, offset);",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_next_and_bit(addr1, addr2, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_next_andnot_bit",
        "/**",
        " * find_next_andnot_bit - find the next set bit in *addr1 excluding all the bits",
        " *                        in *addr2",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_andnot_bit(const unsigned long *addr1,",
        "		const unsigned long *addr2, unsigned long size,",
        "		unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = *addr1 & ~*addr2 & GENMASK(size - 1, offset);",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_next_andnot_bit(addr1, addr2, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_next_or_bit",
        "/**",
        " * find_next_or_bit - find the next set bit in either memory regions",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_or_bit(const unsigned long *addr1,",
        "		const unsigned long *addr2, unsigned long size,",
        "		unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = (*addr1 | *addr2) & GENMASK(size - 1, offset);",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_next_or_bit(addr1, addr2, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_next_zero_bit",
        "/**",
        " * find_next_zero_bit - find the next cleared bit in a memory region",
        " * @addr: The address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number of the next zero bit",
        " * If no bits are zero, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,",
        "				 unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = *addr | ~GENMASK(size - 1, offset);",
        "		return val == ~0UL ? size : ffz(val);",
        "	}",
        "",
        "	return _find_next_zero_bit(addr, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_first_bit",
        "/**",
        " * find_first_bit - find the first set bit in a memory region",
        " * @addr: The address to start the search at",
        " * @size: The maximum number of bits to search",
        " *",
        " * Returns the bit number of the first set bit.",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_first_bit(const unsigned long *addr, unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *addr & GENMASK(size - 1, 0);",
        "",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_first_bit(addr, size);",
        "}",
        "#endif",
        "",
        "/**",
        " * find_nth_bit - find N'th set bit in a memory region",
        " * @addr: The address to start the search at",
        " * @size: The maximum number of bits to search",
        " * @n: The number of set bit, which position is needed, counting from 0",
        " *",
        " * The following is semantically equivalent:",
        " *	 idx = find_nth_bit(addr, size, 0);",
        " *	 idx = find_first_bit(addr, size);",
        " *",
        " * Returns the bit number of the N'th set bit.",
        " * If no such, returns >= @size.",
        " */",
        "static __always_inline",
        "unsigned long find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n)",
        "{",
        "	if (n >= size)",
        "		return size;",
        "",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val =  *addr & GENMASK(size - 1, 0);",
        "",
        "		return val ? fns(val, n) : size;",
        "	}",
        "",
        "	return __find_nth_bit(addr, size, n);",
        "}",
        "",
        "/**",
        " * find_nth_and_bit - find N'th set bit in 2 memory regions",
        " * @addr1: The 1st address to start the search at",
        " * @addr2: The 2nd address to start the search at",
        " * @size: The maximum number of bits to search",
        " * @n: The number of set bit, which position is needed, counting from 0",
        " *",
        " * Returns the bit number of the N'th set bit.",
        " * If no such, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "				unsigned long size, unsigned long n)",
        "{",
        "	if (n >= size)",
        "		return size;",
        "",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val =  *addr1 & *addr2 & GENMASK(size - 1, 0);",
        "",
        "		return val ? fns(val, n) : size;",
        "	}",
        "",
        "	return __find_nth_and_bit(addr1, addr2, size, n);",
        "}",
        "",
        "/**",
        " * find_nth_andnot_bit - find N'th set bit in 2 memory regions,",
        " *			 flipping bits in 2nd region",
        " * @addr1: The 1st address to start the search at",
        " * @addr2: The 2nd address to start the search at",
        " * @size: The maximum number of bits to search",
        " * @n: The number of set bit, which position is needed, counting from 0",
        " *",
        " * Returns the bit number of the N'th set bit.",
        " * If no such, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,",
        "				unsigned long size, unsigned long n)",
        "{",
        "	if (n >= size)",
        "		return size;",
        "",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val =  *addr1 & (~*addr2) & GENMASK(size - 1, 0);",
        "",
        "		return val ? fns(val, n) : size;",
        "	}",
        "",
        "	return __find_nth_andnot_bit(addr1, addr2, size, n);",
        "}",
        "",
        "/**",
        " * find_nth_and_andnot_bit - find N'th set bit in 2 memory regions,",
        " *			     excluding those set in 3rd region",
        " * @addr1: The 1st address to start the search at",
        " * @addr2: The 2nd address to start the search at",
        " * @addr3: The 3rd address to start the search at",
        " * @size: The maximum number of bits to search",
        " * @n: The number of set bit, which position is needed, counting from 0",
        " *",
        " * Returns the bit number of the N'th set bit.",
        " * If no such, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_nth_and_andnot_bit(const unsigned long *addr1,",
        "					const unsigned long *addr2,",
        "					const unsigned long *addr3,",
        "					unsigned long size, unsigned long n)",
        "{",
        "	if (n >= size)",
        "		return size;",
        "",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val =  *addr1 & *addr2 & (~*addr3) & GENMASK(size - 1, 0);",
        "",
        "		return val ? fns(val, n) : size;",
        "	}",
        "",
        "	return __find_nth_and_andnot_bit(addr1, addr2, addr3, size, n);",
        "}",
        "",
        "#ifndef find_first_and_bit",
        "/**",
        " * find_first_and_bit - find the first set bit in both memory regions",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @size: The bitmap size in bits",
        " *",
        " * Returns the bit number for the next set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_first_and_bit(const unsigned long *addr1,",
        "				 const unsigned long *addr2,",
        "				 unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *addr1 & *addr2 & GENMASK(size - 1, 0);",
        "",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_first_and_bit(addr1, addr2, size);",
        "}",
        "#endif",
        "",
        "/**",
        " * find_first_and_and_bit - find the first set bit in 3 memory regions",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @addr3: The third address to base the search on",
        " * @size: The bitmap size in bits",
        " *",
        " * Returns the bit number for the first set bit",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_first_and_and_bit(const unsigned long *addr1,",
        "				     const unsigned long *addr2,",
        "				     const unsigned long *addr3,",
        "				     unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *addr1 & *addr2 & *addr3 & GENMASK(size - 1, 0);",
        "",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_first_and_and_bit(addr1, addr2, addr3, size);",
        "}",
        "",
        "#ifndef find_first_zero_bit",
        "/**",
        " * find_first_zero_bit - find the first cleared bit in a memory region",
        " * @addr: The address to start the search at",
        " * @size: The maximum number of bits to search",
        " *",
        " * Returns the bit number of the first cleared bit.",
        " * If no bits are zero, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_first_zero_bit(const unsigned long *addr, unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *addr | ~GENMASK(size - 1, 0);",
        "",
        "		return val == ~0UL ? size : ffz(val);",
        "	}",
        "",
        "	return _find_first_zero_bit(addr, size);",
        "}",
        "#endif",
        "",
        "#ifndef find_last_bit",
        "/**",
        " * find_last_bit - find the last set bit in a memory region",
        " * @addr: The address to start the search at",
        " * @size: The number of bits to search",
        " *",
        " * Returns the bit number of the last set bit, or size.",
        " */",
        "static __always_inline",
        "unsigned long find_last_bit(const unsigned long *addr, unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *addr & GENMASK(size - 1, 0);",
        "",
        "		return val ? __fls(val) : size;",
        "	}",
        "",
        "	return _find_last_bit(addr, size);",
        "}",
        "#endif",
        "",
        "/**",
        " * find_next_and_bit_wrap - find the next set bit in both memory regions",
        " * @addr1: The first address to base the search on",
        " * @addr2: The second address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit, or first set bit up to @offset",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_and_bit_wrap(const unsigned long *addr1,",
        "					const unsigned long *addr2,",
        "					unsigned long size, unsigned long offset)",
        "{",
        "	unsigned long bit = find_next_and_bit(addr1, addr2, size, offset);",
        "",
        "	if (bit < size || offset == 0)",
        "		return bit;",
        "",
        "	bit = find_first_and_bit(addr1, addr2, offset);",
        "	return bit < offset ? bit : size;",
        "}",
        "",
        "/**",
        " * find_next_bit_wrap - find the next set bit in a memory region",
        " * @addr: The address to base the search on",
        " * @size: The bitmap size in bits",
        " * @offset: The bitnumber to start searching at",
        " *",
        " * Returns the bit number for the next set bit, or first set bit up to @offset",
        " * If no bits are set, returns @size.",
        " */",
        "static __always_inline",
        "unsigned long find_next_bit_wrap(const unsigned long *addr,",
        "					unsigned long size, unsigned long offset)",
        "{",
        "	unsigned long bit = find_next_bit(addr, size, offset);",
        "",
        "	if (bit < size || offset == 0)",
        "		return bit;",
        "",
        "	bit = find_first_bit(addr, offset);",
        "	return bit < offset ? bit : size;",
        "}",
        "",
        "/*",
        " * Helper for for_each_set_bit_wrap(). Make sure you're doing right thing",
        " * before using it alone.",
        " */",
        "static __always_inline",
        "unsigned long __for_each_wrap(const unsigned long *bitmap, unsigned long size,",
        "				 unsigned long start, unsigned long n)",
        "{",
        "	unsigned long bit;",
        "",
        "	/* If not wrapped around */",
        "	if (n > start) {",
        "		/* and have a bit, just return it. */",
        "		bit = find_next_bit(bitmap, size, n);",
        "		if (bit < size)",
        "			return bit;",
        "",
        "		/* Otherwise, wrap around and ... */",
        "		n = 0;",
        "	}",
        "",
        "	/* Search the other part. */",
        "	bit = find_next_bit(bitmap, start, n);",
        "	return bit < start ? bit : size;",
        "}",
        "",
        "/**",
        " * find_next_clump8 - find next 8-bit clump with set bits in a memory region",
        " * @clump: location to store copy of found clump",
        " * @addr: address to base the search on",
        " * @size: bitmap size in number of bits",
        " * @offset: bit offset at which to start searching",
        " *",
        " * Returns the bit offset for the next set clump; the found clump value is",
        " * copied to the location pointed by @clump. If no bits are set, returns @size.",
        " */",
        "extern unsigned long find_next_clump8(unsigned long *clump,",
        "				      const unsigned long *addr,",
        "				      unsigned long size, unsigned long offset);",
        "",
        "#define find_first_clump8(clump, bits, size) \\",
        "	find_next_clump8((clump), (bits), (size), 0)",
        "",
        "#if defined(__LITTLE_ENDIAN)",
        "",
        "static __always_inline",
        "unsigned long find_next_zero_bit_le(const void *addr, unsigned long size, unsigned long offset)",
        "{",
        "	return find_next_zero_bit(addr, size, offset);",
        "}",
        "",
        "static __always_inline",
        "unsigned long find_next_bit_le(const void *addr, unsigned long size, unsigned long offset)",
        "{",
        "	return find_next_bit(addr, size, offset);",
        "}",
        "",
        "static __always_inline",
        "unsigned long find_first_zero_bit_le(const void *addr, unsigned long size)",
        "{",
        "	return find_first_zero_bit(addr, size);",
        "}",
        "",
        "#elif defined(__BIG_ENDIAN)",
        "",
        "#ifndef find_next_zero_bit_le",
        "static __always_inline",
        "unsigned long find_next_zero_bit_le(const void *addr, unsigned",
        "		long size, unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *(const unsigned long *)addr;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = swab(val) | ~GENMASK(size - 1, offset);",
        "		return val == ~0UL ? size : ffz(val);",
        "	}",
        "",
        "	return _find_next_zero_bit_le(addr, size, offset);",
        "}",
        "#endif",
        "",
        "#ifndef find_first_zero_bit_le",
        "static __always_inline",
        "unsigned long find_first_zero_bit_le(const void *addr, unsigned long size)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = swab(*(const unsigned long *)addr) | ~GENMASK(size - 1, 0);",
        "",
        "		return val == ~0UL ? size : ffz(val);",
        "	}",
        "",
        "	return _find_first_zero_bit_le(addr, size);",
        "}",
        "#endif",
        "",
        "#ifndef find_next_bit_le",
        "static __always_inline",
        "unsigned long find_next_bit_le(const void *addr, unsigned",
        "		long size, unsigned long offset)",
        "{",
        "	if (small_const_nbits(size)) {",
        "		unsigned long val = *(const unsigned long *)addr;",
        "",
        "		if (unlikely(offset >= size))",
        "			return size;",
        "",
        "		val = swab(val) & GENMASK(size - 1, offset);",
        "		return val ? __ffs(val) : size;",
        "	}",
        "",
        "	return _find_next_bit_le(addr, size, offset);",
        "}",
        "#endif",
        "",
        "#else",
        "#error \"Please fix <asm/byteorder.h>\"",
        "#endif",
        "",
        "#define for_each_set_bit(bit, addr, size) \\",
        "	for ((bit) = 0; (bit) = find_next_bit((addr), (size), (bit)), (bit) < (size); (bit)++)",
        "",
        "#define for_each_and_bit(bit, addr1, addr2, size) \\",
        "	for ((bit) = 0;									\\",
        "	     (bit) = find_next_and_bit((addr1), (addr2), (size), (bit)), (bit) < (size);\\",
        "	     (bit)++)",
        "",
        "#define for_each_andnot_bit(bit, addr1, addr2, size) \\",
        "	for ((bit) = 0;									\\",
        "	     (bit) = find_next_andnot_bit((addr1), (addr2), (size), (bit)), (bit) < (size);\\",
        "	     (bit)++)",
        "",
        "#define for_each_or_bit(bit, addr1, addr2, size) \\",
        "	for ((bit) = 0;									\\",
        "	     (bit) = find_next_or_bit((addr1), (addr2), (size), (bit)), (bit) < (size);\\",
        "	     (bit)++)",
        "",
        "/* same as for_each_set_bit() but use bit as value to start with */",
        "#define for_each_set_bit_from(bit, addr, size) \\",
        "	for (; (bit) = find_next_bit((addr), (size), (bit)), (bit) < (size); (bit)++)",
        "",
        "#define for_each_clear_bit(bit, addr, size) \\",
        "	for ((bit) = 0;									\\",
        "	     (bit) = find_next_zero_bit((addr), (size), (bit)), (bit) < (size);		\\",
        "	     (bit)++)",
        "",
        "/* same as for_each_clear_bit() but use bit as value to start with */",
        "#define for_each_clear_bit_from(bit, addr, size) \\",
        "	for (; (bit) = find_next_zero_bit((addr), (size), (bit)), (bit) < (size); (bit)++)",
        "",
        "/**",
        " * for_each_set_bitrange - iterate over all set bit ranges [b; e)",
        " * @b: bit offset of start of current bitrange (first set bit)",
        " * @e: bit offset of end of current bitrange (first unset bit)",
        " * @addr: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " */",
        "#define for_each_set_bitrange(b, e, addr, size)			\\",
        "	for ((b) = 0;						\\",
        "	     (b) = find_next_bit((addr), (size), b),		\\",
        "	     (e) = find_next_zero_bit((addr), (size), (b) + 1),	\\",
        "	     (b) < (size);					\\",
        "	     (b) = (e) + 1)",
        "",
        "/**",
        " * for_each_set_bitrange_from - iterate over all set bit ranges [b; e)",
        " * @b: bit offset of start of current bitrange (first set bit); must be initialized",
        " * @e: bit offset of end of current bitrange (first unset bit)",
        " * @addr: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " */",
        "#define for_each_set_bitrange_from(b, e, addr, size)		\\",
        "	for (;							\\",
        "	     (b) = find_next_bit((addr), (size), (b)),		\\",
        "	     (e) = find_next_zero_bit((addr), (size), (b) + 1),	\\",
        "	     (b) < (size);					\\",
        "	     (b) = (e) + 1)",
        "",
        "/**",
        " * for_each_clear_bitrange - iterate over all unset bit ranges [b; e)",
        " * @b: bit offset of start of current bitrange (first unset bit)",
        " * @e: bit offset of end of current bitrange (first set bit)",
        " * @addr: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " */",
        "#define for_each_clear_bitrange(b, e, addr, size)		\\",
        "	for ((b) = 0;						\\",
        "	     (b) = find_next_zero_bit((addr), (size), (b)),	\\",
        "	     (e) = find_next_bit((addr), (size), (b) + 1),	\\",
        "	     (b) < (size);					\\",
        "	     (b) = (e) + 1)",
        "",
        "/**",
        " * for_each_clear_bitrange_from - iterate over all unset bit ranges [b; e)",
        " * @b: bit offset of start of current bitrange (first set bit); must be initialized",
        " * @e: bit offset of end of current bitrange (first unset bit)",
        " * @addr: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " */",
        "#define for_each_clear_bitrange_from(b, e, addr, size)		\\",
        "	for (;							\\",
        "	     (b) = find_next_zero_bit((addr), (size), (b)),	\\",
        "	     (e) = find_next_bit((addr), (size), (b) + 1),	\\",
        "	     (b) < (size);					\\",
        "	     (b) = (e) + 1)",
        "",
        "/**",
        " * for_each_set_bit_wrap - iterate over all set bits starting from @start, and",
        " * wrapping around the end of bitmap.",
        " * @bit: offset for current iteration",
        " * @addr: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " * @start: Starting bit for bitmap traversing, wrapping around the bitmap end",
        " */",
        "#define for_each_set_bit_wrap(bit, addr, size, start) \\",
        "	for ((bit) = find_next_bit_wrap((addr), (size), (start));		\\",
        "	     (bit) < (size);							\\",
        "	     (bit) = __for_each_wrap((addr), (size), (start), (bit) + 1))",
        "",
        "/**",
        " * for_each_set_clump8 - iterate over bitmap for each 8-bit clump with set bits",
        " * @start: bit offset to start search and to store the current iteration offset",
        " * @clump: location to store copy of current 8-bit clump",
        " * @bits: bitmap address to base the search on",
        " * @size: bitmap size in number of bits",
        " */",
        "#define for_each_set_clump8(start, clump, bits, size) \\",
        "	for ((start) = find_first_clump8(&(clump), (bits), (size)); \\",
        "	     (start) < (size); \\",
        "	     (start) = find_next_clump8(&(clump), (bits), (size), (start) + 8))",
        "",
        "#endif /*__LINUX_FIND_H_ */"
    ]
  },
  "include_trace_events_kmem_h": {
    path: "include/trace/events/kmem.h",
    covered: [384],
    totalLines: 415,
    coveredCount: 1,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#undef TRACE_SYSTEM",
        "#define TRACE_SYSTEM kmem",
        "",
        "#if !defined(_TRACE_KMEM_H) || defined(TRACE_HEADER_MULTI_READ)",
        "#define _TRACE_KMEM_H",
        "",
        "#include <linux/types.h>",
        "#include <linux/tracepoint.h>",
        "#include <trace/events/mmflags.h>",
        "",
        "TRACE_EVENT(kmem_cache_alloc,",
        "",
        "	TP_PROTO(unsigned long call_site,",
        "		 const void *ptr,",
        "		 struct kmem_cache *s,",
        "		 gfp_t gfp_flags,",
        "		 int node),",
        "",
        "	TP_ARGS(call_site, ptr, s, gfp_flags, node),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	call_site	)",
        "		__field(	const void *,	ptr		)",
        "		__field(	size_t,		bytes_req	)",
        "		__field(	size_t,		bytes_alloc	)",
        "		__field(	unsigned long,	gfp_flags	)",
        "		__field(	int,		node		)",
        "		__field(	bool,		accounted	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->call_site	= call_site;",
        "		__entry->ptr		= ptr;",
        "		__entry->bytes_req	= s->object_size;",
        "		__entry->bytes_alloc	= s->size;",
        "		__entry->gfp_flags	= (__force unsigned long)gfp_flags;",
        "		__entry->node		= node;",
        "		__entry->accounted	= IS_ENABLED(CONFIG_MEMCG) ?",
        "					  ((gfp_flags & __GFP_ACCOUNT) ||",
        "					  (s->flags & SLAB_ACCOUNT)) : false;",
        "	),",
        "",
        "	TP_printk(\"call_site=%pS ptr=%p bytes_req=%zu bytes_alloc=%zu gfp_flags=%s node=%d accounted=%s\",",
        "		(void *)__entry->call_site,",
        "		__entry->ptr,",
        "		__entry->bytes_req,",
        "		__entry->bytes_alloc,",
        "		show_gfp_flags(__entry->gfp_flags),",
        "		__entry->node,",
        "		__entry->accounted ? \"true\" : \"false\")",
        ");",
        "",
        "TRACE_EVENT(kmalloc,",
        "",
        "	TP_PROTO(unsigned long call_site,",
        "		 const void *ptr,",
        "		 size_t bytes_req,",
        "		 size_t bytes_alloc,",
        "		 gfp_t gfp_flags,",
        "		 int node),",
        "",
        "	TP_ARGS(call_site, ptr, bytes_req, bytes_alloc, gfp_flags, node),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	call_site	)",
        "		__field(	const void *,	ptr		)",
        "		__field(	size_t,		bytes_req	)",
        "		__field(	size_t,		bytes_alloc	)",
        "		__field(	unsigned long,	gfp_flags	)",
        "		__field(	int,		node		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->call_site	= call_site;",
        "		__entry->ptr		= ptr;",
        "		__entry->bytes_req	= bytes_req;",
        "		__entry->bytes_alloc	= bytes_alloc;",
        "		__entry->gfp_flags	= (__force unsigned long)gfp_flags;",
        "		__entry->node		= node;",
        "	),",
        "",
        "	TP_printk(\"call_site=%pS ptr=%p bytes_req=%zu bytes_alloc=%zu gfp_flags=%s node=%d accounted=%s\",",
        "		(void *)__entry->call_site,",
        "		__entry->ptr,",
        "		__entry->bytes_req,",
        "		__entry->bytes_alloc,",
        "		show_gfp_flags(__entry->gfp_flags),",
        "		__entry->node,",
        "		(IS_ENABLED(CONFIG_MEMCG) &&",
        "		 (__entry->gfp_flags & (__force unsigned long)__GFP_ACCOUNT)) ? \"true\" : \"false\")",
        ");",
        "",
        "TRACE_EVENT(kfree,",
        "",
        "	TP_PROTO(unsigned long call_site, const void *ptr),",
        "",
        "	TP_ARGS(call_site, ptr),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	call_site	)",
        "		__field(	const void *,	ptr		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->call_site	= call_site;",
        "		__entry->ptr		= ptr;",
        "	),",
        "",
        "	TP_printk(\"call_site=%pS ptr=%p\",",
        "		  (void *)__entry->call_site, __entry->ptr)",
        ");",
        "",
        "TRACE_EVENT(kmem_cache_free,",
        "",
        "	TP_PROTO(unsigned long call_site, const void *ptr, const struct kmem_cache *s),",
        "",
        "	TP_ARGS(call_site, ptr, s),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	call_site	)",
        "		__field(	const void *,	ptr		)",
        "		__string(	name,		s->name		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->call_site	= call_site;",
        "		__entry->ptr		= ptr;",
        "		__assign_str(name);",
        "	),",
        "",
        "	TP_printk(\"call_site=%pS ptr=%p name=%s\",",
        "		  (void *)__entry->call_site, __entry->ptr, __get_str(name))",
        ");",
        "",
        "TRACE_EVENT(mm_page_free,",
        "",
        "	TP_PROTO(struct page *page, unsigned int order),",
        "",
        "	TP_ARGS(page, order),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn		)",
        "		__field(	unsigned int,	order		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn		= page_to_pfn(page);",
        "		__entry->order		= order;",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx order=%d\",",
        "			pfn_to_page(__entry->pfn),",
        "			__entry->pfn,",
        "			__entry->order)",
        ");",
        "",
        "TRACE_EVENT(mm_page_free_batched,",
        "",
        "	TP_PROTO(struct page *page),",
        "",
        "	TP_ARGS(page),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn		= page_to_pfn(page);",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx order=0\",",
        "			pfn_to_page(__entry->pfn),",
        "			__entry->pfn)",
        ");",
        "",
        "TRACE_EVENT(mm_page_alloc,",
        "",
        "	TP_PROTO(struct page *page, unsigned int order,",
        "			gfp_t gfp_flags, int migratetype),",
        "",
        "	TP_ARGS(page, order, gfp_flags, migratetype),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn		)",
        "		__field(	unsigned int,	order		)",
        "		__field(	unsigned long,	gfp_flags	)",
        "		__field(	int,		migratetype	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn		= page ? page_to_pfn(page) : -1UL;",
        "		__entry->order		= order;",
        "		__entry->gfp_flags	= (__force unsigned long)gfp_flags;",
        "		__entry->migratetype	= migratetype;",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx order=%d migratetype=%d gfp_flags=%s\",",
        "		__entry->pfn != -1UL ? pfn_to_page(__entry->pfn) : NULL,",
        "		__entry->pfn != -1UL ? __entry->pfn : 0,",
        "		__entry->order,",
        "		__entry->migratetype,",
        "		show_gfp_flags(__entry->gfp_flags))",
        ");",
        "",
        "DECLARE_EVENT_CLASS(mm_page,",
        "",
        "	TP_PROTO(struct page *page, unsigned int order, int migratetype,",
        "		 int percpu_refill),",
        "",
        "	TP_ARGS(page, order, migratetype, percpu_refill),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn		)",
        "		__field(	unsigned int,	order		)",
        "		__field(	int,		migratetype	)",
        "		__field(	int,		percpu_refill	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn		= page ? page_to_pfn(page) : -1UL;",
        "		__entry->order		= order;",
        "		__entry->migratetype	= migratetype;",
        "		__entry->percpu_refill	= percpu_refill;",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx order=%u migratetype=%d percpu_refill=%d\",",
        "		__entry->pfn != -1UL ? pfn_to_page(__entry->pfn) : NULL,",
        "		__entry->pfn != -1UL ? __entry->pfn : 0,",
        "		__entry->order,",
        "		__entry->migratetype,",
        "		__entry->percpu_refill)",
        ");",
        "",
        "DEFINE_EVENT(mm_page, mm_page_alloc_zone_locked,",
        "",
        "	TP_PROTO(struct page *page, unsigned int order, int migratetype,",
        "		 int percpu_refill),",
        "",
        "	TP_ARGS(page, order, migratetype, percpu_refill)",
        ");",
        "",
        "TRACE_EVENT(mm_page_pcpu_drain,",
        "",
        "	TP_PROTO(struct page *page, unsigned int order, int migratetype),",
        "",
        "	TP_ARGS(page, order, migratetype),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn		)",
        "		__field(	unsigned int,	order		)",
        "		__field(	int,		migratetype	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn		= page ? page_to_pfn(page) : -1UL;",
        "		__entry->order		= order;",
        "		__entry->migratetype	= migratetype;",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx order=%d migratetype=%d\",",
        "		pfn_to_page(__entry->pfn), __entry->pfn,",
        "		__entry->order, __entry->migratetype)",
        ");",
        "",
        "TRACE_EVENT(mm_page_alloc_extfrag,",
        "",
        "	TP_PROTO(struct page *page,",
        "		int alloc_order, int fallback_order,",
        "		int alloc_migratetype, int fallback_migratetype),",
        "",
        "	TP_ARGS(page,",
        "		alloc_order, fallback_order,",
        "		alloc_migratetype, fallback_migratetype),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	pfn			)",
        "		__field(	int,		alloc_order		)",
        "		__field(	int,		fallback_order		)",
        "		__field(	int,		alloc_migratetype	)",
        "		__field(	int,		fallback_migratetype	)",
        "		__field(	int,		change_ownership	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->pfn			= page_to_pfn(page);",
        "		__entry->alloc_order		= alloc_order;",
        "		__entry->fallback_order		= fallback_order;",
        "		__entry->alloc_migratetype	= alloc_migratetype;",
        "		__entry->fallback_migratetype	= fallback_migratetype;",
        "		__entry->change_ownership	= (alloc_migratetype ==",
        "					get_pageblock_migratetype(page));",
        "	),",
        "",
        "	TP_printk(\"page=%p pfn=0x%lx alloc_order=%d fallback_order=%d pageblock_order=%d alloc_migratetype=%d fallback_migratetype=%d fragmenting=%d change_ownership=%d\",",
        "		pfn_to_page(__entry->pfn),",
        "		__entry->pfn,",
        "		__entry->alloc_order,",
        "		__entry->fallback_order,",
        "		pageblock_order,",
        "		__entry->alloc_migratetype,",
        "		__entry->fallback_migratetype,",
        "		__entry->fallback_order < pageblock_order,",
        "		__entry->change_ownership)",
        ");",
        "",
        "TRACE_EVENT(mm_alloc_contig_migrate_range_info,",
        "",
        "	TP_PROTO(unsigned long start,",
        "		 unsigned long end,",
        "		 unsigned long nr_migrated,",
        "		 unsigned long nr_reclaimed,",
        "		 unsigned long nr_mapped,",
        "		 int migratetype),",
        "",
        "	TP_ARGS(start, end, nr_migrated, nr_reclaimed, nr_mapped, migratetype),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(unsigned long, start)",
        "		__field(unsigned long, end)",
        "		__field(unsigned long, nr_migrated)",
        "		__field(unsigned long, nr_reclaimed)",
        "		__field(unsigned long, nr_mapped)",
        "		__field(int, migratetype)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->start = start;",
        "		__entry->end = end;",
        "		__entry->nr_migrated = nr_migrated;",
        "		__entry->nr_reclaimed = nr_reclaimed;",
        "		__entry->nr_mapped = nr_mapped;",
        "		__entry->migratetype = migratetype;",
        "	),",
        "",
        "	TP_printk(\"start=0x%lx end=0x%lx migratetype=%d nr_migrated=%lu nr_reclaimed=%lu nr_mapped=%lu\",",
        "		  __entry->start,",
        "		  __entry->end,",
        "		  __entry->migratetype,",
        "		  __entry->nr_migrated,",
        "		  __entry->nr_reclaimed,",
        "		  __entry->nr_mapped)",
        ");",
        "",
        "/*",
        " * Required for uniquely and securely identifying mm in rss_stat tracepoint.",
        " */",
        "#ifndef __PTR_TO_HASHVAL",
        "static unsigned int __maybe_unused mm_ptr_to_hash(const void *ptr)",
        "{",
        "	int ret;",
        "	unsigned long hashval;",
        "",
        "	ret = ptr_to_hashval(ptr, &hashval);",
        "	if (ret)",
        "		return 0;",
        "",
        "	/* The hashed value is only 32-bit */",
        "	return (unsigned int)hashval;",
        "}",
        "#define __PTR_TO_HASHVAL",
        "#endif",
        "",
        "#define TRACE_MM_PAGES		\\",
        "	EM(MM_FILEPAGES)	\\",
        "	EM(MM_ANONPAGES)	\\",
        "	EM(MM_SWAPENTS)		\\",
        "	EMe(MM_SHMEMPAGES)",
        "",
        "#undef EM",
        "#undef EMe",
        "",
        "#define EM(a)	TRACE_DEFINE_ENUM(a);",
        "#define EMe(a)	TRACE_DEFINE_ENUM(a);",
        "",
        "TRACE_MM_PAGES",
        "",
        "#undef EM",
        "#undef EMe",
        "",
        "#define EM(a)	{ a, #a },",
        "#define EMe(a)	{ a, #a }",
        "",
        "TRACE_EVENT(rss_stat,",
        "",
        "	TP_PROTO(struct mm_struct *mm,",
        "		int member),",
        "",
        "	TP_ARGS(mm, member),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(unsigned int, mm_id)",
        "		__field(unsigned int, curr)",
        "		__field(int, member)",
        "		__field(long, size)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->mm_id = mm_ptr_to_hash(mm);",
        "		__entry->curr = !!(current->mm == mm);",
        "		__entry->member = member;",
        "		__entry->size = (percpu_counter_sum_positive(&mm->rss_stat[member])",
        "							    << PAGE_SHIFT);",
        "	),",
        "",
        "	TP_printk(\"mm_id=%u curr=%d type=%s size=%ldB\",",
        "		__entry->mm_id,",
        "		__entry->curr,",
        "		__print_symbolic(__entry->member, TRACE_MM_PAGES),",
        "		__entry->size)",
        "	);",
        "#endif /* _TRACE_KMEM_H */",
        "",
        "/* This part must be outside protection */",
        "#include <trace/define_trace.h>"
    ]
  },
  "kernel_bpf_token_c": {
    path: "kernel/bpf/token.c",
    covered: [112, 17, 49, 24, 13, 21, 124, 227, 130, 218, 22, 211, 214, 50, 220],
    totalLines: 255,
    coveredCount: 15,
    coveragePct: 5.9,
    source: [
        "#include <linux/bpf.h>",
        "#include <linux/vmalloc.h>",
        "#include <linux/file.h>",
        "#include <linux/fs.h>",
        "#include <linux/kernel.h>",
        "#include <linux/idr.h>",
        "#include <linux/namei.h>",
        "#include <linux/user_namespace.h>",
        "#include <linux/security.h>",
        "",
        "static bool bpf_ns_capable(struct user_namespace *ns, int cap)",
        "{",
        "	return ns_capable(ns, cap) || (cap != CAP_SYS_ADMIN && ns_capable(ns, CAP_SYS_ADMIN));",
        "}",
        "",
        "bool bpf_token_capable(const struct bpf_token *token, int cap)",
        "{",
        "	struct user_namespace *userns;",
        "",
        "	/* BPF token allows ns_capable() level of capabilities */",
        "	userns = token ? token->userns : &init_user_ns;",
        "	if (!bpf_ns_capable(userns, cap))",
        "		return false;",
        "	if (token && security_bpf_token_capable(token, cap) < 0)",
        "		return false;",
        "	return true;",
        "}",
        "",
        "void bpf_token_inc(struct bpf_token *token)",
        "{",
        "	atomic64_inc(&token->refcnt);",
        "}",
        "",
        "static void bpf_token_free(struct bpf_token *token)",
        "{",
        "	security_bpf_token_free(token);",
        "	put_user_ns(token->userns);",
        "	kfree(token);",
        "}",
        "",
        "static void bpf_token_put_deferred(struct work_struct *work)",
        "{",
        "	struct bpf_token *token = container_of(work, struct bpf_token, work);",
        "",
        "	bpf_token_free(token);",
        "}",
        "",
        "void bpf_token_put(struct bpf_token *token)",
        "{",
        "	if (!token)",
        "		return;",
        "",
        "	if (!atomic64_dec_and_test(&token->refcnt))",
        "		return;",
        "",
        "	INIT_WORK(&token->work, bpf_token_put_deferred);",
        "	schedule_work(&token->work);",
        "}",
        "",
        "static int bpf_token_release(struct inode *inode, struct file *filp)",
        "{",
        "	struct bpf_token *token = filp->private_data;",
        "",
        "	bpf_token_put(token);",
        "	return 0;",
        "}",
        "",
        "static void bpf_token_show_fdinfo(struct seq_file *m, struct file *filp)",
        "{",
        "	struct bpf_token *token = filp->private_data;",
        "	u64 mask;",
        "",
        "	BUILD_BUG_ON(__MAX_BPF_CMD >= 64);",
        "	mask = BIT_ULL(__MAX_BPF_CMD) - 1;",
        "	if ((token->allowed_cmds & mask) == mask)",
        "		seq_printf(m, \"allowed_cmds:\\tany\\n\");",
        "	else",
        "		seq_printf(m, \"allowed_cmds:\\t0x%llx\\n\", token->allowed_cmds);",
        "",
        "	BUILD_BUG_ON(__MAX_BPF_MAP_TYPE >= 64);",
        "	mask = BIT_ULL(__MAX_BPF_MAP_TYPE) - 1;",
        "	if ((token->allowed_maps & mask) == mask)",
        "		seq_printf(m, \"allowed_maps:\\tany\\n\");",
        "	else",
        "		seq_printf(m, \"allowed_maps:\\t0x%llx\\n\", token->allowed_maps);",
        "",
        "	BUILD_BUG_ON(__MAX_BPF_PROG_TYPE >= 64);",
        "	mask = BIT_ULL(__MAX_BPF_PROG_TYPE) - 1;",
        "	if ((token->allowed_progs & mask) == mask)",
        "		seq_printf(m, \"allowed_progs:\\tany\\n\");",
        "	else",
        "		seq_printf(m, \"allowed_progs:\\t0x%llx\\n\", token->allowed_progs);",
        "",
        "	BUILD_BUG_ON(__MAX_BPF_ATTACH_TYPE >= 64);",
        "	mask = BIT_ULL(__MAX_BPF_ATTACH_TYPE) - 1;",
        "	if ((token->allowed_attachs & mask) == mask)",
        "		seq_printf(m, \"allowed_attachs:\\tany\\n\");",
        "	else",
        "		seq_printf(m, \"allowed_attachs:\\t0x%llx\\n\", token->allowed_attachs);",
        "}",
        "",
        "#define BPF_TOKEN_INODE_NAME \"bpf-token\"",
        "",
        "static const struct inode_operations bpf_token_iops = { };",
        "",
        "static const struct file_operations bpf_token_fops = {",
        "	.release	= bpf_token_release,",
        "	.show_fdinfo	= bpf_token_show_fdinfo,",
        "};",
        "",
        "int bpf_token_create(union bpf_attr *attr)",
        "{",
        "	struct bpf_mount_opts *mnt_opts;",
        "	struct bpf_token *token = NULL;",
        "	struct user_namespace *userns;",
        "	struct inode *inode;",
        "	struct file *file;",
        "	CLASS(fd, f)(attr->token_create.bpffs_fd);",
        "	struct path path;",
        "	struct super_block *sb;",
        "	umode_t mode;",
        "	int err, fd;",
        "",
        "	if (fd_empty(f))",
        "		return -EBADF;",
        "",
        "	path = fd_file(f)->f_path;",
        "	sb = path.dentry->d_sb;",
        "",
        "	if (path.dentry != sb->s_root)",
        "		return -EINVAL;",
        "	if (sb->s_op != &bpf_super_ops)",
        "		return -EINVAL;",
        "	err = path_permission(&path, MAY_ACCESS);",
        "	if (err)",
        "		return err;",
        "",
        "	userns = sb->s_user_ns;",
        "	/*",
        "	 * Enforce that creators of BPF tokens are in the same user",
        "	 * namespace as the BPF FS instance. This makes reasoning about",
        "	 * permissions a lot easier and we can always relax this later.",
        "	 */",
        "	if (current_user_ns() != userns)",
        "		return -EPERM;",
        "	if (!ns_capable(userns, CAP_BPF))",
        "		return -EPERM;",
        "",
        "	/* Creating BPF token in init_user_ns doesn't make much sense. */",
        "	if (current_user_ns() == &init_user_ns)",
        "		return -EOPNOTSUPP;",
        "",
        "	mnt_opts = sb->s_fs_info;",
        "	if (mnt_opts->delegate_cmds == 0 &&",
        "	    mnt_opts->delegate_maps == 0 &&",
        "	    mnt_opts->delegate_progs == 0 &&",
        "	    mnt_opts->delegate_attachs == 0)",
        "		return -ENOENT; /* no BPF token delegation is set up */",
        "",
        "	mode = S_IFREG | ((S_IRUSR | S_IWUSR) & ~current_umask());",
        "	inode = bpf_get_inode(sb, NULL, mode);",
        "	if (IS_ERR(inode))",
        "		return PTR_ERR(inode);",
        "",
        "	inode->i_op = &bpf_token_iops;",
        "	inode->i_fop = &bpf_token_fops;",
        "	clear_nlink(inode); /* make sure it is unlinked */",
        "",
        "	file = alloc_file_pseudo(inode, path.mnt, BPF_TOKEN_INODE_NAME, O_RDWR, &bpf_token_fops);",
        "	if (IS_ERR(file)) {",
        "		iput(inode);",
        "		return PTR_ERR(file);",
        "	}",
        "",
        "	token = kzalloc(sizeof(*token), GFP_USER);",
        "	if (!token) {",
        "		err = -ENOMEM;",
        "		goto out_file;",
        "	}",
        "",
        "	atomic64_set(&token->refcnt, 1);",
        "",
        "	/* remember bpffs owning userns for future ns_capable() checks */",
        "	token->userns = get_user_ns(userns);",
        "",
        "	token->allowed_cmds = mnt_opts->delegate_cmds;",
        "	token->allowed_maps = mnt_opts->delegate_maps;",
        "	token->allowed_progs = mnt_opts->delegate_progs;",
        "	token->allowed_attachs = mnt_opts->delegate_attachs;",
        "",
        "	err = security_bpf_token_create(token, attr, &path);",
        "	if (err)",
        "		goto out_token;",
        "",
        "	fd = get_unused_fd_flags(O_CLOEXEC);",
        "	if (fd < 0) {",
        "		err = fd;",
        "		goto out_token;",
        "	}",
        "",
        "	file->private_data = token;",
        "	fd_install(fd, file);",
        "",
        "	return fd;",
        "",
        "out_token:",
        "	bpf_token_free(token);",
        "out_file:",
        "	fput(file);",
        "	return err;",
        "}",
        "",
        "struct bpf_token *bpf_token_get_from_fd(u32 ufd)",
        "{",
        "	CLASS(fd, f)(ufd);",
        "	struct bpf_token *token;",
        "",
        "	if (fd_empty(f))",
        "		return ERR_PTR(-EBADF);",
        "	if (fd_file(f)->f_op != &bpf_token_fops)",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	token = fd_file(f)->private_data;",
        "	bpf_token_inc(token);",
        "",
        "	return token;",
        "}",
        "",
        "bool bpf_token_allow_cmd(const struct bpf_token *token, enum bpf_cmd cmd)",
        "{",
        "	if (!token)",
        "		return false;",
        "	if (!(token->allowed_cmds & BIT_ULL(cmd)))",
        "		return false;",
        "	return security_bpf_token_cmd(token, cmd) == 0;",
        "}",
        "",
        "bool bpf_token_allow_map_type(const struct bpf_token *token, enum bpf_map_type type)",
        "{",
        "	if (!token || type >= __MAX_BPF_MAP_TYPE)",
        "		return false;",
        "",
        "	return token->allowed_maps & BIT_ULL(type);",
        "}",
        "",
        "bool bpf_token_allow_prog_type(const struct bpf_token *token,",
        "			       enum bpf_prog_type prog_type,",
        "			       enum bpf_attach_type attach_type)",
        "{",
        "	if (!token || prog_type >= __MAX_BPF_PROG_TYPE || attach_type >= __MAX_BPF_ATTACH_TYPE)",
        "		return false;",
        "",
        "	return (token->allowed_progs & BIT_ULL(prog_type)) &&",
        "	       (token->allowed_attachs & BIT_ULL(attach_type));",
        "}"
    ]
  },
  "lib_bsearch_c": {
    path: "lib/bsearch.c",
    covered: [33, 32],
    totalLines: 36,
    coveredCount: 2,
    coveragePct: 5.6,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " * A generic implementation of binary search for the Linux kernel",
        " *",
        " * Copyright (C) 2008-2009 Ksplice, Inc.",
        " * Author: Tim Abbott <tabbott@ksplice.com>",
        " */",
        "",
        "#include <linux/export.h>",
        "#include <linux/bsearch.h>",
        "#include <linux/kprobes.h>",
        "",
        "/*",
        " * bsearch - binary search an array of elements",
        " * @key: pointer to item being searched for",
        " * @base: pointer to first element to search",
        " * @num: number of elements",
        " * @size: size of each element",
        " * @cmp: pointer to comparison function",
        " *",
        " * This function does a binary search on the given array.  The",
        " * contents of the array should already be in ascending sorted order",
        " * under the provided comparison function.",
        " *",
        " * Note that the key need not have the same type as the elements in",
        " * the array, e.g. key could be a string and the comparison function",
        " * could compare the string with the struct's name field.  However, if",
        " * the key and elements in the array are of the same type, you can use",
        " * the same comparison function for both sort() and bsearch().",
        " */",
        "void *bsearch(const void *key, const void *base, size_t num, size_t size, cmp_func_t cmp)",
        "{",
        "	return __inline_bsearch(key, base, num, size, cmp);",
        "}",
        "EXPORT_SYMBOL(bsearch);",
        "NOKPROBE_SYMBOL(bsearch);"
    ]
  },
  "kernel_irq_work_c": {
    path: "kernel/irq_work.c",
    covered: [287, 298],
    totalLines: 327,
    coveredCount: 2,
    coveragePct: 0.6,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra",
        " *",
        " * Provides a framework for enqueueing and running callbacks from hardirq",
        " * context. The enqueueing is NMI-safe.",
        " */",
        "",
        "#include <linux/bug.h>",
        "#include <linux/kernel.h>",
        "#include <linux/export.h>",
        "#include <linux/irq_work.h>",
        "#include <linux/percpu.h>",
        "#include <linux/hardirq.h>",
        "#include <linux/irqflags.h>",
        "#include <linux/sched.h>",
        "#include <linux/tick.h>",
        "#include <linux/cpu.h>",
        "#include <linux/notifier.h>",
        "#include <linux/smp.h>",
        "#include <linux/smpboot.h>",
        "#include <asm/processor.h>",
        "#include <linux/kasan.h>",
        "",
        "#include <trace/events/ipi.h>",
        "",
        "static DEFINE_PER_CPU(struct llist_head, raised_list);",
        "static DEFINE_PER_CPU(struct llist_head, lazy_list);",
        "static DEFINE_PER_CPU(struct task_struct *, irq_workd);",
        "",
        "static void wake_irq_workd(void)",
        "{",
        "	struct task_struct *tsk = __this_cpu_read(irq_workd);",
        "",
        "	if (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)",
        "		wake_up_process(tsk);",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "static void irq_work_wake(struct irq_work *entry)",
        "{",
        "	wake_irq_workd();",
        "}",
        "",
        "static DEFINE_PER_CPU(struct irq_work, irq_work_wakeup) =",
        "	IRQ_WORK_INIT_HARD(irq_work_wake);",
        "#endif",
        "",
        "static int irq_workd_should_run(unsigned int cpu)",
        "{",
        "	return !llist_empty(this_cpu_ptr(&lazy_list));",
        "}",
        "",
        "/*",
        " * Claim the entry so that no one else will poke at it.",
        " */",
        "static bool irq_work_claim(struct irq_work *work)",
        "{",
        "	int oflags;",
        "",
        "	oflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);",
        "	/*",
        "	 * If the work is already pending, no need to raise the IPI.",
        "	 * The pairing smp_mb() in irq_work_single() makes sure",
        "	 * everything we did before is visible.",
        "	 */",
        "	if (oflags & IRQ_WORK_PENDING)",
        "		return false;",
        "	return true;",
        "}",
        "",
        "void __weak arch_irq_work_raise(void)",
        "{",
        "	/*",
        "	 * Lame architectures will get the timer tick callback",
        "	 */",
        "}",
        "",
        "static __always_inline void irq_work_raise(struct irq_work *work)",
        "{",
        "	if (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())",
        "		trace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);",
        "",
        "	arch_irq_work_raise();",
        "}",
        "",
        "/* Enqueue on current CPU, work must already be claimed and preempt disabled */",
        "static void __irq_work_queue_local(struct irq_work *work)",
        "{",
        "	struct llist_head *list;",
        "	bool rt_lazy_work = false;",
        "	bool lazy_work = false;",
        "	int work_flags;",
        "",
        "	work_flags = atomic_read(&work->node.a_flags);",
        "	if (work_flags & IRQ_WORK_LAZY)",
        "		lazy_work = true;",
        "	else if (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
        "		 !(work_flags & IRQ_WORK_HARD_IRQ))",
        "		rt_lazy_work = true;",
        "",
        "	if (lazy_work || rt_lazy_work)",
        "		list = this_cpu_ptr(&lazy_list);",
        "	else",
        "		list = this_cpu_ptr(&raised_list);",
        "",
        "	if (!llist_add(&work->node.llist, list))",
        "		return;",
        "",
        "	/* If the work is \"lazy\", handle it from next tick if any */",
        "	if (!lazy_work || tick_nohz_tick_stopped())",
        "		irq_work_raise(work);",
        "}",
        "",
        "/* Enqueue the irq work @work on the current CPU */",
        "bool irq_work_queue(struct irq_work *work)",
        "{",
        "	/* Only queue if not already pending */",
        "	if (!irq_work_claim(work))",
        "		return false;",
        "",
        "	/* Queue the entry and raise the IPI if needed. */",
        "	preempt_disable();",
        "	__irq_work_queue_local(work);",
        "	preempt_enable();",
        "",
        "	return true;",
        "}",
        "EXPORT_SYMBOL_GPL(irq_work_queue);",
        "",
        "/*",
        " * Enqueue the irq_work @work on @cpu unless it's already pending",
        " * somewhere.",
        " *",
        " * Can be re-enqueued while the callback is still in progress.",
        " */",
        "bool irq_work_queue_on(struct irq_work *work, int cpu)",
        "{",
        "#ifndef CONFIG_SMP",
        "	return irq_work_queue(work);",
        "",
        "#else /* CONFIG_SMP: */",
        "	/* All work should have been flushed before going offline */",
        "	WARN_ON_ONCE(cpu_is_offline(cpu));",
        "",
        "	/* Only queue if not already pending */",
        "	if (!irq_work_claim(work))",
        "		return false;",
        "",
        "	kasan_record_aux_stack_noalloc(work);",
        "",
        "	preempt_disable();",
        "	if (cpu != smp_processor_id()) {",
        "		/* Arch remote IPI send/receive backend aren't NMI safe */",
        "		WARN_ON_ONCE(in_nmi());",
        "",
        "		/*",
        "		 * On PREEMPT_RT the items which are not marked as",
        "		 * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work",
        "		 * item is used on the remote CPU to wake the thread.",
        "		 */",
        "		if (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
        "		    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {",
        "",
        "			if (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))",
        "				goto out;",
        "",
        "			work = &per_cpu(irq_work_wakeup, cpu);",
        "			if (!irq_work_claim(work))",
        "				goto out;",
        "		}",
        "",
        "		__smp_call_single_queue(cpu, &work->node.llist);",
        "	} else {",
        "		__irq_work_queue_local(work);",
        "	}",
        "out:",
        "	preempt_enable();",
        "",
        "	return true;",
        "#endif /* CONFIG_SMP */",
        "}",
        "",
        "bool irq_work_needs_cpu(void)",
        "{",
        "	struct llist_head *raised, *lazy;",
        "",
        "	raised = this_cpu_ptr(&raised_list);",
        "	lazy = this_cpu_ptr(&lazy_list);",
        "",
        "	if (llist_empty(raised) || arch_irq_work_has_interrupt())",
        "		if (llist_empty(lazy))",
        "			return false;",
        "",
        "	/* All work should have been flushed before going offline */",
        "	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
        "",
        "	return true;",
        "}",
        "",
        "void irq_work_single(void *arg)",
        "{",
        "	struct irq_work *work = arg;",
        "	int flags;",
        "",
        "	/*",
        "	 * Clear the PENDING bit, after this point the @work can be re-used.",
        "	 * The PENDING bit acts as a lock, and we own it, so we can clear it",
        "	 * without atomic ops.",
        "	 */",
        "	flags = atomic_read(&work->node.a_flags);",
        "	flags &= ~IRQ_WORK_PENDING;",
        "	atomic_set(&work->node.a_flags, flags);",
        "",
        "	/*",
        "	 * See irq_work_claim().",
        "	 */",
        "	smp_mb();",
        "",
        "	lockdep_irq_work_enter(flags);",
        "	work->func(work);",
        "	lockdep_irq_work_exit(flags);",
        "",
        "	/*",
        "	 * Clear the BUSY bit, if set, and return to the free state if no-one",
        "	 * else claimed it meanwhile.",
        "	 */",
        "	(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);",
        "",
        "	if ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
        "	    !arch_irq_work_has_interrupt())",
        "		rcuwait_wake_up(&work->irqwait);",
        "}",
        "",
        "static void irq_work_run_list(struct llist_head *list)",
        "{",
        "	struct irq_work *work, *tmp;",
        "	struct llist_node *llnode;",
        "",
        "	/*",
        "	 * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed",
        "	 * in a per-CPU thread in preemptible context. Only the items which are",
        "	 * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.",
        "	 */",
        "	BUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));",
        "",
        "	if (llist_empty(list))",
        "		return;",
        "",
        "	llnode = llist_del_all(list);",
        "	llist_for_each_entry_safe(work, tmp, llnode, node.llist)",
        "		irq_work_single(work);",
        "}",
        "",
        "/*",
        " * hotplug calls this through:",
        " *  hotplug_cfd() -> flush_smp_call_function_queue()",
        " */",
        "void irq_work_run(void)",
        "{",
        "	irq_work_run_list(this_cpu_ptr(&raised_list));",
        "	if (!IS_ENABLED(CONFIG_PREEMPT_RT))",
        "		irq_work_run_list(this_cpu_ptr(&lazy_list));",
        "	else",
        "		wake_irq_workd();",
        "}",
        "EXPORT_SYMBOL_GPL(irq_work_run);",
        "",
        "void irq_work_tick(void)",
        "{",
        "	struct llist_head *raised = this_cpu_ptr(&raised_list);",
        "",
        "	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())",
        "		irq_work_run_list(raised);",
        "",
        "	if (!IS_ENABLED(CONFIG_PREEMPT_RT))",
        "		irq_work_run_list(this_cpu_ptr(&lazy_list));",
        "	else",
        "		wake_irq_workd();",
        "}",
        "",
        "/*",
        " * Synchronize against the irq_work @entry, ensures the entry is not",
        " * currently in use.",
        " */",
        "void irq_work_sync(struct irq_work *work)",
        "{",
        "	lockdep_assert_irqs_enabled();",
        "	might_sleep();",
        "",
        "	if ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
        "	    !arch_irq_work_has_interrupt()) {",
        "		rcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),",
        "				   TASK_UNINTERRUPTIBLE);",
        "		return;",
        "	}",
        "",
        "	while (irq_work_is_busy(work))",
        "		cpu_relax();",
        "}",
        "EXPORT_SYMBOL_GPL(irq_work_sync);",
        "",
        "static void run_irq_workd(unsigned int cpu)",
        "{",
        "	irq_work_run_list(this_cpu_ptr(&lazy_list));",
        "}",
        "",
        "static void irq_workd_setup(unsigned int cpu)",
        "{",
        "	sched_set_fifo_low(current);",
        "}",
        "",
        "static struct smp_hotplug_thread irqwork_threads = {",
        "	.store                  = &irq_workd,",
        "	.setup			= irq_workd_setup,",
        "	.thread_should_run      = irq_workd_should_run,",
        "	.thread_fn              = run_irq_workd,",
        "	.thread_comm            = \"irq_work/%u\",",
        "};",
        "",
        "static __init int irq_work_init_threads(void)",
        "{",
        "	if (IS_ENABLED(CONFIG_PREEMPT_RT))",
        "		BUG_ON(smpboot_register_percpu_thread(&irqwork_threads));",
        "	return 0;",
        "}",
        "early_initcall(irq_work_init_threads);"
    ]
  },
  "include_linux_rbtree_h": {
    path: "include/linux/rbtree.h",
    covered: [112, 183, 123, 172],
    totalLines: 401,
    coveredCount: 4,
    coveragePct: 1.0,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0-or-later */",
        "/*",
        "  Red Black Trees",
        "  (C) 1999  Andrea Arcangeli <andrea@suse.de>",
        "  ",
        "",
        "  linux/include/linux/rbtree.h",
        "",
        "  To use rbtrees you'll have to implement your own insert and search cores.",
        "  This will avoid us to use callbacks and to drop drammatically performances.",
        "  I know it's not the cleaner way,  but in C (not in C++) to get",
        "  performances and genericity...",
        "",
        "  See Documentation/core-api/rbtree.rst for documentation and samples.",
        "*/",
        "",
        "#ifndef	_LINUX_RBTREE_H",
        "#define	_LINUX_RBTREE_H",
        "",
        "#include <linux/container_of.h>",
        "#include <linux/rbtree_types.h>",
        "",
        "#include <linux/stddef.h>",
        "#include <linux/rcupdate.h>",
        "",
        "#define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))",
        "",
        "#define	rb_entry(ptr, type, member) container_of(ptr, type, member)",
        "",
        "#define RB_EMPTY_ROOT(root)  (READ_ONCE((root)->rb_node) == NULL)",
        "",
        "/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */",
        "#define RB_EMPTY_NODE(node)  \\",
        "	((node)->__rb_parent_color == (unsigned long)(node))",
        "#define RB_CLEAR_NODE(node)  \\",
        "	((node)->__rb_parent_color = (unsigned long)(node))",
        "",
        "",
        "extern void rb_insert_color(struct rb_node *, struct rb_root *);",
        "extern void rb_erase(struct rb_node *, struct rb_root *);",
        "",
        "",
        "/* Find logical next and previous nodes in a tree */",
        "extern struct rb_node *rb_next(const struct rb_node *);",
        "extern struct rb_node *rb_prev(const struct rb_node *);",
        "extern struct rb_node *rb_first(const struct rb_root *);",
        "extern struct rb_node *rb_last(const struct rb_root *);",
        "",
        "/* Postorder iteration - always visit the parent after its children */",
        "extern struct rb_node *rb_first_postorder(const struct rb_root *);",
        "extern struct rb_node *rb_next_postorder(const struct rb_node *);",
        "",
        "/* Fast replacement of a single node without remove/rebalance/add/rebalance */",
        "extern void rb_replace_node(struct rb_node *victim, struct rb_node *new,",
        "			    struct rb_root *root);",
        "extern void rb_replace_node_rcu(struct rb_node *victim, struct rb_node *new,",
        "				struct rb_root *root);",
        "",
        "static inline void rb_link_node(struct rb_node *node, struct rb_node *parent,",
        "				struct rb_node **rb_link)",
        "{",
        "	node->__rb_parent_color = (unsigned long)parent;",
        "	node->rb_left = node->rb_right = NULL;",
        "",
        "	*rb_link = node;",
        "}",
        "",
        "static inline void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,",
        "				    struct rb_node **rb_link)",
        "{",
        "	node->__rb_parent_color = (unsigned long)parent;",
        "	node->rb_left = node->rb_right = NULL;",
        "",
        "	rcu_assign_pointer(*rb_link, node);",
        "}",
        "",
        "#define rb_entry_safe(ptr, type, member) \\",
        "	({ typeof(ptr) ____ptr = (ptr); \\",
        "	   ____ptr ? rb_entry(____ptr, type, member) : NULL; \\",
        "	})",
        "",
        "/**",
        " * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of",
        " * given type allowing the backing memory of @pos to be invalidated",
        " *",
        " * @pos:	the 'type *' to use as a loop cursor.",
        " * @n:		another 'type *' to use as temporary storage",
        " * @root:	'rb_root *' of the rbtree.",
        " * @field:	the name of the rb_node field within 'type'.",
        " *",
        " * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as",
        " * list_for_each_entry_safe() and allows the iteration to continue independent",
        " * of changes to @pos by the body of the loop.",
        " *",
        " * Note, however, that it cannot handle other modifications that re-order the",
        " * rbtree it is iterating over. This includes calling rb_erase() on @pos, as",
        " * rb_erase() may rebalance the tree, causing us to miss some nodes.",
        " */",
        "#define rbtree_postorder_for_each_entry_safe(pos, n, root, field) \\",
        "	for (pos = rb_entry_safe(rb_first_postorder(root), typeof(*pos), field); \\",
        "	     pos && ({ n = rb_entry_safe(rb_next_postorder(&pos->field), \\",
        "			typeof(*pos), field); 1; }); \\",
        "	     pos = n)",
        "",
        "/* Same as rb_first(), but O(1) */",
        "#define rb_first_cached(root) (root)->rb_leftmost",
        "",
        "static inline void rb_insert_color_cached(struct rb_node *node,",
        "					  struct rb_root_cached *root,",
        "					  bool leftmost)",
        "{",
        "	if (leftmost)",
        "		root->rb_leftmost = node;",
        "	rb_insert_color(node, &root->rb_root);",
        "}",
        "",
        "",
        "static inline struct rb_node *",
        "rb_erase_cached(struct rb_node *node, struct rb_root_cached *root)",
        "{",
        "	struct rb_node *leftmost = NULL;",
        "",
        "	if (root->rb_leftmost == node)",
        "		leftmost = root->rb_leftmost = rb_next(node);",
        "",
        "	rb_erase(node, &root->rb_root);",
        "",
        "	return leftmost;",
        "}",
        "",
        "static inline void rb_replace_node_cached(struct rb_node *victim,",
        "					  struct rb_node *new,",
        "					  struct rb_root_cached *root)",
        "{",
        "	if (root->rb_leftmost == victim)",
        "		root->rb_leftmost = new;",
        "	rb_replace_node(victim, new, &root->rb_root);",
        "}",
        "",
        "/*",
        " * The below helper functions use 2 operators with 3 different",
        " * calling conventions. The operators are related like:",
        " *",
        " *	comp(a->key,b) < 0  := less(a,b)",
        " *	comp(a->key,b) > 0  := less(b,a)",
        " *	comp(a->key,b) == 0 := !less(a,b) && !less(b,a)",
        " *",
        " * If these operators define a partial order on the elements we make no",
        " * guarantee on which of the elements matching the key is found. See",
        " * rb_find().",
        " *",
        " * The reason for this is to allow the find() interface without requiring an",
        " * on-stack dummy object, which might not be feasible due to object size.",
        " */",
        "",
        "/**",
        " * rb_add_cached() - insert @node into the leftmost cached tree @tree",
        " * @node: node to insert",
        " * @tree: leftmost cached tree to insert @node into",
        " * @less: operator defining the (partial) node order",
        " *",
        " * Returns @node when it is the new leftmost, or NULL.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_add_cached(struct rb_node *node, struct rb_root_cached *tree,",
        "	      bool (*less)(struct rb_node *, const struct rb_node *))",
        "{",
        "	struct rb_node **link = &tree->rb_root.rb_node;",
        "	struct rb_node *parent = NULL;",
        "	bool leftmost = true;",
        "",
        "	while (*link) {",
        "		parent = *link;",
        "		if (less(node, parent)) {",
        "			link = &parent->rb_left;",
        "		} else {",
        "			link = &parent->rb_right;",
        "			leftmost = false;",
        "		}",
        "	}",
        "",
        "	rb_link_node(node, parent, link);",
        "	rb_insert_color_cached(node, tree, leftmost);",
        "",
        "	return leftmost ? node : NULL;",
        "}",
        "",
        "/**",
        " * rb_add() - insert @node into @tree",
        " * @node: node to insert",
        " * @tree: tree to insert @node into",
        " * @less: operator defining the (partial) node order",
        " */",
        "static __always_inline void",
        "rb_add(struct rb_node *node, struct rb_root *tree,",
        "       bool (*less)(struct rb_node *, const struct rb_node *))",
        "{",
        "	struct rb_node **link = &tree->rb_node;",
        "	struct rb_node *parent = NULL;",
        "",
        "	while (*link) {",
        "		parent = *link;",
        "		if (less(node, parent))",
        "			link = &parent->rb_left;",
        "		else",
        "			link = &parent->rb_right;",
        "	}",
        "",
        "	rb_link_node(node, parent, link);",
        "	rb_insert_color(node, tree);",
        "}",
        "",
        "/**",
        " * rb_find_add() - find equivalent @node in @tree, or add @node",
        " * @node: node to look-for / insert",
        " * @tree: tree to search / modify",
        " * @cmp: operator defining the node order",
        " *",
        " * Returns the rb_node matching @node, or NULL when no match is found and @node",
        " * is inserted.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_find_add(struct rb_node *node, struct rb_root *tree,",
        "	    int (*cmp)(struct rb_node *, const struct rb_node *))",
        "{",
        "	struct rb_node **link = &tree->rb_node;",
        "	struct rb_node *parent = NULL;",
        "	int c;",
        "",
        "	while (*link) {",
        "		parent = *link;",
        "		c = cmp(node, parent);",
        "",
        "		if (c < 0)",
        "			link = &parent->rb_left;",
        "		else if (c > 0)",
        "			link = &parent->rb_right;",
        "		else",
        "			return parent;",
        "	}",
        "",
        "	rb_link_node(node, parent, link);",
        "	rb_insert_color(node, tree);",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * rb_find_add_rcu() - find equivalent @node in @tree, or add @node",
        " * @node: node to look-for / insert",
        " * @tree: tree to search / modify",
        " * @cmp: operator defining the node order",
        " *",
        " * Adds a Store-Release for link_node.",
        " *",
        " * Returns the rb_node matching @node, or NULL when no match is found and @node",
        " * is inserted.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_find_add_rcu(struct rb_node *node, struct rb_root *tree,",
        "		int (*cmp)(struct rb_node *, const struct rb_node *))",
        "{",
        "	struct rb_node **link = &tree->rb_node;",
        "	struct rb_node *parent = NULL;",
        "	int c;",
        "",
        "	while (*link) {",
        "		parent = *link;",
        "		c = cmp(node, parent);",
        "",
        "		if (c < 0)",
        "			link = &parent->rb_left;",
        "		else if (c > 0)",
        "			link = &parent->rb_right;",
        "		else",
        "			return parent;",
        "	}",
        "",
        "	rb_link_node_rcu(node, parent, link);",
        "	rb_insert_color(node, tree);",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * rb_find() - find @key in tree @tree",
        " * @key: key to match",
        " * @tree: tree to search",
        " * @cmp: operator defining the node order",
        " *",
        " * Returns the rb_node matching @key or NULL.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_find(const void *key, const struct rb_root *tree,",
        "	int (*cmp)(const void *key, const struct rb_node *))",
        "{",
        "	struct rb_node *node = tree->rb_node;",
        "",
        "	while (node) {",
        "		int c = cmp(key, node);",
        "",
        "		if (c < 0)",
        "			node = node->rb_left;",
        "		else if (c > 0)",
        "			node = node->rb_right;",
        "		else",
        "			return node;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * rb_find_rcu() - find @key in tree @tree",
        " * @key: key to match",
        " * @tree: tree to search",
        " * @cmp: operator defining the node order",
        " *",
        " * Notably, tree descent vs concurrent tree rotations is unsound and can result",
        " * in false-negatives.",
        " *",
        " * Returns the rb_node matching @key or NULL.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_find_rcu(const void *key, const struct rb_root *tree,",
        "	    int (*cmp)(const void *key, const struct rb_node *))",
        "{",
        "	struct rb_node *node = tree->rb_node;",
        "",
        "	while (node) {",
        "		int c = cmp(key, node);",
        "",
        "		if (c < 0)",
        "			node = rcu_dereference_raw(node->rb_left);",
        "		else if (c > 0)",
        "			node = rcu_dereference_raw(node->rb_right);",
        "		else",
        "			return node;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * rb_find_first() - find the first @key in @tree",
        " * @key: key to match",
        " * @tree: tree to search",
        " * @cmp: operator defining node order",
        " *",
        " * Returns the leftmost node matching @key, or NULL.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_find_first(const void *key, const struct rb_root *tree,",
        "	      int (*cmp)(const void *key, const struct rb_node *))",
        "{",
        "	struct rb_node *node = tree->rb_node;",
        "	struct rb_node *match = NULL;",
        "",
        "	while (node) {",
        "		int c = cmp(key, node);",
        "",
        "		if (c <= 0) {",
        "			if (!c)",
        "				match = node;",
        "			node = node->rb_left;",
        "		} else if (c > 0) {",
        "			node = node->rb_right;",
        "		}",
        "	}",
        "",
        "	return match;",
        "}",
        "",
        "/**",
        " * rb_next_match() - find the next @key in @tree",
        " * @key: key to match",
        " * @tree: tree to search",
        " * @cmp: operator defining node order",
        " *",
        " * Returns the next node matching @key, or NULL.",
        " */",
        "static __always_inline struct rb_node *",
        "rb_next_match(const void *key, struct rb_node *node,",
        "	      int (*cmp)(const void *key, const struct rb_node *))",
        "{",
        "	node = rb_next(node);",
        "	if (node && cmp(key, node))",
        "		node = NULL;",
        "	return node;",
        "}",
        "",
        "/**",
        " * rb_for_each() - iterates a subtree matching @key",
        " * @node: iterator",
        " * @key: key to match",
        " * @tree: tree to search",
        " * @cmp: operator defining node order",
        " */",
        "#define rb_for_each(node, key, tree, cmp) \\",
        "	for ((node) = rb_find_first((key), (tree), (cmp)); \\",
        "	     (node); (node) = rb_next_match((key), (node), (cmp)))",
        "",
        "#endif	/* _LINUX_RBTREE_H */"
    ]
  },
  "include_linux_fsnotify_backend_h": {
    path: "include/linux/fsnotify_backend.h",
    covered: [618],
    totalLines: 900,
    coveredCount: 1,
    coveragePct: 0.1,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "/*",
        " * Filesystem access notification for Linux",
        " *",
        " *  Copyright (C) 2008 Red Hat, Inc., Eric Paris <eparis@redhat.com>",
        " */",
        "",
        "#ifndef __LINUX_FSNOTIFY_BACKEND_H",
        "#define __LINUX_FSNOTIFY_BACKEND_H",
        "",
        "#ifdef __KERNEL__",
        "",
        "#include <linux/idr.h> /* inotify uses this */",
        "#include <linux/fs.h> /* struct inode */",
        "#include <linux/list.h>",
        "#include <linux/path.h> /* struct path */",
        "#include <linux/spinlock.h>",
        "#include <linux/types.h>",
        "#include <linux/atomic.h>",
        "#include <linux/user_namespace.h>",
        "#include <linux/refcount.h>",
        "#include <linux/mempool.h>",
        "#include <linux/sched/mm.h>",
        "",
        "/*",
        " * IN_* from inotfy.h lines up EXACTLY with FS_*, this is so we can easily",
        " * convert between them.  dnotify only needs conversion at watch creation",
        " * so no perf loss there.  fanotify isn't defined yet, so it can use the",
        " * wholes if it needs more events.",
        " */",
        "#define FS_ACCESS		0x00000001	/* File was accessed */",
        "#define FS_MODIFY		0x00000002	/* File was modified */",
        "#define FS_ATTRIB		0x00000004	/* Metadata changed */",
        "#define FS_CLOSE_WRITE		0x00000008	/* Writable file was closed */",
        "#define FS_CLOSE_NOWRITE	0x00000010	/* Unwritable file closed */",
        "#define FS_OPEN			0x00000020	/* File was opened */",
        "#define FS_MOVED_FROM		0x00000040	/* File was moved from X */",
        "#define FS_MOVED_TO		0x00000080	/* File was moved to Y */",
        "#define FS_CREATE		0x00000100	/* Subfile was created */",
        "#define FS_DELETE		0x00000200	/* Subfile was deleted */",
        "#define FS_DELETE_SELF		0x00000400	/* Self was deleted */",
        "#define FS_MOVE_SELF		0x00000800	/* Self was moved */",
        "#define FS_OPEN_EXEC		0x00001000	/* File was opened for exec */",
        "",
        "#define FS_UNMOUNT		0x00002000	/* inode on umount fs */",
        "#define FS_Q_OVERFLOW		0x00004000	/* Event queued overflowed */",
        "#define FS_ERROR		0x00008000	/* Filesystem Error (fanotify) */",
        "",
        "/*",
        " * FS_IN_IGNORED overloads FS_ERROR.  It is only used internally by inotify",
        " * which does not support FS_ERROR.",
        " */",
        "#define FS_IN_IGNORED		0x00008000	/* last inotify event here */",
        "",
        "#define FS_OPEN_PERM		0x00010000	/* open event in an permission hook */",
        "#define FS_ACCESS_PERM		0x00020000	/* access event in a permissions hook */",
        "#define FS_OPEN_EXEC_PERM	0x00040000	/* open/exec event in a permission hook */",
        "",
        "/*",
        " * Set on inode mark that cares about things that happen to its children.",
        " * Always set for dnotify and inotify.",
        " * Set on inode/sb/mount marks that care about parent/name info.",
        " */",
        "#define FS_EVENT_ON_CHILD	0x08000000",
        "",
        "#define FS_RENAME		0x10000000	/* File was renamed */",
        "#define FS_DN_MULTISHOT		0x20000000	/* dnotify multishot */",
        "#define FS_ISDIR		0x40000000	/* event occurred against dir */",
        "",
        "#define FS_MOVE			(FS_MOVED_FROM | FS_MOVED_TO)",
        "",
        "/*",
        " * Directory entry modification events - reported only to directory",
        " * where entry is modified and not to a watching parent.",
        " * The watching parent may get an FS_ATTRIB|FS_EVENT_ON_CHILD event",
        " * when a directory entry inside a child subdir changes.",
        " */",
        "#define ALL_FSNOTIFY_DIRENT_EVENTS (FS_CREATE | FS_DELETE | FS_MOVE | FS_RENAME)",
        "",
        "#define ALL_FSNOTIFY_PERM_EVENTS (FS_OPEN_PERM | FS_ACCESS_PERM | \\",
        "				  FS_OPEN_EXEC_PERM)",
        "",
        "/*",
        " * This is a list of all events that may get sent to a parent that is watching",
        " * with flag FS_EVENT_ON_CHILD based on fs event on a child of that directory.",
        " */",
        "#define FS_EVENTS_POSS_ON_CHILD   (ALL_FSNOTIFY_PERM_EVENTS | \\",
        "				   FS_ACCESS | FS_MODIFY | FS_ATTRIB | \\",
        "				   FS_CLOSE_WRITE | FS_CLOSE_NOWRITE | \\",
        "				   FS_OPEN | FS_OPEN_EXEC)",
        "",
        "/*",
        " * This is a list of all events that may get sent with the parent inode as the",
        " * @to_tell argument of fsnotify().",
        " * It may include events that can be sent to an inode/sb/mount mark, but cannot",
        " * be sent to a parent watching children.",
        " */",
        "#define FS_EVENTS_POSS_TO_PARENT (FS_EVENTS_POSS_ON_CHILD)",
        "",
        "/* Events that can be reported to backends */",
        "#define ALL_FSNOTIFY_EVENTS (ALL_FSNOTIFY_DIRENT_EVENTS | \\",
        "			     FS_EVENTS_POSS_ON_CHILD | \\",
        "			     FS_DELETE_SELF | FS_MOVE_SELF | \\",
        "			     FS_UNMOUNT | FS_Q_OVERFLOW | FS_IN_IGNORED | \\",
        "			     FS_ERROR)",
        "",
        "/* Extra flags that may be reported with event or control handling of events */",
        "#define ALL_FSNOTIFY_FLAGS  (FS_ISDIR | FS_EVENT_ON_CHILD | FS_DN_MULTISHOT)",
        "",
        "#define ALL_FSNOTIFY_BITS   (ALL_FSNOTIFY_EVENTS | ALL_FSNOTIFY_FLAGS)",
        "",
        "struct fsnotify_group;",
        "struct fsnotify_event;",
        "struct fsnotify_mark;",
        "struct fsnotify_event_private_data;",
        "struct fsnotify_fname;",
        "struct fsnotify_iter_info;",
        "",
        "struct mem_cgroup;",
        "",
        "/*",
        " * Each group much define these ops.  The fsnotify infrastructure will call",
        " * these operations for each relevant group.",
        " *",
        " * handle_event - main call for a group to handle an fs event",
        " * @group:	group to notify",
        " * @mask:	event type and flags",
        " * @data:	object that event happened on",
        " * @data_type:	type of object for fanotify_data_XXX() accessors",
        " * @dir:	optional directory associated with event -",
        " *		if @file_name is not NULL, this is the directory that",
        " *		@file_name is relative to",
        " * @file_name:	optional file name associated with event",
        " * @cookie:	inotify rename cookie",
        " * @iter_info:	array of marks from this group that are interested in the event",
        " *",
        " * handle_inode_event - simple variant of handle_event() for groups that only",
        " *		have inode marks and don't have ignore mask",
        " * @mark:	mark to notify",
        " * @mask:	event type and flags",
        " * @inode:	inode that event happened on",
        " * @dir:	optional directory associated with event -",
        " *		if @file_name is not NULL, this is the directory that",
        " *		@file_name is relative to.",
        " *		Either @inode or @dir must be non-NULL.",
        " * @file_name:	optional file name associated with event",
        " * @cookie:	inotify rename cookie",
        " *",
        " * free_group_priv - called when a group refcnt hits 0 to clean up the private union",
        " * freeing_mark - called when a mark is being destroyed for some reason.  The group",
        " *		MUST be holding a reference on each mark and that reference must be",
        " *		dropped in this function.  inotify uses this function to send",
        " *		userspace messages that marks have been removed.",
        " */",
        "struct fsnotify_ops {",
        "	int (*handle_event)(struct fsnotify_group *group, u32 mask,",
        "			    const void *data, int data_type, struct inode *dir,",
        "			    const struct qstr *file_name, u32 cookie,",
        "			    struct fsnotify_iter_info *iter_info);",
        "	int (*handle_inode_event)(struct fsnotify_mark *mark, u32 mask,",
        "			    struct inode *inode, struct inode *dir,",
        "			    const struct qstr *file_name, u32 cookie);",
        "	void (*free_group_priv)(struct fsnotify_group *group);",
        "	void (*freeing_mark)(struct fsnotify_mark *mark, struct fsnotify_group *group);",
        "	void (*free_event)(struct fsnotify_group *group, struct fsnotify_event *event);",
        "	/* called on final put+free to free memory */",
        "	void (*free_mark)(struct fsnotify_mark *mark);",
        "};",
        "",
        "/*",
        " * all of the information about the original object we want to now send to",
        " * a group.  If you want to carry more info from the accessing task to the",
        " * listener this structure is where you need to be adding fields.",
        " */",
        "struct fsnotify_event {",
        "	struct list_head list;",
        "};",
        "",
        "/*",
        " * fsnotify group priorities.",
        " * Events are sent in order from highest priority to lowest priority.",
        " */",
        "enum fsnotify_group_prio {",
        "	FSNOTIFY_PRIO_NORMAL = 0,	/* normal notifiers, no permissions */",
        "	FSNOTIFY_PRIO_CONTENT,		/* fanotify permission events */",
        "	FSNOTIFY_PRIO_PRE_CONTENT,	/* fanotify pre-content events */",
        "	__FSNOTIFY_PRIO_NUM",
        "};",
        "",
        "/*",
        " * A group is a \"thing\" that wants to receive notification about filesystem",
        " * events.  The mask holds the subset of event types this group cares about.",
        " * refcnt on a group is up to the implementor and at any moment if it goes 0",
        " * everything will be cleaned up.",
        " */",
        "struct fsnotify_group {",
        "	const struct fsnotify_ops *ops;	/* how this group handles things */",
        "",
        "	/*",
        "	 * How the refcnt is used is up to each group.  When the refcnt hits 0",
        "	 * fsnotify will clean up all of the resources associated with this group.",
        "	 * As an example, the dnotify group will always have a refcnt=1 and that",
        "	 * will never change.  Inotify, on the other hand, has a group per",
        "	 * inotify_init() and the refcnt will hit 0 only when that fd has been",
        "	 * closed.",
        "	 */",
        "	refcount_t refcnt;		/* things with interest in this group */",
        "",
        "	/* needed to send notification to userspace */",
        "	spinlock_t notification_lock;		/* protect the notification_list */",
        "	struct list_head notification_list;	/* list of event_holder this group needs to send to userspace */",
        "	wait_queue_head_t notification_waitq;	/* read() on the notification file blocks on this waitq */",
        "	unsigned int q_len;			/* events on the queue */",
        "	unsigned int max_events;		/* maximum events allowed on the list */",
        "	enum fsnotify_group_prio priority;	/* priority for sending events */",
        "	bool shutdown;		/* group is being shut down, don't queue more events */",
        "",
        "#define FSNOTIFY_GROUP_USER	0x01 /* user allocated group */",
        "#define FSNOTIFY_GROUP_DUPS	0x02 /* allow multiple marks per object */",
        "	int flags;",
        "	unsigned int owner_flags;	/* stored flags of mark_mutex owner */",
        "",
        "	/* stores all fastpath marks assoc with this group so they can be cleaned on unregister */",
        "	struct mutex mark_mutex;	/* protect marks_list */",
        "	atomic_t user_waits;		/* Number of tasks waiting for user",
        "					 * response */",
        "	struct list_head marks_list;	/* all inode marks for this group */",
        "",
        "	struct fasync_struct *fsn_fa;    /* async notification */",
        "",
        "	struct fsnotify_event *overflow_event;	/* Event we queue when the",
        "						 * notification list is too",
        "						 * full */",
        "",
        "	struct mem_cgroup *memcg;	/* memcg to charge allocations */",
        "",
        "	/* groups can define private fields here or use the void *private */",
        "	union {",
        "		void *private;",
        "#ifdef CONFIG_INOTIFY_USER",
        "		struct inotify_group_private_data {",
        "			spinlock_t	idr_lock;",
        "			struct idr      idr;",
        "			struct ucounts *ucounts;",
        "		} inotify_data;",
        "#endif",
        "#ifdef CONFIG_FANOTIFY",
        "		struct fanotify_group_private_data {",
        "			/* Hash table of events for merge */",
        "			struct hlist_head *merge_hash;",
        "			/* allows a group to block waiting for a userspace response */",
        "			struct list_head access_list;",
        "			wait_queue_head_t access_waitq;",
        "			int flags;           /* flags from fanotify_init() */",
        "			int f_flags; /* event_f_flags from fanotify_init() */",
        "			struct ucounts *ucounts;",
        "			mempool_t error_events_pool;",
        "		} fanotify_data;",
        "#endif /* CONFIG_FANOTIFY */",
        "	};",
        "};",
        "",
        "/*",
        " * These helpers are used to prevent deadlock when reclaiming inodes with",
        " * evictable marks of the same group that is allocating a new mark.",
        " */",
        "static inline void fsnotify_group_lock(struct fsnotify_group *group)",
        "{",
        "	mutex_lock(&group->mark_mutex);",
        "	group->owner_flags = memalloc_nofs_save();",
        "}",
        "",
        "static inline void fsnotify_group_unlock(struct fsnotify_group *group)",
        "{",
        "	memalloc_nofs_restore(group->owner_flags);",
        "	mutex_unlock(&group->mark_mutex);",
        "}",
        "",
        "static inline void fsnotify_group_assert_locked(struct fsnotify_group *group)",
        "{",
        "	WARN_ON_ONCE(!mutex_is_locked(&group->mark_mutex));",
        "	WARN_ON_ONCE(!(current->flags & PF_MEMALLOC_NOFS));",
        "}",
        "",
        "/* When calling fsnotify tell it if the data is a path or inode */",
        "enum fsnotify_data_type {",
        "	FSNOTIFY_EVENT_NONE,",
        "	FSNOTIFY_EVENT_PATH,",
        "	FSNOTIFY_EVENT_INODE,",
        "	FSNOTIFY_EVENT_DENTRY,",
        "	FSNOTIFY_EVENT_ERROR,",
        "};",
        "",
        "struct fs_error_report {",
        "	int error;",
        "	struct inode *inode;",
        "	struct super_block *sb;",
        "};",
        "",
        "static inline struct inode *fsnotify_data_inode(const void *data, int data_type)",
        "{",
        "	switch (data_type) {",
        "	case FSNOTIFY_EVENT_INODE:",
        "		return (struct inode *)data;",
        "	case FSNOTIFY_EVENT_DENTRY:",
        "		return d_inode(data);",
        "	case FSNOTIFY_EVENT_PATH:",
        "		return d_inode(((const struct path *)data)->dentry);",
        "	case FSNOTIFY_EVENT_ERROR:",
        "		return ((struct fs_error_report *)data)->inode;",
        "	default:",
        "		return NULL;",
        "	}",
        "}",
        "",
        "static inline struct dentry *fsnotify_data_dentry(const void *data, int data_type)",
        "{",
        "	switch (data_type) {",
        "	case FSNOTIFY_EVENT_DENTRY:",
        "		/* Non const is needed for dget() */",
        "		return (struct dentry *)data;",
        "	case FSNOTIFY_EVENT_PATH:",
        "		return ((const struct path *)data)->dentry;",
        "	default:",
        "		return NULL;",
        "	}",
        "}",
        "",
        "static inline const struct path *fsnotify_data_path(const void *data,",
        "						    int data_type)",
        "{",
        "	switch (data_type) {",
        "	case FSNOTIFY_EVENT_PATH:",
        "		return data;",
        "	default:",
        "		return NULL;",
        "	}",
        "}",
        "",
        "static inline struct super_block *fsnotify_data_sb(const void *data,",
        "						   int data_type)",
        "{",
        "	switch (data_type) {",
        "	case FSNOTIFY_EVENT_INODE:",
        "		return ((struct inode *)data)->i_sb;",
        "	case FSNOTIFY_EVENT_DENTRY:",
        "		return ((struct dentry *)data)->d_sb;",
        "	case FSNOTIFY_EVENT_PATH:",
        "		return ((const struct path *)data)->dentry->d_sb;",
        "	case FSNOTIFY_EVENT_ERROR:",
        "		return ((struct fs_error_report *) data)->sb;",
        "	default:",
        "		return NULL;",
        "	}",
        "}",
        "",
        "static inline struct fs_error_report *fsnotify_data_error_report(",
        "							const void *data,",
        "							int data_type)",
        "{",
        "	switch (data_type) {",
        "	case FSNOTIFY_EVENT_ERROR:",
        "		return (struct fs_error_report *) data;",
        "	default:",
        "		return NULL;",
        "	}",
        "}",
        "",
        "/*",
        " * Index to merged marks iterator array that correlates to a type of watch.",
        " * The type of watched object can be deduced from the iterator type, but not",
        " * the other way around, because an event can match different watched objects",
        " * of the same object type.",
        " * For example, both parent and child are watching an object of type inode.",
        " */",
        "enum fsnotify_iter_type {",
        "	FSNOTIFY_ITER_TYPE_INODE,",
        "	FSNOTIFY_ITER_TYPE_VFSMOUNT,",
        "	FSNOTIFY_ITER_TYPE_SB,",
        "	FSNOTIFY_ITER_TYPE_PARENT,",
        "	FSNOTIFY_ITER_TYPE_INODE2,",
        "	FSNOTIFY_ITER_TYPE_COUNT",
        "};",
        "",
        "/* The type of object that a mark is attached to */",
        "enum fsnotify_obj_type {",
        "	FSNOTIFY_OBJ_TYPE_ANY = -1,",
        "	FSNOTIFY_OBJ_TYPE_INODE,",
        "	FSNOTIFY_OBJ_TYPE_VFSMOUNT,",
        "	FSNOTIFY_OBJ_TYPE_SB,",
        "	FSNOTIFY_OBJ_TYPE_COUNT,",
        "	FSNOTIFY_OBJ_TYPE_DETACHED = FSNOTIFY_OBJ_TYPE_COUNT",
        "};",
        "",
        "static inline bool fsnotify_valid_obj_type(unsigned int obj_type)",
        "{",
        "	return (obj_type < FSNOTIFY_OBJ_TYPE_COUNT);",
        "}",
        "",
        "struct fsnotify_iter_info {",
        "	struct fsnotify_mark *marks[FSNOTIFY_ITER_TYPE_COUNT];",
        "	struct fsnotify_group *current_group;",
        "	unsigned int report_mask;",
        "	int srcu_idx;",
        "};",
        "",
        "static inline bool fsnotify_iter_should_report_type(",
        "		struct fsnotify_iter_info *iter_info, int iter_type)",
        "{",
        "	return (iter_info->report_mask & (1U << iter_type));",
        "}",
        "",
        "static inline void fsnotify_iter_set_report_type(",
        "		struct fsnotify_iter_info *iter_info, int iter_type)",
        "{",
        "	iter_info->report_mask |= (1U << iter_type);",
        "}",
        "",
        "static inline struct fsnotify_mark *fsnotify_iter_mark(",
        "		struct fsnotify_iter_info *iter_info, int iter_type)",
        "{",
        "	if (fsnotify_iter_should_report_type(iter_info, iter_type))",
        "		return iter_info->marks[iter_type];",
        "	return NULL;",
        "}",
        "",
        "static inline int fsnotify_iter_step(struct fsnotify_iter_info *iter, int type,",
        "				     struct fsnotify_mark **markp)",
        "{",
        "	while (type < FSNOTIFY_ITER_TYPE_COUNT) {",
        "		*markp = fsnotify_iter_mark(iter, type);",
        "		if (*markp)",
        "			break;",
        "		type++;",
        "	}",
        "	return type;",
        "}",
        "",
        "#define FSNOTIFY_ITER_FUNCS(name, NAME) \\",
        "static inline struct fsnotify_mark *fsnotify_iter_##name##_mark( \\",
        "		struct fsnotify_iter_info *iter_info) \\",
        "{ \\",
        "	return fsnotify_iter_mark(iter_info, FSNOTIFY_ITER_TYPE_##NAME); \\",
        "}",
        "",
        "FSNOTIFY_ITER_FUNCS(inode, INODE)",
        "FSNOTIFY_ITER_FUNCS(parent, PARENT)",
        "FSNOTIFY_ITER_FUNCS(vfsmount, VFSMOUNT)",
        "FSNOTIFY_ITER_FUNCS(sb, SB)",
        "",
        "#define fsnotify_foreach_iter_type(type) \\",
        "	for (type = 0; type < FSNOTIFY_ITER_TYPE_COUNT; type++)",
        "#define fsnotify_foreach_iter_mark_type(iter, mark, type) \\",
        "	for (type = 0; \\",
        "	     type = fsnotify_iter_step(iter, type, &mark), \\",
        "	     type < FSNOTIFY_ITER_TYPE_COUNT; \\",
        "	     type++)",
        "",
        "/*",
        " * Inode/vfsmount/sb point to this structure which tracks all marks attached to",
        " * the inode/vfsmount/sb. The reference to inode/vfsmount/sb is held by this",
        " * structure. We destroy this structure when there are no more marks attached",
        " * to it. The structure is protected by fsnotify_mark_srcu.",
        " */",
        "struct fsnotify_mark_connector {",
        "	spinlock_t lock;",
        "	unsigned char type;	/* Type of object [lock] */",
        "	unsigned char prio;	/* Highest priority group */",
        "#define FSNOTIFY_CONN_FLAG_IS_WATCHED	0x01",
        "#define FSNOTIFY_CONN_FLAG_HAS_IREF	0x02",
        "	unsigned short flags;	/* flags [lock] */",
        "	union {",
        "		/* Object pointer [lock] */",
        "		void *obj;",
        "		/* Used listing heads to free after srcu period expires */",
        "		struct fsnotify_mark_connector *destroy_next;",
        "	};",
        "	struct hlist_head list;",
        "};",
        "",
        "/*",
        " * Container for per-sb fsnotify state (sb marks and more).",
        " * Attached lazily on first marked object on the sb and freed when killing sb.",
        " */",
        "struct fsnotify_sb_info {",
        "	struct fsnotify_mark_connector __rcu *sb_marks;",
        "	/*",
        "	 * Number of inode/mount/sb objects that are being watched in this sb.",
        "	 * Note that inodes objects are currently double-accounted.",
        "	 *",
        "	 * The value in watched_objects[prio] is the number of objects that are",
        "	 * watched by groups of priority >= prio, so watched_objects[0] is the",
        "	 * total number of watched objects in this sb.",
        "	 */",
        "	atomic_long_t watched_objects[__FSNOTIFY_PRIO_NUM];",
        "};",
        "",
        "static inline struct fsnotify_sb_info *fsnotify_sb_info(struct super_block *sb)",
        "{",
        "#ifdef CONFIG_FSNOTIFY",
        "	return READ_ONCE(sb->s_fsnotify_info);",
        "#else",
        "	return NULL;",
        "#endif",
        "}",
        "",
        "static inline atomic_long_t *fsnotify_sb_watched_objects(struct super_block *sb)",
        "{",
        "	return &fsnotify_sb_info(sb)->watched_objects[0];",
        "}",
        "",
        "/*",
        " * A mark is simply an object attached to an in core inode which allows an",
        " * fsnotify listener to indicate they are either no longer interested in events",
        " * of a type matching mask or only interested in those events.",
        " *",
        " * These are flushed when an inode is evicted from core and may be flushed",
        " * when the inode is modified (as seen by fsnotify_access).  Some fsnotify",
        " * users (such as dnotify) will flush these when the open fd is closed and not",
        " * at inode eviction or modification.",
        " *",
        " * Text in brackets is showing the lock(s) protecting modifications of a",
        " * particular entry. obj_lock means either inode->i_lock or",
        " * mnt->mnt_root->d_lock depending on the mark type.",
        " */",
        "struct fsnotify_mark {",
        "	/* Mask this mark is for [mark->lock, group->mark_mutex] */",
        "	__u32 mask;",
        "	/* We hold one for presence in g_list. Also one ref for each 'thing'",
        "	 * in kernel that found and may be using this mark. */",
        "	refcount_t refcnt;",
        "	/* Group this mark is for. Set on mark creation, stable until last ref",
        "	 * is dropped */",
        "	struct fsnotify_group *group;",
        "	/* List of marks by group->marks_list. Also reused for queueing",
        "	 * mark into destroy_list when it's waiting for the end of SRCU period",
        "	 * before it can be freed. [group->mark_mutex] */",
        "	struct list_head g_list;",
        "	/* Protects inode / mnt pointers, flags, masks */",
        "	spinlock_t lock;",
        "	/* List of marks for inode / vfsmount [connector->lock, mark ref] */",
        "	struct hlist_node obj_list;",
        "	/* Head of list of marks for an object [mark ref] */",
        "	struct fsnotify_mark_connector *connector;",
        "	/* Events types and flags to ignore [mark->lock, group->mark_mutex] */",
        "	__u32 ignore_mask;",
        "	/* General fsnotify mark flags */",
        "#define FSNOTIFY_MARK_FLAG_ALIVE		0x0001",
        "#define FSNOTIFY_MARK_FLAG_ATTACHED		0x0002",
        "	/* inotify mark flags */",
        "#define FSNOTIFY_MARK_FLAG_EXCL_UNLINK		0x0010",
        "#define FSNOTIFY_MARK_FLAG_IN_ONESHOT		0x0020",
        "	/* fanotify mark flags */",
        "#define FSNOTIFY_MARK_FLAG_IGNORED_SURV_MODIFY	0x0100",
        "#define FSNOTIFY_MARK_FLAG_NO_IREF		0x0200",
        "#define FSNOTIFY_MARK_FLAG_HAS_IGNORE_FLAGS	0x0400",
        "#define FSNOTIFY_MARK_FLAG_HAS_FSID		0x0800",
        "#define FSNOTIFY_MARK_FLAG_WEAK_FSID		0x1000",
        "	unsigned int flags;		/* flags [mark->lock] */",
        "};",
        "",
        "#ifdef CONFIG_FSNOTIFY",
        "",
        "/* called from the vfs helpers */",
        "",
        "/* main fsnotify call to send events */",
        "extern int fsnotify(__u32 mask, const void *data, int data_type,",
        "		    struct inode *dir, const struct qstr *name,",
        "		    struct inode *inode, u32 cookie);",
        "extern int __fsnotify_parent(struct dentry *dentry, __u32 mask, const void *data,",
        "			   int data_type);",
        "extern void __fsnotify_inode_delete(struct inode *inode);",
        "extern void __fsnotify_vfsmount_delete(struct vfsmount *mnt);",
        "extern void fsnotify_sb_delete(struct super_block *sb);",
        "extern void fsnotify_sb_free(struct super_block *sb);",
        "extern u32 fsnotify_get_cookie(void);",
        "",
        "static inline __u32 fsnotify_parent_needed_mask(__u32 mask)",
        "{",
        "	/* FS_EVENT_ON_CHILD is set on marks that want parent/name info */",
        "	if (!(mask & FS_EVENT_ON_CHILD))",
        "		return 0;",
        "	/*",
        "	 * This object might be watched by a mark that cares about parent/name",
        "	 * info, does it care about the specific set of events that can be",
        "	 * reported with parent/name info?",
        "	 */",
        "	return mask & FS_EVENTS_POSS_TO_PARENT;",
        "}",
        "",
        "static inline int fsnotify_inode_watches_children(struct inode *inode)",
        "{",
        "	__u32 parent_mask = READ_ONCE(inode->i_fsnotify_mask);",
        "",
        "	/* FS_EVENT_ON_CHILD is set if the inode may care */",
        "	if (!(parent_mask & FS_EVENT_ON_CHILD))",
        "		return 0;",
        "	/* this inode might care about child events, does it care about the",
        "	 * specific set of events that can happen on a child? */",
        "	return parent_mask & FS_EVENTS_POSS_ON_CHILD;",
        "}",
        "",
        "/*",
        " * Update the dentry with a flag indicating the interest of its parent to receive",
        " * filesystem events when those events happens to this dentry->d_inode.",
        " */",
        "static inline void fsnotify_update_flags(struct dentry *dentry)",
        "{",
        "	assert_spin_locked(&dentry->d_lock);",
        "",
        "	/*",
        "	 * Serialisation of setting PARENT_WATCHED on the dentries is provided",
        "	 * by d_lock. If inotify_inode_watched changes after we have taken",
        "	 * d_lock, the following fsnotify_set_children_dentry_flags call will",
        "	 * find our entry, so it will spin until we complete here, and update",
        "	 * us with the new state.",
        "	 */",
        "	if (fsnotify_inode_watches_children(dentry->d_parent->d_inode))",
        "		dentry->d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED;",
        "	else",
        "		dentry->d_flags &= ~DCACHE_FSNOTIFY_PARENT_WATCHED;",
        "}",
        "",
        "/* called from fsnotify listeners, such as fanotify or dnotify */",
        "",
        "/* create a new group */",
        "extern struct fsnotify_group *fsnotify_alloc_group(",
        "				const struct fsnotify_ops *ops,",
        "				int flags);",
        "/* get reference to a group */",
        "extern void fsnotify_get_group(struct fsnotify_group *group);",
        "/* drop reference on a group from fsnotify_alloc_group */",
        "extern void fsnotify_put_group(struct fsnotify_group *group);",
        "/* group destruction begins, stop queuing new events */",
        "extern void fsnotify_group_stop_queueing(struct fsnotify_group *group);",
        "/* destroy group */",
        "extern void fsnotify_destroy_group(struct fsnotify_group *group);",
        "/* fasync handler function */",
        "extern int fsnotify_fasync(int fd, struct file *file, int on);",
        "/* Free event from memory */",
        "extern void fsnotify_destroy_event(struct fsnotify_group *group,",
        "				   struct fsnotify_event *event);",
        "/* attach the event to the group notification queue */",
        "extern int fsnotify_insert_event(struct fsnotify_group *group,",
        "				 struct fsnotify_event *event,",
        "				 int (*merge)(struct fsnotify_group *,",
        "					      struct fsnotify_event *),",
        "				 void (*insert)(struct fsnotify_group *,",
        "						struct fsnotify_event *));",
        "",
        "static inline int fsnotify_add_event(struct fsnotify_group *group,",
        "				     struct fsnotify_event *event,",
        "				     int (*merge)(struct fsnotify_group *,",
        "						  struct fsnotify_event *))",
        "{",
        "	return fsnotify_insert_event(group, event, merge, NULL);",
        "}",
        "",
        "/* Queue overflow event to a notification group */",
        "static inline void fsnotify_queue_overflow(struct fsnotify_group *group)",
        "{",
        "	fsnotify_add_event(group, group->overflow_event, NULL);",
        "}",
        "",
        "static inline bool fsnotify_is_overflow_event(u32 mask)",
        "{",
        "	return mask & FS_Q_OVERFLOW;",
        "}",
        "",
        "static inline bool fsnotify_notify_queue_is_empty(struct fsnotify_group *group)",
        "{",
        "	assert_spin_locked(&group->notification_lock);",
        "",
        "	return list_empty(&group->notification_list);",
        "}",
        "",
        "extern bool fsnotify_notify_queue_is_empty(struct fsnotify_group *group);",
        "/* return, but do not dequeue the first event on the notification queue */",
        "extern struct fsnotify_event *fsnotify_peek_first_event(struct fsnotify_group *group);",
        "/* return AND dequeue the first event on the notification queue */",
        "extern struct fsnotify_event *fsnotify_remove_first_event(struct fsnotify_group *group);",
        "/* Remove event queued in the notification list */",
        "extern void fsnotify_remove_queued_event(struct fsnotify_group *group,",
        "					 struct fsnotify_event *event);",
        "",
        "/* functions used to manipulate the marks attached to inodes */",
        "",
        "/*",
        " * Canonical \"ignore mask\" including event flags.",
        " *",
        " * Note the subtle semantic difference from the legacy ->ignored_mask.",
        " * ->ignored_mask traditionally only meant which events should be ignored,",
        " * while ->ignore_mask also includes flags regarding the type of objects on",
        " * which events should be ignored.",
        " */",
        "static inline __u32 fsnotify_ignore_mask(struct fsnotify_mark *mark)",
        "{",
        "	__u32 ignore_mask = mark->ignore_mask;",
        "",
        "	/* The event flags in ignore mask take effect */",
        "	if (mark->flags & FSNOTIFY_MARK_FLAG_HAS_IGNORE_FLAGS)",
        "		return ignore_mask;",
        "",
        "	/*",
        "	 * Legacy behavior:",
        "	 * - Always ignore events on dir",
        "	 * - Ignore events on child if parent is watching children",
        "	 */",
        "	ignore_mask |= FS_ISDIR;",
        "	ignore_mask &= ~FS_EVENT_ON_CHILD;",
        "	ignore_mask |= mark->mask & FS_EVENT_ON_CHILD;",
        "",
        "	return ignore_mask;",
        "}",
        "",
        "/* Legacy ignored_mask - only event types to ignore */",
        "static inline __u32 fsnotify_ignored_events(struct fsnotify_mark *mark)",
        "{",
        "	return mark->ignore_mask & ALL_FSNOTIFY_EVENTS;",
        "}",
        "",
        "/*",
        " * Check if mask (or ignore mask) should be applied depending if victim is a",
        " * directory and whether it is reported to a watching parent.",
        " */",
        "static inline bool fsnotify_mask_applicable(__u32 mask, bool is_dir,",
        "					    int iter_type)",
        "{",
        "	/* Should mask be applied to a directory? */",
        "	if (is_dir && !(mask & FS_ISDIR))",
        "		return false;",
        "",
        "	/* Should mask be applied to a child? */",
        "	if (iter_type == FSNOTIFY_ITER_TYPE_PARENT &&",
        "	    !(mask & FS_EVENT_ON_CHILD))",
        "		return false;",
        "",
        "	return true;",
        "}",
        "",
        "/*",
        " * Effective ignore mask taking into account if event victim is a",
        " * directory and whether it is reported to a watching parent.",
        " */",
        "static inline __u32 fsnotify_effective_ignore_mask(struct fsnotify_mark *mark,",
        "						   bool is_dir, int iter_type)",
        "{",
        "	__u32 ignore_mask = fsnotify_ignored_events(mark);",
        "",
        "	if (!ignore_mask)",
        "		return 0;",
        "",
        "	/* For non-dir and non-child, no need to consult the event flags */",
        "	if (!is_dir && iter_type != FSNOTIFY_ITER_TYPE_PARENT)",
        "		return ignore_mask;",
        "",
        "	ignore_mask = fsnotify_ignore_mask(mark);",
        "	if (!fsnotify_mask_applicable(ignore_mask, is_dir, iter_type))",
        "		return 0;",
        "",
        "	return ignore_mask & ALL_FSNOTIFY_EVENTS;",
        "}",
        "",
        "/* Get mask for calculating object interest taking ignore mask into account */",
        "static inline __u32 fsnotify_calc_mask(struct fsnotify_mark *mark)",
        "{",
        "	__u32 mask = mark->mask;",
        "",
        "	if (!fsnotify_ignored_events(mark))",
        "		return mask;",
        "",
        "	/* Interest in FS_MODIFY may be needed for clearing ignore mask */",
        "	if (!(mark->flags & FSNOTIFY_MARK_FLAG_IGNORED_SURV_MODIFY))",
        "		mask |= FS_MODIFY;",
        "",
        "	/*",
        "	 * If mark is interested in ignoring events on children, the object must",
        "	 * show interest in those events for fsnotify_parent() to notice it.",
        "	 */",
        "	return mask | mark->ignore_mask;",
        "}",
        "",
        "/* Get mask of events for a list of marks */",
        "extern __u32 fsnotify_conn_mask(struct fsnotify_mark_connector *conn);",
        "/* Calculate mask of events for a list of marks */",
        "extern void fsnotify_recalc_mask(struct fsnotify_mark_connector *conn);",
        "extern void fsnotify_init_mark(struct fsnotify_mark *mark,",
        "			       struct fsnotify_group *group);",
        "/* Find mark belonging to given group in the list of marks */",
        "struct fsnotify_mark *fsnotify_find_mark(void *obj, unsigned int obj_type,",
        "					 struct fsnotify_group *group);",
        "/* attach the mark to the object */",
        "int fsnotify_add_mark(struct fsnotify_mark *mark, void *obj,",
        "		      unsigned int obj_type, int add_flags);",
        "int fsnotify_add_mark_locked(struct fsnotify_mark *mark, void *obj,",
        "			     unsigned int obj_type, int add_flags);",
        "",
        "/* attach the mark to the inode */",
        "static inline int fsnotify_add_inode_mark(struct fsnotify_mark *mark,",
        "					  struct inode *inode,",
        "					  int add_flags)",
        "{",
        "	return fsnotify_add_mark(mark, inode, FSNOTIFY_OBJ_TYPE_INODE,",
        "				 add_flags);",
        "}",
        "static inline int fsnotify_add_inode_mark_locked(struct fsnotify_mark *mark,",
        "						 struct inode *inode,",
        "						 int add_flags)",
        "{",
        "	return fsnotify_add_mark_locked(mark, inode, FSNOTIFY_OBJ_TYPE_INODE,",
        "					add_flags);",
        "}",
        "",
        "static inline struct fsnotify_mark *fsnotify_find_inode_mark(",
        "						struct inode *inode,",
        "						struct fsnotify_group *group)",
        "{",
        "	return fsnotify_find_mark(inode, FSNOTIFY_OBJ_TYPE_INODE, group);",
        "}",
        "",
        "/* given a group and a mark, flag mark to be freed when all references are dropped */",
        "extern void fsnotify_destroy_mark(struct fsnotify_mark *mark,",
        "				  struct fsnotify_group *group);",
        "/* detach mark from inode / mount list, group list, drop inode reference */",
        "extern void fsnotify_detach_mark(struct fsnotify_mark *mark);",
        "/* free mark */",
        "extern void fsnotify_free_mark(struct fsnotify_mark *mark);",
        "/* Wait until all marks queued for destruction are destroyed */",
        "extern void fsnotify_wait_marks_destroyed(void);",
        "/* Clear all of the marks of a group attached to a given object type */",
        "extern void fsnotify_clear_marks_by_group(struct fsnotify_group *group,",
        "					  unsigned int obj_type);",
        "/* run all the marks in a group, and clear all of the vfsmount marks */",
        "static inline void fsnotify_clear_vfsmount_marks_by_group(struct fsnotify_group *group)",
        "{",
        "	fsnotify_clear_marks_by_group(group, FSNOTIFY_OBJ_TYPE_VFSMOUNT);",
        "}",
        "/* run all the marks in a group, and clear all of the inode marks */",
        "static inline void fsnotify_clear_inode_marks_by_group(struct fsnotify_group *group)",
        "{",
        "	fsnotify_clear_marks_by_group(group, FSNOTIFY_OBJ_TYPE_INODE);",
        "}",
        "/* run all the marks in a group, and clear all of the sn marks */",
        "static inline void fsnotify_clear_sb_marks_by_group(struct fsnotify_group *group)",
        "{",
        "	fsnotify_clear_marks_by_group(group, FSNOTIFY_OBJ_TYPE_SB);",
        "}",
        "extern void fsnotify_get_mark(struct fsnotify_mark *mark);",
        "extern void fsnotify_put_mark(struct fsnotify_mark *mark);",
        "extern void fsnotify_finish_user_wait(struct fsnotify_iter_info *iter_info);",
        "extern bool fsnotify_prepare_user_wait(struct fsnotify_iter_info *iter_info);",
        "",
        "static inline void fsnotify_init_event(struct fsnotify_event *event)",
        "{",
        "	INIT_LIST_HEAD(&event->list);",
        "}",
        "",
        "#else",
        "",
        "static inline int fsnotify(__u32 mask, const void *data, int data_type,",
        "			   struct inode *dir, const struct qstr *name,",
        "			   struct inode *inode, u32 cookie)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline int __fsnotify_parent(struct dentry *dentry, __u32 mask,",
        "				  const void *data, int data_type)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void __fsnotify_inode_delete(struct inode *inode)",
        "{}",
        "",
        "static inline void __fsnotify_vfsmount_delete(struct vfsmount *mnt)",
        "{}",
        "",
        "static inline void fsnotify_sb_delete(struct super_block *sb)",
        "{}",
        "",
        "static inline void fsnotify_sb_free(struct super_block *sb)",
        "{}",
        "",
        "static inline void fsnotify_update_flags(struct dentry *dentry)",
        "{}",
        "",
        "static inline u32 fsnotify_get_cookie(void)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void fsnotify_unmount_inodes(struct super_block *sb)",
        "{}",
        "",
        "#endif	/* CONFIG_FSNOTIFY */",
        "",
        "#endif	/* __KERNEL __ */",
        "",
        "#endif	/* __LINUX_FSNOTIFY_BACKEND_H */"
    ]
  },
  "arch_x86_include_asm_cpufeature_h": {
    path: "arch/x86/include/asm/cpufeature.h",
    covered: [178, 191],
    totalLines: 218,
    coveredCount: 2,
    coveragePct: 0.9,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _ASM_X86_CPUFEATURE_H",
        "#define _ASM_X86_CPUFEATURE_H",
        "",
        "#include <asm/processor.h>",
        "",
        "#if defined(__KERNEL__) && !defined(__ASSEMBLY__)",
        "",
        "#include <asm/asm.h>",
        "#include <linux/bitops.h>",
        "#include <asm/alternative.h>",
        "",
        "enum cpuid_leafs",
        "{",
        "	CPUID_1_EDX		= 0,",
        "	CPUID_8000_0001_EDX,",
        "	CPUID_8086_0001_EDX,",
        "	CPUID_LNX_1,",
        "	CPUID_1_ECX,",
        "	CPUID_C000_0001_EDX,",
        "	CPUID_8000_0001_ECX,",
        "	CPUID_LNX_2,",
        "	CPUID_LNX_3,",
        "	CPUID_7_0_EBX,",
        "	CPUID_D_1_EAX,",
        "	CPUID_LNX_4,",
        "	CPUID_7_1_EAX,",
        "	CPUID_8000_0008_EBX,",
        "	CPUID_6_EAX,",
        "	CPUID_8000_000A_EDX,",
        "	CPUID_7_ECX,",
        "	CPUID_8000_0007_EBX,",
        "	CPUID_7_EDX,",
        "	CPUID_8000_001F_EAX,",
        "	CPUID_8000_0021_EAX,",
        "	CPUID_LNX_5,",
        "	NR_CPUID_WORDS,",
        "};",
        "",
        "#define X86_CAP_FMT_NUM \"%d:%d\"",
        "#define x86_cap_flag_num(flag) ((flag) >> 5), ((flag) & 31)",
        "",
        "extern const char * const x86_cap_flags[NCAPINTS*32];",
        "extern const char * const x86_power_flags[32];",
        "#define X86_CAP_FMT \"%s\"",
        "#define x86_cap_flag(flag) x86_cap_flags[flag]",
        "",
        "/*",
        " * In order to save room, we index into this array by doing",
        " * X86_BUG_<name> - NCAPINTS*32.",
        " */",
        "extern const char * const x86_bug_flags[NBUGINTS*32];",
        "",
        "#define test_cpu_cap(c, bit)						\\",
        "	 arch_test_bit(bit, (unsigned long *)((c)->x86_capability))",
        "",
        "/*",
        " * There are 32 bits/features in each mask word.  The high bits",
        " * (selected with (bit>>5) give us the word number and the low 5",
        " * bits give us the bit/feature number inside the word.",
        " * (1UL<<((bit)&31) gives us a mask for the feature_bit so we can",
        " * see if it is set in the mask word.",
        " */",
        "#define CHECK_BIT_IN_MASK_WORD(maskname, word, bit)	\\",
        "	(((bit)>>5)==(word) && (1UL<<((bit)&31) & maskname##word ))",
        "",
        "/*",
        " * {REQUIRED,DISABLED}_MASK_CHECK below may seem duplicated with the",
        " * following BUILD_BUG_ON_ZERO() check but when NCAPINTS gets changed, all",
        " * header macros which use NCAPINTS need to be changed. The duplicated macro",
        " * use causes the compiler to issue errors for all headers so that all usage",
        " * sites can be corrected.",
        " */",
        "#define REQUIRED_MASK_BIT_SET(feature_bit)		\\",
        "	 ( CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  0, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  1, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  2, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  3, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  4, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  5, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  6, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  7, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  8, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK,  9, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 10, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 11, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 12, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 13, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 14, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 15, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 16, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 17, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 18, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 19, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 20, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 21, feature_bit) ||	\\",
        "	   REQUIRED_MASK_CHECK					  ||	\\",
        "	   BUILD_BUG_ON_ZERO(NCAPINTS != 22))",
        "",
        "#define DISABLED_MASK_BIT_SET(feature_bit)				\\",
        "	 ( CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  0, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  1, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  2, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  3, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  4, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  5, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  6, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  7, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  8, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  9, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 10, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 11, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 12, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 13, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 14, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 15, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 16, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 17, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 18, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 19, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 20, feature_bit) ||	\\",
        "	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 21, feature_bit) ||	\\",
        "	   DISABLED_MASK_CHECK					  ||	\\",
        "	   BUILD_BUG_ON_ZERO(NCAPINTS != 22))",
        "",
        "#define cpu_has(c, bit)							\\",
        "	(__builtin_constant_p(bit) && REQUIRED_MASK_BIT_SET(bit) ? 1 :	\\",
        "	 test_cpu_cap(c, bit))",
        "",
        "#define this_cpu_has(bit)						\\",
        "	(__builtin_constant_p(bit) && REQUIRED_MASK_BIT_SET(bit) ? 1 :	\\",
        "	 x86_this_cpu_test_bit(bit, cpu_info.x86_capability))",
        "",
        "/*",
        " * This macro is for detection of features which need kernel",
        " * infrastructure to be used.  It may *not* directly test the CPU",
        " * itself.  Use the cpu_has() family if you want true runtime",
        " * testing of CPU features, like in hypervisor code where you are",
        " * supporting a possible guest feature where host support for it",
        " * is not relevant.",
        " */",
        "#define cpu_feature_enabled(bit)	\\",
        "	(__builtin_constant_p(bit) && DISABLED_MASK_BIT_SET(bit) ? 0 : static_cpu_has(bit))",
        "",
        "#define boot_cpu_has(bit)	cpu_has(&boot_cpu_data, bit)",
        "",
        "#define set_cpu_cap(c, bit)	set_bit(bit, (unsigned long *)((c)->x86_capability))",
        "",
        "extern void setup_clear_cpu_cap(unsigned int bit);",
        "extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);",
        "",
        "#define setup_force_cpu_cap(bit) do {			\\",
        "							\\",
        "	if (!boot_cpu_has(bit))				\\",
        "		WARN_ON(alternatives_patched);		\\",
        "							\\",
        "	set_cpu_cap(&boot_cpu_data, bit);		\\",
        "	set_bit(bit, (unsigned long *)cpu_caps_set);	\\",
        "} while (0)",
        "",
        "#define setup_force_cpu_bug(bit) setup_force_cpu_cap(bit)",
        "",
        "/*",
        " * Static testing of CPU features. Used the same as boot_cpu_has(). It",
        " * statically patches the target code for additional performance. Use",
        " * static_cpu_has() only in fast paths, where every cycle counts. Which",
        " * means that the boot_cpu_has() variant is already fast enough for the",
        " * majority of cases and you should stick to using it as it is generally",
        " * only two instructions: a RIP-relative MOV and a TEST.",
        " *",
        " * Do not use an \"m\" constraint for [cap_byte] here: gcc doesn't know",
        " * that this is only used on a fallback path and will sometimes cause",
        " * it to manifest the address of boot_cpu_data in a register, fouling",
        " * the mainline (post-initialization) code.",
        " */",
        "static __always_inline bool _static_cpu_has(u16 bit)",
        "{",
        "	asm goto(ALTERNATIVE_TERNARY(\"jmp 6f\", %c[feature], \"\", \"jmp %l[t_no]\")",
        "		\".pushsection .altinstr_aux,\\\"ax\\\"\\n\"",
        "		\"6:\\n\"",
        "		\" testb %[bitnum], %a[cap_byte]\\n\"",
        "		\" jnz %l[t_yes]\\n\"",
        "		\" jmp %l[t_no]\\n\"",
        "		\".popsection\\n\"",
        "		 : : [feature]  \"i\" (bit),",
        "		     [bitnum]   \"i\" (1 << (bit & 7)),",
        "		     [cap_byte] \"i\" (&((const char *)boot_cpu_data.x86_capability)[bit >> 3])",
        "		 : : t_yes, t_no);",
        "t_yes:",
        "	return true;",
        "t_no:",
        "	return false;",
        "}",
        "",
        "#define static_cpu_has(bit)					\\",
        "(								\\",
        "	__builtin_constant_p(boot_cpu_has(bit)) ?		\\",
        "		boot_cpu_has(bit) :				\\",
        "		_static_cpu_has(bit)				\\",
        ")",
        "",
        "#define cpu_has_bug(c, bit)		cpu_has(c, (bit))",
        "#define set_cpu_bug(c, bit)		set_cpu_cap(c, (bit))",
        "#define clear_cpu_bug(c, bit)		clear_cpu_cap(c, (bit))",
        "",
        "#define static_cpu_has_bug(bit)		static_cpu_has((bit))",
        "#define boot_cpu_has_bug(bit)		cpu_has_bug(&boot_cpu_data, (bit))",
        "#define boot_cpu_set_bug(bit)		set_cpu_cap(&boot_cpu_data, (bit))",
        "",
        "#define MAX_CPU_FEATURES		(NCAPINTS * 32)",
        "#define cpu_have_feature		boot_cpu_has",
        "",
        "#define CPU_FEATURE_TYPEFMT		\"x86,ven%04Xfam%04Xmod%04X\"",
        "#define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \\",
        "					boot_cpu_data.x86_model",
        "",
        "#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */",
        "#endif /* _ASM_X86_CPUFEATURE_H */"
    ]
  },
  "mm_swap_c": {
    path: "mm/swap.c",
    covered: [154, 164, 934, 109, 75, 121, 76, 155, 955, 953, 98, 501, 194, 915, 167, 429, 192, 201, 159, 119, 443, 93, 136, 112, 181, 910, 495, 917, 184, 137],
    totalLines: 1063,
    coveredCount: 30,
    coveragePct: 2.8,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " *  linux/mm/swap.c",
        " *",
        " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
        " */",
        "",
        "/*",
        " * This file contains the default values for the operation of the",
        " * Linux VM subsystem. Fine-tuning documentation can be found in",
        " * Documentation/admin-guide/sysctl/vm.rst.",
        " * Started 18.12.91",
        " * Swap aging added 23.2.95, Stephen Tweedie.",
        " * Buffermem limits added 12.3.98, Rik van Riel.",
        " */",
        "",
        "#include <linux/mm.h>",
        "#include <linux/sched.h>",
        "#include <linux/kernel_stat.h>",
        "#include <linux/swap.h>",
        "#include <linux/mman.h>",
        "#include <linux/pagemap.h>",
        "#include <linux/pagevec.h>",
        "#include <linux/init.h>",
        "#include <linux/export.h>",
        "#include <linux/mm_inline.h>",
        "#include <linux/percpu_counter.h>",
        "#include <linux/memremap.h>",
        "#include <linux/percpu.h>",
        "#include <linux/cpu.h>",
        "#include <linux/notifier.h>",
        "#include <linux/backing-dev.h>",
        "#include <linux/memcontrol.h>",
        "#include <linux/gfp.h>",
        "#include <linux/uio.h>",
        "#include <linux/hugetlb.h>",
        "#include <linux/page_idle.h>",
        "#include <linux/local_lock.h>",
        "#include <linux/buffer_head.h>",
        "",
        "#include \"internal.h\"",
        "",
        "#define CREATE_TRACE_POINTS",
        "#include <trace/events/pagemap.h>",
        "",
        "/* How many pages do we try to swap or page in/out together? As a power of 2 */",
        "int page_cluster;",
        "const int page_cluster_max = 31;",
        "",
        "struct cpu_fbatches {",
        "	/*",
        "	 * The following folio batches are grouped together because they are protected",
        "	 * by disabling preemption (and interrupts remain enabled).",
        "	 */",
        "	local_lock_t lock;",
        "	struct folio_batch lru_add;",
        "	struct folio_batch lru_deactivate_file;",
        "	struct folio_batch lru_deactivate;",
        "	struct folio_batch lru_lazyfree;",
        "#ifdef CONFIG_SMP",
        "	struct folio_batch lru_activate;",
        "#endif",
        "	/* Protecting the following batches which require disabling interrupts */",
        "	local_lock_t lock_irq;",
        "	struct folio_batch lru_move_tail;",
        "};",
        "",
        "static DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {",
        "	.lock = INIT_LOCAL_LOCK(lock),",
        "	.lock_irq = INIT_LOCAL_LOCK(lock_irq),",
        "};",
        "",
        "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
        "		unsigned long *flagsp)",
        "{",
        "	if (folio_test_lru(folio)) {",
        "		folio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
        "		lruvec_del_folio(*lruvecp, folio);",
        "		__folio_clear_lru_flags(folio);",
        "	}",
        "}",
        "",
        "/*",
        " * This path almost never happens for VM activity - pages are normally freed",
        " * in batches.  But it gets used by networking - and for compound pages.",
        " */",
        "static void page_cache_release(struct folio *folio)",
        "{",
        "	struct lruvec *lruvec = NULL;",
        "	unsigned long flags;",
        "",
        "	__page_cache_release(folio, &lruvec, &flags);",
        "	if (lruvec)",
        "		unlock_page_lruvec_irqrestore(lruvec, flags);",
        "}",
        "",
        "void __folio_put(struct folio *folio)",
        "{",
        "	if (unlikely(folio_is_zone_device(folio))) {",
        "		free_zone_device_folio(folio);",
        "		return;",
        "	}",
        "",
        "	if (folio_test_hugetlb(folio)) {",
        "		free_huge_folio(folio);",
        "		return;",
        "	}",
        "",
        "	page_cache_release(folio);",
        "	folio_unqueue_deferred_split(folio);",
        "	mem_cgroup_uncharge(folio);",
        "	free_unref_page(&folio->page, folio_order(folio));",
        "}",
        "EXPORT_SYMBOL(__folio_put);",
        "",
        "typedef void (*move_fn_t)(struct lruvec *lruvec, struct folio *folio);",
        "",
        "static void lru_add(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	int was_unevictable = folio_test_clear_unevictable(folio);",
        "	long nr_pages = folio_nr_pages(folio);",
        "",
        "	VM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
        "",
        "	/*",
        "	 * Is an smp_mb__after_atomic() still required here, before",
        "	 * folio_evictable() tests the mlocked flag, to rule out the possibility",
        "	 * of stranding an evictable folio on an unevictable LRU?  I think",
        "	 * not, because __munlock_folio() only clears the mlocked flag",
        "	 * while the LRU lock is held.",
        "	 *",
        "	 * (That is not true of __page_cache_release(), and not necessarily",
        "	 * true of folios_put(): but those only clear the mlocked flag after",
        "	 * folio_put_testzero() has excluded any other users of the folio.)",
        "	 */",
        "	if (folio_evictable(folio)) {",
        "		if (was_unevictable)",
        "			__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
        "	} else {",
        "		folio_clear_active(folio);",
        "		folio_set_unevictable(folio);",
        "		/*",
        "		 * folio->mlock_count = !!folio_test_mlocked(folio)?",
        "		 * But that leaves __mlock_folio() in doubt whether another",
        "		 * actor has already counted the mlock or not.  Err on the",
        "		 * safe side, underestimate, let page reclaim fix it, rather",
        "		 * than leaving a page on the unevictable LRU indefinitely.",
        "		 */",
        "		folio->mlock_count = 0;",
        "		if (!was_unevictable)",
        "			__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
        "	}",
        "",
        "	lruvec_add_folio(lruvec, folio);",
        "	trace_mm_lru_insertion(folio);",
        "}",
        "",
        "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
        "{",
        "	int i;",
        "	struct lruvec *lruvec = NULL;",
        "	unsigned long flags = 0;",
        "",
        "	for (i = 0; i < folio_batch_count(fbatch); i++) {",
        "		struct folio *folio = fbatch->folios[i];",
        "",
        "		folio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
        "		move_fn(lruvec, folio);",
        "",
        "		folio_set_lru(folio);",
        "	}",
        "",
        "	if (lruvec)",
        "		unlock_page_lruvec_irqrestore(lruvec, flags);",
        "	folios_put(fbatch);",
        "}",
        "",
        "static void __folio_batch_add_and_move(struct folio_batch __percpu *fbatch,",
        "		struct folio *folio, move_fn_t move_fn,",
        "		bool on_lru, bool disable_irq)",
        "{",
        "	unsigned long flags;",
        "",
        "	if (on_lru && !folio_test_clear_lru(folio))",
        "		return;",
        "",
        "	folio_get(folio);",
        "",
        "	if (disable_irq)",
        "		local_lock_irqsave(&cpu_fbatches.lock_irq, flags);",
        "	else",
        "		local_lock(&cpu_fbatches.lock);",
        "",
        "	if (!folio_batch_add(this_cpu_ptr(fbatch), folio) || folio_test_large(folio) ||",
        "	    lru_cache_disabled())",
        "		folio_batch_move_lru(this_cpu_ptr(fbatch), move_fn);",
        "",
        "	if (disable_irq)",
        "		local_unlock_irqrestore(&cpu_fbatches.lock_irq, flags);",
        "	else",
        "		local_unlock(&cpu_fbatches.lock);",
        "}",
        "",
        "#define folio_batch_add_and_move(folio, op, on_lru)						\\",
        "	__folio_batch_add_and_move(								\\",
        "		&cpu_fbatches.op,								\\",
        "		folio,										\\",
        "		op,										\\",
        "		on_lru,										\\",
        "		offsetof(struct cpu_fbatches, op) >= offsetof(struct cpu_fbatches, lock_irq)	\\",
        "	)",
        "",
        "static void lru_move_tail(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	if (folio_test_unevictable(folio))",
        "		return;",
        "",
        "	lruvec_del_folio(lruvec, folio);",
        "	folio_clear_active(folio);",
        "	lruvec_add_folio_tail(lruvec, folio);",
        "	__count_vm_events(PGROTATED, folio_nr_pages(folio));",
        "}",
        "",
        "/*",
        " * Writeback is about to end against a folio which has been marked for",
        " * immediate reclaim.  If it still appears to be reclaimable, move it",
        " * to the tail of the inactive list.",
        " *",
        " * folio_rotate_reclaimable() must disable IRQs, to prevent nasty races.",
        " */",
        "void folio_rotate_reclaimable(struct folio *folio)",
        "{",
        "	if (folio_test_locked(folio) || folio_test_dirty(folio) ||",
        "	    folio_test_unevictable(folio))",
        "		return;",
        "",
        "	folio_batch_add_and_move(folio, lru_move_tail, true);",
        "}",
        "",
        "void lru_note_cost(struct lruvec *lruvec, bool file,",
        "		   unsigned int nr_io, unsigned int nr_rotated)",
        "{",
        "	unsigned long cost;",
        "",
        "	/*",
        "	 * Reflect the relative cost of incurring IO and spending CPU",
        "	 * time on rotations. This doesn't attempt to make a precise",
        "	 * comparison, it just says: if reloads are about comparable",
        "	 * between the LRU lists, or rotations are overwhelmingly",
        "	 * different between them, adjust scan balance for CPU work.",
        "	 */",
        "	cost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;",
        "",
        "	do {",
        "		unsigned long lrusize;",
        "",
        "		/*",
        "		 * Hold lruvec->lru_lock is safe here, since",
        "		 * 1) The pinned lruvec in reclaim, or",
        "		 * 2) From a pre-LRU page during refault (which also holds the",
        "		 *    rcu lock, so would be safe even if the page was on the LRU",
        "		 *    and could move simultaneously to a new lruvec).",
        "		 */",
        "		spin_lock_irq(&lruvec->lru_lock);",
        "		/* Record cost event */",
        "		if (file)",
        "			lruvec->file_cost += cost;",
        "		else",
        "			lruvec->anon_cost += cost;",
        "",
        "		/*",
        "		 * Decay previous events",
        "		 *",
        "		 * Because workloads change over time (and to avoid",
        "		 * overflow) we keep these statistics as a floating",
        "		 * average, which ends up weighing recent refaults",
        "		 * more than old ones.",
        "		 */",
        "		lrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +",
        "			  lruvec_page_state(lruvec, NR_ACTIVE_ANON) +",
        "			  lruvec_page_state(lruvec, NR_INACTIVE_FILE) +",
        "			  lruvec_page_state(lruvec, NR_ACTIVE_FILE);",
        "",
        "		if (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {",
        "			lruvec->file_cost /= 2;",
        "			lruvec->anon_cost /= 2;",
        "		}",
        "		spin_unlock_irq(&lruvec->lru_lock);",
        "	} while ((lruvec = parent_lruvec(lruvec)));",
        "}",
        "",
        "void lru_note_cost_refault(struct folio *folio)",
        "{",
        "	lru_note_cost(folio_lruvec(folio), folio_is_file_lru(folio),",
        "		      folio_nr_pages(folio), 0);",
        "}",
        "",
        "static void lru_activate(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	long nr_pages = folio_nr_pages(folio);",
        "",
        "	if (folio_test_active(folio) || folio_test_unevictable(folio))",
        "		return;",
        "",
        "",
        "	lruvec_del_folio(lruvec, folio);",
        "	folio_set_active(folio);",
        "	lruvec_add_folio(lruvec, folio);",
        "	trace_mm_lru_activate(folio);",
        "",
        "	__count_vm_events(PGACTIVATE, nr_pages);",
        "	__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE, nr_pages);",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "static void folio_activate_drain(int cpu)",
        "{",
        "	struct folio_batch *fbatch = &per_cpu(cpu_fbatches.lru_activate, cpu);",
        "",
        "	if (folio_batch_count(fbatch))",
        "		folio_batch_move_lru(fbatch, lru_activate);",
        "}",
        "",
        "void folio_activate(struct folio *folio)",
        "{",
        "	if (folio_test_active(folio) || folio_test_unevictable(folio))",
        "		return;",
        "",
        "	folio_batch_add_and_move(folio, lru_activate, true);",
        "}",
        "",
        "#else",
        "static inline void folio_activate_drain(int cpu)",
        "{",
        "}",
        "",
        "void folio_activate(struct folio *folio)",
        "{",
        "	struct lruvec *lruvec;",
        "",
        "	if (!folio_test_clear_lru(folio))",
        "		return;",
        "",
        "	lruvec = folio_lruvec_lock_irq(folio);",
        "	lru_activate(lruvec, folio);",
        "	unlock_page_lruvec_irq(lruvec);",
        "	folio_set_lru(folio);",
        "}",
        "#endif",
        "",
        "static void __lru_cache_activate_folio(struct folio *folio)",
        "{",
        "	struct folio_batch *fbatch;",
        "	int i;",
        "",
        "	local_lock(&cpu_fbatches.lock);",
        "	fbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
        "",
        "	/*",
        "	 * Search backwards on the optimistic assumption that the folio being",
        "	 * activated has just been added to this batch. Note that only",
        "	 * the local batch is examined as a !LRU folio could be in the",
        "	 * process of being released, reclaimed, migrated or on a remote",
        "	 * batch that is currently being drained. Furthermore, marking",
        "	 * a remote batch's folio active potentially hits a race where",
        "	 * a folio is marked active just after it is added to the inactive",
        "	 * list causing accounting errors and BUG_ON checks to trigger.",
        "	 */",
        "	for (i = folio_batch_count(fbatch) - 1; i >= 0; i--) {",
        "		struct folio *batch_folio = fbatch->folios[i];",
        "",
        "		if (batch_folio == folio) {",
        "			folio_set_active(folio);",
        "			break;",
        "		}",
        "	}",
        "",
        "	local_unlock(&cpu_fbatches.lock);",
        "}",
        "",
        "#ifdef CONFIG_LRU_GEN",
        "static void folio_inc_refs(struct folio *folio)",
        "{",
        "	unsigned long new_flags, old_flags = READ_ONCE(folio->flags);",
        "",
        "	if (folio_test_unevictable(folio))",
        "		return;",
        "",
        "	if (!folio_test_referenced(folio)) {",
        "		folio_set_referenced(folio);",
        "		return;",
        "	}",
        "",
        "	if (!folio_test_workingset(folio)) {",
        "		folio_set_workingset(folio);",
        "		return;",
        "	}",
        "",
        "	/* see the comment on MAX_NR_TIERS */",
        "	do {",
        "		new_flags = old_flags & LRU_REFS_MASK;",
        "		if (new_flags == LRU_REFS_MASK)",
        "			break;",
        "",
        "		new_flags += BIT(LRU_REFS_PGOFF);",
        "		new_flags |= old_flags & ~LRU_REFS_MASK;",
        "	} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));",
        "}",
        "#else",
        "static void folio_inc_refs(struct folio *folio)",
        "{",
        "}",
        "#endif /* CONFIG_LRU_GEN */",
        "",
        "/**",
        " * folio_mark_accessed - Mark a folio as having seen activity.",
        " * @folio: The folio to mark.",
        " *",
        " * This function will perform one of the following transitions:",
        " *",
        " * * inactive,unreferenced	->	inactive,referenced",
        " * * inactive,referenced	->	active,unreferenced",
        " * * active,unreferenced	->	active,referenced",
        " *",
        " * When a newly allocated folio is not yet visible, so safe for non-atomic ops,",
        " * __folio_set_referenced() may be substituted for folio_mark_accessed().",
        " */",
        "void folio_mark_accessed(struct folio *folio)",
        "{",
        "	if (lru_gen_enabled()) {",
        "		folio_inc_refs(folio);",
        "		return;",
        "	}",
        "",
        "	if (!folio_test_referenced(folio)) {",
        "		folio_set_referenced(folio);",
        "	} else if (folio_test_unevictable(folio)) {",
        "		/*",
        "		 * Unevictable pages are on the \"LRU_UNEVICTABLE\" list. But,",
        "		 * this list is never rotated or maintained, so marking an",
        "		 * unevictable page accessed has no effect.",
        "		 */",
        "	} else if (!folio_test_active(folio)) {",
        "		/*",
        "		 * If the folio is on the LRU, queue it for activation via",
        "		 * cpu_fbatches.lru_activate. Otherwise, assume the folio is in a",
        "		 * folio_batch, mark it active and it'll be moved to the active",
        "		 * LRU on the next drain.",
        "		 */",
        "		if (folio_test_lru(folio))",
        "			folio_activate(folio);",
        "		else",
        "			__lru_cache_activate_folio(folio);",
        "		folio_clear_referenced(folio);",
        "		workingset_activation(folio);",
        "	}",
        "	if (folio_test_idle(folio))",
        "		folio_clear_idle(folio);",
        "}",
        "EXPORT_SYMBOL(folio_mark_accessed);",
        "",
        "/**",
        " * folio_add_lru - Add a folio to an LRU list.",
        " * @folio: The folio to be added to the LRU.",
        " *",
        " * Queue the folio for addition to the LRU. The decision on whether",
        " * to add the page to the [in]active [file|anon] list is deferred until the",
        " * folio_batch is drained. This gives a chance for the caller of folio_add_lru()",
        " * have the folio added to the active list using folio_mark_accessed().",
        " */",
        "void folio_add_lru(struct folio *folio)",
        "{",
        "	VM_BUG_ON_FOLIO(folio_test_active(folio) &&",
        "			folio_test_unevictable(folio), folio);",
        "	VM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
        "",
        "	/* see the comment in lru_gen_add_folio() */",
        "	if (lru_gen_enabled() && !folio_test_unevictable(folio) &&",
        "	    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))",
        "		folio_set_active(folio);",
        "",
        "	folio_batch_add_and_move(folio, lru_add, false);",
        "}",
        "EXPORT_SYMBOL(folio_add_lru);",
        "",
        "/**",
        " * folio_add_lru_vma() - Add a folio to the appropate LRU list for this VMA.",
        " * @folio: The folio to be added to the LRU.",
        " * @vma: VMA in which the folio is mapped.",
        " *",
        " * If the VMA is mlocked, @folio is added to the unevictable list.",
        " * Otherwise, it is treated the same way as folio_add_lru().",
        " */",
        "void folio_add_lru_vma(struct folio *folio, struct vm_area_struct *vma)",
        "{",
        "	VM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
        "",
        "	if (unlikely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED))",
        "		mlock_new_folio(folio);",
        "	else",
        "		folio_add_lru(folio);",
        "}",
        "",
        "/*",
        " * If the folio cannot be invalidated, it is moved to the",
        " * inactive list to speed up its reclaim.  It is moved to the",
        " * head of the list, rather than the tail, to give the flusher",
        " * threads some time to write it out, as this is much more",
        " * effective than the single-page writeout from reclaim.",
        " *",
        " * If the folio isn't mapped and dirty/writeback, the folio",
        " * could be reclaimed asap using the reclaim flag.",
        " *",
        " * 1. active, mapped folio -> none",
        " * 2. active, dirty/writeback folio -> inactive, head, reclaim",
        " * 3. inactive, mapped folio -> none",
        " * 4. inactive, dirty/writeback folio -> inactive, head, reclaim",
        " * 5. inactive, clean -> inactive, tail",
        " * 6. Others -> none",
        " *",
        " * In 4, it moves to the head of the inactive list so the folio is",
        " * written out by flusher threads as this is much more efficient",
        " * than the single-page writeout from reclaim.",
        " */",
        "static void lru_deactivate_file(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	bool active = folio_test_active(folio);",
        "	long nr_pages = folio_nr_pages(folio);",
        "",
        "	if (folio_test_unevictable(folio))",
        "		return;",
        "",
        "	/* Some processes are using the folio */",
        "	if (folio_mapped(folio))",
        "		return;",
        "",
        "	lruvec_del_folio(lruvec, folio);",
        "	folio_clear_active(folio);",
        "	folio_clear_referenced(folio);",
        "",
        "	if (folio_test_writeback(folio) || folio_test_dirty(folio)) {",
        "		/*",
        "		 * Setting the reclaim flag could race with",
        "		 * folio_end_writeback() and confuse readahead.  But the",
        "		 * race window is _really_ small and  it's not a critical",
        "		 * problem.",
        "		 */",
        "		lruvec_add_folio(lruvec, folio);",
        "		folio_set_reclaim(folio);",
        "	} else {",
        "		/*",
        "		 * The folio's writeback ended while it was in the batch.",
        "		 * We move that folio to the tail of the inactive list.",
        "		 */",
        "		lruvec_add_folio_tail(lruvec, folio);",
        "		__count_vm_events(PGROTATED, nr_pages);",
        "	}",
        "",
        "	if (active) {",
        "		__count_vm_events(PGDEACTIVATE, nr_pages);",
        "		__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
        "				     nr_pages);",
        "	}",
        "}",
        "",
        "static void lru_deactivate(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	long nr_pages = folio_nr_pages(folio);",
        "",
        "	if (folio_test_unevictable(folio) || !(folio_test_active(folio) || lru_gen_enabled()))",
        "		return;",
        "",
        "	lruvec_del_folio(lruvec, folio);",
        "	folio_clear_active(folio);",
        "	folio_clear_referenced(folio);",
        "	lruvec_add_folio(lruvec, folio);",
        "",
        "	__count_vm_events(PGDEACTIVATE, nr_pages);",
        "	__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE, nr_pages);",
        "}",
        "",
        "static void lru_lazyfree(struct lruvec *lruvec, struct folio *folio)",
        "{",
        "	long nr_pages = folio_nr_pages(folio);",
        "",
        "	if (!folio_test_anon(folio) || !folio_test_swapbacked(folio) ||",
        "	    folio_test_swapcache(folio) || folio_test_unevictable(folio))",
        "		return;",
        "",
        "	lruvec_del_folio(lruvec, folio);",
        "	folio_clear_active(folio);",
        "	folio_clear_referenced(folio);",
        "	/*",
        "	 * Lazyfree folios are clean anonymous folios.  They have",
        "	 * the swapbacked flag cleared, to distinguish them from normal",
        "	 * anonymous folios",
        "	 */",
        "	folio_clear_swapbacked(folio);",
        "	lruvec_add_folio(lruvec, folio);",
        "",
        "	__count_vm_events(PGLAZYFREE, nr_pages);",
        "	__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE, nr_pages);",
        "}",
        "",
        "/*",
        " * Drain pages out of the cpu's folio_batch.",
        " * Either \"cpu\" is the current CPU, and preemption has already been",
        " * disabled; or \"cpu\" is being hot-unplugged, and is already dead.",
        " */",
        "void lru_add_drain_cpu(int cpu)",
        "{",
        "	struct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
        "	struct folio_batch *fbatch = &fbatches->lru_add;",
        "",
        "	if (folio_batch_count(fbatch))",
        "		folio_batch_move_lru(fbatch, lru_add);",
        "",
        "	fbatch = &fbatches->lru_move_tail;",
        "	/* Disabling interrupts below acts as a compiler barrier. */",
        "	if (data_race(folio_batch_count(fbatch))) {",
        "		unsigned long flags;",
        "",
        "		/* No harm done if a racing interrupt already did this */",
        "		local_lock_irqsave(&cpu_fbatches.lock_irq, flags);",
        "		folio_batch_move_lru(fbatch, lru_move_tail);",
        "		local_unlock_irqrestore(&cpu_fbatches.lock_irq, flags);",
        "	}",
        "",
        "	fbatch = &fbatches->lru_deactivate_file;",
        "	if (folio_batch_count(fbatch))",
        "		folio_batch_move_lru(fbatch, lru_deactivate_file);",
        "",
        "	fbatch = &fbatches->lru_deactivate;",
        "	if (folio_batch_count(fbatch))",
        "		folio_batch_move_lru(fbatch, lru_deactivate);",
        "",
        "	fbatch = &fbatches->lru_lazyfree;",
        "	if (folio_batch_count(fbatch))",
        "		folio_batch_move_lru(fbatch, lru_lazyfree);",
        "",
        "	folio_activate_drain(cpu);",
        "}",
        "",
        "/**",
        " * deactivate_file_folio() - Deactivate a file folio.",
        " * @folio: Folio to deactivate.",
        " *",
        " * This function hints to the VM that @folio is a good reclaim candidate,",
        " * for example if its invalidation fails due to the folio being dirty",
        " * or under writeback.",
        " *",
        " * Context: Caller holds a reference on the folio.",
        " */",
        "void deactivate_file_folio(struct folio *folio)",
        "{",
        "	/* Deactivating an unevictable folio will not accelerate reclaim */",
        "	if (folio_test_unevictable(folio))",
        "		return;",
        "",
        "	folio_batch_add_and_move(folio, lru_deactivate_file, true);",
        "}",
        "",
        "/*",
        " * folio_deactivate - deactivate a folio",
        " * @folio: folio to deactivate",
        " *",
        " * folio_deactivate() moves @folio to the inactive list if @folio was on the",
        " * active list and was not unevictable. This is done to accelerate the",
        " * reclaim of @folio.",
        " */",
        "void folio_deactivate(struct folio *folio)",
        "{",
        "	if (folio_test_unevictable(folio) || !(folio_test_active(folio) || lru_gen_enabled()))",
        "		return;",
        "",
        "	folio_batch_add_and_move(folio, lru_deactivate, true);",
        "}",
        "",
        "/**",
        " * folio_mark_lazyfree - make an anon folio lazyfree",
        " * @folio: folio to deactivate",
        " *",
        " * folio_mark_lazyfree() moves @folio to the inactive file list.",
        " * This is done to accelerate the reclaim of @folio.",
        " */",
        "void folio_mark_lazyfree(struct folio *folio)",
        "{",
        "	if (!folio_test_anon(folio) || !folio_test_swapbacked(folio) ||",
        "	    folio_test_swapcache(folio) || folio_test_unevictable(folio))",
        "		return;",
        "",
        "	folio_batch_add_and_move(folio, lru_lazyfree, true);",
        "}",
        "",
        "void lru_add_drain(void)",
        "{",
        "	local_lock(&cpu_fbatches.lock);",
        "	lru_add_drain_cpu(smp_processor_id());",
        "	local_unlock(&cpu_fbatches.lock);",
        "	mlock_drain_local();",
        "}",
        "",
        "/*",
        " * It's called from per-cpu workqueue context in SMP case so",
        " * lru_add_drain_cpu and invalidate_bh_lrus_cpu should run on",
        " * the same cpu. It shouldn't be a problem in !SMP case since",
        " * the core is only one and the locks will disable preemption.",
        " */",
        "static void lru_add_and_bh_lrus_drain(void)",
        "{",
        "	local_lock(&cpu_fbatches.lock);",
        "	lru_add_drain_cpu(smp_processor_id());",
        "	local_unlock(&cpu_fbatches.lock);",
        "	invalidate_bh_lrus_cpu();",
        "	mlock_drain_local();",
        "}",
        "",
        "void lru_add_drain_cpu_zone(struct zone *zone)",
        "{",
        "	local_lock(&cpu_fbatches.lock);",
        "	lru_add_drain_cpu(smp_processor_id());",
        "	drain_local_pages(zone);",
        "	local_unlock(&cpu_fbatches.lock);",
        "	mlock_drain_local();",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "",
        "static DEFINE_PER_CPU(struct work_struct, lru_add_drain_work);",
        "",
        "static void lru_add_drain_per_cpu(struct work_struct *dummy)",
        "{",
        "	lru_add_and_bh_lrus_drain();",
        "}",
        "",
        "static bool cpu_needs_drain(unsigned int cpu)",
        "{",
        "	struct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
        "",
        "	/* Check these in order of likelihood that they're not zero */",
        "	return folio_batch_count(&fbatches->lru_add) ||",
        "		folio_batch_count(&fbatches->lru_move_tail) ||",
        "		folio_batch_count(&fbatches->lru_deactivate_file) ||",
        "		folio_batch_count(&fbatches->lru_deactivate) ||",
        "		folio_batch_count(&fbatches->lru_lazyfree) ||",
        "		folio_batch_count(&fbatches->lru_activate) ||",
        "		need_mlock_drain(cpu) ||",
        "		has_bh_in_lru(cpu, NULL);",
        "}",
        "",
        "/*",
        " * Doesn't need any cpu hotplug locking because we do rely on per-cpu",
        " * kworkers being shut down before our page_alloc_cpu_dead callback is",
        " * executed on the offlined cpu.",
        " * Calling this function with cpu hotplug locks held can actually lead",
        " * to obscure indirect dependencies via WQ context.",
        " */",
        "static inline void __lru_add_drain_all(bool force_all_cpus)",
        "{",
        "	/*",
        "	 * lru_drain_gen - Global pages generation number",
        "	 *",
        "	 * (A) Definition: global lru_drain_gen = x implies that all generations",
        "	 *     0 < n <= x are already *scheduled* for draining.",
        "	 *",
        "	 * This is an optimization for the highly-contended use case where a",
        "	 * user space workload keeps constantly generating a flow of pages for",
        "	 * each CPU.",
        "	 */",
        "	static unsigned int lru_drain_gen;",
        "	static struct cpumask has_work;",
        "	static DEFINE_MUTEX(lock);",
        "	unsigned cpu, this_gen;",
        "",
        "	/*",
        "	 * Make sure nobody triggers this path before mm_percpu_wq is fully",
        "	 * initialized.",
        "	 */",
        "	if (WARN_ON(!mm_percpu_wq))",
        "		return;",
        "",
        "	/*",
        "	 * Guarantee folio_batch counter stores visible by this CPU",
        "	 * are visible to other CPUs before loading the current drain",
        "	 * generation.",
        "	 */",
        "	smp_mb();",
        "",
        "	/*",
        "	 * (B) Locally cache global LRU draining generation number",
        "	 *",
        "	 * The read barrier ensures that the counter is loaded before the mutex",
        "	 * is taken. It pairs with smp_mb() inside the mutex critical section",
        "	 * at (D).",
        "	 */",
        "	this_gen = smp_load_acquire(&lru_drain_gen);",
        "",
        "	mutex_lock(&lock);",
        "",
        "	/*",
        "	 * (C) Exit the draining operation if a newer generation, from another",
        "	 * lru_add_drain_all(), was already scheduled for draining. Check (A).",
        "	 */",
        "	if (unlikely(this_gen != lru_drain_gen && !force_all_cpus))",
        "		goto done;",
        "",
        "	/*",
        "	 * (D) Increment global generation number",
        "	 *",
        "	 * Pairs with smp_load_acquire() at (B), outside of the critical",
        "	 * section. Use a full memory barrier to guarantee that the",
        "	 * new global drain generation number is stored before loading",
        "	 * folio_batch counters.",
        "	 *",
        "	 * This pairing must be done here, before the for_each_online_cpu loop",
        "	 * below which drains the page vectors.",
        "	 *",
        "	 * Let x, y, and z represent some system CPU numbers, where x < y < z.",
        "	 * Assume CPU #z is in the middle of the for_each_online_cpu loop",
        "	 * below and has already reached CPU #y's per-cpu data. CPU #x comes",
        "	 * along, adds some pages to its per-cpu vectors, then calls",
        "	 * lru_add_drain_all().",
        "	 *",
        "	 * If the paired barrier is done at any later step, e.g. after the",
        "	 * loop, CPU #x will just exit at (C) and miss flushing out all of its",
        "	 * added pages.",
        "	 */",
        "	WRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);",
        "	smp_mb();",
        "",
        "	cpumask_clear(&has_work);",
        "	for_each_online_cpu(cpu) {",
        "		struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);",
        "",
        "		if (cpu_needs_drain(cpu)) {",
        "			INIT_WORK(work, lru_add_drain_per_cpu);",
        "			queue_work_on(cpu, mm_percpu_wq, work);",
        "			__cpumask_set_cpu(cpu, &has_work);",
        "		}",
        "	}",
        "",
        "	for_each_cpu(cpu, &has_work)",
        "		flush_work(&per_cpu(lru_add_drain_work, cpu));",
        "",
        "done:",
        "	mutex_unlock(&lock);",
        "}",
        "",
        "void lru_add_drain_all(void)",
        "{",
        "	__lru_add_drain_all(false);",
        "}",
        "#else",
        "void lru_add_drain_all(void)",
        "{",
        "	lru_add_drain();",
        "}",
        "#endif /* CONFIG_SMP */",
        "",
        "atomic_t lru_disable_count = ATOMIC_INIT(0);",
        "",
        "/*",
        " * lru_cache_disable() needs to be called before we start compiling",
        " * a list of folios to be migrated using folio_isolate_lru().",
        " * It drains folios on LRU cache and then disable on all cpus until",
        " * lru_cache_enable is called.",
        " *",
        " * Must be paired with a call to lru_cache_enable().",
        " */",
        "void lru_cache_disable(void)",
        "{",
        "	atomic_inc(&lru_disable_count);",
        "	/*",
        "	 * Readers of lru_disable_count are protected by either disabling",
        "	 * preemption or rcu_read_lock:",
        "	 *",
        "	 * preempt_disable, local_irq_disable  [bh_lru_lock()]",
        "	 * rcu_read_lock		       [rt_spin_lock CONFIG_PREEMPT_RT]",
        "	 * preempt_disable		       [local_lock !CONFIG_PREEMPT_RT]",
        "	 *",
        "	 * Since v5.1 kernel, synchronize_rcu() is guaranteed to wait on",
        "	 * preempt_disable() regions of code. So any CPU which sees",
        "	 * lru_disable_count = 0 will have exited the critical",
        "	 * section when synchronize_rcu() returns.",
        "	 */",
        "	synchronize_rcu_expedited();",
        "#ifdef CONFIG_SMP",
        "	__lru_add_drain_all(true);",
        "#else",
        "	lru_add_and_bh_lrus_drain();",
        "#endif",
        "}",
        "",
        "/**",
        " * folios_put_refs - Reduce the reference count on a batch of folios.",
        " * @folios: The folios.",
        " * @refs: The number of refs to subtract from each folio.",
        " *",
        " * Like folio_put(), but for a batch of folios.  This is more efficient",
        " * than writing the loop yourself as it will optimise the locks which need",
        " * to be taken if the folios are freed.  The folios batch is returned",
        " * empty and ready to be reused for another batch; there is no need",
        " * to reinitialise it.  If @refs is NULL, we subtract one from each",
        " * folio refcount.",
        " *",
        " * Context: May be called in process or interrupt context, but not in NMI",
        " * context.  May be called while holding a spinlock.",
        " */",
        "void folios_put_refs(struct folio_batch *folios, unsigned int *refs)",
        "{",
        "	int i, j;",
        "	struct lruvec *lruvec = NULL;",
        "	unsigned long flags = 0;",
        "",
        "	for (i = 0, j = 0; i < folios->nr; i++) {",
        "		struct folio *folio = folios->folios[i];",
        "		unsigned int nr_refs = refs ? refs[i] : 1;",
        "",
        "		if (is_huge_zero_folio(folio))",
        "			continue;",
        "",
        "		if (folio_is_zone_device(folio)) {",
        "			if (lruvec) {",
        "				unlock_page_lruvec_irqrestore(lruvec, flags);",
        "				lruvec = NULL;",
        "			}",
        "			if (put_devmap_managed_folio_refs(folio, nr_refs))",
        "				continue;",
        "			if (folio_ref_sub_and_test(folio, nr_refs))",
        "				free_zone_device_folio(folio);",
        "			continue;",
        "		}",
        "",
        "		if (!folio_ref_sub_and_test(folio, nr_refs))",
        "			continue;",
        "",
        "		/* hugetlb has its own memcg */",
        "		if (folio_test_hugetlb(folio)) {",
        "			if (lruvec) {",
        "				unlock_page_lruvec_irqrestore(lruvec, flags);",
        "				lruvec = NULL;",
        "			}",
        "			free_huge_folio(folio);",
        "			continue;",
        "		}",
        "		folio_unqueue_deferred_split(folio);",
        "		__page_cache_release(folio, &lruvec, &flags);",
        "",
        "		if (j != i)",
        "			folios->folios[j] = folio;",
        "		j++;",
        "	}",
        "	if (lruvec)",
        "		unlock_page_lruvec_irqrestore(lruvec, flags);",
        "	if (!j) {",
        "		folio_batch_reinit(folios);",
        "		return;",
        "	}",
        "",
        "	folios->nr = j;",
        "	mem_cgroup_uncharge_folios(folios);",
        "	free_unref_folios(folios);",
        "}",
        "EXPORT_SYMBOL(folios_put_refs);",
        "",
        "/**",
        " * release_pages - batched put_page()",
        " * @arg: array of pages to release",
        " * @nr: number of pages",
        " *",
        " * Decrement the reference count on all the pages in @arg.  If it",
        " * fell to zero, remove the page from the LRU and free it.",
        " *",
        " * Note that the argument can be an array of pages, encoded pages,",
        " * or folio pointers. We ignore any encoded bits, and turn any of",
        " * them into just a folio that gets free'd.",
        " */",
        "void release_pages(release_pages_arg arg, int nr)",
        "{",
        "	struct folio_batch fbatch;",
        "	int refs[PAGEVEC_SIZE];",
        "	struct encoded_page **encoded = arg.encoded_pages;",
        "	int i;",
        "",
        "	folio_batch_init(&fbatch);",
        "	for (i = 0; i < nr; i++) {",
        "		/* Turn any of the argument types into a folio */",
        "		struct folio *folio = page_folio(encoded_page_ptr(encoded[i]));",
        "",
        "		/* Is our next entry actually \"nr_pages\" -> \"nr_refs\" ? */",
        "		refs[fbatch.nr] = 1;",
        "		if (unlikely(encoded_page_flags(encoded[i]) &",
        "			     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
        "			refs[fbatch.nr] = encoded_nr_pages(encoded[++i]);",
        "",
        "		if (folio_batch_add(&fbatch, folio) > 0)",
        "			continue;",
        "		folios_put_refs(&fbatch, refs);",
        "	}",
        "",
        "	if (fbatch.nr)",
        "		folios_put_refs(&fbatch, refs);",
        "}",
        "EXPORT_SYMBOL(release_pages);",
        "",
        "/*",
        " * The folios which we're about to release may be in the deferred lru-addition",
        " * queues.  That would prevent them from really being freed right now.  That's",
        " * OK from a correctness point of view but is inefficient - those folios may be",
        " * cache-warm and we want to give them back to the page allocator ASAP.",
        " *",
        " * So __folio_batch_release() will drain those queues here.",
        " * folio_batch_move_lru() calls folios_put() directly to avoid",
        " * mutual recursion.",
        " */",
        "void __folio_batch_release(struct folio_batch *fbatch)",
        "{",
        "	if (!fbatch->percpu_pvec_drained) {",
        "		lru_add_drain();",
        "		fbatch->percpu_pvec_drained = true;",
        "	}",
        "	folios_put(fbatch);",
        "}",
        "EXPORT_SYMBOL(__folio_batch_release);",
        "",
        "/**",
        " * folio_batch_remove_exceptionals() - Prune non-folios from a batch.",
        " * @fbatch: The batch to prune",
        " *",
        " * find_get_entries() fills a batch with both folios and shadow/swap/DAX",
        " * entries.  This function prunes all the non-folio entries from @fbatch",
        " * without leaving holes, so that it can be passed on to folio-only batch",
        " * operations.",
        " */",
        "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
        "{",
        "	unsigned int i, j;",
        "",
        "	for (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
        "		struct folio *folio = fbatch->folios[i];",
        "		if (!xa_is_value(folio))",
        "			fbatch->folios[j++] = folio;",
        "	}",
        "	fbatch->nr = j;",
        "}",
        "",
        "/*",
        " * Perform any setup for the swap system",
        " */",
        "void __init swap_setup(void)",
        "{",
        "	unsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
        "",
        "	/* Use a smaller cluster for small-memory machines */",
        "	if (megs < 16)",
        "		page_cluster = 2;",
        "	else",
        "		page_cluster = 3;",
        "	/*",
        "	 * Right now other parts of the system means that we",
        "	 * _really_ don't want to cluster much more",
        "	 */",
        "}"
    ]
  },
  "include_trace_events_ext4_h": {
    path: "include/trace/events/ext4.h",
    covered: [2314, 2292],
    totalLines: 2986,
    coveredCount: 2,
    coveragePct: 0.1,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#undef TRACE_SYSTEM",
        "#define TRACE_SYSTEM ext4",
        "",
        "#if !defined(_TRACE_EXT4_H) || defined(TRACE_HEADER_MULTI_READ)",
        "#define _TRACE_EXT4_H",
        "",
        "#include <linux/writeback.h>",
        "#include <linux/tracepoint.h>",
        "",
        "struct ext4_allocation_context;",
        "struct ext4_allocation_request;",
        "struct ext4_extent;",
        "struct ext4_prealloc_space;",
        "struct ext4_inode_info;",
        "struct mpage_da_data;",
        "struct ext4_map_blocks;",
        "struct extent_status;",
        "struct ext4_fsmap;",
        "struct partial_cluster;",
        "",
        "#define EXT4_I(inode) (container_of(inode, struct ext4_inode_info, vfs_inode))",
        "",
        "#define show_mballoc_flags(flags) __print_flags(flags, \"|\",	\\",
        "	{ EXT4_MB_HINT_MERGE,		\"HINT_MERGE\" },		\\",
        "	{ EXT4_MB_HINT_RESERVED,	\"HINT_RESV\" },		\\",
        "	{ EXT4_MB_HINT_METADATA,	\"HINT_MDATA\" },		\\",
        "	{ EXT4_MB_HINT_FIRST,		\"HINT_FIRST\" },		\\",
        "	{ EXT4_MB_HINT_BEST,		\"HINT_BEST\" },		\\",
        "	{ EXT4_MB_HINT_DATA,		\"HINT_DATA\" },		\\",
        "	{ EXT4_MB_HINT_NOPREALLOC,	\"HINT_NOPREALLOC\" },	\\",
        "	{ EXT4_MB_HINT_GROUP_ALLOC,	\"HINT_GRP_ALLOC\" },	\\",
        "	{ EXT4_MB_HINT_GOAL_ONLY,	\"HINT_GOAL_ONLY\" },	\\",
        "	{ EXT4_MB_HINT_TRY_GOAL,	\"HINT_TRY_GOAL\" },	\\",
        "	{ EXT4_MB_DELALLOC_RESERVED,	\"DELALLOC_RESV\" },	\\",
        "	{ EXT4_MB_STREAM_ALLOC,		\"STREAM_ALLOC\" },	\\",
        "	{ EXT4_MB_USE_ROOT_BLOCKS,	\"USE_ROOT_BLKS\" },	\\",
        "	{ EXT4_MB_USE_RESERVED,		\"USE_RESV\" },		\\",
        "	{ EXT4_MB_STRICT_CHECK,		\"STRICT_CHECK\" })",
        "",
        "#define show_map_flags(flags) __print_flags(flags, \"|\",			\\",
        "	{ EXT4_GET_BLOCKS_CREATE,		\"CREATE\" },		\\",
        "	{ EXT4_GET_BLOCKS_UNWRIT_EXT,		\"UNWRIT\" },		\\",
        "	{ EXT4_GET_BLOCKS_DELALLOC_RESERVE,	\"DELALLOC\" },		\\",
        "	{ EXT4_GET_BLOCKS_PRE_IO,		\"PRE_IO\" },		\\",
        "	{ EXT4_GET_BLOCKS_CONVERT,		\"CONVERT\" },		\\",
        "	{ EXT4_GET_BLOCKS_METADATA_NOFAIL,	\"METADATA_NOFAIL\" },	\\",
        "	{ EXT4_GET_BLOCKS_NO_NORMALIZE,		\"NO_NORMALIZE\" },	\\",
        "	{ EXT4_GET_BLOCKS_CONVERT_UNWRITTEN,	\"CONVERT_UNWRITTEN\" },  \\",
        "	{ EXT4_GET_BLOCKS_ZERO,			\"ZERO\" },		\\",
        "	{ EXT4_GET_BLOCKS_IO_SUBMIT,		\"IO_SUBMIT\" },		\\",
        "	{ EXT4_EX_NOCACHE,			\"EX_NOCACHE\" })",
        "",
        "/*",
        " * __print_flags() requires that all enum values be wrapped in the",
        " * TRACE_DEFINE_ENUM macro so that the enum value can be encoded in the ftrace",
        " * ring buffer.",
        " */",
        "TRACE_DEFINE_ENUM(BH_New);",
        "TRACE_DEFINE_ENUM(BH_Mapped);",
        "TRACE_DEFINE_ENUM(BH_Unwritten);",
        "TRACE_DEFINE_ENUM(BH_Boundary);",
        "",
        "#define show_mflags(flags) __print_flags(flags, \"\",	\\",
        "	{ EXT4_MAP_NEW,		\"N\" },			\\",
        "	{ EXT4_MAP_MAPPED,	\"M\" },			\\",
        "	{ EXT4_MAP_UNWRITTEN,	\"U\" },			\\",
        "	{ EXT4_MAP_BOUNDARY,	\"B\" })",
        "",
        "#define show_free_flags(flags) __print_flags(flags, \"|\",	\\",
        "	{ EXT4_FREE_BLOCKS_METADATA,		\"METADATA\" },	\\",
        "	{ EXT4_FREE_BLOCKS_FORGET,		\"FORGET\" },	\\",
        "	{ EXT4_FREE_BLOCKS_VALIDATED,		\"VALIDATED\" },	\\",
        "	{ EXT4_FREE_BLOCKS_NO_QUOT_UPDATE,	\"NO_QUOTA\" },	\\",
        "	{ EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER,\"1ST_CLUSTER\" },\\",
        "	{ EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER,	\"LAST_CLUSTER\" })",
        "",
        "TRACE_DEFINE_ENUM(ES_WRITTEN_B);",
        "TRACE_DEFINE_ENUM(ES_UNWRITTEN_B);",
        "TRACE_DEFINE_ENUM(ES_DELAYED_B);",
        "TRACE_DEFINE_ENUM(ES_HOLE_B);",
        "TRACE_DEFINE_ENUM(ES_REFERENCED_B);",
        "",
        "#define show_extent_status(status) __print_flags(status, \"\",	\\",
        "	{ EXTENT_STATUS_WRITTEN,	\"W\" },			\\",
        "	{ EXTENT_STATUS_UNWRITTEN,	\"U\" },			\\",
        "	{ EXTENT_STATUS_DELAYED,	\"D\" },			\\",
        "	{ EXTENT_STATUS_HOLE,		\"H\" },			\\",
        "	{ EXTENT_STATUS_REFERENCED,	\"R\" })",
        "",
        "#define show_falloc_mode(mode) __print_flags(mode, \"|\",		\\",
        "	{ FALLOC_FL_KEEP_SIZE,		\"KEEP_SIZE\"},		\\",
        "	{ FALLOC_FL_PUNCH_HOLE,		\"PUNCH_HOLE\"},		\\",
        "	{ FALLOC_FL_COLLAPSE_RANGE,	\"COLLAPSE_RANGE\"},	\\",
        "	{ FALLOC_FL_ZERO_RANGE,		\"ZERO_RANGE\"})",
        "",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_XATTR);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_CROSS_RENAME);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_JOURNAL_FLAG_CHANGE);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_NOMEM);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_SWAP_BOOT);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_RESIZE);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_RENAME_DIR);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_FALLOC_RANGE);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_INODE_JOURNAL_DATA);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_ENCRYPTED_FILENAME);",
        "TRACE_DEFINE_ENUM(EXT4_FC_REASON_MAX);",
        "",
        "#define show_fc_reason(reason)						\\",
        "	__print_symbolic(reason,					\\",
        "		{ EXT4_FC_REASON_XATTR,		\"XATTR\"},		\\",
        "		{ EXT4_FC_REASON_CROSS_RENAME,	\"CROSS_RENAME\"},	\\",
        "		{ EXT4_FC_REASON_JOURNAL_FLAG_CHANGE, \"JOURNAL_FLAG_CHANGE\"}, \\",
        "		{ EXT4_FC_REASON_NOMEM,	\"NO_MEM\"},			\\",
        "		{ EXT4_FC_REASON_SWAP_BOOT,	\"SWAP_BOOT\"},		\\",
        "		{ EXT4_FC_REASON_RESIZE,	\"RESIZE\"},		\\",
        "		{ EXT4_FC_REASON_RENAME_DIR,	\"RENAME_DIR\"},		\\",
        "		{ EXT4_FC_REASON_FALLOC_RANGE,	\"FALLOC_RANGE\"},	\\",
        "		{ EXT4_FC_REASON_INODE_JOURNAL_DATA,	\"INODE_JOURNAL_DATA\"}, \\",
        "		{ EXT4_FC_REASON_ENCRYPTED_FILENAME,	\"ENCRYPTED_FILENAME\"})",
        "",
        "TRACE_DEFINE_ENUM(CR_POWER2_ALIGNED);",
        "TRACE_DEFINE_ENUM(CR_GOAL_LEN_FAST);",
        "TRACE_DEFINE_ENUM(CR_BEST_AVAIL_LEN);",
        "TRACE_DEFINE_ENUM(CR_GOAL_LEN_SLOW);",
        "TRACE_DEFINE_ENUM(CR_ANY_FREE);",
        "",
        "#define show_criteria(cr)                                               \\",
        "	__print_symbolic(cr,                                            \\",
        "			 { CR_POWER2_ALIGNED, \"CR_POWER2_ALIGNED\" },	\\",
        "			 { CR_GOAL_LEN_FAST, \"CR_GOAL_LEN_FAST\" },      \\",
        "			 { CR_BEST_AVAIL_LEN, \"CR_BEST_AVAIL_LEN\" },    \\",
        "			 { CR_GOAL_LEN_SLOW, \"CR_GOAL_LEN_SLOW\" },      \\",
        "			 { CR_ANY_FREE, \"CR_ANY_FREE\" })",
        "",
        "TRACE_EVENT(ext4_other_inode_update_time,",
        "	TP_PROTO(struct inode *inode, ino_t orig_ino),",
        "",
        "	TP_ARGS(inode, orig_ino),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	ino_t,	orig_ino		)",
        "		__field(	uid_t,	uid			)",
        "		__field(	gid_t,	gid			)",
        "		__field(	__u16, mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->orig_ino = orig_ino;",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->uid	= i_uid_read(inode);",
        "		__entry->gid	= i_gid_read(inode);",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d orig_ino %lu ino %lu mode 0%o uid %u gid %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->orig_ino,",
        "		  (unsigned long) __entry->ino, __entry->mode,",
        "		  __entry->uid, __entry->gid)",
        ");",
        "",
        "TRACE_EVENT(ext4_free_inode,",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	uid_t,	uid			)",
        "		__field(	gid_t,	gid			)",
        "		__field(	__u64, blocks			)",
        "		__field(	__u16, mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->uid	= i_uid_read(inode);",
        "		__entry->gid	= i_gid_read(inode);",
        "		__entry->blocks	= inode->i_blocks;",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o uid %u gid %u blocks %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->mode,",
        "		  __entry->uid, __entry->gid, __entry->blocks)",
        ");",
        "",
        "TRACE_EVENT(ext4_request_inode,",
        "	TP_PROTO(struct inode *dir, int mode),",
        "",
        "	TP_ARGS(dir, mode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	dir			)",
        "		__field(	__u16, mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= dir->i_sb->s_dev;",
        "		__entry->dir	= dir->i_ino;",
        "		__entry->mode	= mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d dir %lu mode 0%o\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->dir, __entry->mode)",
        ");",
        "",
        "TRACE_EVENT(ext4_allocate_inode,",
        "	TP_PROTO(struct inode *inode, struct inode *dir, int mode),",
        "",
        "	TP_ARGS(inode, dir, mode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	ino_t,	dir			)",
        "		__field(	__u16,	mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->dir	= dir->i_ino;",
        "		__entry->mode	= mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu dir %lu mode 0%o\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned long) __entry->dir, __entry->mode)",
        ");",
        "",
        "TRACE_EVENT(ext4_evict_inode,",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	int,	nlink			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->nlink	= inode->i_nlink;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu nlink %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->nlink)",
        ");",
        "",
        "TRACE_EVENT(ext4_drop_inode,",
        "	TP_PROTO(struct inode *inode, int drop),",
        "",
        "	TP_ARGS(inode, drop),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	int,	drop			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->drop	= drop;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu drop %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->drop)",
        ");",
        "",
        "TRACE_EVENT(ext4_nfs_commit_metadata,",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino)",
        ");",
        "",
        "TRACE_EVENT(ext4_mark_inode_dirty,",
        "	TP_PROTO(struct inode *inode, unsigned long IP),",
        "",
        "	TP_ARGS(inode, IP),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(unsigned long,	ip			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->ip	= IP;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu caller %pS\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, (void *)__entry->ip)",
        ");",
        "",
        "TRACE_EVENT(ext4_begin_ordered_truncate,",
        "	TP_PROTO(struct inode *inode, loff_t new_size),",
        "",
        "	TP_ARGS(inode, new_size),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	new_size		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->new_size	= new_size;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu new_size %lld\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->new_size)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__write_begin,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len),",
        "",
        "	TP_ARGS(inode, pos, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	pos			)",
        "		__field(	unsigned int, len		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->pos	= pos;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu pos %lld len %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->pos, __entry->len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__write_begin, ext4_write_begin,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len),",
        "",
        "	TP_ARGS(inode, pos, len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__write_begin, ext4_da_write_begin,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len),",
        "",
        "	TP_ARGS(inode, pos, len)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__write_end,",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,",
        "			unsigned int copied),",
        "",
        "	TP_ARGS(inode, pos, len, copied),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	pos			)",
        "		__field(	unsigned int, len		)",
        "		__field(	unsigned int, copied		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->pos	= pos;",
        "		__entry->len	= len;",
        "		__entry->copied	= copied;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu pos %lld len %u copied %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->pos, __entry->len, __entry->copied)",
        ");",
        "",
        "DEFINE_EVENT(ext4__write_end, ext4_write_end,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,",
        "		 unsigned int copied),",
        "",
        "	TP_ARGS(inode, pos, len, copied)",
        ");",
        "",
        "DEFINE_EVENT(ext4__write_end, ext4_journalled_write_end,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,",
        "		 unsigned int copied),",
        "",
        "	TP_ARGS(inode, pos, len, copied)",
        ");",
        "",
        "DEFINE_EVENT(ext4__write_end, ext4_da_write_end,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,",
        "		 unsigned int copied),",
        "",
        "	TP_ARGS(inode, pos, len, copied)",
        ");",
        "",
        "TRACE_EVENT(ext4_writepages,",
        "	TP_PROTO(struct inode *inode, struct writeback_control *wbc),",
        "",
        "	TP_ARGS(inode, wbc),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	long,	nr_to_write		)",
        "		__field(	long,	pages_skipped		)",
        "		__field(	loff_t,	range_start		)",
        "		__field(	loff_t,	range_end		)",
        "		__field(       pgoff_t,	writeback_index		)",
        "		__field(	int,	sync_mode		)",
        "		__field(	char,	for_kupdate		)",
        "		__field(	char,	range_cyclic		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->nr_to_write	= wbc->nr_to_write;",
        "		__entry->pages_skipped	= wbc->pages_skipped;",
        "		__entry->range_start	= wbc->range_start;",
        "		__entry->range_end	= wbc->range_end;",
        "		__entry->writeback_index = inode->i_mapping->writeback_index;",
        "		__entry->sync_mode	= wbc->sync_mode;",
        "		__entry->for_kupdate	= wbc->for_kupdate;",
        "		__entry->range_cyclic	= wbc->range_cyclic;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu nr_to_write %ld pages_skipped %ld \"",
        "		  \"range_start %lld range_end %lld sync_mode %d \"",
        "		  \"for_kupdate %d range_cyclic %d writeback_index %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->nr_to_write,",
        "		  __entry->pages_skipped, __entry->range_start,",
        "		  __entry->range_end, __entry->sync_mode,",
        "		  __entry->for_kupdate, __entry->range_cyclic,",
        "		  (unsigned long) __entry->writeback_index)",
        ");",
        "",
        "TRACE_EVENT(ext4_da_write_pages,",
        "	TP_PROTO(struct inode *inode, pgoff_t first_page,",
        "		 struct writeback_control *wbc),",
        "",
        "	TP_ARGS(inode, first_page, wbc),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(      pgoff_t,	first_page		)",
        "		__field(	 long,	nr_to_write		)",
        "		__field(	  int,	sync_mode		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->first_page	= first_page;",
        "		__entry->nr_to_write	= wbc->nr_to_write;",
        "		__entry->sync_mode	= wbc->sync_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu first_page %lu nr_to_write %ld \"",
        "		  \"sync_mode %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->first_page,",
        "		  __entry->nr_to_write, __entry->sync_mode)",
        ");",
        "",
        "TRACE_EVENT(ext4_da_write_pages_extent,",
        "	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map),",
        "",
        "	TP_ARGS(inode, map),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	lblk			)",
        "		__field(	__u32,	len			)",
        "		__field(	__u32,	flags			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->lblk		= map->m_lblk;",
        "		__entry->len		= map->m_len;",
        "		__entry->flags		= map->m_flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %llu len %u flags %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->lblk, __entry->len,",
        "		  show_mflags(__entry->flags))",
        ");",
        "",
        "TRACE_EVENT(ext4_writepages_result,",
        "	TP_PROTO(struct inode *inode, struct writeback_control *wbc,",
        "			int ret, int pages_written),",
        "",
        "	TP_ARGS(inode, wbc, ret, pages_written),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	int,	ret			)",
        "		__field(	int,	pages_written		)",
        "		__field(	long,	pages_skipped		)",
        "		__field(       pgoff_t,	writeback_index		)",
        "		__field(	int,	sync_mode		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->ret		= ret;",
        "		__entry->pages_written	= pages_written;",
        "		__entry->pages_skipped	= wbc->pages_skipped;",
        "		__entry->writeback_index = inode->i_mapping->writeback_index;",
        "		__entry->sync_mode	= wbc->sync_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu ret %d pages_written %d pages_skipped %ld \"",
        "		  \"sync_mode %d writeback_index %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->ret,",
        "		  __entry->pages_written, __entry->pages_skipped,",
        "		  __entry->sync_mode,",
        "		  (unsigned long) __entry->writeback_index)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__folio_op,",
        "	TP_PROTO(struct inode *inode, struct folio *folio),",
        "",
        "	TP_ARGS(inode, folio),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	pgoff_t, index			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->index	= folio->index;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu folio_index %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned long) __entry->index)",
        ");",
        "",
        "DEFINE_EVENT(ext4__folio_op, ext4_read_folio,",
        "",
        "	TP_PROTO(struct inode *inode, struct folio *folio),",
        "",
        "	TP_ARGS(inode, folio)",
        ");",
        "",
        "DEFINE_EVENT(ext4__folio_op, ext4_release_folio,",
        "",
        "	TP_PROTO(struct inode *inode, struct folio *folio),",
        "",
        "	TP_ARGS(inode, folio)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4_invalidate_folio_op,",
        "	TP_PROTO(struct folio *folio, size_t offset, size_t length),",
        "",
        "	TP_ARGS(folio, offset, length),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	pgoff_t, index			)",
        "		__field(	size_t, offset			)",
        "		__field(	size_t, length			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= folio->mapping->host->i_sb->s_dev;",
        "		__entry->ino	= folio->mapping->host->i_ino;",
        "		__entry->index	= folio->index;",
        "		__entry->offset	= offset;",
        "		__entry->length	= length;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu folio_index %lu offset %zu length %zu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned long) __entry->index,",
        "		  __entry->offset, __entry->length)",
        ");",
        "",
        "DEFINE_EVENT(ext4_invalidate_folio_op, ext4_invalidate_folio,",
        "	TP_PROTO(struct folio *folio, size_t offset, size_t length),",
        "",
        "	TP_ARGS(folio, offset, length)",
        ");",
        "",
        "DEFINE_EVENT(ext4_invalidate_folio_op, ext4_journalled_invalidate_folio,",
        "	TP_PROTO(struct folio *folio, size_t offset, size_t length),",
        "",
        "	TP_ARGS(folio, offset, length)",
        ");",
        "",
        "TRACE_EVENT(ext4_discard_blocks,",
        "	TP_PROTO(struct super_block *sb, unsigned long long blk,",
        "			unsigned long long count),",
        "",
        "	TP_ARGS(sb, blk, count),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u64,	blk			)",
        "		__field(	__u64,	count			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->blk	= blk;",
        "		__entry->count	= count;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d blk %llu count %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->blk, __entry->count)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__mb_new_pa,",
        "	TP_PROTO(struct ext4_allocation_context *ac,",
        "		 struct ext4_prealloc_space *pa),",
        "",
        "	TP_ARGS(ac, pa),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	pa_pstart		)",
        "		__field(	__u64,	pa_lstart		)",
        "		__field(	__u32,	pa_len			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= ac->ac_sb->s_dev;",
        "		__entry->ino		= ac->ac_inode->i_ino;",
        "		__entry->pa_pstart	= pa->pa_pstart;",
        "		__entry->pa_lstart	= pa->pa_lstart;",
        "		__entry->pa_len		= pa->pa_len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu pstart %llu len %u lstart %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->pa_pstart, __entry->pa_len, __entry->pa_lstart)",
        ");",
        "",
        "DEFINE_EVENT(ext4__mb_new_pa, ext4_mb_new_inode_pa,",
        "",
        "	TP_PROTO(struct ext4_allocation_context *ac,",
        "		 struct ext4_prealloc_space *pa),",
        "",
        "	TP_ARGS(ac, pa)",
        ");",
        "",
        "DEFINE_EVENT(ext4__mb_new_pa, ext4_mb_new_group_pa,",
        "",
        "	TP_PROTO(struct ext4_allocation_context *ac,",
        "		 struct ext4_prealloc_space *pa),",
        "",
        "	TP_ARGS(ac, pa)",
        ");",
        "",
        "TRACE_EVENT(ext4_mb_release_inode_pa,",
        "	TP_PROTO(struct ext4_prealloc_space *pa,",
        "		 unsigned long long block, unsigned int count),",
        "",
        "	TP_ARGS(pa, block, count),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	block			)",
        "		__field(	__u32,	count			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= pa->pa_inode->i_sb->s_dev;",
        "		__entry->ino		= pa->pa_inode->i_ino;",
        "		__entry->block		= block;",
        "		__entry->count		= count;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu block %llu count %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->block, __entry->count)",
        ");",
        "",
        "TRACE_EVENT(ext4_mb_release_group_pa,",
        "	TP_PROTO(struct super_block *sb, struct ext4_prealloc_space *pa),",
        "",
        "	TP_ARGS(sb, pa),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u64,	pa_pstart		)",
        "		__field(	__u32,	pa_len			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->pa_pstart	= pa->pa_pstart;",
        "		__entry->pa_len		= pa->pa_len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d pstart %llu len %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->pa_pstart, __entry->pa_len)",
        ");",
        "",
        "TRACE_EVENT(ext4_discard_preallocations,",
        "	TP_PROTO(struct inode *inode, unsigned int len),",
        "",
        "	TP_ARGS(inode, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	unsigned int,	len		)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu len: %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->len)",
        ");",
        "",
        "TRACE_EVENT(ext4_mb_discard_preallocations,",
        "	TP_PROTO(struct super_block *sb, int needed),",
        "",
        "	TP_ARGS(sb, needed),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	int,	needed			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->needed	= needed;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d needed %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->needed)",
        ");",
        "",
        "TRACE_EVENT(ext4_request_blocks,",
        "	TP_PROTO(struct ext4_allocation_request *ar),",
        "",
        "	TP_ARGS(ar),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	unsigned int, len		)",
        "		__field(	__u32,  logical			)",
        "		__field(	__u32,	lleft			)",
        "		__field(	__u32,	lright			)",
        "		__field(	__u64,	goal			)",
        "		__field(	__u64,	pleft			)",
        "		__field(	__u64,	pright			)",
        "		__field(	unsigned int, flags		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= ar->inode->i_sb->s_dev;",
        "		__entry->ino	= ar->inode->i_ino;",
        "		__entry->len	= ar->len;",
        "		__entry->logical = ar->logical;",
        "		__entry->goal	= ar->goal;",
        "		__entry->lleft	= ar->lleft;",
        "		__entry->lright	= ar->lright;",
        "		__entry->pleft	= ar->pleft;",
        "		__entry->pright	= ar->pright;",
        "		__entry->flags	= ar->flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu flags %s len %u lblk %u goal %llu \"",
        "		  \"lleft %u lright %u pleft %llu pright %llu \",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, show_mballoc_flags(__entry->flags),",
        "		  __entry->len, __entry->logical, __entry->goal,",
        "		  __entry->lleft, __entry->lright, __entry->pleft,",
        "		  __entry->pright)",
        ");",
        "",
        "TRACE_EVENT(ext4_allocate_blocks,",
        "	TP_PROTO(struct ext4_allocation_request *ar, unsigned long long block),",
        "",
        "	TP_ARGS(ar, block),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	block			)",
        "		__field(	unsigned int, len		)",
        "		__field(	__u32,  logical			)",
        "		__field(	__u32,	lleft			)",
        "		__field(	__u32,	lright			)",
        "		__field(	__u64,	goal			)",
        "		__field(	__u64,	pleft			)",
        "		__field(	__u64,	pright			)",
        "		__field(	unsigned int, flags		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= ar->inode->i_sb->s_dev;",
        "		__entry->ino	= ar->inode->i_ino;",
        "		__entry->block	= block;",
        "		__entry->len	= ar->len;",
        "		__entry->logical = ar->logical;",
        "		__entry->goal	= ar->goal;",
        "		__entry->lleft	= ar->lleft;",
        "		__entry->lright	= ar->lright;",
        "		__entry->pleft	= ar->pleft;",
        "		__entry->pright	= ar->pright;",
        "		__entry->flags	= ar->flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu flags %s len %u block %llu lblk %u \"",
        "		  \"goal %llu lleft %u lright %u pleft %llu pright %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, show_mballoc_flags(__entry->flags),",
        "		  __entry->len, __entry->block, __entry->logical,",
        "		  __entry->goal,  __entry->lleft, __entry->lright,",
        "		  __entry->pleft, __entry->pright)",
        ");",
        "",
        "TRACE_EVENT(ext4_free_blocks,",
        "	TP_PROTO(struct inode *inode, __u64 block, unsigned long count,",
        "		 int flags),",
        "",
        "	TP_ARGS(inode, block, count, flags),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	block			)",
        "		__field(	unsigned long,	count		)",
        "		__field(	int,	flags			)",
        "		__field(	__u16,	mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->block		= block;",
        "		__entry->count		= count;",
        "		__entry->flags		= flags;",
        "		__entry->mode		= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o block %llu count %lu flags %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->mode, __entry->block, __entry->count,",
        "		  show_free_flags(__entry->flags))",
        ");",
        "",
        "TRACE_EVENT(ext4_sync_file_enter,",
        "	TP_PROTO(struct file *file, int datasync),",
        "",
        "	TP_ARGS(file, datasync),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	ino_t,	parent			)",
        "		__field(	int,	datasync		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		struct dentry *dentry = file->f_path.dentry;",
        "",
        "		__entry->dev		= dentry->d_sb->s_dev;",
        "		__entry->ino		= d_inode(dentry)->i_ino;",
        "		__entry->datasync	= datasync;",
        "		__entry->parent		= d_inode(dentry->d_parent)->i_ino;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu parent %lu datasync %d \",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned long) __entry->parent, __entry->datasync)",
        ");",
        "",
        "TRACE_EVENT(ext4_sync_file_exit,",
        "	TP_PROTO(struct inode *inode, int ret),",
        "",
        "	TP_ARGS(inode, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	int,	ret			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->ret		= ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu ret %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->ret)",
        ");",
        "",
        "TRACE_EVENT(ext4_sync_fs,",
        "	TP_PROTO(struct super_block *sb, int wait),",
        "",
        "	TP_ARGS(sb, wait),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	int,	wait			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->wait	= wait;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d wait %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->wait)",
        ");",
        "",
        "TRACE_EVENT(ext4_alloc_da_blocks,",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field( unsigned int,	data_blocks		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->data_blocks = EXT4_I(inode)->i_reserved_data_blocks;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu reserved_data_blocks %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->data_blocks)",
        ");",
        "",
        "TRACE_EVENT(ext4_mballoc_alloc,",
        "	TP_PROTO(struct ext4_allocation_context *ac),",
        "",
        "	TP_ARGS(ac),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u32, 	orig_logical		)",
        "		__field(	  int,	orig_start		)",
        "		__field(	__u32, 	orig_group		)",
        "		__field(	  int,	orig_len		)",
        "		__field(	__u32, 	goal_logical		)",
        "		__field(	  int,	goal_start		)",
        "		__field(	__u32, 	goal_group		)",
        "		__field(	  int,	goal_len		)",
        "		__field(	__u32, 	result_logical		)",
        "		__field(	  int,	result_start		)",
        "		__field(	__u32, 	result_group		)",
        "		__field(	  int,	result_len		)",
        "		__field(	__u16,	found			)",
        "		__field(	__u16,	groups			)",
        "		__field(	__u16,	buddy			)",
        "		__field(	__u16,	flags			)",
        "		__field(	__u16,	tail			)",
        "		__field(	__u8,	cr			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= ac->ac_inode->i_sb->s_dev;",
        "		__entry->ino		= ac->ac_inode->i_ino;",
        "		__entry->orig_logical	= ac->ac_o_ex.fe_logical;",
        "		__entry->orig_start	= ac->ac_o_ex.fe_start;",
        "		__entry->orig_group	= ac->ac_o_ex.fe_group;",
        "		__entry->orig_len	= ac->ac_o_ex.fe_len;",
        "		__entry->goal_logical	= ac->ac_g_ex.fe_logical;",
        "		__entry->goal_start	= ac->ac_g_ex.fe_start;",
        "		__entry->goal_group	= ac->ac_g_ex.fe_group;",
        "		__entry->goal_len	= ac->ac_g_ex.fe_len;",
        "		__entry->result_logical	= ac->ac_f_ex.fe_logical;",
        "		__entry->result_start	= ac->ac_f_ex.fe_start;",
        "		__entry->result_group	= ac->ac_f_ex.fe_group;",
        "		__entry->result_len	= ac->ac_f_ex.fe_len;",
        "		__entry->found		= ac->ac_found;",
        "		__entry->flags		= ac->ac_flags;",
        "		__entry->groups		= ac->ac_groups_scanned;",
        "		__entry->buddy		= ac->ac_buddy;",
        "		__entry->tail		= ac->ac_tail;",
        "		__entry->cr		= ac->ac_criteria;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d inode %lu orig %u/%d/%u@%u goal %u/%d/%u@%u \"",
        "		  \"result %u/%d/%u@%u blks %u grps %u cr %s flags %s \"",
        "		  \"tail %u broken %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->orig_group, __entry->orig_start,",
        "		  __entry->orig_len, __entry->orig_logical,",
        "		  __entry->goal_group, __entry->goal_start,",
        "		  __entry->goal_len, __entry->goal_logical,",
        "		  __entry->result_group, __entry->result_start,",
        "		  __entry->result_len, __entry->result_logical,",
        "		  __entry->found, __entry->groups, show_criteria(__entry->cr),",
        "		  show_mballoc_flags(__entry->flags), __entry->tail,",
        "		  __entry->buddy ? 1 << __entry->buddy : 0)",
        ");",
        "",
        "TRACE_EVENT(ext4_mballoc_prealloc,",
        "	TP_PROTO(struct ext4_allocation_context *ac),",
        "",
        "	TP_ARGS(ac),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u32, 	orig_logical		)",
        "		__field(	  int,	orig_start		)",
        "		__field(	__u32, 	orig_group		)",
        "		__field(	  int,	orig_len		)",
        "		__field(	__u32, 	result_logical		)",
        "		__field(	  int,	result_start		)",
        "		__field(	__u32, 	result_group		)",
        "		__field(	  int,	result_len		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= ac->ac_inode->i_sb->s_dev;",
        "		__entry->ino		= ac->ac_inode->i_ino;",
        "		__entry->orig_logical	= ac->ac_o_ex.fe_logical;",
        "		__entry->orig_start	= ac->ac_o_ex.fe_start;",
        "		__entry->orig_group	= ac->ac_o_ex.fe_group;",
        "		__entry->orig_len	= ac->ac_o_ex.fe_len;",
        "		__entry->result_logical	= ac->ac_b_ex.fe_logical;",
        "		__entry->result_start	= ac->ac_b_ex.fe_start;",
        "		__entry->result_group	= ac->ac_b_ex.fe_group;",
        "		__entry->result_len	= ac->ac_b_ex.fe_len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d inode %lu orig %u/%d/%u@%u result %u/%d/%u@%u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->orig_group, __entry->orig_start,",
        "		  __entry->orig_len, __entry->orig_logical,",
        "		  __entry->result_group, __entry->result_start,",
        "		  __entry->result_len, __entry->result_logical)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__mballoc,",
        "	TP_PROTO(struct super_block *sb,",
        "		 struct inode *inode,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, inode, group, start, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	  int,	result_start		)",
        "		__field(	__u32, 	result_group		)",
        "		__field(	  int,	result_len		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->ino		= inode ? inode->i_ino : 0;",
        "		__entry->result_start	= start;",
        "		__entry->result_group	= group;",
        "		__entry->result_len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d inode %lu extent %u/%d/%d \",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->result_group, __entry->result_start,",
        "		  __entry->result_len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__mballoc, ext4_mballoc_discard,",
        "",
        "	TP_PROTO(struct super_block *sb,",
        "		 struct inode *inode,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, inode, group, start, len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__mballoc, ext4_mballoc_free,",
        "",
        "	TP_PROTO(struct super_block *sb,",
        "		 struct inode *inode,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, inode, group, start, len)",
        ");",
        "",
        "TRACE_EVENT(ext4_forget,",
        "	TP_PROTO(struct inode *inode, int is_metadata, __u64 block),",
        "",
        "	TP_ARGS(inode, is_metadata, block),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	block			)",
        "		__field(	int,	is_metadata		)",
        "		__field(	__u16,	mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->block	= block;",
        "		__entry->is_metadata = is_metadata;",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o is_metadata %d block %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->mode, __entry->is_metadata, __entry->block)",
        ");",
        "",
        "TRACE_EVENT(ext4_da_update_reserve_space,",
        "	TP_PROTO(struct inode *inode, int used_blocks, int quota_claim),",
        "",
        "	TP_ARGS(inode, used_blocks, quota_claim),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	i_blocks		)",
        "		__field(	int,	used_blocks		)",
        "		__field(	int,	reserved_data_blocks	)",
        "		__field(	int,	quota_claim		)",
        "		__field(	__u16,	mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->i_blocks = inode->i_blocks;",
        "		__entry->used_blocks = used_blocks;",
        "		__entry->reserved_data_blocks =",
        "				EXT4_I(inode)->i_reserved_data_blocks;",
        "		__entry->quota_claim = quota_claim;",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o i_blocks %llu used_blocks %d \"",
        "		  \"reserved_data_blocks %d quota_claim %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->mode, __entry->i_blocks,",
        "		  __entry->used_blocks, __entry->reserved_data_blocks,",
        "		  __entry->quota_claim)",
        ");",
        "",
        "TRACE_EVENT(ext4_da_reserve_space,",
        "	TP_PROTO(struct inode *inode, int nr_resv),",
        "",
        "	TP_ARGS(inode, nr_resv),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	i_blocks		)",
        "		__field(	int,	reserve_blocks		)",
        "		__field(	int,	reserved_data_blocks	)",
        "		__field(	__u16,  mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->i_blocks = inode->i_blocks;",
        "		__entry->reserve_blocks = nr_resv;",
        "		__entry->reserved_data_blocks = EXT4_I(inode)->i_reserved_data_blocks;",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o i_blocks %llu reserve_blocks %d\"",
        "		  \"reserved_data_blocks %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->mode, __entry->i_blocks,",
        "		  __entry->reserve_blocks, __entry->reserved_data_blocks)",
        ");",
        "",
        "TRACE_EVENT(ext4_da_release_space,",
        "	TP_PROTO(struct inode *inode, int freed_blocks),",
        "",
        "	TP_ARGS(inode, freed_blocks),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	__u64,	i_blocks		)",
        "		__field(	int,	freed_blocks		)",
        "		__field(	int,	reserved_data_blocks	)",
        "		__field(	__u16,  mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->i_blocks = inode->i_blocks;",
        "		__entry->freed_blocks = freed_blocks;",
        "		__entry->reserved_data_blocks = EXT4_I(inode)->i_reserved_data_blocks;",
        "		__entry->mode	= inode->i_mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu mode 0%o i_blocks %llu freed_blocks %d \"",
        "		  \"reserved_data_blocks %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->mode, __entry->i_blocks,",
        "		  __entry->freed_blocks, __entry->reserved_data_blocks)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__bitmap_load,",
        "	TP_PROTO(struct super_block *sb, unsigned long group),",
        "",
        "	TP_ARGS(sb, group),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u32,	group			)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->group	= group;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d group %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->group)",
        ");",
        "",
        "DEFINE_EVENT(ext4__bitmap_load, ext4_mb_bitmap_load,",
        "",
        "	TP_PROTO(struct super_block *sb, unsigned long group),",
        "",
        "	TP_ARGS(sb, group)",
        ");",
        "",
        "DEFINE_EVENT(ext4__bitmap_load, ext4_mb_buddy_bitmap_load,",
        "",
        "	TP_PROTO(struct super_block *sb, unsigned long group),",
        "",
        "	TP_ARGS(sb, group)",
        ");",
        "",
        "DEFINE_EVENT(ext4__bitmap_load, ext4_load_inode_bitmap,",
        "",
        "	TP_PROTO(struct super_block *sb, unsigned long group),",
        "",
        "	TP_ARGS(sb, group)",
        ");",
        "",
        "TRACE_EVENT(ext4_read_block_bitmap_load,",
        "	TP_PROTO(struct super_block *sb, unsigned long group, bool prefetch),",
        "",
        "	TP_ARGS(sb, group, prefetch),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u32,	group			)",
        "		__field(	bool,	prefetch		)",
        "",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->group	= group;",
        "		__entry->prefetch = prefetch;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d group %u prefetch %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->group, __entry->prefetch)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__fallocate_mode,",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len, int mode),",
        "",
        "	TP_ARGS(inode, offset, len, mode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	offset			)",
        "		__field(	loff_t, len			)",
        "		__field(	int,	mode			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->offset	= offset;",
        "		__entry->len	= len;",
        "		__entry->mode	= mode;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu offset %lld len %lld mode %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->offset, __entry->len,",
        "		  show_falloc_mode(__entry->mode))",
        ");",
        "",
        "DEFINE_EVENT(ext4__fallocate_mode, ext4_fallocate_enter,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len, int mode),",
        "",
        "	TP_ARGS(inode, offset, len, mode)",
        ");",
        "",
        "DEFINE_EVENT(ext4__fallocate_mode, ext4_punch_hole,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len, int mode),",
        "",
        "	TP_ARGS(inode, offset, len, mode)",
        ");",
        "",
        "DEFINE_EVENT(ext4__fallocate_mode, ext4_zero_range,",
        "",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len, int mode),",
        "",
        "	TP_ARGS(inode, offset, len, mode)",
        ");",
        "",
        "TRACE_EVENT(ext4_fallocate_exit,",
        "	TP_PROTO(struct inode *inode, loff_t offset,",
        "		 unsigned int max_blocks, int ret),",
        "",
        "	TP_ARGS(inode, offset, max_blocks, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	pos			)",
        "		__field(	unsigned int,	blocks		)",
        "		__field(	int, 	ret			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->pos	= offset;",
        "		__entry->blocks	= max_blocks;",
        "		__entry->ret	= ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu pos %lld blocks %u ret %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->pos, __entry->blocks,",
        "		  __entry->ret)",
        ");",
        "",
        "TRACE_EVENT(ext4_unlink_enter,",
        "	TP_PROTO(struct inode *parent, struct dentry *dentry),",
        "",
        "	TP_ARGS(parent, dentry),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	ino_t,	parent			)",
        "		__field(	loff_t,	size			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= dentry->d_sb->s_dev;",
        "		__entry->ino		= d_inode(dentry)->i_ino;",
        "		__entry->parent		= parent->i_ino;",
        "		__entry->size		= d_inode(dentry)->i_size;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu size %lld parent %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->size,",
        "		  (unsigned long) __entry->parent)",
        ");",
        "",
        "TRACE_EVENT(ext4_unlink_exit,",
        "	TP_PROTO(struct dentry *dentry, int ret),",
        "",
        "	TP_ARGS(dentry, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	int,	ret			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= dentry->d_sb->s_dev;",
        "		__entry->ino		= d_inode(dentry)->i_ino;",
        "		__entry->ret		= ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu ret %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->ret)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__truncate,",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	__u64,		blocks		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev    = inode->i_sb->s_dev;",
        "		__entry->ino    = inode->i_ino;",
        "		__entry->blocks	= inode->i_blocks;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu blocks %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->blocks)",
        ");",
        "",
        "DEFINE_EVENT(ext4__truncate, ext4_truncate_enter,",
        "",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode)",
        ");",
        "",
        "DEFINE_EVENT(ext4__truncate, ext4_truncate_exit,",
        "",
        "	TP_PROTO(struct inode *inode),",
        "",
        "	TP_ARGS(inode)",
        ");",
        "",
        "/* 'ux' is the unwritten extent. */",
        "TRACE_EVENT(ext4_ext_convert_to_initialized_enter,",
        "	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map,",
        "		 struct ext4_extent *ux),",
        "",
        "	TP_ARGS(inode, map, ux),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_lblk_t,	m_lblk	)",
        "		__field(	unsigned,	m_len	)",
        "		__field(	ext4_lblk_t,	u_lblk	)",
        "		__field(	unsigned,	u_len	)",
        "		__field(	ext4_fsblk_t,	u_pblk	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->m_lblk		= map->m_lblk;",
        "		__entry->m_len		= map->m_len;",
        "		__entry->u_lblk		= le32_to_cpu(ux->ee_block);",
        "		__entry->u_len		= ext4_ext_get_actual_len(ux);",
        "		__entry->u_pblk		= ext4_ext_pblock(ux);",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu m_lblk %u m_len %u u_lblk %u u_len %u \"",
        "		  \"u_pblk %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->m_lblk, __entry->m_len,",
        "		  __entry->u_lblk, __entry->u_len, __entry->u_pblk)",
        ");",
        "",
        "/*",
        " * 'ux' is the unwritten extent.",
        " * 'ix' is the initialized extent to which blocks are transferred.",
        " */",
        "TRACE_EVENT(ext4_ext_convert_to_initialized_fastpath,",
        "	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map,",
        "		 struct ext4_extent *ux, struct ext4_extent *ix),",
        "",
        "	TP_ARGS(inode, map, ux, ix),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_lblk_t,	m_lblk	)",
        "		__field(	unsigned,	m_len	)",
        "		__field(	ext4_lblk_t,	u_lblk	)",
        "		__field(	unsigned,	u_len	)",
        "		__field(	ext4_fsblk_t,	u_pblk	)",
        "		__field(	ext4_lblk_t,	i_lblk	)",
        "		__field(	unsigned,	i_len	)",
        "		__field(	ext4_fsblk_t,	i_pblk	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->m_lblk		= map->m_lblk;",
        "		__entry->m_len		= map->m_len;",
        "		__entry->u_lblk		= le32_to_cpu(ux->ee_block);",
        "		__entry->u_len		= ext4_ext_get_actual_len(ux);",
        "		__entry->u_pblk		= ext4_ext_pblock(ux);",
        "		__entry->i_lblk		= le32_to_cpu(ix->ee_block);",
        "		__entry->i_len		= ext4_ext_get_actual_len(ix);",
        "		__entry->i_pblk		= ext4_ext_pblock(ix);",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu m_lblk %u m_len %u \"",
        "		  \"u_lblk %u u_len %u u_pblk %llu \"",
        "		  \"i_lblk %u i_len %u i_pblk %llu \",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->m_lblk, __entry->m_len,",
        "		  __entry->u_lblk, __entry->u_len, __entry->u_pblk,",
        "		  __entry->i_lblk, __entry->i_len, __entry->i_pblk)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__map_blocks_enter,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk,",
        "		 unsigned int len, unsigned int flags),",
        "",
        "	TP_ARGS(inode, lblk, len, flags),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	unsigned int,	len		)",
        "		__field(	unsigned int,	flags		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev    = inode->i_sb->s_dev;",
        "		__entry->ino    = inode->i_ino;",
        "		__entry->lblk	= lblk;",
        "		__entry->len	= len;",
        "		__entry->flags	= flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %u len %u flags %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->len, show_map_flags(__entry->flags))",
        ");",
        "",
        "DEFINE_EVENT(ext4__map_blocks_enter, ext4_ext_map_blocks_enter,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk,",
        "		 unsigned len, unsigned flags),",
        "",
        "	TP_ARGS(inode, lblk, len, flags)",
        ");",
        "",
        "DEFINE_EVENT(ext4__map_blocks_enter, ext4_ind_map_blocks_enter,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk,",
        "		 unsigned len, unsigned flags),",
        "",
        "	TP_ARGS(inode, lblk, len, flags)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__map_blocks_exit,",
        "	TP_PROTO(struct inode *inode, unsigned flags, struct ext4_map_blocks *map,",
        "		 int ret),",
        "",
        "	TP_ARGS(inode, flags, map, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	unsigned int,	flags		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	unsigned int,	len		)",
        "		__field(	unsigned int,	mflags		)",
        "		__field(	int,		ret		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev    = inode->i_sb->s_dev;",
        "		__entry->ino    = inode->i_ino;",
        "		__entry->flags	= flags;",
        "		__entry->pblk	= map->m_pblk;",
        "		__entry->lblk	= map->m_lblk;",
        "		__entry->len	= map->m_len;",
        "		__entry->mflags	= map->m_flags;",
        "		__entry->ret	= ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu flags %s lblk %u pblk %llu len %u \"",
        "		  \"mflags %s ret %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  show_map_flags(__entry->flags), __entry->lblk, __entry->pblk,",
        "		  __entry->len, show_mflags(__entry->mflags), __entry->ret)",
        ");",
        "",
        "DEFINE_EVENT(ext4__map_blocks_exit, ext4_ext_map_blocks_exit,",
        "	TP_PROTO(struct inode *inode, unsigned flags,",
        "		 struct ext4_map_blocks *map, int ret),",
        "",
        "	TP_ARGS(inode, flags, map, ret)",
        ");",
        "",
        "DEFINE_EVENT(ext4__map_blocks_exit, ext4_ind_map_blocks_exit,",
        "	TP_PROTO(struct inode *inode, unsigned flags,",
        "		 struct ext4_map_blocks *map, int ret),",
        "",
        "	TP_ARGS(inode, flags, map, ret)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_load_extent,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk, ext4_fsblk_t pblk),",
        "",
        "	TP_ARGS(inode, lblk, pblk),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev    = inode->i_sb->s_dev;",
        "		__entry->ino    = inode->i_ino;",
        "		__entry->pblk	= pblk;",
        "		__entry->lblk	= lblk;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %u pblk %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->pblk)",
        ");",
        "",
        "TRACE_EVENT(ext4_load_inode,",
        "	TP_PROTO(struct super_block *sb, unsigned long ino),",
        "",
        "	TP_ARGS(sb, ino),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev		)",
        "		__field(	ino_t,	ino		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->ino		= ino;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %ld\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino)",
        ");",
        "",
        "TRACE_EVENT(ext4_journal_start_sb,",
        "	TP_PROTO(struct super_block *sb, int blocks, int rsv_blocks,",
        "		 int revoke_creds, int type, unsigned long IP),",
        "",
        "	TP_ARGS(sb, blocks, rsv_blocks, revoke_creds, type, IP),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	unsigned long,	ip		)",
        "		__field(	int,		blocks		)",
        "		__field(	int,		rsv_blocks	)",
        "		__field(	int,		revoke_creds	)",
        "		__field(	int,		type		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		 = sb->s_dev;",
        "		__entry->ip		 = IP;",
        "		__entry->blocks		 = blocks;",
        "		__entry->rsv_blocks	 = rsv_blocks;",
        "		__entry->revoke_creds	 = revoke_creds;",
        "		__entry->type		 = type;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d blocks %d, rsv_blocks %d, revoke_creds %d,\"",
        "		  \" type %d, caller %pS\", MAJOR(__entry->dev),",
        "		  MINOR(__entry->dev), __entry->blocks, __entry->rsv_blocks,",
        "		  __entry->revoke_creds, __entry->type, (void *)__entry->ip)",
        ");",
        "",
        "TRACE_EVENT(ext4_journal_start_inode,",
        "	TP_PROTO(struct inode *inode, int blocks, int rsv_blocks,",
        "		 int revoke_creds, int type, unsigned long IP),",
        "",
        "	TP_ARGS(inode, blocks, rsv_blocks, revoke_creds, type, IP),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	unsigned long,	ino		)",
        "		__field(	dev_t,		dev		)",
        "		__field(	unsigned long,	ip		)",
        "		__field(	int,		blocks		)",
        "		__field(	int,		rsv_blocks	)",
        "		__field(	int,		revoke_creds	)",
        "		__field(	int,		type		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		 = inode->i_sb->s_dev;",
        "		__entry->ip		 = IP;",
        "		__entry->blocks		 = blocks;",
        "		__entry->rsv_blocks	 = rsv_blocks;",
        "		__entry->revoke_creds	 = revoke_creds;",
        "		__entry->type		 = type;",
        "		__entry->ino		 = inode->i_ino;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d blocks %d, rsv_blocks %d, revoke_creds %d,\"",
        "		  \" type %d, ino %lu, caller %pS\", MAJOR(__entry->dev),",
        "		  MINOR(__entry->dev), __entry->blocks, __entry->rsv_blocks,",
        "		  __entry->revoke_creds, __entry->type, __entry->ino,",
        "		  (void *)__entry->ip)",
        ");",
        "",
        "TRACE_EVENT(ext4_journal_start_reserved,",
        "	TP_PROTO(struct super_block *sb, int blocks, unsigned long IP),",
        "",
        "	TP_ARGS(sb, blocks, IP),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(unsigned long,	ip			)",
        "		__field(	  int,	blocks			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		 = sb->s_dev;",
        "		__entry->ip		 = IP;",
        "		__entry->blocks		 = blocks;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d blocks, %d caller %pS\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->blocks, (void *)__entry->ip)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__trim,",
        "	TP_PROTO(struct super_block *sb,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, group, start, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	int,	dev_major		)",
        "		__field(	int,	dev_minor		)",
        "		__field(	__u32, 	group			)",
        "		__field(	int,	start			)",
        "		__field(	int,	len			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev_major	= MAJOR(sb->s_dev);",
        "		__entry->dev_minor	= MINOR(sb->s_dev);",
        "		__entry->group		= group;",
        "		__entry->start		= start;",
        "		__entry->len		= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d group %u, start %d, len %d\",",
        "		  __entry->dev_major, __entry->dev_minor,",
        "		  __entry->group, __entry->start, __entry->len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__trim, ext4_trim_extent,",
        "",
        "	TP_PROTO(struct super_block *sb,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, group, start, len)",
        ");",
        "",
        "DEFINE_EVENT(ext4__trim, ext4_trim_all_free,",
        "",
        "	TP_PROTO(struct super_block *sb,",
        "		 ext4_group_t group,",
        "		 ext4_grpblk_t start,",
        "		 ext4_grpblk_t len),",
        "",
        "	TP_ARGS(sb, group, start, len)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_handle_unwritten_extents,",
        "	TP_PROTO(struct inode *inode, struct ext4_map_blocks *map, int flags,",
        "		 unsigned int allocated, ext4_fsblk_t newblock),",
        "",
        "	TP_ARGS(inode, map, flags, allocated, newblock),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	int,		flags		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	unsigned int,	len		)",
        "		__field(	unsigned int,	allocated	)",
        "		__field(	ext4_fsblk_t,	newblk		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->flags		= flags;",
        "		__entry->lblk		= map->m_lblk;",
        "		__entry->pblk		= map->m_pblk;",
        "		__entry->len		= map->m_len;",
        "		__entry->allocated	= allocated;",
        "		__entry->newblk		= newblock;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu m_lblk %u m_pblk %llu m_len %u flags %s \"",
        "		  \"allocated %d newblock %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->lblk, (unsigned long long) __entry->pblk,",
        "		  __entry->len, show_map_flags(__entry->flags),",
        "		  (unsigned int) __entry->allocated,",
        "		  (unsigned long long) __entry->newblk)",
        ");",
        "",
        "TRACE_EVENT(ext4_get_implied_cluster_alloc_exit,",
        "	TP_PROTO(struct super_block *sb, struct ext4_map_blocks *map, int ret),",
        "",
        "	TP_ARGS(sb, map, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	unsigned int,	flags	)",
        "		__field(	ext4_lblk_t,	lblk	)",
        "		__field(	ext4_fsblk_t,	pblk	)",
        "		__field(	unsigned int,	len	)",
        "		__field(	int,		ret	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->flags	= map->m_flags;",
        "		__entry->lblk	= map->m_lblk;",
        "		__entry->pblk	= map->m_pblk;",
        "		__entry->len	= map->m_len;",
        "		__entry->ret	= ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d m_lblk %u m_pblk %llu m_len %u m_flags %s ret %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->lblk, (unsigned long long) __entry->pblk,",
        "		  __entry->len, show_mflags(__entry->flags), __entry->ret)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_show_extent,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk, ext4_fsblk_t pblk,",
        "		 unsigned short len),",
        "",
        "	TP_ARGS(inode, lblk, pblk, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_fsblk_t,	pblk	)",
        "		__field(	ext4_lblk_t,	lblk	)",
        "		__field(	unsigned short,	len	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->pblk	= pblk;",
        "		__entry->lblk	= lblk;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %u pblk %llu len %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->lblk,",
        "		  (unsigned long long) __entry->pblk,",
        "		  (unsigned short) __entry->len)",
        ");",
        "",
        "TRACE_EVENT(ext4_remove_blocks,",
        "	TP_PROTO(struct inode *inode, struct ext4_extent *ex,",
        "		 ext4_lblk_t from, ext4_fsblk_t to,",
        "		 struct partial_cluster *pc),",
        "",
        "	TP_ARGS(inode, ex, from, to, pc),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_lblk_t,	from	)",
        "		__field(	ext4_lblk_t,	to	)",
        "		__field(	ext4_fsblk_t,	ee_pblk	)",
        "		__field(	ext4_lblk_t,	ee_lblk	)",
        "		__field(	unsigned short,	ee_len	)",
        "		__field(	ext4_fsblk_t,	pc_pclu	)",
        "		__field(	ext4_lblk_t,	pc_lblk	)",
        "		__field(	int,		pc_state)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->from		= from;",
        "		__entry->to		= to;",
        "		__entry->ee_pblk	= ext4_ext_pblock(ex);",
        "		__entry->ee_lblk	= le32_to_cpu(ex->ee_block);",
        "		__entry->ee_len		= ext4_ext_get_actual_len(ex);",
        "		__entry->pc_pclu	= pc->pclu;",
        "		__entry->pc_lblk	= pc->lblk;",
        "		__entry->pc_state	= pc->state;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu extent [%u(%llu), %u]\"",
        "		  \"from %u to %u partial [pclu %lld lblk %u state %d]\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->ee_lblk,",
        "		  (unsigned long long) __entry->ee_pblk,",
        "		  (unsigned short) __entry->ee_len,",
        "		  (unsigned) __entry->from,",
        "		  (unsigned) __entry->to,",
        "		  (long long) __entry->pc_pclu,",
        "		  (unsigned int) __entry->pc_lblk,",
        "		  (int) __entry->pc_state)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_rm_leaf,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t start,",
        "		 struct ext4_extent *ex,",
        "		 struct partial_cluster *pc),",
        "",
        "	TP_ARGS(inode, start, ex, pc),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_lblk_t,	start	)",
        "		__field(	ext4_lblk_t,	ee_lblk	)",
        "		__field(	ext4_fsblk_t,	ee_pblk	)",
        "		__field(	short,		ee_len	)",
        "		__field(	ext4_fsblk_t,	pc_pclu	)",
        "		__field(	ext4_lblk_t,	pc_lblk	)",
        "		__field(	int,		pc_state)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->start		= start;",
        "		__entry->ee_lblk	= le32_to_cpu(ex->ee_block);",
        "		__entry->ee_pblk	= ext4_ext_pblock(ex);",
        "		__entry->ee_len		= ext4_ext_get_actual_len(ex);",
        "		__entry->pc_pclu	= pc->pclu;",
        "		__entry->pc_lblk	= pc->lblk;",
        "		__entry->pc_state	= pc->state;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu start_lblk %u last_extent [%u(%llu), %u]\"",
        "		  \"partial [pclu %lld lblk %u state %d]\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->start,",
        "		  (unsigned) __entry->ee_lblk,",
        "		  (unsigned long long) __entry->ee_pblk,",
        "		  (unsigned short) __entry->ee_len,",
        "		  (long long) __entry->pc_pclu,",
        "		  (unsigned int) __entry->pc_lblk,",
        "		  (int) __entry->pc_state)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_rm_idx,",
        "	TP_PROTO(struct inode *inode, ext4_fsblk_t pblk),",
        "",
        "	TP_ARGS(inode, pblk),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_fsblk_t,	pblk	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->pblk	= pblk;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu index_pblk %llu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned long long) __entry->pblk)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_remove_space,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t start,",
        "		 ext4_lblk_t end, int depth),",
        "",
        "	TP_ARGS(inode, start, end, depth),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev	)",
        "		__field(	ino_t,		ino	)",
        "		__field(	ext4_lblk_t,	start	)",
        "		__field(	ext4_lblk_t,	end	)",
        "		__field(	int,		depth	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->start	= start;",
        "		__entry->end	= end;",
        "		__entry->depth	= depth;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu since %u end %u depth %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->start,",
        "		  (unsigned) __entry->end,",
        "		  __entry->depth)",
        ");",
        "",
        "TRACE_EVENT(ext4_ext_remove_space_done,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t start, ext4_lblk_t end,",
        "		 int depth, struct partial_cluster *pc, __le16 eh_entries),",
        "",
        "	TP_ARGS(inode, start, end, depth, pc, eh_entries),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	start		)",
        "		__field(	ext4_lblk_t,	end		)",
        "		__field(	int,		depth		)",
        "		__field(	ext4_fsblk_t,	pc_pclu		)",
        "		__field(	ext4_lblk_t,	pc_lblk		)",
        "		__field(	int,		pc_state	)",
        "		__field(	unsigned short,	eh_entries	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->start		= start;",
        "		__entry->end		= end;",
        "		__entry->depth		= depth;",
        "		__entry->pc_pclu	= pc->pclu;",
        "		__entry->pc_lblk	= pc->lblk;",
        "		__entry->pc_state	= pc->state;",
        "		__entry->eh_entries	= le16_to_cpu(eh_entries);",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu since %u end %u depth %d \"",
        "		  \"partial [pclu %lld lblk %u state %d] \"",
        "		  \"remaining_entries %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  (unsigned) __entry->start,",
        "		  (unsigned) __entry->end,",
        "		  __entry->depth,",
        "		  (long long) __entry->pc_pclu,",
        "		  (unsigned int) __entry->pc_lblk,",
        "		  (int) __entry->pc_state,",
        "		  (unsigned short) __entry->eh_entries)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__es_extent,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es),",
        "",
        "	TP_ARGS(inode, es),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	ext4_lblk_t,	len		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	char, status	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= es->es_lblk;",
        "		__entry->len	= es->es_len;",
        "		__entry->pblk	= ext4_es_show_pblock(es);",
        "		__entry->status	= ext4_es_status(es);",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu es [%u/%u) mapped %llu status %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->len,",
        "		  __entry->pblk, show_extent_status(__entry->status))",
        ");",
        "",
        "DEFINE_EVENT(ext4__es_extent, ext4_es_insert_extent,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es),",
        "",
        "	TP_ARGS(inode, es)",
        ");",
        "",
        "DEFINE_EVENT(ext4__es_extent, ext4_es_cache_extent,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es),",
        "",
        "	TP_ARGS(inode, es)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_remove_extent,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk, ext4_lblk_t len),",
        "",
        "	TP_ARGS(inode, lblk, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	ino_t,	ino			)",
        "		__field(	loff_t,	lblk			)",
        "		__field(	loff_t,	len			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= lblk;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu es [%lld/%lld)\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->len)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_find_extent_range_enter,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk),",
        "",
        "	TP_ARGS(inode, lblk),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= lblk;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->lblk)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_find_extent_range_exit,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es),",
        "",
        "	TP_ARGS(inode, es),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	ext4_lblk_t,	len		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	char, status	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= es->es_lblk;",
        "		__entry->len	= es->es_len;",
        "		__entry->pblk	= ext4_es_show_pblock(es);",
        "		__entry->status	= ext4_es_status(es);",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu es [%u/%u) mapped %llu status %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->len,",
        "		  __entry->pblk, show_extent_status(__entry->status))",
        ");",
        "",
        "TRACE_EVENT(ext4_es_lookup_extent_enter,",
        "	TP_PROTO(struct inode *inode, ext4_lblk_t lblk),",
        "",
        "	TP_ARGS(inode, lblk),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= lblk;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu lblk %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->lblk)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_lookup_extent_exit,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es,",
        "		 int found),",
        "",
        "	TP_ARGS(inode, es, found),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	ext4_lblk_t,	len		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	char,		status		)",
        "		__field(	int,		found		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->lblk	= es->es_lblk;",
        "		__entry->len	= es->es_len;",
        "		__entry->pblk	= ext4_es_show_pblock(es);",
        "		__entry->status	= ext4_es_status(es);",
        "		__entry->found	= found;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu found %d [%u/%u) %llu %s\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino, __entry->found,",
        "		  __entry->lblk, __entry->len,",
        "		  __entry->found ? __entry->pblk : 0,",
        "		  show_extent_status(__entry->found ? __entry->status : 0))",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4__es_shrink_enter,",
        "	TP_PROTO(struct super_block *sb, int nr_to_scan, int cache_cnt),",
        "",
        "	TP_ARGS(sb, nr_to_scan, cache_cnt),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	int,	nr_to_scan		)",
        "		__field(	int,	cache_cnt		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->nr_to_scan	= nr_to_scan;",
        "		__entry->cache_cnt	= cache_cnt;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d nr_to_scan %d cache_cnt %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->nr_to_scan, __entry->cache_cnt)",
        ");",
        "",
        "DEFINE_EVENT(ext4__es_shrink_enter, ext4_es_shrink_count,",
        "	TP_PROTO(struct super_block *sb, int nr_to_scan, int cache_cnt),",
        "",
        "	TP_ARGS(sb, nr_to_scan, cache_cnt)",
        ");",
        "",
        "DEFINE_EVENT(ext4__es_shrink_enter, ext4_es_shrink_scan_enter,",
        "	TP_PROTO(struct super_block *sb, int nr_to_scan, int cache_cnt),",
        "",
        "	TP_ARGS(sb, nr_to_scan, cache_cnt)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_shrink_scan_exit,",
        "	TP_PROTO(struct super_block *sb, int nr_shrunk, int cache_cnt),",
        "",
        "	TP_ARGS(sb, nr_shrunk, cache_cnt),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	int,	nr_shrunk		)",
        "		__field(	int,	cache_cnt		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->nr_shrunk	= nr_shrunk;",
        "		__entry->cache_cnt	= cache_cnt;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d nr_shrunk %d cache_cnt %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->nr_shrunk, __entry->cache_cnt)",
        ");",
        "",
        "TRACE_EVENT(ext4_collapse_range,",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len),",
        "",
        "	TP_ARGS(inode, offset, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t,	dev)",
        "		__field(ino_t,	ino)",
        "		__field(loff_t,	offset)",
        "		__field(loff_t, len)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->offset	= offset;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu offset %lld len %lld\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->offset, __entry->len)",
        ");",
        "",
        "TRACE_EVENT(ext4_insert_range,",
        "	TP_PROTO(struct inode *inode, loff_t offset, loff_t len),",
        "",
        "	TP_ARGS(inode, offset, len),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t,	dev)",
        "		__field(ino_t,	ino)",
        "		__field(loff_t,	offset)",
        "		__field(loff_t, len)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= inode->i_sb->s_dev;",
        "		__entry->ino	= inode->i_ino;",
        "		__entry->offset	= offset;",
        "		__entry->len	= len;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu offset %lld len %lld\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->offset, __entry->len)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_shrink,",
        "	TP_PROTO(struct super_block *sb, int nr_shrunk, u64 scan_time,",
        "		 int nr_skipped, int retried),",
        "",
        "	TP_ARGS(sb, nr_shrunk, scan_time, nr_skipped, retried),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	int,		nr_shrunk	)",
        "		__field(	unsigned long long, scan_time	)",
        "		__field(	int,		nr_skipped	)",
        "		__field(	int,		retried		)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= sb->s_dev;",
        "		__entry->nr_shrunk	= nr_shrunk;",
        "		__entry->scan_time	= div_u64(scan_time, 1000);",
        "		__entry->nr_skipped	= nr_skipped;",
        "		__entry->retried	= retried;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d nr_shrunk %d, scan_time %llu \"",
        "		  \"nr_skipped %d retried %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->nr_shrunk,",
        "		  __entry->scan_time, __entry->nr_skipped, __entry->retried)",
        ");",
        "",
        "TRACE_EVENT(ext4_es_insert_delayed_extent,",
        "	TP_PROTO(struct inode *inode, struct extent_status *es,",
        "		 bool lclu_allocated, bool end_allocated),",
        "",
        "	TP_ARGS(inode, es, lclu_allocated, end_allocated),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,		dev		)",
        "		__field(	ino_t,		ino		)",
        "		__field(	ext4_lblk_t,	lblk		)",
        "		__field(	ext4_lblk_t,	len		)",
        "		__field(	ext4_fsblk_t,	pblk		)",
        "		__field(	char,		status		)",
        "		__field(	bool,		lclu_allocated	)",
        "		__field(	bool,		end_allocated	)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev		= inode->i_sb->s_dev;",
        "		__entry->ino		= inode->i_ino;",
        "		__entry->lblk		= es->es_lblk;",
        "		__entry->len		= es->es_len;",
        "		__entry->pblk		= ext4_es_show_pblock(es);",
        "		__entry->status		= ext4_es_status(es);",
        "		__entry->lclu_allocated	= lclu_allocated;",
        "		__entry->end_allocated	= end_allocated;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d ino %lu es [%u/%u) mapped %llu status %s \"",
        "		  \"allocated %d %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  (unsigned long) __entry->ino,",
        "		  __entry->lblk, __entry->len,",
        "		  __entry->pblk, show_extent_status(__entry->status),",
        "		  __entry->lclu_allocated, __entry->end_allocated)",
        ");",
        "",
        "/* fsmap traces */",
        "DECLARE_EVENT_CLASS(ext4_fsmap_class,",
        "	TP_PROTO(struct super_block *sb, u32 keydev, u32 agno, u64 bno, u64 len,",
        "		 u64 owner),",
        "	TP_ARGS(sb, keydev, agno, bno, len, owner),",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(dev_t, keydev)",
        "		__field(u32, agno)",
        "		__field(u64, bno)",
        "		__field(u64, len)",
        "		__field(u64, owner)",
        "	),",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_bdev->bd_dev;",
        "		__entry->keydev = new_decode_dev(keydev);",
        "		__entry->agno = agno;",
        "		__entry->bno = bno;",
        "		__entry->len = len;",
        "		__entry->owner = owner;",
        "	),",
        "	TP_printk(\"dev %d:%d keydev %d:%d agno %u bno %llu len %llu owner %lld\\n\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  MAJOR(__entry->keydev), MINOR(__entry->keydev),",
        "		  __entry->agno,",
        "		  __entry->bno,",
        "		  __entry->len,",
        "		  __entry->owner)",
        ")",
        "#define DEFINE_FSMAP_EVENT(name) \\",
        "DEFINE_EVENT(ext4_fsmap_class, name, \\",
        "	TP_PROTO(struct super_block *sb, u32 keydev, u32 agno, u64 bno, u64 len, \\",
        "		 u64 owner), \\",
        "	TP_ARGS(sb, keydev, agno, bno, len, owner))",
        "DEFINE_FSMAP_EVENT(ext4_fsmap_low_key);",
        "DEFINE_FSMAP_EVENT(ext4_fsmap_high_key);",
        "DEFINE_FSMAP_EVENT(ext4_fsmap_mapping);",
        "",
        "DECLARE_EVENT_CLASS(ext4_getfsmap_class,",
        "	TP_PROTO(struct super_block *sb, struct ext4_fsmap *fsmap),",
        "	TP_ARGS(sb, fsmap),",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(dev_t, keydev)",
        "		__field(u64, block)",
        "		__field(u64, len)",
        "		__field(u64, owner)",
        "		__field(u64, flags)",
        "	),",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_bdev->bd_dev;",
        "		__entry->keydev = new_decode_dev(fsmap->fmr_device);",
        "		__entry->block = fsmap->fmr_physical;",
        "		__entry->len = fsmap->fmr_length;",
        "		__entry->owner = fsmap->fmr_owner;",
        "		__entry->flags = fsmap->fmr_flags;",
        "	),",
        "	TP_printk(\"dev %d:%d keydev %d:%d block %llu len %llu owner %lld flags 0x%llx\\n\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  MAJOR(__entry->keydev), MINOR(__entry->keydev),",
        "		  __entry->block,",
        "		  __entry->len,",
        "		  __entry->owner,",
        "		  __entry->flags)",
        ")",
        "#define DEFINE_GETFSMAP_EVENT(name) \\",
        "DEFINE_EVENT(ext4_getfsmap_class, name, \\",
        "	TP_PROTO(struct super_block *sb, struct ext4_fsmap *fsmap), \\",
        "	TP_ARGS(sb, fsmap))",
        "DEFINE_GETFSMAP_EVENT(ext4_getfsmap_low_key);",
        "DEFINE_GETFSMAP_EVENT(ext4_getfsmap_high_key);",
        "DEFINE_GETFSMAP_EVENT(ext4_getfsmap_mapping);",
        "",
        "TRACE_EVENT(ext4_shutdown,",
        "	TP_PROTO(struct super_block *sb, unsigned long flags),",
        "",
        "	TP_ARGS(sb, flags),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(     unsigned,	flags			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->flags	= flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d flags %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->flags)",
        ");",
        "",
        "TRACE_EVENT(ext4_error,",
        "	TP_PROTO(struct super_block *sb, const char *function,",
        "		 unsigned int line),",
        "",
        "	TP_ARGS(sb, function, line),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field( const char *,	function		)",
        "		__field(     unsigned,	line			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->function = function;",
        "		__entry->line	= line;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d function %s line %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->function, __entry->line)",
        ");",
        "",
        "TRACE_EVENT(ext4_prefetch_bitmaps,",
        "	    TP_PROTO(struct super_block *sb, ext4_group_t group,",
        "		     ext4_group_t next, unsigned int prefetch_ios),",
        "",
        "	TP_ARGS(sb, group, next, prefetch_ios),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u32,	group			)",
        "		__field(	__u32,	next			)",
        "		__field(	__u32,	ios			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->group	= group;",
        "		__entry->next	= next;",
        "		__entry->ios	= prefetch_ios;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d group %u next %u ios %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->group, __entry->next, __entry->ios)",
        ");",
        "",
        "TRACE_EVENT(ext4_lazy_itable_init,",
        "	    TP_PROTO(struct super_block *sb, ext4_group_t group),",
        "",
        "	TP_ARGS(sb, group),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(	dev_t,	dev			)",
        "		__field(	__u32,	group			)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->group	= group;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d group %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->group)",
        ");",
        "",
        "TRACE_EVENT(ext4_fc_replay_scan,",
        "	TP_PROTO(struct super_block *sb, int error, int off),",
        "",
        "	TP_ARGS(sb, error, off),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(int, error)",
        "		__field(int, off)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_dev;",
        "		__entry->error = error;",
        "		__entry->off = off;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d error %d, off %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->error, __entry->off)",
        ");",
        "",
        "TRACE_EVENT(ext4_fc_replay,",
        "	TP_PROTO(struct super_block *sb, int tag, int ino, int priv1, int priv2),",
        "",
        "	TP_ARGS(sb, tag, ino, priv1, priv2),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(int, tag)",
        "		__field(int, ino)",
        "		__field(int, priv1)",
        "		__field(int, priv2)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_dev;",
        "		__entry->tag = tag;",
        "		__entry->ino = ino;",
        "		__entry->priv1 = priv1;",
        "		__entry->priv2 = priv2;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d: tag %d, ino %d, data1 %d, data2 %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->tag, __entry->ino, __entry->priv1, __entry->priv2)",
        ");",
        "",
        "TRACE_EVENT(ext4_fc_commit_start,",
        "	TP_PROTO(struct super_block *sb, tid_t commit_tid),",
        "",
        "	TP_ARGS(sb, commit_tid),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(tid_t, tid)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_dev;",
        "		__entry->tid = commit_tid;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d tid %u\", MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->tid)",
        ");",
        "",
        "TRACE_EVENT(ext4_fc_commit_stop,",
        "	    TP_PROTO(struct super_block *sb, int nblks, int reason,",
        "		     tid_t commit_tid),",
        "",
        "	TP_ARGS(sb, nblks, reason, commit_tid),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(int, nblks)",
        "		__field(int, reason)",
        "		__field(int, num_fc)",
        "		__field(int, num_fc_ineligible)",
        "		__field(int, nblks_agg)",
        "		__field(tid_t, tid)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev = sb->s_dev;",
        "		__entry->nblks = nblks;",
        "		__entry->reason = reason;",
        "		__entry->num_fc = EXT4_SB(sb)->s_fc_stats.fc_num_commits;",
        "		__entry->num_fc_ineligible =",
        "			EXT4_SB(sb)->s_fc_stats.fc_ineligible_commits;",
        "		__entry->nblks_agg = EXT4_SB(sb)->s_fc_stats.fc_numblks;",
        "		__entry->tid = commit_tid;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d nblks %d, reason %d, fc = %d, ineligible = %d, agg_nblks %d, tid %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->nblks, __entry->reason, __entry->num_fc,",
        "		  __entry->num_fc_ineligible, __entry->nblks_agg, __entry->tid)",
        ");",
        "",
        "#define FC_REASON_NAME_STAT(reason)					\\",
        "	show_fc_reason(reason),						\\",
        "	__entry->fc_ineligible_rc[reason]",
        "",
        "TRACE_EVENT(ext4_fc_stats,",
        "	TP_PROTO(struct super_block *sb),",
        "",
        "	TP_ARGS(sb),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__array(unsigned int, fc_ineligible_rc, EXT4_FC_REASON_MAX)",
        "		__field(unsigned long, fc_commits)",
        "		__field(unsigned long, fc_ineligible_commits)",
        "		__field(unsigned long, fc_numblks)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		int i;",
        "",
        "		__entry->dev = sb->s_dev;",
        "		for (i = 0; i < EXT4_FC_REASON_MAX; i++) {",
        "			__entry->fc_ineligible_rc[i] =",
        "				EXT4_SB(sb)->s_fc_stats.fc_ineligible_reason_count[i];",
        "		}",
        "		__entry->fc_commits = EXT4_SB(sb)->s_fc_stats.fc_num_commits;",
        "		__entry->fc_ineligible_commits =",
        "			EXT4_SB(sb)->s_fc_stats.fc_ineligible_commits;",
        "		__entry->fc_numblks = EXT4_SB(sb)->s_fc_stats.fc_numblks;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d fc ineligible reasons:\\n\"",
        "		  \"%s:%u, %s:%u, %s:%u, %s:%u, %s:%u, %s:%u, %s:%u, %s:%u, %s:%u, %s:%u\"",
        "		  \"num_commits:%lu, ineligible: %lu, numblks: %lu\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_XATTR),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_CROSS_RENAME),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_JOURNAL_FLAG_CHANGE),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_NOMEM),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_SWAP_BOOT),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_RESIZE),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_RENAME_DIR),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_FALLOC_RANGE),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_INODE_JOURNAL_DATA),",
        "		  FC_REASON_NAME_STAT(EXT4_FC_REASON_ENCRYPTED_FILENAME),",
        "		  __entry->fc_commits, __entry->fc_ineligible_commits,",
        "		  __entry->fc_numblks)",
        ");",
        "",
        "DECLARE_EVENT_CLASS(ext4_fc_track_dentry,",
        "",
        "	TP_PROTO(handle_t *handle, struct inode *inode,",
        "		 struct dentry *dentry, int ret),",
        "",
        "	TP_ARGS(handle, inode, dentry, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(tid_t, t_tid)",
        "		__field(ino_t, i_ino)",
        "		__field(tid_t, i_sync_tid)",
        "		__field(int, error)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		struct ext4_inode_info *ei = EXT4_I(inode);",
        "",
        "		__entry->dev = inode->i_sb->s_dev;",
        "		__entry->t_tid = handle->h_transaction->t_tid;",
        "		__entry->i_ino = inode->i_ino;",
        "		__entry->i_sync_tid = ei->i_sync_tid;",
        "		__entry->error = ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d, t_tid %u, ino %lu, i_sync_tid %u, error %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->t_tid, __entry->i_ino, __entry->i_sync_tid,",
        "		  __entry->error",
        "	)",
        ");",
        "",
        "#define DEFINE_EVENT_CLASS_DENTRY(__type)				\\",
        "DEFINE_EVENT(ext4_fc_track_dentry, ext4_fc_track_##__type,		\\",
        "	TP_PROTO(handle_t *handle, struct inode *inode,			\\",
        "		 struct dentry *dentry, int ret),			\\",
        "	TP_ARGS(handle, inode, dentry, ret)				\\",
        ")",
        "",
        "DEFINE_EVENT_CLASS_DENTRY(create);",
        "DEFINE_EVENT_CLASS_DENTRY(link);",
        "DEFINE_EVENT_CLASS_DENTRY(unlink);",
        "",
        "TRACE_EVENT(ext4_fc_track_inode,",
        "	TP_PROTO(handle_t *handle, struct inode *inode, int ret),",
        "",
        "	TP_ARGS(handle, inode, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(tid_t, t_tid)",
        "		__field(ino_t, i_ino)",
        "		__field(tid_t, i_sync_tid)",
        "		__field(int, error)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		struct ext4_inode_info *ei = EXT4_I(inode);",
        "",
        "		__entry->dev = inode->i_sb->s_dev;",
        "		__entry->t_tid = handle->h_transaction->t_tid;",
        "		__entry->i_ino = inode->i_ino;",
        "		__entry->i_sync_tid = ei->i_sync_tid;",
        "		__entry->error = ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d:%d, t_tid %u, inode %lu, i_sync_tid %u, error %d\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->t_tid, __entry->i_ino, __entry->i_sync_tid,",
        "		  __entry->error)",
        "	);",
        "",
        "TRACE_EVENT(ext4_fc_track_range,",
        "	TP_PROTO(handle_t *handle, struct inode *inode,",
        "		 long start, long end, int ret),",
        "",
        "	TP_ARGS(handle, inode, start, end, ret),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(tid_t, t_tid)",
        "		__field(ino_t, i_ino)",
        "		__field(tid_t, i_sync_tid)",
        "		__field(long, start)",
        "		__field(long, end)",
        "		__field(int, error)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		struct ext4_inode_info *ei = EXT4_I(inode);",
        "",
        "		__entry->dev = inode->i_sb->s_dev;",
        "		__entry->t_tid = handle->h_transaction->t_tid;",
        "		__entry->i_ino = inode->i_ino;",
        "		__entry->i_sync_tid = ei->i_sync_tid;",
        "		__entry->start = start;",
        "		__entry->end = end;",
        "		__entry->error = ret;",
        "	),",
        "",
        "	TP_printk(\"dev %d:%d, t_tid %u, inode %lu, i_sync_tid %u, error %d, start %ld, end %ld\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->t_tid, __entry->i_ino, __entry->i_sync_tid,",
        "		  __entry->error, __entry->start, __entry->end)",
        "	);",
        "",
        "TRACE_EVENT(ext4_fc_cleanup,",
        "	TP_PROTO(journal_t *journal, int full, tid_t tid),",
        "",
        "	TP_ARGS(journal, full, tid),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t, dev)",
        "		__field(int, j_fc_off)",
        "		__field(int, full)",
        "		__field(tid_t, tid)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		struct super_block *sb = journal->j_private;",
        "",
        "		__entry->dev = sb->s_dev;",
        "		__entry->j_fc_off = journal->j_fc_off;",
        "		__entry->full = full;",
        "		__entry->tid = tid;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d, j_fc_off %d, full %d, tid %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->j_fc_off, __entry->full, __entry->tid)",
        "	);",
        "",
        "TRACE_EVENT(ext4_update_sb,",
        "	TP_PROTO(struct super_block *sb, ext4_fsblk_t fsblk,",
        "		 unsigned int flags),",
        "",
        "	TP_ARGS(sb, fsblk, flags),",
        "",
        "	TP_STRUCT__entry(",
        "		__field(dev_t,		dev)",
        "		__field(ext4_fsblk_t,	fsblk)",
        "		__field(unsigned int,	flags)",
        "	),",
        "",
        "	TP_fast_assign(",
        "		__entry->dev	= sb->s_dev;",
        "		__entry->fsblk	= fsblk;",
        "		__entry->flags	= flags;",
        "	),",
        "",
        "	TP_printk(\"dev %d,%d fsblk %llu flags %u\",",
        "		  MAJOR(__entry->dev), MINOR(__entry->dev),",
        "		  __entry->fsblk, __entry->flags)",
        ");",
        "",
        "#endif /* _TRACE_EXT4_H */",
        "",
        "/* This part must be outside protection */",
        "#include <trace/define_trace.h>"
    ]
  },
  "mm_vmalloc_c": {
    path: "mm/vmalloc.c",
    covered: [1924, 1234, 166, 416, 4630, 1987, 4566, 374, 427, 3984, 1148, 719, 1221, 1982, 1533, 614, 379, 1999, 1113, 4595, 3924, 1324, 1654, 1249, 407, 497, 4592, 2375, 3943, 406, 3371, 3113, 3788, 2000, 1337, 1231, 1213, 289, 759, 228, 3587, 3387, 457, 3782, 531, 1673, 3925, 1718, 4588, 267, 3911, 1779, 2372, 358, 1347, 1663, 1135, 2443, 381, 1544, 2445, 1011, 4743, 510, 3859, 4686, 1512, 4653, 1179, 3123, 384, 4672, 4559, 3944, 1330, 985, 222, 1520, 4632, 172, 2020, 188, 383, 789, 153, 1144, 3715, 1006, 587, 3151, 1747, 3770, 1536, 3613, 180, 279, 618, 3571, 426, 455, 3796, 3647, 1836, 1531, 635, 740, 3721, 3709, 2005, 1833, 542, 3606, 730, 3373, 1722, 3846, 3644, 982, 1013, 1783, 2348, 3386, 529, 1644, 1328, 524, 4785, 1957, 585, 3548, 80, 588, 357, 4563, 3586, 560, 2036, 3669, 547, 307, 3348, 2376, 443, 1180, 1721, 2366, 1348, 453, 3547, 955, 3591, 1314, 1342, 1243, 303, 1508, 3566, 609, 3983, 83, 565, 4687, 2351, 1719, 2430, 3225, 762, 4641, 512, 616, 216, 4748, 755, 1137],
    totalLines: 5233,
    coveredCount: 174,
    coveragePct: 3.3,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " *  Copyright (C) 1993  Linus Torvalds",
        " *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999",
        " *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000",
        " *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002",
        " *  Numa awareness, Christoph Lameter, SGI, June 2005",
        " *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019",
        " */",
        "",
        "#include <linux/vmalloc.h>",
        "#include <linux/mm.h>",
        "#include <linux/module.h>",
        "#include <linux/highmem.h>",
        "#include <linux/sched/signal.h>",
        "#include <linux/slab.h>",
        "#include <linux/spinlock.h>",
        "#include <linux/interrupt.h>",
        "#include <linux/proc_fs.h>",
        "#include <linux/seq_file.h>",
        "#include <linux/set_memory.h>",
        "#include <linux/debugobjects.h>",
        "#include <linux/kallsyms.h>",
        "#include <linux/list.h>",
        "#include <linux/notifier.h>",
        "#include <linux/rbtree.h>",
        "#include <linux/xarray.h>",
        "#include <linux/io.h>",
        "#include <linux/rcupdate.h>",
        "#include <linux/pfn.h>",
        "#include <linux/kmemleak.h>",
        "#include <linux/atomic.h>",
        "#include <linux/compiler.h>",
        "#include <linux/memcontrol.h>",
        "#include <linux/llist.h>",
        "#include <linux/uio.h>",
        "#include <linux/bitops.h>",
        "#include <linux/rbtree_augmented.h>",
        "#include <linux/overflow.h>",
        "#include <linux/pgtable.h>",
        "#include <linux/hugetlb.h>",
        "#include <linux/sched/mm.h>",
        "#include <asm/tlbflush.h>",
        "#include <asm/shmparam.h>",
        "#include <linux/page_owner.h>",
        "",
        "#define CREATE_TRACE_POINTS",
        "#include <trace/events/vmalloc.h>",
        "",
        "#include \"internal.h\"",
        "#include \"pgalloc-track.h\"",
        "",
        "#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP",
        "static unsigned int __ro_after_init ioremap_max_page_shift = BITS_PER_LONG - 1;",
        "",
        "static int __init set_nohugeiomap(char *str)",
        "{",
        "	ioremap_max_page_shift = PAGE_SHIFT;",
        "	return 0;",
        "}",
        "early_param(\"nohugeiomap\", set_nohugeiomap);",
        "#else /* CONFIG_HAVE_ARCH_HUGE_VMAP */",
        "static const unsigned int ioremap_max_page_shift = PAGE_SHIFT;",
        "#endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */",
        "",
        "#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC",
        "static bool __ro_after_init vmap_allow_huge = true;",
        "",
        "static int __init set_nohugevmalloc(char *str)",
        "{",
        "	vmap_allow_huge = false;",
        "	return 0;",
        "}",
        "early_param(\"nohugevmalloc\", set_nohugevmalloc);",
        "#else /* CONFIG_HAVE_ARCH_HUGE_VMALLOC */",
        "static const bool vmap_allow_huge = false;",
        "#endif	/* CONFIG_HAVE_ARCH_HUGE_VMALLOC */",
        "",
        "bool is_vmalloc_addr(const void *x)",
        "{",
        "	unsigned long addr = (unsigned long)kasan_reset_tag(x);",
        "",
        "	return addr >= VMALLOC_START && addr < VMALLOC_END;",
        "}",
        "EXPORT_SYMBOL(is_vmalloc_addr);",
        "",
        "struct vfree_deferred {",
        "	struct llist_head list;",
        "	struct work_struct wq;",
        "};",
        "static DEFINE_PER_CPU(struct vfree_deferred, vfree_deferred);",
        "",
        "/*** Page table manipulation functions ***/",
        "static int vmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift, pgtbl_mod_mask *mask)",
        "{",
        "	pte_t *pte;",
        "	u64 pfn;",
        "	struct page *page;",
        "	unsigned long size = PAGE_SIZE;",
        "",
        "	pfn = phys_addr >> PAGE_SHIFT;",
        "	pte = pte_alloc_kernel_track(pmd, addr, mask);",
        "	if (!pte)",
        "		return -ENOMEM;",
        "	do {",
        "		if (unlikely(!pte_none(ptep_get(pte)))) {",
        "			if (pfn_valid(pfn)) {",
        "				page = pfn_to_page(pfn);",
        "				dump_page(page, \"remapping already mapped page\");",
        "			}",
        "			BUG();",
        "		}",
        "",
        "#ifdef CONFIG_HUGETLB_PAGE",
        "		size = arch_vmap_pte_range_map_size(addr, end, pfn, max_page_shift);",
        "		if (size != PAGE_SIZE) {",
        "			pte_t entry = pfn_pte(pfn, prot);",
        "",
        "			entry = arch_make_huge_pte(entry, ilog2(size), 0);",
        "			set_huge_pte_at(&init_mm, addr, pte, entry, size);",
        "			pfn += PFN_DOWN(size);",
        "			continue;",
        "		}",
        "#endif",
        "		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));",
        "		pfn++;",
        "	} while (pte += PFN_DOWN(size), addr += size, addr != end);",
        "	*mask |= PGTBL_PTE_MODIFIED;",
        "	return 0;",
        "}",
        "",
        "static int vmap_try_huge_pmd(pmd_t *pmd, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift)",
        "{",
        "	if (max_page_shift < PMD_SHIFT)",
        "		return 0;",
        "",
        "	if (!arch_vmap_pmd_supported(prot))",
        "		return 0;",
        "",
        "	if ((end - addr) != PMD_SIZE)",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(addr, PMD_SIZE))",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(phys_addr, PMD_SIZE))",
        "		return 0;",
        "",
        "	if (pmd_present(*pmd) && !pmd_free_pte_page(pmd, addr))",
        "		return 0;",
        "",
        "	return pmd_set_huge(pmd, phys_addr, prot);",
        "}",
        "",
        "static int vmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift, pgtbl_mod_mask *mask)",
        "{",
        "	pmd_t *pmd;",
        "	unsigned long next;",
        "",
        "	pmd = pmd_alloc_track(&init_mm, pud, addr, mask);",
        "	if (!pmd)",
        "		return -ENOMEM;",
        "	do {",
        "		next = pmd_addr_end(addr, end);",
        "",
        "		if (vmap_try_huge_pmd(pmd, addr, next, phys_addr, prot,",
        "					max_page_shift)) {",
        "			*mask |= PGTBL_PMD_MODIFIED;",
        "			continue;",
        "		}",
        "",
        "		if (vmap_pte_range(pmd, addr, next, phys_addr, prot, max_page_shift, mask))",
        "			return -ENOMEM;",
        "	} while (pmd++, phys_addr += (next - addr), addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_try_huge_pud(pud_t *pud, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift)",
        "{",
        "	if (max_page_shift < PUD_SHIFT)",
        "		return 0;",
        "",
        "	if (!arch_vmap_pud_supported(prot))",
        "		return 0;",
        "",
        "	if ((end - addr) != PUD_SIZE)",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(addr, PUD_SIZE))",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(phys_addr, PUD_SIZE))",
        "		return 0;",
        "",
        "	if (pud_present(*pud) && !pud_free_pmd_page(pud, addr))",
        "		return 0;",
        "",
        "	return pud_set_huge(pud, phys_addr, prot);",
        "}",
        "",
        "static int vmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift, pgtbl_mod_mask *mask)",
        "{",
        "	pud_t *pud;",
        "	unsigned long next;",
        "",
        "	pud = pud_alloc_track(&init_mm, p4d, addr, mask);",
        "	if (!pud)",
        "		return -ENOMEM;",
        "	do {",
        "		next = pud_addr_end(addr, end);",
        "",
        "		if (vmap_try_huge_pud(pud, addr, next, phys_addr, prot,",
        "					max_page_shift)) {",
        "			*mask |= PGTBL_PUD_MODIFIED;",
        "			continue;",
        "		}",
        "",
        "		if (vmap_pmd_range(pud, addr, next, phys_addr, prot,",
        "					max_page_shift, mask))",
        "			return -ENOMEM;",
        "	} while (pud++, phys_addr += (next - addr), addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_try_huge_p4d(p4d_t *p4d, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift)",
        "{",
        "	if (max_page_shift < P4D_SHIFT)",
        "		return 0;",
        "",
        "	if (!arch_vmap_p4d_supported(prot))",
        "		return 0;",
        "",
        "	if ((end - addr) != P4D_SIZE)",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(addr, P4D_SIZE))",
        "		return 0;",
        "",
        "	if (!IS_ALIGNED(phys_addr, P4D_SIZE))",
        "		return 0;",
        "",
        "	if (p4d_present(*p4d) && !p4d_free_pud_page(p4d, addr))",
        "		return 0;",
        "",
        "	return p4d_set_huge(p4d, phys_addr, prot);",
        "}",
        "",
        "static int vmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift, pgtbl_mod_mask *mask)",
        "{",
        "	p4d_t *p4d;",
        "	unsigned long next;",
        "",
        "	p4d = p4d_alloc_track(&init_mm, pgd, addr, mask);",
        "	if (!p4d)",
        "		return -ENOMEM;",
        "	do {",
        "		next = p4d_addr_end(addr, end);",
        "",
        "		if (vmap_try_huge_p4d(p4d, addr, next, phys_addr, prot,",
        "					max_page_shift)) {",
        "			*mask |= PGTBL_P4D_MODIFIED;",
        "			continue;",
        "		}",
        "",
        "		if (vmap_pud_range(p4d, addr, next, phys_addr, prot,",
        "					max_page_shift, mask))",
        "			return -ENOMEM;",
        "	} while (p4d++, phys_addr += (next - addr), addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_range_noflush(unsigned long addr, unsigned long end,",
        "			phys_addr_t phys_addr, pgprot_t prot,",
        "			unsigned int max_page_shift)",
        "{",
        "	pgd_t *pgd;",
        "	unsigned long start;",
        "	unsigned long next;",
        "	int err;",
        "	pgtbl_mod_mask mask = 0;",
        "",
        "	might_sleep();",
        "	BUG_ON(addr >= end);",
        "",
        "	start = addr;",
        "	pgd = pgd_offset_k(addr);",
        "	do {",
        "		next = pgd_addr_end(addr, end);",
        "		err = vmap_p4d_range(pgd, addr, next, phys_addr, prot,",
        "					max_page_shift, &mask);",
        "		if (err)",
        "			break;",
        "	} while (pgd++, phys_addr += (next - addr), addr = next, addr != end);",
        "",
        "	if (mask & ARCH_PAGE_TABLE_SYNC_MASK)",
        "		arch_sync_kernel_mappings(start, end);",
        "",
        "	return err;",
        "}",
        "",
        "int vmap_page_range(unsigned long addr, unsigned long end,",
        "		    phys_addr_t phys_addr, pgprot_t prot)",
        "{",
        "	int err;",
        "",
        "	err = vmap_range_noflush(addr, end, phys_addr, pgprot_nx(prot),",
        "				 ioremap_max_page_shift);",
        "	flush_cache_vmap(addr, end);",
        "	if (!err)",
        "		err = kmsan_ioremap_page_range(addr, end, phys_addr, prot,",
        "					       ioremap_max_page_shift);",
        "	return err;",
        "}",
        "",
        "int ioremap_page_range(unsigned long addr, unsigned long end,",
        "		phys_addr_t phys_addr, pgprot_t prot)",
        "{",
        "	struct vm_struct *area;",
        "",
        "	area = find_vm_area((void *)addr);",
        "	if (!area || !(area->flags & VM_IOREMAP)) {",
        "		WARN_ONCE(1, \"vm_area at addr %lx is not marked as VM_IOREMAP\\n\", addr);",
        "		return -EINVAL;",
        "	}",
        "	if (addr != (unsigned long)area->addr ||",
        "	    (void *)end != area->addr + get_vm_area_size(area)) {",
        "		WARN_ONCE(1, \"ioremap request [%lx,%lx) doesn't match vm_area [%lx, %lx)\\n\",",
        "			  addr, end, (long)area->addr,",
        "			  (long)area->addr + get_vm_area_size(area));",
        "		return -ERANGE;",
        "	}",
        "	return vmap_page_range(addr, end, phys_addr, prot);",
        "}",
        "",
        "static void vunmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,",
        "			     pgtbl_mod_mask *mask)",
        "{",
        "	pte_t *pte;",
        "",
        "	pte = pte_offset_kernel(pmd, addr);",
        "	do {",
        "		pte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);",
        "		WARN_ON(!pte_none(ptent) && !pte_present(ptent));",
        "	} while (pte++, addr += PAGE_SIZE, addr != end);",
        "	*mask |= PGTBL_PTE_MODIFIED;",
        "}",
        "",
        "static void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,",
        "			     pgtbl_mod_mask *mask)",
        "{",
        "	pmd_t *pmd;",
        "	unsigned long next;",
        "	int cleared;",
        "",
        "	pmd = pmd_offset(pud, addr);",
        "	do {",
        "		next = pmd_addr_end(addr, end);",
        "",
        "		cleared = pmd_clear_huge(pmd);",
        "		if (cleared || pmd_bad(*pmd))",
        "			*mask |= PGTBL_PMD_MODIFIED;",
        "",
        "		if (cleared)",
        "			continue;",
        "		if (pmd_none_or_clear_bad(pmd))",
        "			continue;",
        "		vunmap_pte_range(pmd, addr, next, mask);",
        "",
        "		cond_resched();",
        "	} while (pmd++, addr = next, addr != end);",
        "}",
        "",
        "static void vunmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,",
        "			     pgtbl_mod_mask *mask)",
        "{",
        "	pud_t *pud;",
        "	unsigned long next;",
        "	int cleared;",
        "",
        "	pud = pud_offset(p4d, addr);",
        "	do {",
        "		next = pud_addr_end(addr, end);",
        "",
        "		cleared = pud_clear_huge(pud);",
        "		if (cleared || pud_bad(*pud))",
        "			*mask |= PGTBL_PUD_MODIFIED;",
        "",
        "		if (cleared)",
        "			continue;",
        "		if (pud_none_or_clear_bad(pud))",
        "			continue;",
        "		vunmap_pmd_range(pud, addr, next, mask);",
        "	} while (pud++, addr = next, addr != end);",
        "}",
        "",
        "static void vunmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,",
        "			     pgtbl_mod_mask *mask)",
        "{",
        "	p4d_t *p4d;",
        "	unsigned long next;",
        "",
        "	p4d = p4d_offset(pgd, addr);",
        "	do {",
        "		next = p4d_addr_end(addr, end);",
        "",
        "		p4d_clear_huge(p4d);",
        "		if (p4d_bad(*p4d))",
        "			*mask |= PGTBL_P4D_MODIFIED;",
        "",
        "		if (p4d_none_or_clear_bad(p4d))",
        "			continue;",
        "		vunmap_pud_range(p4d, addr, next, mask);",
        "	} while (p4d++, addr = next, addr != end);",
        "}",
        "",
        "/*",
        " * vunmap_range_noflush is similar to vunmap_range, but does not",
        " * flush caches or TLBs.",
        " *",
        " * The caller is responsible for calling flush_cache_vmap() before calling",
        " * this function, and flush_tlb_kernel_range after it has returned",
        " * successfully (and before the addresses are expected to cause a page fault",
        " * or be re-mapped for something else, if TLB flushes are being delayed or",
        " * coalesced).",
        " *",
        " * This is an internal function only. Do not use outside mm/.",
        " */",
        "void __vunmap_range_noflush(unsigned long start, unsigned long end)",
        "{",
        "	unsigned long next;",
        "	pgd_t *pgd;",
        "	unsigned long addr = start;",
        "	pgtbl_mod_mask mask = 0;",
        "",
        "	BUG_ON(addr >= end);",
        "	pgd = pgd_offset_k(addr);",
        "	do {",
        "		next = pgd_addr_end(addr, end);",
        "		if (pgd_bad(*pgd))",
        "			mask |= PGTBL_PGD_MODIFIED;",
        "		if (pgd_none_or_clear_bad(pgd))",
        "			continue;",
        "		vunmap_p4d_range(pgd, addr, next, &mask);",
        "	} while (pgd++, addr = next, addr != end);",
        "",
        "	if (mask & ARCH_PAGE_TABLE_SYNC_MASK)",
        "		arch_sync_kernel_mappings(start, end);",
        "}",
        "",
        "void vunmap_range_noflush(unsigned long start, unsigned long end)",
        "{",
        "	kmsan_vunmap_range_noflush(start, end);",
        "	__vunmap_range_noflush(start, end);",
        "}",
        "",
        "/**",
        " * vunmap_range - unmap kernel virtual addresses",
        " * @addr: start of the VM area to unmap",
        " * @end: end of the VM area to unmap (non-inclusive)",
        " *",
        " * Clears any present PTEs in the virtual address range, flushes TLBs and",
        " * caches. Any subsequent access to the address before it has been re-mapped",
        " * is a kernel bug.",
        " */",
        "void vunmap_range(unsigned long addr, unsigned long end)",
        "{",
        "	flush_cache_vunmap(addr, end);",
        "	vunmap_range_noflush(addr, end);",
        "	flush_tlb_kernel_range(addr, end);",
        "}",
        "",
        "static int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,",
        "		unsigned long end, pgprot_t prot, struct page **pages, int *nr,",
        "		pgtbl_mod_mask *mask)",
        "{",
        "	pte_t *pte;",
        "",
        "	/*",
        "	 * nr is a running index into the array which helps higher level",
        "	 * callers keep track of where we're up to.",
        "	 */",
        "",
        "	pte = pte_alloc_kernel_track(pmd, addr, mask);",
        "	if (!pte)",
        "		return -ENOMEM;",
        "	do {",
        "		struct page *page = pages[*nr];",
        "",
        "		if (WARN_ON(!pte_none(ptep_get(pte))))",
        "			return -EBUSY;",
        "		if (WARN_ON(!page))",
        "			return -ENOMEM;",
        "		if (WARN_ON(!pfn_valid(page_to_pfn(page))))",
        "			return -EINVAL;",
        "",
        "		set_pte_at(&init_mm, addr, pte, mk_pte(page, prot));",
        "		(*nr)++;",
        "	} while (pte++, addr += PAGE_SIZE, addr != end);",
        "	*mask |= PGTBL_PTE_MODIFIED;",
        "	return 0;",
        "}",
        "",
        "static int vmap_pages_pmd_range(pud_t *pud, unsigned long addr,",
        "		unsigned long end, pgprot_t prot, struct page **pages, int *nr,",
        "		pgtbl_mod_mask *mask)",
        "{",
        "	pmd_t *pmd;",
        "	unsigned long next;",
        "",
        "	pmd = pmd_alloc_track(&init_mm, pud, addr, mask);",
        "	if (!pmd)",
        "		return -ENOMEM;",
        "	do {",
        "		next = pmd_addr_end(addr, end);",
        "		if (vmap_pages_pte_range(pmd, addr, next, prot, pages, nr, mask))",
        "			return -ENOMEM;",
        "	} while (pmd++, addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_pages_pud_range(p4d_t *p4d, unsigned long addr,",
        "		unsigned long end, pgprot_t prot, struct page **pages, int *nr,",
        "		pgtbl_mod_mask *mask)",
        "{",
        "	pud_t *pud;",
        "	unsigned long next;",
        "",
        "	pud = pud_alloc_track(&init_mm, p4d, addr, mask);",
        "	if (!pud)",
        "		return -ENOMEM;",
        "	do {",
        "		next = pud_addr_end(addr, end);",
        "		if (vmap_pages_pmd_range(pud, addr, next, prot, pages, nr, mask))",
        "			return -ENOMEM;",
        "	} while (pud++, addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_pages_p4d_range(pgd_t *pgd, unsigned long addr,",
        "		unsigned long end, pgprot_t prot, struct page **pages, int *nr,",
        "		pgtbl_mod_mask *mask)",
        "{",
        "	p4d_t *p4d;",
        "	unsigned long next;",
        "",
        "	p4d = p4d_alloc_track(&init_mm, pgd, addr, mask);",
        "	if (!p4d)",
        "		return -ENOMEM;",
        "	do {",
        "		next = p4d_addr_end(addr, end);",
        "		if (vmap_pages_pud_range(p4d, addr, next, prot, pages, nr, mask))",
        "			return -ENOMEM;",
        "	} while (p4d++, addr = next, addr != end);",
        "	return 0;",
        "}",
        "",
        "static int vmap_small_pages_range_noflush(unsigned long addr, unsigned long end,",
        "		pgprot_t prot, struct page **pages)",
        "{",
        "	unsigned long start = addr;",
        "	pgd_t *pgd;",
        "	unsigned long next;",
        "	int err = 0;",
        "	int nr = 0;",
        "	pgtbl_mod_mask mask = 0;",
        "",
        "	BUG_ON(addr >= end);",
        "	pgd = pgd_offset_k(addr);",
        "	do {",
        "		next = pgd_addr_end(addr, end);",
        "		if (pgd_bad(*pgd))",
        "			mask |= PGTBL_PGD_MODIFIED;",
        "		err = vmap_pages_p4d_range(pgd, addr, next, prot, pages, &nr, &mask);",
        "		if (err)",
        "			break;",
        "	} while (pgd++, addr = next, addr != end);",
        "",
        "	if (mask & ARCH_PAGE_TABLE_SYNC_MASK)",
        "		arch_sync_kernel_mappings(start, end);",
        "",
        "	return err;",
        "}",
        "",
        "/*",
        " * vmap_pages_range_noflush is similar to vmap_pages_range, but does not",
        " * flush caches.",
        " *",
        " * The caller is responsible for calling flush_cache_vmap() after this",
        " * function returns successfully and before the addresses are accessed.",
        " *",
        " * This is an internal function only. Do not use outside mm/.",
        " */",
        "int __vmap_pages_range_noflush(unsigned long addr, unsigned long end,",
        "		pgprot_t prot, struct page **pages, unsigned int page_shift)",
        "{",
        "	unsigned int i, nr = (end - addr) >> PAGE_SHIFT;",
        "",
        "	WARN_ON(page_shift < PAGE_SHIFT);",
        "",
        "	if (!IS_ENABLED(CONFIG_HAVE_ARCH_HUGE_VMALLOC) ||",
        "			page_shift == PAGE_SHIFT)",
        "		return vmap_small_pages_range_noflush(addr, end, prot, pages);",
        "",
        "	for (i = 0; i < nr; i += 1U << (page_shift - PAGE_SHIFT)) {",
        "		int err;",
        "",
        "		err = vmap_range_noflush(addr, addr + (1UL << page_shift),",
        "					page_to_phys(pages[i]), prot,",
        "					page_shift);",
        "		if (err)",
        "			return err;",
        "",
        "		addr += 1UL << page_shift;",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "int vmap_pages_range_noflush(unsigned long addr, unsigned long end,",
        "		pgprot_t prot, struct page **pages, unsigned int page_shift)",
        "{",
        "	int ret = kmsan_vmap_pages_range_noflush(addr, end, prot, pages,",
        "						 page_shift);",
        "",
        "	if (ret)",
        "		return ret;",
        "	return __vmap_pages_range_noflush(addr, end, prot, pages, page_shift);",
        "}",
        "",
        "/**",
        " * vmap_pages_range - map pages to a kernel virtual address",
        " * @addr: start of the VM area to map",
        " * @end: end of the VM area to map (non-inclusive)",
        " * @prot: page protection flags to use",
        " * @pages: pages to map (always PAGE_SIZE pages)",
        " * @page_shift: maximum shift that the pages may be mapped with, @pages must",
        " * be aligned and contiguous up to at least this shift.",
        " *",
        " * RETURNS:",
        " * 0 on success, -errno on failure.",
        " */",
        "int vmap_pages_range(unsigned long addr, unsigned long end,",
        "		pgprot_t prot, struct page **pages, unsigned int page_shift)",
        "{",
        "	int err;",
        "",
        "	err = vmap_pages_range_noflush(addr, end, prot, pages, page_shift);",
        "	flush_cache_vmap(addr, end);",
        "	return err;",
        "}",
        "",
        "static int check_sparse_vm_area(struct vm_struct *area, unsigned long start,",
        "				unsigned long end)",
        "{",
        "	might_sleep();",
        "	if (WARN_ON_ONCE(area->flags & VM_FLUSH_RESET_PERMS))",
        "		return -EINVAL;",
        "	if (WARN_ON_ONCE(area->flags & VM_NO_GUARD))",
        "		return -EINVAL;",
        "	if (WARN_ON_ONCE(!(area->flags & VM_SPARSE)))",
        "		return -EINVAL;",
        "	if ((end - start) >> PAGE_SHIFT > totalram_pages())",
        "		return -E2BIG;",
        "	if (start < (unsigned long)area->addr ||",
        "	    (void *)end > area->addr + get_vm_area_size(area))",
        "		return -ERANGE;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * vm_area_map_pages - map pages inside given sparse vm_area",
        " * @area: vm_area",
        " * @start: start address inside vm_area",
        " * @end: end address inside vm_area",
        " * @pages: pages to map (always PAGE_SIZE pages)",
        " */",
        "int vm_area_map_pages(struct vm_struct *area, unsigned long start,",
        "		      unsigned long end, struct page **pages)",
        "{",
        "	int err;",
        "",
        "	err = check_sparse_vm_area(area, start, end);",
        "	if (err)",
        "		return err;",
        "",
        "	return vmap_pages_range(start, end, PAGE_KERNEL, pages, PAGE_SHIFT);",
        "}",
        "",
        "/**",
        " * vm_area_unmap_pages - unmap pages inside given sparse vm_area",
        " * @area: vm_area",
        " * @start: start address inside vm_area",
        " * @end: end address inside vm_area",
        " */",
        "void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,",
        "			 unsigned long end)",
        "{",
        "	if (check_sparse_vm_area(area, start, end))",
        "		return;",
        "",
        "	vunmap_range(start, end);",
        "}",
        "",
        "int is_vmalloc_or_module_addr(const void *x)",
        "{",
        "	/*",
        "	 * ARM, x86-64 and sparc64 put modules in a special place,",
        "	 * and fall back on vmalloc() if that fails. Others",
        "	 * just put it in the vmalloc space.",
        "	 */",
        "#if defined(CONFIG_EXECMEM) && defined(MODULES_VADDR)",
        "	unsigned long addr = (unsigned long)kasan_reset_tag(x);",
        "	if (addr >= MODULES_VADDR && addr < MODULES_END)",
        "		return 1;",
        "#endif",
        "	return is_vmalloc_addr(x);",
        "}",
        "EXPORT_SYMBOL_GPL(is_vmalloc_or_module_addr);",
        "",
        "/*",
        " * Walk a vmap address to the struct page it maps. Huge vmap mappings will",
        " * return the tail page that corresponds to the base page address, which",
        " * matches small vmap mappings.",
        " */",
        "struct page *vmalloc_to_page(const void *vmalloc_addr)",
        "{",
        "	unsigned long addr = (unsigned long) vmalloc_addr;",
        "	struct page *page = NULL;",
        "	pgd_t *pgd = pgd_offset_k(addr);",
        "	p4d_t *p4d;",
        "	pud_t *pud;",
        "	pmd_t *pmd;",
        "	pte_t *ptep, pte;",
        "",
        "	/*",
        "	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for",
        "	 * architectures that do not vmalloc module space",
        "	 */",
        "	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));",
        "",
        "	if (pgd_none(*pgd))",
        "		return NULL;",
        "	if (WARN_ON_ONCE(pgd_leaf(*pgd)))",
        "		return NULL; /* XXX: no allowance for huge pgd */",
        "	if (WARN_ON_ONCE(pgd_bad(*pgd)))",
        "		return NULL;",
        "",
        "	p4d = p4d_offset(pgd, addr);",
        "	if (p4d_none(*p4d))",
        "		return NULL;",
        "	if (p4d_leaf(*p4d))",
        "		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);",
        "	if (WARN_ON_ONCE(p4d_bad(*p4d)))",
        "		return NULL;",
        "",
        "	pud = pud_offset(p4d, addr);",
        "	if (pud_none(*pud))",
        "		return NULL;",
        "	if (pud_leaf(*pud))",
        "		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);",
        "	if (WARN_ON_ONCE(pud_bad(*pud)))",
        "		return NULL;",
        "",
        "	pmd = pmd_offset(pud, addr);",
        "	if (pmd_none(*pmd))",
        "		return NULL;",
        "	if (pmd_leaf(*pmd))",
        "		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);",
        "	if (WARN_ON_ONCE(pmd_bad(*pmd)))",
        "		return NULL;",
        "",
        "	ptep = pte_offset_kernel(pmd, addr);",
        "	pte = ptep_get(ptep);",
        "	if (pte_present(pte))",
        "		page = pte_page(pte);",
        "",
        "	return page;",
        "}",
        "EXPORT_SYMBOL(vmalloc_to_page);",
        "",
        "/*",
        " * Map a vmalloc()-space virtual address to the physical page frame number.",
        " */",
        "unsigned long vmalloc_to_pfn(const void *vmalloc_addr)",
        "{",
        "	return page_to_pfn(vmalloc_to_page(vmalloc_addr));",
        "}",
        "EXPORT_SYMBOL(vmalloc_to_pfn);",
        "",
        "",
        "/*** Global kva allocator ***/",
        "",
        "#define DEBUG_AUGMENT_PROPAGATE_CHECK 0",
        "#define DEBUG_AUGMENT_LOWEST_MATCH_CHECK 0",
        "",
        "",
        "static DEFINE_SPINLOCK(free_vmap_area_lock);",
        "static bool vmap_initialized __read_mostly;",
        "",
        "/*",
        " * This kmem_cache is used for vmap_area objects. Instead of",
        " * allocating from slab we reuse an object from this cache to",
        " * make things faster. Especially in \"no edge\" splitting of",
        " * free block.",
        " */",
        "static struct kmem_cache *vmap_area_cachep;",
        "",
        "/*",
        " * This linked list is used in pair with free_vmap_area_root.",
        " * It gives O(1) access to prev/next to perform fast coalescing.",
        " */",
        "static LIST_HEAD(free_vmap_area_list);",
        "",
        "/*",
        " * This augment red-black tree represents the free vmap space.",
        " * All vmap_area objects in this tree are sorted by va->va_start",
        " * address. It is used for allocation and merging when a vmap",
        " * object is released.",
        " *",
        " * Each vmap_area node contains a maximum available free block",
        " * of its sub-tree, right or left. Therefore it is possible to",
        " * find a lowest match of free area.",
        " */",
        "static struct rb_root free_vmap_area_root = RB_ROOT;",
        "",
        "/*",
        " * Preload a CPU with one object for \"no edge\" split case. The",
        " * aim is to get rid of allocations from the atomic context, thus",
        " * to use more permissive allocation masks.",
        " */",
        "static DEFINE_PER_CPU(struct vmap_area *, ne_fit_preload_node);",
        "",
        "/*",
        " * This structure defines a single, solid model where a list and",
        " * rb-tree are part of one entity protected by the lock. Nodes are",
        " * sorted in ascending order, thus for O(1) access to left/right",
        " * neighbors a list is used as well as for sequential traversal.",
        " */",
        "struct rb_list {",
        "	struct rb_root root;",
        "	struct list_head head;",
        "	spinlock_t lock;",
        "};",
        "",
        "/*",
        " * A fast size storage contains VAs up to 1M size. A pool consists",
        " * of linked between each other ready to go VAs of certain sizes.",
        " * An index in the pool-array corresponds to number of pages + 1.",
        " */",
        "#define MAX_VA_SIZE_PAGES 256",
        "",
        "struct vmap_pool {",
        "	struct list_head head;",
        "	unsigned long len;",
        "};",
        "",
        "/*",
        " * An effective vmap-node logic. Users make use of nodes instead",
        " * of a global heap. It allows to balance an access and mitigate",
        " * contention.",
        " */",
        "static struct vmap_node {",
        "	/* Simple size segregated storage. */",
        "	struct vmap_pool pool[MAX_VA_SIZE_PAGES];",
        "	spinlock_t pool_lock;",
        "	bool skip_populate;",
        "",
        "	/* Bookkeeping data of this node. */",
        "	struct rb_list busy;",
        "	struct rb_list lazy;",
        "",
        "	/*",
        "	 * Ready-to-free areas.",
        "	 */",
        "	struct list_head purge_list;",
        "	struct work_struct purge_work;",
        "	unsigned long nr_purged;",
        "} single;",
        "",
        "/*",
        " * Initial setup consists of one single node, i.e. a balancing",
        " * is fully disabled. Later on, after vmap is initialized these",
        " * parameters are updated based on a system capacity.",
        " */",
        "static struct vmap_node *vmap_nodes = &single;",
        "static __read_mostly unsigned int nr_vmap_nodes = 1;",
        "static __read_mostly unsigned int vmap_zone_size = 1;",
        "",
        "static inline unsigned int",
        "addr_to_node_id(unsigned long addr)",
        "{",
        "	return (addr / vmap_zone_size) % nr_vmap_nodes;",
        "}",
        "",
        "static inline struct vmap_node *",
        "addr_to_node(unsigned long addr)",
        "{",
        "	return &vmap_nodes[addr_to_node_id(addr)];",
        "}",
        "",
        "static inline struct vmap_node *",
        "id_to_node(unsigned int id)",
        "{",
        "	return &vmap_nodes[id % nr_vmap_nodes];",
        "}",
        "",
        "/*",
        " * We use the value 0 to represent \"no node\", that is why",
        " * an encoded value will be the node-id incremented by 1.",
        " * It is always greater then 0. A valid node_id which can",
        " * be encoded is [0:nr_vmap_nodes - 1]. If a passed node_id",
        " * is not valid 0 is returned.",
        " */",
        "static unsigned int",
        "encode_vn_id(unsigned int node_id)",
        "{",
        "	/* Can store U8_MAX [0:254] nodes. */",
        "	if (node_id < nr_vmap_nodes)",
        "		return (node_id + 1) << BITS_PER_BYTE;",
        "",
        "	/* Warn and no node encoded. */",
        "	WARN_ONCE(1, \"Encode wrong node id (%u)\\n\", node_id);",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Returns an encoded node-id, the valid range is within",
        " * [0:nr_vmap_nodes-1] values. Otherwise nr_vmap_nodes is",
        " * returned if extracted data is wrong.",
        " */",
        "static unsigned int",
        "decode_vn_id(unsigned int val)",
        "{",
        "	unsigned int node_id = (val >> BITS_PER_BYTE) - 1;",
        "",
        "	/* Can store U8_MAX [0:254] nodes. */",
        "	if (node_id < nr_vmap_nodes)",
        "		return node_id;",
        "",
        "	/* If it was _not_ zero, warn. */",
        "	WARN_ONCE(node_id != UINT_MAX,",
        "		\"Decode wrong node id (%d)\\n\", node_id);",
        "",
        "	return nr_vmap_nodes;",
        "}",
        "",
        "static bool",
        "is_vn_id_valid(unsigned int node_id)",
        "{",
        "	if (node_id < nr_vmap_nodes)",
        "		return true;",
        "",
        "	return false;",
        "}",
        "",
        "static __always_inline unsigned long",
        "va_size(struct vmap_area *va)",
        "{",
        "	return (va->va_end - va->va_start);",
        "}",
        "",
        "static __always_inline unsigned long",
        "get_subtree_max_size(struct rb_node *node)",
        "{",
        "	struct vmap_area *va;",
        "",
        "	va = rb_entry_safe(node, struct vmap_area, rb_node);",
        "	return va ? va->subtree_max_size : 0;",
        "}",
        "",
        "RB_DECLARE_CALLBACKS_MAX(static, free_vmap_area_rb_augment_cb,",
        "	struct vmap_area, rb_node, unsigned long, subtree_max_size, va_size)",
        "",
        "static void reclaim_and_purge_vmap_areas(void);",
        "static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);",
        "static void drain_vmap_area_work(struct work_struct *work);",
        "static DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);",
        "",
        "static atomic_long_t nr_vmalloc_pages;",
        "",
        "unsigned long vmalloc_nr_pages(void)",
        "{",
        "	return atomic_long_read(&nr_vmalloc_pages);",
        "}",
        "",
        "static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)",
        "{",
        "	struct rb_node *n = root->rb_node;",
        "",
        "	addr = (unsigned long)kasan_reset_tag((void *)addr);",
        "",
        "	while (n) {",
        "		struct vmap_area *va;",
        "",
        "		va = rb_entry(n, struct vmap_area, rb_node);",
        "		if (addr < va->va_start)",
        "			n = n->rb_left;",
        "		else if (addr >= va->va_end)",
        "			n = n->rb_right;",
        "		else",
        "			return va;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/* Look up the first VA which satisfies addr < va_end, NULL if none. */",
        "static struct vmap_area *",
        "__find_vmap_area_exceed_addr(unsigned long addr, struct rb_root *root)",
        "{",
        "	struct vmap_area *va = NULL;",
        "	struct rb_node *n = root->rb_node;",
        "",
        "	addr = (unsigned long)kasan_reset_tag((void *)addr);",
        "",
        "	while (n) {",
        "		struct vmap_area *tmp;",
        "",
        "		tmp = rb_entry(n, struct vmap_area, rb_node);",
        "		if (tmp->va_end > addr) {",
        "			va = tmp;",
        "			if (tmp->va_start <= addr)",
        "				break;",
        "",
        "			n = n->rb_left;",
        "		} else",
        "			n = n->rb_right;",
        "	}",
        "",
        "	return va;",
        "}",
        "",
        "/*",
        " * Returns a node where a first VA, that satisfies addr < va_end, resides.",
        " * If success, a node is locked. A user is responsible to unlock it when a",
        " * VA is no longer needed to be accessed.",
        " *",
        " * Returns NULL if nothing found.",
        " */",
        "static struct vmap_node *",
        "find_vmap_area_exceed_addr_lock(unsigned long addr, struct vmap_area **va)",
        "{",
        "	unsigned long va_start_lowest;",
        "	struct vmap_node *vn;",
        "	int i;",
        "",
        "repeat:",
        "	for (i = 0, va_start_lowest = 0; i < nr_vmap_nodes; i++) {",
        "		vn = &vmap_nodes[i];",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		*va = __find_vmap_area_exceed_addr(addr, &vn->busy.root);",
        "",
        "		if (*va)",
        "			if (!va_start_lowest || (*va)->va_start < va_start_lowest)",
        "				va_start_lowest = (*va)->va_start;",
        "		spin_unlock(&vn->busy.lock);",
        "	}",
        "",
        "	/*",
        "	 * Check if found VA exists, it might have gone away.  In this case we",
        "	 * repeat the search because a VA has been removed concurrently and we",
        "	 * need to proceed to the next one, which is a rare case.",
        "	 */",
        "	if (va_start_lowest) {",
        "		vn = addr_to_node(va_start_lowest);",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		*va = __find_vmap_area(va_start_lowest, &vn->busy.root);",
        "",
        "		if (*va)",
        "			return vn;",
        "",
        "		spin_unlock(&vn->busy.lock);",
        "		goto repeat;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/*",
        " * This function returns back addresses of parent node",
        " * and its left or right link for further processing.",
        " *",
        " * Otherwise NULL is returned. In that case all further",
        " * steps regarding inserting of conflicting overlap range",
        " * have to be declined and actually considered as a bug.",
        " */",
        "static __always_inline struct rb_node **",
        "find_va_links(struct vmap_area *va,",
        "	struct rb_root *root, struct rb_node *from,",
        "	struct rb_node **parent)",
        "{",
        "	struct vmap_area *tmp_va;",
        "	struct rb_node **link;",
        "",
        "	if (root) {",
        "		link = &root->rb_node;",
        "		if (unlikely(!*link)) {",
        "			*parent = NULL;",
        "			return link;",
        "		}",
        "	} else {",
        "		link = &from;",
        "	}",
        "",
        "	/*",
        "	 * Go to the bottom of the tree. When we hit the last point",
        "	 * we end up with parent rb_node and correct direction, i name",
        "	 * it link, where the new va->rb_node will be attached to.",
        "	 */",
        "	do {",
        "		tmp_va = rb_entry(*link, struct vmap_area, rb_node);",
        "",
        "		/*",
        "		 * During the traversal we also do some sanity check.",
        "		 * Trigger the BUG() if there are sides(left/right)",
        "		 * or full overlaps.",
        "		 */",
        "		if (va->va_end <= tmp_va->va_start)",
        "			link = &(*link)->rb_left;",
        "		else if (va->va_start >= tmp_va->va_end)",
        "			link = &(*link)->rb_right;",
        "		else {",
        "			WARN(1, \"vmalloc bug: 0x%lx-0x%lx overlaps with 0x%lx-0x%lx\\n\",",
        "				va->va_start, va->va_end, tmp_va->va_start, tmp_va->va_end);",
        "",
        "			return NULL;",
        "		}",
        "	} while (*link);",
        "",
        "	*parent = &tmp_va->rb_node;",
        "	return link;",
        "}",
        "",
        "static __always_inline struct list_head *",
        "get_va_next_sibling(struct rb_node *parent, struct rb_node **link)",
        "{",
        "	struct list_head *list;",
        "",
        "	if (unlikely(!parent))",
        "		/*",
        "		 * The red-black tree where we try to find VA neighbors",
        "		 * before merging or inserting is empty, i.e. it means",
        "		 * there is no free vmap space. Normally it does not",
        "		 * happen but we handle this case anyway.",
        "		 */",
        "		return NULL;",
        "",
        "	list = &rb_entry(parent, struct vmap_area, rb_node)->list;",
        "	return (&parent->rb_right == link ? list->next : list);",
        "}",
        "",
        "static __always_inline void",
        "__link_va(struct vmap_area *va, struct rb_root *root,",
        "	struct rb_node *parent, struct rb_node **link,",
        "	struct list_head *head, bool augment)",
        "{",
        "	/*",
        "	 * VA is still not in the list, but we can",
        "	 * identify its future previous list_head node.",
        "	 */",
        "	if (likely(parent)) {",
        "		head = &rb_entry(parent, struct vmap_area, rb_node)->list;",
        "		if (&parent->rb_right != link)",
        "			head = head->prev;",
        "	}",
        "",
        "	/* Insert to the rb-tree */",
        "	rb_link_node(&va->rb_node, parent, link);",
        "	if (augment) {",
        "		/*",
        "		 * Some explanation here. Just perform simple insertion",
        "		 * to the tree. We do not set va->subtree_max_size to",
        "		 * its current size before calling rb_insert_augmented().",
        "		 * It is because we populate the tree from the bottom",
        "		 * to parent levels when the node _is_ in the tree.",
        "		 *",
        "		 * Therefore we set subtree_max_size to zero after insertion,",
        "		 * to let __augment_tree_propagate_from() puts everything to",
        "		 * the correct order later on.",
        "		 */",
        "		rb_insert_augmented(&va->rb_node,",
        "			root, &free_vmap_area_rb_augment_cb);",
        "		va->subtree_max_size = 0;",
        "	} else {",
        "		rb_insert_color(&va->rb_node, root);",
        "	}",
        "",
        "	/* Address-sort this list */",
        "	list_add(&va->list, head);",
        "}",
        "",
        "static __always_inline void",
        "link_va(struct vmap_area *va, struct rb_root *root,",
        "	struct rb_node *parent, struct rb_node **link,",
        "	struct list_head *head)",
        "{",
        "	__link_va(va, root, parent, link, head, false);",
        "}",
        "",
        "static __always_inline void",
        "link_va_augment(struct vmap_area *va, struct rb_root *root,",
        "	struct rb_node *parent, struct rb_node **link,",
        "	struct list_head *head)",
        "{",
        "	__link_va(va, root, parent, link, head, true);",
        "}",
        "",
        "static __always_inline void",
        "__unlink_va(struct vmap_area *va, struct rb_root *root, bool augment)",
        "{",
        "	if (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))",
        "		return;",
        "",
        "	if (augment)",
        "		rb_erase_augmented(&va->rb_node,",
        "			root, &free_vmap_area_rb_augment_cb);",
        "	else",
        "		rb_erase(&va->rb_node, root);",
        "",
        "	list_del_init(&va->list);",
        "	RB_CLEAR_NODE(&va->rb_node);",
        "}",
        "",
        "static __always_inline void",
        "unlink_va(struct vmap_area *va, struct rb_root *root)",
        "{",
        "	__unlink_va(va, root, false);",
        "}",
        "",
        "static __always_inline void",
        "unlink_va_augment(struct vmap_area *va, struct rb_root *root)",
        "{",
        "	__unlink_va(va, root, true);",
        "}",
        "",
        "#if DEBUG_AUGMENT_PROPAGATE_CHECK",
        "/*",
        " * Gets called when remove the node and rotate.",
        " */",
        "static __always_inline unsigned long",
        "compute_subtree_max_size(struct vmap_area *va)",
        "{",
        "	return max3(va_size(va),",
        "		get_subtree_max_size(va->rb_node.rb_left),",
        "		get_subtree_max_size(va->rb_node.rb_right));",
        "}",
        "",
        "static void",
        "augment_tree_propagate_check(void)",
        "{",
        "	struct vmap_area *va;",
        "	unsigned long computed_size;",
        "",
        "	list_for_each_entry(va, &free_vmap_area_list, list) {",
        "		computed_size = compute_subtree_max_size(va);",
        "		if (computed_size != va->subtree_max_size)",
        "			pr_emerg(\"tree is corrupted: %lu, %lu\\n\",",
        "				va_size(va), va->subtree_max_size);",
        "	}",
        "}",
        "#endif",
        "",
        "/*",
        " * This function populates subtree_max_size from bottom to upper",
        " * levels starting from VA point. The propagation must be done",
        " * when VA size is modified by changing its va_start/va_end. Or",
        " * in case of newly inserting of VA to the tree.",
        " *",
        " * It means that __augment_tree_propagate_from() must be called:",
        " * - After VA has been inserted to the tree(free path);",
        " * - After VA has been shrunk(allocation path);",
        " * - After VA has been increased(merging path).",
        " *",
        " * Please note that, it does not mean that upper parent nodes",
        " * and their subtree_max_size are recalculated all the time up",
        " * to the root node.",
        " *",
        " *       4--8",
        " *        /\\",
        " *       /  \\",
        " *      /    \\",
        " *    2--2  8--8",
        " *",
        " * For example if we modify the node 4, shrinking it to 2, then",
        " * no any modification is required. If we shrink the node 2 to 1",
        " * its subtree_max_size is updated only, and set to 1. If we shrink",
        " * the node 8 to 6, then its subtree_max_size is set to 6 and parent",
        " * node becomes 4--6.",
        " */",
        "static __always_inline void",
        "augment_tree_propagate_from(struct vmap_area *va)",
        "{",
        "	/*",
        "	 * Populate the tree from bottom towards the root until",
        "	 * the calculated maximum available size of checked node",
        "	 * is equal to its current one.",
        "	 */",
        "	free_vmap_area_rb_augment_cb_propagate(&va->rb_node, NULL);",
        "",
        "#if DEBUG_AUGMENT_PROPAGATE_CHECK",
        "	augment_tree_propagate_check();",
        "#endif",
        "}",
        "",
        "static void",
        "insert_vmap_area(struct vmap_area *va,",
        "	struct rb_root *root, struct list_head *head)",
        "{",
        "	struct rb_node **link;",
        "	struct rb_node *parent;",
        "",
        "	link = find_va_links(va, root, NULL, &parent);",
        "	if (link)",
        "		link_va(va, root, parent, link, head);",
        "}",
        "",
        "static void",
        "insert_vmap_area_augment(struct vmap_area *va,",
        "	struct rb_node *from, struct rb_root *root,",
        "	struct list_head *head)",
        "{",
        "	struct rb_node **link;",
        "	struct rb_node *parent;",
        "",
        "	if (from)",
        "		link = find_va_links(va, NULL, from, &parent);",
        "	else",
        "		link = find_va_links(va, root, NULL, &parent);",
        "",
        "	if (link) {",
        "		link_va_augment(va, root, parent, link, head);",
        "		augment_tree_propagate_from(va);",
        "	}",
        "}",
        "",
        "/*",
        " * Merge de-allocated chunk of VA memory with previous",
        " * and next free blocks. If coalesce is not done a new",
        " * free area is inserted. If VA has been merged, it is",
        " * freed.",
        " *",
        " * Please note, it can return NULL in case of overlap",
        " * ranges, followed by WARN() report. Despite it is a",
        " * buggy behaviour, a system can be alive and keep",
        " * ongoing.",
        " */",
        "static __always_inline struct vmap_area *",
        "__merge_or_add_vmap_area(struct vmap_area *va,",
        "	struct rb_root *root, struct list_head *head, bool augment)",
        "{",
        "	struct vmap_area *sibling;",
        "	struct list_head *next;",
        "	struct rb_node **link;",
        "	struct rb_node *parent;",
        "	bool merged = false;",
        "",
        "	/*",
        "	 * Find a place in the tree where VA potentially will be",
        "	 * inserted, unless it is merged with its sibling/siblings.",
        "	 */",
        "	link = find_va_links(va, root, NULL, &parent);",
        "	if (!link)",
        "		return NULL;",
        "",
        "	/*",
        "	 * Get next node of VA to check if merging can be done.",
        "	 */",
        "	next = get_va_next_sibling(parent, link);",
        "	if (unlikely(next == NULL))",
        "		goto insert;",
        "",
        "	/*",
        "	 * start            end",
        "	 * |                |",
        "	 * |<------VA------>|<-----Next----->|",
        "	 *                  |                |",
        "	 *                  start            end",
        "	 */",
        "	if (next != head) {",
        "		sibling = list_entry(next, struct vmap_area, list);",
        "		if (sibling->va_start == va->va_end) {",
        "			sibling->va_start = va->va_start;",
        "",
        "			/* Free vmap_area object. */",
        "			kmem_cache_free(vmap_area_cachep, va);",
        "",
        "			/* Point to the new merged area. */",
        "			va = sibling;",
        "			merged = true;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * start            end",
        "	 * |                |",
        "	 * |<-----Prev----->|<------VA------>|",
        "	 *                  |                |",
        "	 *                  start            end",
        "	 */",
        "	if (next->prev != head) {",
        "		sibling = list_entry(next->prev, struct vmap_area, list);",
        "		if (sibling->va_end == va->va_start) {",
        "			/*",
        "			 * If both neighbors are coalesced, it is important",
        "			 * to unlink the \"next\" node first, followed by merging",
        "			 * with \"previous\" one. Otherwise the tree might not be",
        "			 * fully populated if a sibling's augmented value is",
        "			 * \"normalized\" because of rotation operations.",
        "			 */",
        "			if (merged)",
        "				__unlink_va(va, root, augment);",
        "",
        "			sibling->va_end = va->va_end;",
        "",
        "			/* Free vmap_area object. */",
        "			kmem_cache_free(vmap_area_cachep, va);",
        "",
        "			/* Point to the new merged area. */",
        "			va = sibling;",
        "			merged = true;",
        "		}",
        "	}",
        "",
        "insert:",
        "	if (!merged)",
        "		__link_va(va, root, parent, link, head, augment);",
        "",
        "	return va;",
        "}",
        "",
        "static __always_inline struct vmap_area *",
        "merge_or_add_vmap_area(struct vmap_area *va,",
        "	struct rb_root *root, struct list_head *head)",
        "{",
        "	return __merge_or_add_vmap_area(va, root, head, false);",
        "}",
        "",
        "static __always_inline struct vmap_area *",
        "merge_or_add_vmap_area_augment(struct vmap_area *va,",
        "	struct rb_root *root, struct list_head *head)",
        "{",
        "	va = __merge_or_add_vmap_area(va, root, head, true);",
        "	if (va)",
        "		augment_tree_propagate_from(va);",
        "",
        "	return va;",
        "}",
        "",
        "static __always_inline bool",
        "is_within_this_va(struct vmap_area *va, unsigned long size,",
        "	unsigned long align, unsigned long vstart)",
        "{",
        "	unsigned long nva_start_addr;",
        "",
        "	if (va->va_start > vstart)",
        "		nva_start_addr = ALIGN(va->va_start, align);",
        "	else",
        "		nva_start_addr = ALIGN(vstart, align);",
        "",
        "	/* Can be overflowed due to big size or alignment. */",
        "	if (nva_start_addr + size < nva_start_addr ||",
        "			nva_start_addr < vstart)",
        "		return false;",
        "",
        "	return (nva_start_addr + size <= va->va_end);",
        "}",
        "",
        "/*",
        " * Find the first free block(lowest start address) in the tree,",
        " * that will accomplish the request corresponding to passing",
        " * parameters. Please note, with an alignment bigger than PAGE_SIZE,",
        " * a search length is adjusted to account for worst case alignment",
        " * overhead.",
        " */",
        "static __always_inline struct vmap_area *",
        "find_vmap_lowest_match(struct rb_root *root, unsigned long size,",
        "	unsigned long align, unsigned long vstart, bool adjust_search_size)",
        "{",
        "	struct vmap_area *va;",
        "	struct rb_node *node;",
        "	unsigned long length;",
        "",
        "	/* Start from the root. */",
        "	node = root->rb_node;",
        "",
        "	/* Adjust the search size for alignment overhead. */",
        "	length = adjust_search_size ? size + align - 1 : size;",
        "",
        "	while (node) {",
        "		va = rb_entry(node, struct vmap_area, rb_node);",
        "",
        "		if (get_subtree_max_size(node->rb_left) >= length &&",
        "				vstart < va->va_start) {",
        "			node = node->rb_left;",
        "		} else {",
        "			if (is_within_this_va(va, size, align, vstart))",
        "				return va;",
        "",
        "			/*",
        "			 * Does not make sense to go deeper towards the right",
        "			 * sub-tree if it does not have a free block that is",
        "			 * equal or bigger to the requested search length.",
        "			 */",
        "			if (get_subtree_max_size(node->rb_right) >= length) {",
        "				node = node->rb_right;",
        "				continue;",
        "			}",
        "",
        "			/*",
        "			 * OK. We roll back and find the first right sub-tree,",
        "			 * that will satisfy the search criteria. It can happen",
        "			 * due to \"vstart\" restriction or an alignment overhead",
        "			 * that is bigger then PAGE_SIZE.",
        "			 */",
        "			while ((node = rb_parent(node))) {",
        "				va = rb_entry(node, struct vmap_area, rb_node);",
        "				if (is_within_this_va(va, size, align, vstart))",
        "					return va;",
        "",
        "				if (get_subtree_max_size(node->rb_right) >= length &&",
        "						vstart <= va->va_start) {",
        "					/*",
        "					 * Shift the vstart forward. Please note, we update it with",
        "					 * parent's start address adding \"1\" because we do not want",
        "					 * to enter same sub-tree after it has already been checked",
        "					 * and no suitable free block found there.",
        "					 */",
        "					vstart = va->va_start + 1;",
        "					node = node->rb_right;",
        "					break;",
        "				}",
        "			}",
        "		}",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK",
        "#include <linux/random.h>",
        "",
        "static struct vmap_area *",
        "find_vmap_lowest_linear_match(struct list_head *head, unsigned long size,",
        "	unsigned long align, unsigned long vstart)",
        "{",
        "	struct vmap_area *va;",
        "",
        "	list_for_each_entry(va, head, list) {",
        "		if (!is_within_this_va(va, size, align, vstart))",
        "			continue;",
        "",
        "		return va;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "static void",
        "find_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,",
        "			     unsigned long size, unsigned long align)",
        "{",
        "	struct vmap_area *va_1, *va_2;",
        "	unsigned long vstart;",
        "	unsigned int rnd;",
        "",
        "	get_random_bytes(&rnd, sizeof(rnd));",
        "	vstart = VMALLOC_START + rnd;",
        "",
        "	va_1 = find_vmap_lowest_match(root, size, align, vstart, false);",
        "	va_2 = find_vmap_lowest_linear_match(head, size, align, vstart);",
        "",
        "	if (va_1 != va_2)",
        "		pr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",",
        "			va_1, va_2, vstart);",
        "}",
        "#endif",
        "",
        "enum fit_type {",
        "	NOTHING_FIT = 0,",
        "	FL_FIT_TYPE = 1,	/* full fit */",
        "	LE_FIT_TYPE = 2,	/* left edge fit */",
        "	RE_FIT_TYPE = 3,	/* right edge fit */",
        "	NE_FIT_TYPE = 4		/* no edge fit */",
        "};",
        "",
        "static __always_inline enum fit_type",
        "classify_va_fit_type(struct vmap_area *va,",
        "	unsigned long nva_start_addr, unsigned long size)",
        "{",
        "	enum fit_type type;",
        "",
        "	/* Check if it is within VA. */",
        "	if (nva_start_addr < va->va_start ||",
        "			nva_start_addr + size > va->va_end)",
        "		return NOTHING_FIT;",
        "",
        "	/* Now classify. */",
        "	if (va->va_start == nva_start_addr) {",
        "		if (va->va_end == nva_start_addr + size)",
        "			type = FL_FIT_TYPE;",
        "		else",
        "			type = LE_FIT_TYPE;",
        "	} else if (va->va_end == nva_start_addr + size) {",
        "		type = RE_FIT_TYPE;",
        "	} else {",
        "		type = NE_FIT_TYPE;",
        "	}",
        "",
        "	return type;",
        "}",
        "",
        "static __always_inline int",
        "va_clip(struct rb_root *root, struct list_head *head,",
        "		struct vmap_area *va, unsigned long nva_start_addr,",
        "		unsigned long size)",
        "{",
        "	struct vmap_area *lva = NULL;",
        "	enum fit_type type = classify_va_fit_type(va, nva_start_addr, size);",
        "",
        "	if (type == FL_FIT_TYPE) {",
        "		/*",
        "		 * No need to split VA, it fully fits.",
        "		 *",
        "		 * |               |",
        "		 * V      NVA      V",
        "		 * |---------------|",
        "		 */",
        "		unlink_va_augment(va, root);",
        "		kmem_cache_free(vmap_area_cachep, va);",
        "	} else if (type == LE_FIT_TYPE) {",
        "		/*",
        "		 * Split left edge of fit VA.",
        "		 *",
        "		 * |       |",
        "		 * V  NVA  V   R",
        "		 * |-------|-------|",
        "		 */",
        "		va->va_start += size;",
        "	} else if (type == RE_FIT_TYPE) {",
        "		/*",
        "		 * Split right edge of fit VA.",
        "		 *",
        "		 *         |       |",
        "		 *     L   V  NVA  V",
        "		 * |-------|-------|",
        "		 */",
        "		va->va_end = nva_start_addr;",
        "	} else if (type == NE_FIT_TYPE) {",
        "		/*",
        "		 * Split no edge of fit VA.",
        "		 *",
        "		 *     |       |",
        "		 *   L V  NVA  V R",
        "		 * |---|-------|---|",
        "		 */",
        "		lva = __this_cpu_xchg(ne_fit_preload_node, NULL);",
        "		if (unlikely(!lva)) {",
        "			/*",
        "			 * For percpu allocator we do not do any pre-allocation",
        "			 * and leave it as it is. The reason is it most likely",
        "			 * never ends up with NE_FIT_TYPE splitting. In case of",
        "			 * percpu allocations offsets and sizes are aligned to",
        "			 * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE",
        "			 * are its main fitting cases.",
        "			 *",
        "			 * There are a few exceptions though, as an example it is",
        "			 * a first allocation (early boot up) when we have \"one\"",
        "			 * big free space that has to be split.",
        "			 *",
        "			 * Also we can hit this path in case of regular \"vmap\"",
        "			 * allocations, if \"this\" current CPU was not preloaded.",
        "			 * See the comment in alloc_vmap_area() why. If so, then",
        "			 * GFP_NOWAIT is used instead to get an extra object for",
        "			 * split purpose. That is rare and most time does not",
        "			 * occur.",
        "			 *",
        "			 * What happens if an allocation gets failed. Basically,",
        "			 * an \"overflow\" path is triggered to purge lazily freed",
        "			 * areas to free some memory, then, the \"retry\" path is",
        "			 * triggered to repeat one more time. See more details",
        "			 * in alloc_vmap_area() function.",
        "			 */",
        "			lva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);",
        "			if (!lva)",
        "				return -1;",
        "		}",
        "",
        "		/*",
        "		 * Build the remainder.",
        "		 */",
        "		lva->va_start = va->va_start;",
        "		lva->va_end = nva_start_addr;",
        "",
        "		/*",
        "		 * Shrink this VA to remaining size.",
        "		 */",
        "		va->va_start = nva_start_addr + size;",
        "	} else {",
        "		return -1;",
        "	}",
        "",
        "	if (type != FL_FIT_TYPE) {",
        "		augment_tree_propagate_from(va);",
        "",
        "		if (lva)	/* type == NE_FIT_TYPE */",
        "			insert_vmap_area_augment(lva, &va->rb_node, root, head);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static unsigned long",
        "va_alloc(struct vmap_area *va,",
        "		struct rb_root *root, struct list_head *head,",
        "		unsigned long size, unsigned long align,",
        "		unsigned long vstart, unsigned long vend)",
        "{",
        "	unsigned long nva_start_addr;",
        "	int ret;",
        "",
        "	if (va->va_start > vstart)",
        "		nva_start_addr = ALIGN(va->va_start, align);",
        "	else",
        "		nva_start_addr = ALIGN(vstart, align);",
        "",
        "	/* Check the \"vend\" restriction. */",
        "	if (nva_start_addr + size > vend)",
        "		return vend;",
        "",
        "	/* Update the free vmap_area. */",
        "	ret = va_clip(root, head, va, nva_start_addr, size);",
        "	if (WARN_ON_ONCE(ret))",
        "		return vend;",
        "",
        "	return nva_start_addr;",
        "}",
        "",
        "/*",
        " * Returns a start address of the newly allocated area, if success.",
        " * Otherwise a vend is returned that indicates failure.",
        " */",
        "static __always_inline unsigned long",
        "__alloc_vmap_area(struct rb_root *root, struct list_head *head,",
        "	unsigned long size, unsigned long align,",
        "	unsigned long vstart, unsigned long vend)",
        "{",
        "	bool adjust_search_size = true;",
        "	unsigned long nva_start_addr;",
        "	struct vmap_area *va;",
        "",
        "	/*",
        "	 * Do not adjust when:",
        "	 *   a) align <= PAGE_SIZE, because it does not make any sense.",
        "	 *      All blocks(their start addresses) are at least PAGE_SIZE",
        "	 *      aligned anyway;",
        "	 *   b) a short range where a requested size corresponds to exactly",
        "	 *      specified [vstart:vend] interval and an alignment > PAGE_SIZE.",
        "	 *      With adjusted search length an allocation would not succeed.",
        "	 */",
        "	if (align <= PAGE_SIZE || (align > PAGE_SIZE && (vend - vstart) == size))",
        "		adjust_search_size = false;",
        "",
        "	va = find_vmap_lowest_match(root, size, align, vstart, adjust_search_size);",
        "	if (unlikely(!va))",
        "		return vend;",
        "",
        "	nva_start_addr = va_alloc(va, root, head, size, align, vstart, vend);",
        "	if (nva_start_addr == vend)",
        "		return vend;",
        "",
        "#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK",
        "	find_vmap_lowest_match_check(root, head, size, align);",
        "#endif",
        "",
        "	return nva_start_addr;",
        "}",
        "",
        "/*",
        " * Free a region of KVA allocated by alloc_vmap_area",
        " */",
        "static void free_vmap_area(struct vmap_area *va)",
        "{",
        "	struct vmap_node *vn = addr_to_node(va->va_start);",
        "",
        "	/*",
        "	 * Remove from the busy tree/list.",
        "	 */",
        "	spin_lock(&vn->busy.lock);",
        "	unlink_va(va, &vn->busy.root);",
        "	spin_unlock(&vn->busy.lock);",
        "",
        "	/*",
        "	 * Insert/Merge it back to the free tree/list.",
        "	 */",
        "	spin_lock(&free_vmap_area_lock);",
        "	merge_or_add_vmap_area_augment(va, &free_vmap_area_root, &free_vmap_area_list);",
        "	spin_unlock(&free_vmap_area_lock);",
        "}",
        "",
        "static inline void",
        "preload_this_cpu_lock(spinlock_t *lock, gfp_t gfp_mask, int node)",
        "{",
        "	struct vmap_area *va = NULL, *tmp;",
        "",
        "	/*",
        "	 * Preload this CPU with one extra vmap_area object. It is used",
        "	 * when fit type of free area is NE_FIT_TYPE. It guarantees that",
        "	 * a CPU that does an allocation is preloaded.",
        "	 *",
        "	 * We do it in non-atomic context, thus it allows us to use more",
        "	 * permissive allocation masks to be more stable under low memory",
        "	 * condition and high memory pressure.",
        "	 */",
        "	if (!this_cpu_read(ne_fit_preload_node))",
        "		va = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);",
        "",
        "	spin_lock(lock);",
        "",
        "	tmp = NULL;",
        "	if (va && !__this_cpu_try_cmpxchg(ne_fit_preload_node, &tmp, va))",
        "		kmem_cache_free(vmap_area_cachep, va);",
        "}",
        "",
        "static struct vmap_pool *",
        "size_to_va_pool(struct vmap_node *vn, unsigned long size)",
        "{",
        "	unsigned int idx = (size - 1) / PAGE_SIZE;",
        "",
        "	if (idx < MAX_VA_SIZE_PAGES)",
        "		return &vn->pool[idx];",
        "",
        "	return NULL;",
        "}",
        "",
        "static bool",
        "node_pool_add_va(struct vmap_node *n, struct vmap_area *va)",
        "{",
        "	struct vmap_pool *vp;",
        "",
        "	vp = size_to_va_pool(n, va_size(va));",
        "	if (!vp)",
        "		return false;",
        "",
        "	spin_lock(&n->pool_lock);",
        "	list_add(&va->list, &vp->head);",
        "	WRITE_ONCE(vp->len, vp->len + 1);",
        "	spin_unlock(&n->pool_lock);",
        "",
        "	return true;",
        "}",
        "",
        "static struct vmap_area *",
        "node_pool_del_va(struct vmap_node *vn, unsigned long size,",
        "		unsigned long align, unsigned long vstart,",
        "		unsigned long vend)",
        "{",
        "	struct vmap_area *va = NULL;",
        "	struct vmap_pool *vp;",
        "	int err = 0;",
        "",
        "	vp = size_to_va_pool(vn, size);",
        "	if (!vp || list_empty(&vp->head))",
        "		return NULL;",
        "",
        "	spin_lock(&vn->pool_lock);",
        "	if (!list_empty(&vp->head)) {",
        "		va = list_first_entry(&vp->head, struct vmap_area, list);",
        "",
        "		if (IS_ALIGNED(va->va_start, align)) {",
        "			/*",
        "			 * Do some sanity check and emit a warning",
        "			 * if one of below checks detects an error.",
        "			 */",
        "			err |= (va_size(va) != size);",
        "			err |= (va->va_start < vstart);",
        "			err |= (va->va_end > vend);",
        "",
        "			if (!WARN_ON_ONCE(err)) {",
        "				list_del_init(&va->list);",
        "				WRITE_ONCE(vp->len, vp->len - 1);",
        "			} else {",
        "				va = NULL;",
        "			}",
        "		} else {",
        "			list_move_tail(&va->list, &vp->head);",
        "			va = NULL;",
        "		}",
        "	}",
        "	spin_unlock(&vn->pool_lock);",
        "",
        "	return va;",
        "}",
        "",
        "static struct vmap_area *",
        "node_alloc(unsigned long size, unsigned long align,",
        "		unsigned long vstart, unsigned long vend,",
        "		unsigned long *addr, unsigned int *vn_id)",
        "{",
        "	struct vmap_area *va;",
        "",
        "	*vn_id = 0;",
        "	*addr = vend;",
        "",
        "	/*",
        "	 * Fallback to a global heap if not vmalloc or there",
        "	 * is only one node.",
        "	 */",
        "	if (vstart != VMALLOC_START || vend != VMALLOC_END ||",
        "			nr_vmap_nodes == 1)",
        "		return NULL;",
        "",
        "	*vn_id = raw_smp_processor_id() % nr_vmap_nodes;",
        "	va = node_pool_del_va(id_to_node(*vn_id), size, align, vstart, vend);",
        "	*vn_id = encode_vn_id(*vn_id);",
        "",
        "	if (va)",
        "		*addr = va->va_start;",
        "",
        "	return va;",
        "}",
        "",
        "static inline void setup_vmalloc_vm(struct vm_struct *vm,",
        "	struct vmap_area *va, unsigned long flags, const void *caller)",
        "{",
        "	vm->flags = flags;",
        "	vm->addr = (void *)va->va_start;",
        "	vm->size = va_size(va);",
        "	vm->caller = caller;",
        "	va->vm = vm;",
        "}",
        "",
        "/*",
        " * Allocate a region of KVA of the specified size and alignment, within the",
        " * vstart and vend. If vm is passed in, the two will also be bound.",
        " */",
        "static struct vmap_area *alloc_vmap_area(unsigned long size,",
        "				unsigned long align,",
        "				unsigned long vstart, unsigned long vend,",
        "				int node, gfp_t gfp_mask,",
        "				unsigned long va_flags, struct vm_struct *vm)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	unsigned long freed;",
        "	unsigned long addr;",
        "	unsigned int vn_id;",
        "	int purged = 0;",
        "	int ret;",
        "",
        "	if (unlikely(!size || offset_in_page(size) || !is_power_of_2(align)))",
        "		return ERR_PTR(-EINVAL);",
        "",
        "	if (unlikely(!vmap_initialized))",
        "		return ERR_PTR(-EBUSY);",
        "",
        "	might_sleep();",
        "",
        "	/*",
        "	 * If a VA is obtained from a global heap(if it fails here)",
        "	 * it is anyway marked with this \"vn_id\" so it is returned",
        "	 * to this pool's node later. Such way gives a possibility",
        "	 * to populate pools based on users demand.",
        "	 *",
        "	 * On success a ready to go VA is returned.",
        "	 */",
        "	va = node_alloc(size, align, vstart, vend, &addr, &vn_id);",
        "	if (!va) {",
        "		gfp_mask = gfp_mask & GFP_RECLAIM_MASK;",
        "",
        "		va = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);",
        "		if (unlikely(!va))",
        "			return ERR_PTR(-ENOMEM);",
        "",
        "		/*",
        "		 * Only scan the relevant parts containing pointers to other objects",
        "		 * to avoid false negatives.",
        "		 */",
        "		kmemleak_scan_area(&va->rb_node, SIZE_MAX, gfp_mask);",
        "	}",
        "",
        "retry:",
        "	if (addr == vend) {",
        "		preload_this_cpu_lock(&free_vmap_area_lock, gfp_mask, node);",
        "		addr = __alloc_vmap_area(&free_vmap_area_root, &free_vmap_area_list,",
        "			size, align, vstart, vend);",
        "		spin_unlock(&free_vmap_area_lock);",
        "	}",
        "",
        "	trace_alloc_vmap_area(addr, size, align, vstart, vend, addr == vend);",
        "",
        "	/*",
        "	 * If an allocation fails, the \"vend\" address is",
        "	 * returned. Therefore trigger the overflow path.",
        "	 */",
        "	if (unlikely(addr == vend))",
        "		goto overflow;",
        "",
        "	va->va_start = addr;",
        "	va->va_end = addr + size;",
        "	va->vm = NULL;",
        "	va->flags = (va_flags | vn_id);",
        "",
        "	if (vm) {",
        "		vm->addr = (void *)va->va_start;",
        "		vm->size = va_size(va);",
        "		va->vm = vm;",
        "	}",
        "",
        "	vn = addr_to_node(va->va_start);",
        "",
        "	spin_lock(&vn->busy.lock);",
        "	insert_vmap_area(va, &vn->busy.root, &vn->busy.head);",
        "	spin_unlock(&vn->busy.lock);",
        "",
        "	BUG_ON(!IS_ALIGNED(va->va_start, align));",
        "	BUG_ON(va->va_start < vstart);",
        "	BUG_ON(va->va_end > vend);",
        "",
        "	ret = kasan_populate_vmalloc(addr, size);",
        "	if (ret) {",
        "		free_vmap_area(va);",
        "		return ERR_PTR(ret);",
        "	}",
        "",
        "	return va;",
        "",
        "overflow:",
        "	if (!purged) {",
        "		reclaim_and_purge_vmap_areas();",
        "		purged = 1;",
        "		goto retry;",
        "	}",
        "",
        "	freed = 0;",
        "	blocking_notifier_call_chain(&vmap_notify_list, 0, &freed);",
        "",
        "	if (freed > 0) {",
        "		purged = 0;",
        "		goto retry;",
        "	}",
        "",
        "	if (!(gfp_mask & __GFP_NOWARN) && printk_ratelimit())",
        "		pr_warn(\"vmalloc_node_range for size %lu failed: Address range restricted to %#lx - %#lx\\n\",",
        "				size, vstart, vend);",
        "",
        "	kmem_cache_free(vmap_area_cachep, va);",
        "	return ERR_PTR(-EBUSY);",
        "}",
        "",
        "int register_vmap_purge_notifier(struct notifier_block *nb)",
        "{",
        "	return blocking_notifier_chain_register(&vmap_notify_list, nb);",
        "}",
        "EXPORT_SYMBOL_GPL(register_vmap_purge_notifier);",
        "",
        "int unregister_vmap_purge_notifier(struct notifier_block *nb)",
        "{",
        "	return blocking_notifier_chain_unregister(&vmap_notify_list, nb);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_vmap_purge_notifier);",
        "",
        "/*",
        " * lazy_max_pages is the maximum amount of virtual address space we gather up",
        " * before attempting to purge with a TLB flush.",
        " *",
        " * There is a tradeoff here: a larger number will cover more kernel page tables",
        " * and take slightly longer to purge, but it will linearly reduce the number of",
        " * global TLB flushes that must be performed. It would seem natural to scale",
        " * this number up linearly with the number of CPUs (because vmapping activity",
        " * could also scale linearly with the number of CPUs), however it is likely",
        " * that in practice, workloads might be constrained in other ways that mean",
        " * vmap activity will not scale linearly with CPUs. Also, I want to be",
        " * conservative and not introduce a big latency on huge systems, so go with",
        " * a less aggressive log scale. It will still be an improvement over the old",
        " * code, and it will be simple to change the scale factor if we find that it",
        " * becomes a problem on bigger systems.",
        " */",
        "static unsigned long lazy_max_pages(void)",
        "{",
        "	unsigned int log;",
        "",
        "	log = fls(num_online_cpus());",
        "",
        "	return log * (32UL * 1024 * 1024 / PAGE_SIZE);",
        "}",
        "",
        "static atomic_long_t vmap_lazy_nr = ATOMIC_LONG_INIT(0);",
        "",
        "/*",
        " * Serialize vmap purging.  There is no actual critical section protected",
        " * by this lock, but we want to avoid concurrent calls for performance",
        " * reasons and to make the pcpu_get_vm_areas more deterministic.",
        " */",
        "static DEFINE_MUTEX(vmap_purge_lock);",
        "",
        "/* for per-CPU blocks */",
        "static void purge_fragmented_blocks_allcpus(void);",
        "static cpumask_t purge_nodes;",
        "",
        "static void",
        "reclaim_list_global(struct list_head *head)",
        "{",
        "	struct vmap_area *va, *n;",
        "",
        "	if (list_empty(head))",
        "		return;",
        "",
        "	spin_lock(&free_vmap_area_lock);",
        "	list_for_each_entry_safe(va, n, head, list)",
        "		merge_or_add_vmap_area_augment(va,",
        "			&free_vmap_area_root, &free_vmap_area_list);",
        "	spin_unlock(&free_vmap_area_lock);",
        "}",
        "",
        "static void",
        "decay_va_pool_node(struct vmap_node *vn, bool full_decay)",
        "{",
        "	LIST_HEAD(decay_list);",
        "	struct rb_root decay_root = RB_ROOT;",
        "	struct vmap_area *va, *nva;",
        "	unsigned long n_decay;",
        "	int i;",
        "",
        "	for (i = 0; i < MAX_VA_SIZE_PAGES; i++) {",
        "		LIST_HEAD(tmp_list);",
        "",
        "		if (list_empty(&vn->pool[i].head))",
        "			continue;",
        "",
        "		/* Detach the pool, so no-one can access it. */",
        "		spin_lock(&vn->pool_lock);",
        "		list_replace_init(&vn->pool[i].head, &tmp_list);",
        "		spin_unlock(&vn->pool_lock);",
        "",
        "		if (full_decay)",
        "			WRITE_ONCE(vn->pool[i].len, 0);",
        "",
        "		/* Decay a pool by ~25% out of left objects. */",
        "		n_decay = vn->pool[i].len >> 2;",
        "",
        "		list_for_each_entry_safe(va, nva, &tmp_list, list) {",
        "			list_del_init(&va->list);",
        "			merge_or_add_vmap_area(va, &decay_root, &decay_list);",
        "",
        "			if (!full_decay) {",
        "				WRITE_ONCE(vn->pool[i].len, vn->pool[i].len - 1);",
        "",
        "				if (!--n_decay)",
        "					break;",
        "			}",
        "		}",
        "",
        "		/*",
        "		 * Attach the pool back if it has been partly decayed.",
        "		 * Please note, it is supposed that nobody(other contexts)",
        "		 * can populate the pool therefore a simple list replace",
        "		 * operation takes place here.",
        "		 */",
        "		if (!full_decay && !list_empty(&tmp_list)) {",
        "			spin_lock(&vn->pool_lock);",
        "			list_replace_init(&tmp_list, &vn->pool[i].head);",
        "			spin_unlock(&vn->pool_lock);",
        "		}",
        "	}",
        "",
        "	reclaim_list_global(&decay_list);",
        "}",
        "",
        "static void",
        "kasan_release_vmalloc_node(struct vmap_node *vn)",
        "{",
        "	struct vmap_area *va;",
        "	unsigned long start, end;",
        "",
        "	start = list_first_entry(&vn->purge_list, struct vmap_area, list)->va_start;",
        "	end = list_last_entry(&vn->purge_list, struct vmap_area, list)->va_end;",
        "",
        "	list_for_each_entry(va, &vn->purge_list, list) {",
        "		if (is_vmalloc_or_module_addr((void *) va->va_start))",
        "			kasan_release_vmalloc(va->va_start, va->va_end,",
        "				va->va_start, va->va_end,",
        "				KASAN_VMALLOC_PAGE_RANGE);",
        "	}",
        "",
        "	kasan_release_vmalloc(start, end, start, end, KASAN_VMALLOC_TLB_FLUSH);",
        "}",
        "",
        "static void purge_vmap_node(struct work_struct *work)",
        "{",
        "	struct vmap_node *vn = container_of(work,",
        "		struct vmap_node, purge_work);",
        "	unsigned long nr_purged_pages = 0;",
        "	struct vmap_area *va, *n_va;",
        "	LIST_HEAD(local_list);",
        "",
        "	if (IS_ENABLED(CONFIG_KASAN_VMALLOC))",
        "		kasan_release_vmalloc_node(vn);",
        "",
        "	vn->nr_purged = 0;",
        "",
        "	list_for_each_entry_safe(va, n_va, &vn->purge_list, list) {",
        "		unsigned long nr = va_size(va) >> PAGE_SHIFT;",
        "		unsigned int vn_id = decode_vn_id(va->flags);",
        "",
        "		list_del_init(&va->list);",
        "",
        "		nr_purged_pages += nr;",
        "		vn->nr_purged++;",
        "",
        "		if (is_vn_id_valid(vn_id) && !vn->skip_populate)",
        "			if (node_pool_add_va(vn, va))",
        "				continue;",
        "",
        "		/* Go back to global. */",
        "		list_add(&va->list, &local_list);",
        "	}",
        "",
        "	atomic_long_sub(nr_purged_pages, &vmap_lazy_nr);",
        "",
        "	reclaim_list_global(&local_list);",
        "}",
        "",
        "/*",
        " * Purges all lazily-freed vmap areas.",
        " */",
        "static bool __purge_vmap_area_lazy(unsigned long start, unsigned long end,",
        "		bool full_pool_decay)",
        "{",
        "	unsigned long nr_purged_areas = 0;",
        "	unsigned int nr_purge_helpers;",
        "	unsigned int nr_purge_nodes;",
        "	struct vmap_node *vn;",
        "	int i;",
        "",
        "	lockdep_assert_held(&vmap_purge_lock);",
        "",
        "	/*",
        "	 * Use cpumask to mark which node has to be processed.",
        "	 */",
        "	purge_nodes = CPU_MASK_NONE;",
        "",
        "	for (i = 0; i < nr_vmap_nodes; i++) {",
        "		vn = &vmap_nodes[i];",
        "",
        "		INIT_LIST_HEAD(&vn->purge_list);",
        "		vn->skip_populate = full_pool_decay;",
        "		decay_va_pool_node(vn, full_pool_decay);",
        "",
        "		if (RB_EMPTY_ROOT(&vn->lazy.root))",
        "			continue;",
        "",
        "		spin_lock(&vn->lazy.lock);",
        "		WRITE_ONCE(vn->lazy.root.rb_node, NULL);",
        "		list_replace_init(&vn->lazy.head, &vn->purge_list);",
        "		spin_unlock(&vn->lazy.lock);",
        "",
        "		start = min(start, list_first_entry(&vn->purge_list,",
        "			struct vmap_area, list)->va_start);",
        "",
        "		end = max(end, list_last_entry(&vn->purge_list,",
        "			struct vmap_area, list)->va_end);",
        "",
        "		cpumask_set_cpu(i, &purge_nodes);",
        "	}",
        "",
        "	nr_purge_nodes = cpumask_weight(&purge_nodes);",
        "	if (nr_purge_nodes > 0) {",
        "		flush_tlb_kernel_range(start, end);",
        "",
        "		/* One extra worker is per a lazy_max_pages() full set minus one. */",
        "		nr_purge_helpers = atomic_long_read(&vmap_lazy_nr) / lazy_max_pages();",
        "		nr_purge_helpers = clamp(nr_purge_helpers, 1U, nr_purge_nodes) - 1;",
        "",
        "		for_each_cpu(i, &purge_nodes) {",
        "			vn = &vmap_nodes[i];",
        "",
        "			if (nr_purge_helpers > 0) {",
        "				INIT_WORK(&vn->purge_work, purge_vmap_node);",
        "",
        "				if (cpumask_test_cpu(i, cpu_online_mask))",
        "					schedule_work_on(i, &vn->purge_work);",
        "				else",
        "					schedule_work(&vn->purge_work);",
        "",
        "				nr_purge_helpers--;",
        "			} else {",
        "				vn->purge_work.func = NULL;",
        "				purge_vmap_node(&vn->purge_work);",
        "				nr_purged_areas += vn->nr_purged;",
        "			}",
        "		}",
        "",
        "		for_each_cpu(i, &purge_nodes) {",
        "			vn = &vmap_nodes[i];",
        "",
        "			if (vn->purge_work.func) {",
        "				flush_work(&vn->purge_work);",
        "				nr_purged_areas += vn->nr_purged;",
        "			}",
        "		}",
        "	}",
        "",
        "	trace_purge_vmap_area_lazy(start, end, nr_purged_areas);",
        "	return nr_purged_areas > 0;",
        "}",
        "",
        "/*",
        " * Reclaim vmap areas by purging fragmented blocks and purge_vmap_area_list.",
        " */",
        "static void reclaim_and_purge_vmap_areas(void)",
        "",
        "{",
        "	mutex_lock(&vmap_purge_lock);",
        "	purge_fragmented_blocks_allcpus();",
        "	__purge_vmap_area_lazy(ULONG_MAX, 0, true);",
        "	mutex_unlock(&vmap_purge_lock);",
        "}",
        "",
        "static void drain_vmap_area_work(struct work_struct *work)",
        "{",
        "	mutex_lock(&vmap_purge_lock);",
        "	__purge_vmap_area_lazy(ULONG_MAX, 0, false);",
        "	mutex_unlock(&vmap_purge_lock);",
        "}",
        "",
        "/*",
        " * Free a vmap area, caller ensuring that the area has been unmapped,",
        " * unlinked and flush_cache_vunmap had been called for the correct",
        " * range previously.",
        " */",
        "static void free_vmap_area_noflush(struct vmap_area *va)",
        "{",
        "	unsigned long nr_lazy_max = lazy_max_pages();",
        "	unsigned long va_start = va->va_start;",
        "	unsigned int vn_id = decode_vn_id(va->flags);",
        "	struct vmap_node *vn;",
        "	unsigned long nr_lazy;",
        "",
        "	if (WARN_ON_ONCE(!list_empty(&va->list)))",
        "		return;",
        "",
        "	nr_lazy = atomic_long_add_return(va_size(va) >> PAGE_SHIFT,",
        "					 &vmap_lazy_nr);",
        "",
        "	/*",
        "	 * If it was request by a certain node we would like to",
        "	 * return it to that node, i.e. its pool for later reuse.",
        "	 */",
        "	vn = is_vn_id_valid(vn_id) ?",
        "		id_to_node(vn_id):addr_to_node(va->va_start);",
        "",
        "	spin_lock(&vn->lazy.lock);",
        "	insert_vmap_area(va, &vn->lazy.root, &vn->lazy.head);",
        "	spin_unlock(&vn->lazy.lock);",
        "",
        "	trace_free_vmap_area_noflush(va_start, nr_lazy, nr_lazy_max);",
        "",
        "	/* After this point, we may free va at any time */",
        "	if (unlikely(nr_lazy > nr_lazy_max))",
        "		schedule_work(&drain_vmap_work);",
        "}",
        "",
        "/*",
        " * Free and unmap a vmap area",
        " */",
        "static void free_unmap_vmap_area(struct vmap_area *va)",
        "{",
        "	flush_cache_vunmap(va->va_start, va->va_end);",
        "	vunmap_range_noflush(va->va_start, va->va_end);",
        "	if (debug_pagealloc_enabled_static())",
        "		flush_tlb_kernel_range(va->va_start, va->va_end);",
        "",
        "	free_vmap_area_noflush(va);",
        "}",
        "",
        "struct vmap_area *find_vmap_area(unsigned long addr)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	int i, j;",
        "",
        "	if (unlikely(!vmap_initialized))",
        "		return NULL;",
        "",
        "	/*",
        "	 * An addr_to_node_id(addr) converts an address to a node index",
        "	 * where a VA is located. If VA spans several zones and passed",
        "	 * addr is not the same as va->va_start, what is not common, we",
        "	 * may need to scan extra nodes. See an example:",
        "	 *",
        "	 *      <----va---->",
        "	 * -|-----|-----|-----|-----|-",
        "	 *     1     2     0     1",
        "	 *",
        "	 * VA resides in node 1 whereas it spans 1, 2 an 0. If passed",
        "	 * addr is within 2 or 0 nodes we should do extra work.",
        "	 */",
        "	i = j = addr_to_node_id(addr);",
        "	do {",
        "		vn = &vmap_nodes[i];",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		va = __find_vmap_area(addr, &vn->busy.root);",
        "		spin_unlock(&vn->busy.lock);",
        "",
        "		if (va)",
        "			return va;",
        "	} while ((i = (i + 1) % nr_vmap_nodes) != j);",
        "",
        "	return NULL;",
        "}",
        "",
        "static struct vmap_area *find_unlink_vmap_area(unsigned long addr)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	int i, j;",
        "",
        "	/*",
        "	 * Check the comment in the find_vmap_area() about the loop.",
        "	 */",
        "	i = j = addr_to_node_id(addr);",
        "	do {",
        "		vn = &vmap_nodes[i];",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		va = __find_vmap_area(addr, &vn->busy.root);",
        "		if (va)",
        "			unlink_va(va, &vn->busy.root);",
        "		spin_unlock(&vn->busy.lock);",
        "",
        "		if (va)",
        "			return va;",
        "	} while ((i = (i + 1) % nr_vmap_nodes) != j);",
        "",
        "	return NULL;",
        "}",
        "",
        "/*** Per cpu kva allocator ***/",
        "",
        "/*",
        " * vmap space is limited especially on 32 bit architectures. Ensure there is",
        " * room for at least 16 percpu vmap blocks per CPU.",
        " */",
        "/*",
        " * If we had a constant VMALLOC_START and VMALLOC_END, we'd like to be able",
        " * to #define VMALLOC_SPACE		(VMALLOC_END-VMALLOC_START). Guess",
        " * instead (we just need a rough idea)",
        " */",
        "#if BITS_PER_LONG == 32",
        "#define VMALLOC_SPACE		(128UL*1024*1024)",
        "#else",
        "#define VMALLOC_SPACE		(128UL*1024*1024*1024)",
        "#endif",
        "",
        "#define VMALLOC_PAGES		(VMALLOC_SPACE / PAGE_SIZE)",
        "#define VMAP_MAX_ALLOC		BITS_PER_LONG	/* 256K with 4K pages */",
        "#define VMAP_BBMAP_BITS_MAX	1024	/* 4MB with 4K pages */",
        "#define VMAP_BBMAP_BITS_MIN	(VMAP_MAX_ALLOC*2)",
        "#define VMAP_MIN(x, y)		((x) < (y) ? (x) : (y)) /* can't use min() */",
        "#define VMAP_MAX(x, y)		((x) > (y) ? (x) : (y)) /* can't use max() */",
        "#define VMAP_BBMAP_BITS		\\",
        "		VMAP_MIN(VMAP_BBMAP_BITS_MAX,	\\",
        "		VMAP_MAX(VMAP_BBMAP_BITS_MIN,	\\",
        "			VMALLOC_PAGES / roundup_pow_of_two(NR_CPUS) / 16))",
        "",
        "#define VMAP_BLOCK_SIZE		(VMAP_BBMAP_BITS * PAGE_SIZE)",
        "",
        "/*",
        " * Purge threshold to prevent overeager purging of fragmented blocks for",
        " * regular operations: Purge if vb->free is less than 1/4 of the capacity.",
        " */",
        "#define VMAP_PURGE_THRESHOLD	(VMAP_BBMAP_BITS / 4)",
        "",
        "#define VMAP_RAM		0x1 /* indicates vm_map_ram area*/",
        "#define VMAP_BLOCK		0x2 /* mark out the vmap_block sub-type*/",
        "#define VMAP_FLAGS_MASK		0x3",
        "",
        "struct vmap_block_queue {",
        "	spinlock_t lock;",
        "	struct list_head free;",
        "",
        "	/*",
        "	 * An xarray requires an extra memory dynamically to",
        "	 * be allocated. If it is an issue, we can use rb-tree",
        "	 * instead.",
        "	 */",
        "	struct xarray vmap_blocks;",
        "};",
        "",
        "struct vmap_block {",
        "	spinlock_t lock;",
        "	struct vmap_area *va;",
        "	unsigned long free, dirty;",
        "	DECLARE_BITMAP(used_map, VMAP_BBMAP_BITS);",
        "	unsigned long dirty_min, dirty_max; /*< dirty range */",
        "	struct list_head free_list;",
        "	struct rcu_head rcu_head;",
        "	struct list_head purge;",
        "	unsigned int cpu;",
        "};",
        "",
        "/* Queue of free and dirty vmap blocks, for allocation and flushing purposes */",
        "static DEFINE_PER_CPU(struct vmap_block_queue, vmap_block_queue);",
        "",
        "/*",
        " * In order to fast access to any \"vmap_block\" associated with a",
        " * specific address, we use a hash.",
        " *",
        " * A per-cpu vmap_block_queue is used in both ways, to serialize",
        " * an access to free block chains among CPUs(alloc path) and it",
        " * also acts as a vmap_block hash(alloc/free paths). It means we",
        " * overload it, since we already have the per-cpu array which is",
        " * used as a hash table. When used as a hash a 'cpu' passed to",
        " * per_cpu() is not actually a CPU but rather a hash index.",
        " *",
        " * A hash function is addr_to_vb_xa() which hashes any address",
        " * to a specific index(in a hash) it belongs to. This then uses a",
        " * per_cpu() macro to access an array with generated index.",
        " *",
        " * An example:",
        " *",
        " *  CPU_1  CPU_2  CPU_0",
        " *    |      |      |",
        " *    V      V      V",
        " * 0     10     20     30     40     50     60",
        " * |------|------|------|------|------|------|...<vmap address space>",
        " *   CPU0   CPU1   CPU2   CPU0   CPU1   CPU2",
        " *",
        " * - CPU_1 invokes vm_unmap_ram(6), 6 belongs to CPU0 zone, thus",
        " *   it access: CPU0/INDEX0 -> vmap_blocks -> xa_lock;",
        " *",
        " * - CPU_2 invokes vm_unmap_ram(11), 11 belongs to CPU1 zone, thus",
        " *   it access: CPU1/INDEX1 -> vmap_blocks -> xa_lock;",
        " *",
        " * - CPU_0 invokes vm_unmap_ram(20), 20 belongs to CPU2 zone, thus",
        " *   it access: CPU2/INDEX2 -> vmap_blocks -> xa_lock.",
        " *",
        " * This technique almost always avoids lock contention on insert/remove,",
        " * however xarray spinlocks protect against any contention that remains.",
        " */",
        "static struct xarray *",
        "addr_to_vb_xa(unsigned long addr)",
        "{",
        "	int index = (addr / VMAP_BLOCK_SIZE) % nr_cpu_ids;",
        "",
        "	/*",
        "	 * Please note, nr_cpu_ids points on a highest set",
        "	 * possible bit, i.e. we never invoke cpumask_next()",
        "	 * if an index points on it which is nr_cpu_ids - 1.",
        "	 */",
        "	if (!cpu_possible(index))",
        "		index = cpumask_next(index, cpu_possible_mask);",
        "",
        "	return &per_cpu(vmap_block_queue, index).vmap_blocks;",
        "}",
        "",
        "/*",
        " * We should probably have a fallback mechanism to allocate virtual memory",
        " * out of partially filled vmap blocks. However vmap block sizing should be",
        " * fairly reasonable according to the vmalloc size, so it shouldn't be a",
        " * big problem.",
        " */",
        "",
        "static unsigned long addr_to_vb_idx(unsigned long addr)",
        "{",
        "	addr -= VMALLOC_START & ~(VMAP_BLOCK_SIZE-1);",
        "	addr /= VMAP_BLOCK_SIZE;",
        "	return addr;",
        "}",
        "",
        "static void *vmap_block_vaddr(unsigned long va_start, unsigned long pages_off)",
        "{",
        "	unsigned long addr;",
        "",
        "	addr = va_start + (pages_off << PAGE_SHIFT);",
        "	BUG_ON(addr_to_vb_idx(addr) != addr_to_vb_idx(va_start));",
        "	return (void *)addr;",
        "}",
        "",
        "/**",
        " * new_vmap_block - allocates new vmap_block and occupies 2^order pages in this",
        " *                  block. Of course pages number can't exceed VMAP_BBMAP_BITS",
        " * @order:    how many 2^order pages should be occupied in newly allocated block",
        " * @gfp_mask: flags for the page level allocator",
        " *",
        " * Return: virtual address in a newly allocated block or ERR_PTR(-errno)",
        " */",
        "static void *new_vmap_block(unsigned int order, gfp_t gfp_mask)",
        "{",
        "	struct vmap_block_queue *vbq;",
        "	struct vmap_block *vb;",
        "	struct vmap_area *va;",
        "	struct xarray *xa;",
        "	unsigned long vb_idx;",
        "	int node, err;",
        "	void *vaddr;",
        "",
        "	node = numa_node_id();",
        "",
        "	vb = kmalloc_node(sizeof(struct vmap_block),",
        "			gfp_mask & GFP_RECLAIM_MASK, node);",
        "	if (unlikely(!vb))",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	va = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE,",
        "					VMALLOC_START, VMALLOC_END,",
        "					node, gfp_mask,",
        "					VMAP_RAM|VMAP_BLOCK, NULL);",
        "	if (IS_ERR(va)) {",
        "		kfree(vb);",
        "		return ERR_CAST(va);",
        "	}",
        "",
        "	vaddr = vmap_block_vaddr(va->va_start, 0);",
        "	spin_lock_init(&vb->lock);",
        "	vb->va = va;",
        "	/* At least something should be left free */",
        "	BUG_ON(VMAP_BBMAP_BITS <= (1UL << order));",
        "	bitmap_zero(vb->used_map, VMAP_BBMAP_BITS);",
        "	vb->free = VMAP_BBMAP_BITS - (1UL << order);",
        "	vb->dirty = 0;",
        "	vb->dirty_min = VMAP_BBMAP_BITS;",
        "	vb->dirty_max = 0;",
        "	bitmap_set(vb->used_map, 0, (1UL << order));",
        "	INIT_LIST_HEAD(&vb->free_list);",
        "	vb->cpu = raw_smp_processor_id();",
        "",
        "	xa = addr_to_vb_xa(va->va_start);",
        "	vb_idx = addr_to_vb_idx(va->va_start);",
        "	err = xa_insert(xa, vb_idx, vb, gfp_mask);",
        "	if (err) {",
        "		kfree(vb);",
        "		free_vmap_area(va);",
        "		return ERR_PTR(err);",
        "	}",
        "	/*",
        "	 * list_add_tail_rcu could happened in another core",
        "	 * rather than vb->cpu due to task migration, which",
        "	 * is safe as list_add_tail_rcu will ensure the list's",
        "	 * integrity together with list_for_each_rcu from read",
        "	 * side.",
        "	 */",
        "	vbq = per_cpu_ptr(&vmap_block_queue, vb->cpu);",
        "	spin_lock(&vbq->lock);",
        "	list_add_tail_rcu(&vb->free_list, &vbq->free);",
        "	spin_unlock(&vbq->lock);",
        "",
        "	return vaddr;",
        "}",
        "",
        "static void free_vmap_block(struct vmap_block *vb)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_block *tmp;",
        "	struct xarray *xa;",
        "",
        "	xa = addr_to_vb_xa(vb->va->va_start);",
        "	tmp = xa_erase(xa, addr_to_vb_idx(vb->va->va_start));",
        "	BUG_ON(tmp != vb);",
        "",
        "	vn = addr_to_node(vb->va->va_start);",
        "	spin_lock(&vn->busy.lock);",
        "	unlink_va(vb->va, &vn->busy.root);",
        "	spin_unlock(&vn->busy.lock);",
        "",
        "	free_vmap_area_noflush(vb->va);",
        "	kfree_rcu(vb, rcu_head);",
        "}",
        "",
        "static bool purge_fragmented_block(struct vmap_block *vb,",
        "		struct list_head *purge_list, bool force_purge)",
        "{",
        "	struct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, vb->cpu);",
        "",
        "	if (vb->free + vb->dirty != VMAP_BBMAP_BITS ||",
        "	    vb->dirty == VMAP_BBMAP_BITS)",
        "		return false;",
        "",
        "	/* Don't overeagerly purge usable blocks unless requested */",
        "	if (!(force_purge || vb->free < VMAP_PURGE_THRESHOLD))",
        "		return false;",
        "",
        "	/* prevent further allocs after releasing lock */",
        "	WRITE_ONCE(vb->free, 0);",
        "	/* prevent purging it again */",
        "	WRITE_ONCE(vb->dirty, VMAP_BBMAP_BITS);",
        "	vb->dirty_min = 0;",
        "	vb->dirty_max = VMAP_BBMAP_BITS;",
        "	spin_lock(&vbq->lock);",
        "	list_del_rcu(&vb->free_list);",
        "	spin_unlock(&vbq->lock);",
        "	list_add_tail(&vb->purge, purge_list);",
        "	return true;",
        "}",
        "",
        "static void free_purged_blocks(struct list_head *purge_list)",
        "{",
        "	struct vmap_block *vb, *n_vb;",
        "",
        "	list_for_each_entry_safe(vb, n_vb, purge_list, purge) {",
        "		list_del(&vb->purge);",
        "		free_vmap_block(vb);",
        "	}",
        "}",
        "",
        "static void purge_fragmented_blocks(int cpu)",
        "{",
        "	LIST_HEAD(purge);",
        "	struct vmap_block *vb;",
        "	struct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);",
        "",
        "	rcu_read_lock();",
        "	list_for_each_entry_rcu(vb, &vbq->free, free_list) {",
        "		unsigned long free = READ_ONCE(vb->free);",
        "		unsigned long dirty = READ_ONCE(vb->dirty);",
        "",
        "		if (free + dirty != VMAP_BBMAP_BITS ||",
        "		    dirty == VMAP_BBMAP_BITS)",
        "			continue;",
        "",
        "		spin_lock(&vb->lock);",
        "		purge_fragmented_block(vb, &purge, true);",
        "		spin_unlock(&vb->lock);",
        "	}",
        "	rcu_read_unlock();",
        "	free_purged_blocks(&purge);",
        "}",
        "",
        "static void purge_fragmented_blocks_allcpus(void)",
        "{",
        "	int cpu;",
        "",
        "	for_each_possible_cpu(cpu)",
        "		purge_fragmented_blocks(cpu);",
        "}",
        "",
        "static void *vb_alloc(unsigned long size, gfp_t gfp_mask)",
        "{",
        "	struct vmap_block_queue *vbq;",
        "	struct vmap_block *vb;",
        "	void *vaddr = NULL;",
        "	unsigned int order;",
        "",
        "	BUG_ON(offset_in_page(size));",
        "	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);",
        "	if (WARN_ON(size == 0)) {",
        "		/*",
        "		 * Allocating 0 bytes isn't what caller wants since",
        "		 * get_order(0) returns funny result. Just warn and terminate",
        "		 * early.",
        "		 */",
        "		return ERR_PTR(-EINVAL);",
        "	}",
        "	order = get_order(size);",
        "",
        "	rcu_read_lock();",
        "	vbq = raw_cpu_ptr(&vmap_block_queue);",
        "	list_for_each_entry_rcu(vb, &vbq->free, free_list) {",
        "		unsigned long pages_off;",
        "",
        "		if (READ_ONCE(vb->free) < (1UL << order))",
        "			continue;",
        "",
        "		spin_lock(&vb->lock);",
        "		if (vb->free < (1UL << order)) {",
        "			spin_unlock(&vb->lock);",
        "			continue;",
        "		}",
        "",
        "		pages_off = VMAP_BBMAP_BITS - vb->free;",
        "		vaddr = vmap_block_vaddr(vb->va->va_start, pages_off);",
        "		WRITE_ONCE(vb->free, vb->free - (1UL << order));",
        "		bitmap_set(vb->used_map, pages_off, (1UL << order));",
        "		if (vb->free == 0) {",
        "			spin_lock(&vbq->lock);",
        "			list_del_rcu(&vb->free_list);",
        "			spin_unlock(&vbq->lock);",
        "		}",
        "",
        "		spin_unlock(&vb->lock);",
        "		break;",
        "	}",
        "",
        "	rcu_read_unlock();",
        "",
        "	/* Allocate new block if nothing was found */",
        "	if (!vaddr)",
        "		vaddr = new_vmap_block(order, gfp_mask);",
        "",
        "	return vaddr;",
        "}",
        "",
        "static void vb_free(unsigned long addr, unsigned long size)",
        "{",
        "	unsigned long offset;",
        "	unsigned int order;",
        "	struct vmap_block *vb;",
        "	struct xarray *xa;",
        "",
        "	BUG_ON(offset_in_page(size));",
        "	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);",
        "",
        "	flush_cache_vunmap(addr, addr + size);",
        "",
        "	order = get_order(size);",
        "	offset = (addr & (VMAP_BLOCK_SIZE - 1)) >> PAGE_SHIFT;",
        "",
        "	xa = addr_to_vb_xa(addr);",
        "	vb = xa_load(xa, addr_to_vb_idx(addr));",
        "",
        "	spin_lock(&vb->lock);",
        "	bitmap_clear(vb->used_map, offset, (1UL << order));",
        "	spin_unlock(&vb->lock);",
        "",
        "	vunmap_range_noflush(addr, addr + size);",
        "",
        "	if (debug_pagealloc_enabled_static())",
        "		flush_tlb_kernel_range(addr, addr + size);",
        "",
        "	spin_lock(&vb->lock);",
        "",
        "	/* Expand the not yet TLB flushed dirty range */",
        "	vb->dirty_min = min(vb->dirty_min, offset);",
        "	vb->dirty_max = max(vb->dirty_max, offset + (1UL << order));",
        "",
        "	WRITE_ONCE(vb->dirty, vb->dirty + (1UL << order));",
        "	if (vb->dirty == VMAP_BBMAP_BITS) {",
        "		BUG_ON(vb->free);",
        "		spin_unlock(&vb->lock);",
        "		free_vmap_block(vb);",
        "	} else",
        "		spin_unlock(&vb->lock);",
        "}",
        "",
        "static void _vm_unmap_aliases(unsigned long start, unsigned long end, int flush)",
        "{",
        "	LIST_HEAD(purge_list);",
        "	int cpu;",
        "",
        "	if (unlikely(!vmap_initialized))",
        "		return;",
        "",
        "	mutex_lock(&vmap_purge_lock);",
        "",
        "	for_each_possible_cpu(cpu) {",
        "		struct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);",
        "		struct vmap_block *vb;",
        "		unsigned long idx;",
        "",
        "		rcu_read_lock();",
        "		xa_for_each(&vbq->vmap_blocks, idx, vb) {",
        "			spin_lock(&vb->lock);",
        "",
        "			/*",
        "			 * Try to purge a fragmented block first. If it's",
        "			 * not purgeable, check whether there is dirty",
        "			 * space to be flushed.",
        "			 */",
        "			if (!purge_fragmented_block(vb, &purge_list, false) &&",
        "			    vb->dirty_max && vb->dirty != VMAP_BBMAP_BITS) {",
        "				unsigned long va_start = vb->va->va_start;",
        "				unsigned long s, e;",
        "",
        "				s = va_start + (vb->dirty_min << PAGE_SHIFT);",
        "				e = va_start + (vb->dirty_max << PAGE_SHIFT);",
        "",
        "				start = min(s, start);",
        "				end   = max(e, end);",
        "",
        "				/* Prevent that this is flushed again */",
        "				vb->dirty_min = VMAP_BBMAP_BITS;",
        "				vb->dirty_max = 0;",
        "",
        "				flush = 1;",
        "			}",
        "			spin_unlock(&vb->lock);",
        "		}",
        "		rcu_read_unlock();",
        "	}",
        "	free_purged_blocks(&purge_list);",
        "",
        "	if (!__purge_vmap_area_lazy(start, end, false) && flush)",
        "		flush_tlb_kernel_range(start, end);",
        "	mutex_unlock(&vmap_purge_lock);",
        "}",
        "",
        "/**",
        " * vm_unmap_aliases - unmap outstanding lazy aliases in the vmap layer",
        " *",
        " * The vmap/vmalloc layer lazily flushes kernel virtual mappings primarily",
        " * to amortize TLB flushing overheads. What this means is that any page you",
        " * have now, may, in a former life, have been mapped into kernel virtual",
        " * address by the vmap layer and so there might be some CPUs with TLB entries",
        " * still referencing that page (additional to the regular 1:1 kernel mapping).",
        " *",
        " * vm_unmap_aliases flushes all such lazy mappings. After it returns, we can",
        " * be sure that none of the pages we have control over will have any aliases",
        " * from the vmap layer.",
        " */",
        "void vm_unmap_aliases(void)",
        "{",
        "	unsigned long start = ULONG_MAX, end = 0;",
        "	int flush = 0;",
        "",
        "	_vm_unmap_aliases(start, end, flush);",
        "}",
        "EXPORT_SYMBOL_GPL(vm_unmap_aliases);",
        "",
        "/**",
        " * vm_unmap_ram - unmap linear kernel address space set up by vm_map_ram",
        " * @mem: the pointer returned by vm_map_ram",
        " * @count: the count passed to that vm_map_ram call (cannot unmap partial)",
        " */",
        "void vm_unmap_ram(const void *mem, unsigned int count)",
        "{",
        "	unsigned long size = (unsigned long)count << PAGE_SHIFT;",
        "	unsigned long addr = (unsigned long)kasan_reset_tag(mem);",
        "	struct vmap_area *va;",
        "",
        "	might_sleep();",
        "	BUG_ON(!addr);",
        "	BUG_ON(addr < VMALLOC_START);",
        "	BUG_ON(addr > VMALLOC_END);",
        "	BUG_ON(!PAGE_ALIGNED(addr));",
        "",
        "	kasan_poison_vmalloc(mem, size);",
        "",
        "	if (likely(count <= VMAP_MAX_ALLOC)) {",
        "		debug_check_no_locks_freed(mem, size);",
        "		vb_free(addr, size);",
        "		return;",
        "	}",
        "",
        "	va = find_unlink_vmap_area(addr);",
        "	if (WARN_ON_ONCE(!va))",
        "		return;",
        "",
        "	debug_check_no_locks_freed((void *)va->va_start, va_size(va));",
        "	free_unmap_vmap_area(va);",
        "}",
        "EXPORT_SYMBOL(vm_unmap_ram);",
        "",
        "/**",
        " * vm_map_ram - map pages linearly into kernel virtual address (vmalloc space)",
        " * @pages: an array of pointers to the pages to be mapped",
        " * @count: number of pages",
        " * @node: prefer to allocate data structures on this node",
        " *",
        " * If you use this function for less than VMAP_MAX_ALLOC pages, it could be",
        " * faster than vmap so it's good.  But if you mix long-life and short-life",
        " * objects with vm_map_ram(), it could consume lots of address space through",
        " * fragmentation (especially on a 32bit machine).  You could see failures in",
        " * the end.  Please use this function for short-lived objects.",
        " *",
        " * Returns: a pointer to the address that has been mapped, or %NULL on failure",
        " */",
        "void *vm_map_ram(struct page **pages, unsigned int count, int node)",
        "{",
        "	unsigned long size = (unsigned long)count << PAGE_SHIFT;",
        "	unsigned long addr;",
        "	void *mem;",
        "",
        "	if (likely(count <= VMAP_MAX_ALLOC)) {",
        "		mem = vb_alloc(size, GFP_KERNEL);",
        "		if (IS_ERR(mem))",
        "			return NULL;",
        "		addr = (unsigned long)mem;",
        "	} else {",
        "		struct vmap_area *va;",
        "		va = alloc_vmap_area(size, PAGE_SIZE,",
        "				VMALLOC_START, VMALLOC_END,",
        "				node, GFP_KERNEL, VMAP_RAM,",
        "				NULL);",
        "		if (IS_ERR(va))",
        "			return NULL;",
        "",
        "		addr = va->va_start;",
        "		mem = (void *)addr;",
        "	}",
        "",
        "	if (vmap_pages_range(addr, addr + size, PAGE_KERNEL,",
        "				pages, PAGE_SHIFT) < 0) {",
        "		vm_unmap_ram(mem, count);",
        "		return NULL;",
        "	}",
        "",
        "	/*",
        "	 * Mark the pages as accessible, now that they are mapped.",
        "	 * With hardware tag-based KASAN, marking is skipped for",
        "	 * non-VM_ALLOC mappings, see __kasan_unpoison_vmalloc().",
        "	 */",
        "	mem = kasan_unpoison_vmalloc(mem, size, KASAN_VMALLOC_PROT_NORMAL);",
        "",
        "	return mem;",
        "}",
        "EXPORT_SYMBOL(vm_map_ram);",
        "",
        "static struct vm_struct *vmlist __initdata;",
        "",
        "static inline unsigned int vm_area_page_order(struct vm_struct *vm)",
        "{",
        "#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC",
        "	return vm->page_order;",
        "#else",
        "	return 0;",
        "#endif",
        "}",
        "",
        "unsigned int get_vm_area_page_order(struct vm_struct *vm)",
        "{",
        "	return vm_area_page_order(vm);",
        "}",
        "",
        "static inline void set_vm_area_page_order(struct vm_struct *vm, unsigned int order)",
        "{",
        "#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC",
        "	vm->page_order = order;",
        "#else",
        "	BUG_ON(order != 0);",
        "#endif",
        "}",
        "",
        "/**",
        " * vm_area_add_early - add vmap area early during boot",
        " * @vm: vm_struct to add",
        " *",
        " * This function is used to add fixed kernel vm area to vmlist before",
        " * vmalloc_init() is called.  @vm->addr, @vm->size, and @vm->flags",
        " * should contain proper values and the other fields should be zero.",
        " *",
        " * DO NOT USE THIS FUNCTION UNLESS YOU KNOW WHAT YOU'RE DOING.",
        " */",
        "void __init vm_area_add_early(struct vm_struct *vm)",
        "{",
        "	struct vm_struct *tmp, **p;",
        "",
        "	BUG_ON(vmap_initialized);",
        "	for (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {",
        "		if (tmp->addr >= vm->addr) {",
        "			BUG_ON(tmp->addr < vm->addr + vm->size);",
        "			break;",
        "		} else",
        "			BUG_ON(tmp->addr + tmp->size > vm->addr);",
        "	}",
        "	vm->next = *p;",
        "	*p = vm;",
        "}",
        "",
        "/**",
        " * vm_area_register_early - register vmap area early during boot",
        " * @vm: vm_struct to register",
        " * @align: requested alignment",
        " *",
        " * This function is used to register kernel vm area before",
        " * vmalloc_init() is called.  @vm->size and @vm->flags should contain",
        " * proper values on entry and other fields should be zero.  On return,",
        " * vm->addr contains the allocated address.",
        " *",
        " * DO NOT USE THIS FUNCTION UNLESS YOU KNOW WHAT YOU'RE DOING.",
        " */",
        "void __init vm_area_register_early(struct vm_struct *vm, size_t align)",
        "{",
        "	unsigned long addr = ALIGN(VMALLOC_START, align);",
        "	struct vm_struct *cur, **p;",
        "",
        "	BUG_ON(vmap_initialized);",
        "",
        "	for (p = &vmlist; (cur = *p) != NULL; p = &cur->next) {",
        "		if ((unsigned long)cur->addr - addr >= vm->size)",
        "			break;",
        "		addr = ALIGN((unsigned long)cur->addr + cur->size, align);",
        "	}",
        "",
        "	BUG_ON(addr > VMALLOC_END - vm->size);",
        "	vm->addr = (void *)addr;",
        "	vm->next = *p;",
        "	*p = vm;",
        "	kasan_populate_early_vm_area_shadow(vm->addr, vm->size);",
        "}",
        "",
        "static void clear_vm_uninitialized_flag(struct vm_struct *vm)",
        "{",
        "	/*",
        "	 * Before removing VM_UNINITIALIZED,",
        "	 * we should make sure that vm has proper values.",
        "	 * Pair with smp_rmb() in show_numa_info().",
        "	 */",
        "	smp_wmb();",
        "	vm->flags &= ~VM_UNINITIALIZED;",
        "}",
        "",
        "struct vm_struct *__get_vm_area_node(unsigned long size,",
        "		unsigned long align, unsigned long shift, unsigned long flags,",
        "		unsigned long start, unsigned long end, int node,",
        "		gfp_t gfp_mask, const void *caller)",
        "{",
        "	struct vmap_area *va;",
        "	struct vm_struct *area;",
        "	unsigned long requested_size = size;",
        "",
        "	BUG_ON(in_interrupt());",
        "	size = ALIGN(size, 1ul << shift);",
        "	if (unlikely(!size))",
        "		return NULL;",
        "",
        "	if (flags & VM_IOREMAP)",
        "		align = 1ul << clamp_t(int, get_count_order_long(size),",
        "				       PAGE_SHIFT, IOREMAP_MAX_ORDER);",
        "",
        "	area = kzalloc_node(sizeof(*area), gfp_mask & GFP_RECLAIM_MASK, node);",
        "	if (unlikely(!area))",
        "		return NULL;",
        "",
        "	if (!(flags & VM_NO_GUARD))",
        "		size += PAGE_SIZE;",
        "",
        "	area->flags = flags;",
        "	area->caller = caller;",
        "",
        "	va = alloc_vmap_area(size, align, start, end, node, gfp_mask, 0, area);",
        "	if (IS_ERR(va)) {",
        "		kfree(area);",
        "		return NULL;",
        "	}",
        "",
        "	/*",
        "	 * Mark pages for non-VM_ALLOC mappings as accessible. Do it now as a",
        "	 * best-effort approach, as they can be mapped outside of vmalloc code.",
        "	 * For VM_ALLOC mappings, the pages are marked as accessible after",
        "	 * getting mapped in __vmalloc_node_range().",
        "	 * With hardware tag-based KASAN, marking is skipped for",
        "	 * non-VM_ALLOC mappings, see __kasan_unpoison_vmalloc().",
        "	 */",
        "	if (!(flags & VM_ALLOC))",
        "		area->addr = kasan_unpoison_vmalloc(area->addr, requested_size,",
        "						    KASAN_VMALLOC_PROT_NORMAL);",
        "",
        "	return area;",
        "}",
        "",
        "struct vm_struct *__get_vm_area_caller(unsigned long size, unsigned long flags,",
        "				       unsigned long start, unsigned long end,",
        "				       const void *caller)",
        "{",
        "	return __get_vm_area_node(size, 1, PAGE_SHIFT, flags, start, end,",
        "				  NUMA_NO_NODE, GFP_KERNEL, caller);",
        "}",
        "",
        "/**",
        " * get_vm_area - reserve a contiguous kernel virtual area",
        " * @size:	 size of the area",
        " * @flags:	 %VM_IOREMAP for I/O mappings or VM_ALLOC",
        " *",
        " * Search an area of @size in the kernel virtual mapping area,",
        " * and reserved it for out purposes.  Returns the area descriptor",
        " * on success or %NULL on failure.",
        " *",
        " * Return: the area descriptor on success or %NULL on failure.",
        " */",
        "struct vm_struct *get_vm_area(unsigned long size, unsigned long flags)",
        "{",
        "	return __get_vm_area_node(size, 1, PAGE_SHIFT, flags,",
        "				  VMALLOC_START, VMALLOC_END,",
        "				  NUMA_NO_NODE, GFP_KERNEL,",
        "				  __builtin_return_address(0));",
        "}",
        "",
        "struct vm_struct *get_vm_area_caller(unsigned long size, unsigned long flags,",
        "				const void *caller)",
        "{",
        "	return __get_vm_area_node(size, 1, PAGE_SHIFT, flags,",
        "				  VMALLOC_START, VMALLOC_END,",
        "				  NUMA_NO_NODE, GFP_KERNEL, caller);",
        "}",
        "",
        "/**",
        " * find_vm_area - find a continuous kernel virtual area",
        " * @addr:	  base address",
        " *",
        " * Search for the kernel VM area starting at @addr, and return it.",
        " * It is up to the caller to do all required locking to keep the returned",
        " * pointer valid.",
        " *",
        " * Return: the area descriptor on success or %NULL on failure.",
        " */",
        "struct vm_struct *find_vm_area(const void *addr)",
        "{",
        "	struct vmap_area *va;",
        "",
        "	va = find_vmap_area((unsigned long)addr);",
        "	if (!va)",
        "		return NULL;",
        "",
        "	return va->vm;",
        "}",
        "",
        "/**",
        " * remove_vm_area - find and remove a continuous kernel virtual area",
        " * @addr:	    base address",
        " *",
        " * Search for the kernel VM area starting at @addr, and remove it.",
        " * This function returns the found VM area, but using it is NOT safe",
        " * on SMP machines, except for its size or flags.",
        " *",
        " * Return: the area descriptor on success or %NULL on failure.",
        " */",
        "struct vm_struct *remove_vm_area(const void *addr)",
        "{",
        "	struct vmap_area *va;",
        "	struct vm_struct *vm;",
        "",
        "	might_sleep();",
        "",
        "	if (WARN(!PAGE_ALIGNED(addr), \"Trying to vfree() bad address (%p)\\n\",",
        "			addr))",
        "		return NULL;",
        "",
        "	va = find_unlink_vmap_area((unsigned long)addr);",
        "	if (!va || !va->vm)",
        "		return NULL;",
        "	vm = va->vm;",
        "",
        "	debug_check_no_locks_freed(vm->addr, get_vm_area_size(vm));",
        "	debug_check_no_obj_freed(vm->addr, get_vm_area_size(vm));",
        "	kasan_free_module_shadow(vm);",
        "	kasan_poison_vmalloc(vm->addr, get_vm_area_size(vm));",
        "",
        "	free_unmap_vmap_area(va);",
        "	return vm;",
        "}",
        "",
        "static inline void set_area_direct_map(const struct vm_struct *area,",
        "				       int (*set_direct_map)(struct page *page))",
        "{",
        "	int i;",
        "",
        "	/* HUGE_VMALLOC passes small pages to set_direct_map */",
        "	for (i = 0; i < area->nr_pages; i++)",
        "		if (page_address(area->pages[i]))",
        "			set_direct_map(area->pages[i]);",
        "}",
        "",
        "/*",
        " * Flush the vm mapping and reset the direct map.",
        " */",
        "static void vm_reset_perms(struct vm_struct *area)",
        "{",
        "	unsigned long start = ULONG_MAX, end = 0;",
        "	unsigned int page_order = vm_area_page_order(area);",
        "	int flush_dmap = 0;",
        "	int i;",
        "",
        "	/*",
        "	 * Find the start and end range of the direct mappings to make sure that",
        "	 * the vm_unmap_aliases() flush includes the direct map.",
        "	 */",
        "	for (i = 0; i < area->nr_pages; i += 1U << page_order) {",
        "		unsigned long addr = (unsigned long)page_address(area->pages[i]);",
        "",
        "		if (addr) {",
        "			unsigned long page_size;",
        "",
        "			page_size = PAGE_SIZE << page_order;",
        "			start = min(addr, start);",
        "			end = max(addr + page_size, end);",
        "			flush_dmap = 1;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * Set direct map to something invalid so that it won't be cached if",
        "	 * there are any accesses after the TLB flush, then flush the TLB and",
        "	 * reset the direct map permissions to the default.",
        "	 */",
        "	set_area_direct_map(area, set_direct_map_invalid_noflush);",
        "	_vm_unmap_aliases(start, end, flush_dmap);",
        "	set_area_direct_map(area, set_direct_map_default_noflush);",
        "}",
        "",
        "static void delayed_vfree_work(struct work_struct *w)",
        "{",
        "	struct vfree_deferred *p = container_of(w, struct vfree_deferred, wq);",
        "	struct llist_node *t, *llnode;",
        "",
        "	llist_for_each_safe(llnode, t, llist_del_all(&p->list))",
        "		vfree(llnode);",
        "}",
        "",
        "/**",
        " * vfree_atomic - release memory allocated by vmalloc()",
        " * @addr:	  memory base address",
        " *",
        " * This one is just like vfree() but can be called in any atomic context",
        " * except NMIs.",
        " */",
        "void vfree_atomic(const void *addr)",
        "{",
        "	struct vfree_deferred *p = raw_cpu_ptr(&vfree_deferred);",
        "",
        "	BUG_ON(in_nmi());",
        "	kmemleak_free(addr);",
        "",
        "	/*",
        "	 * Use raw_cpu_ptr() because this can be called from preemptible",
        "	 * context. Preemption is absolutely fine here, because the llist_add()",
        "	 * implementation is lockless, so it works even if we are adding to",
        "	 * another cpu's list. schedule_work() should be fine with this too.",
        "	 */",
        "	if (addr && llist_add((struct llist_node *)addr, &p->list))",
        "		schedule_work(&p->wq);",
        "}",
        "",
        "/**",
        " * vfree - Release memory allocated by vmalloc()",
        " * @addr:  Memory base address",
        " *",
        " * Free the virtually continuous memory area starting at @addr, as obtained",
        " * from one of the vmalloc() family of APIs.  This will usually also free the",
        " * physical memory underlying the virtual allocation, but that memory is",
        " * reference counted, so it will not be freed until the last user goes away.",
        " *",
        " * If @addr is NULL, no operation is performed.",
        " *",
        " * Context:",
        " * May sleep if called *not* from interrupt context.",
        " * Must not be called in NMI context (strictly speaking, it could be",
        " * if we have CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG, but making the calling",
        " * conventions for vfree() arch-dependent would be a really bad idea).",
        " */",
        "void vfree(const void *addr)",
        "{",
        "	struct vm_struct *vm;",
        "	int i;",
        "",
        "	if (unlikely(in_interrupt())) {",
        "		vfree_atomic(addr);",
        "		return;",
        "	}",
        "",
        "	BUG_ON(in_nmi());",
        "	kmemleak_free(addr);",
        "	might_sleep();",
        "",
        "	if (!addr)",
        "		return;",
        "",
        "	vm = remove_vm_area(addr);",
        "	if (unlikely(!vm)) {",
        "		WARN(1, KERN_ERR \"Trying to vfree() nonexistent vm area (%p)\\n\",",
        "				addr);",
        "		return;",
        "	}",
        "",
        "	if (unlikely(vm->flags & VM_FLUSH_RESET_PERMS))",
        "		vm_reset_perms(vm);",
        "	for (i = 0; i < vm->nr_pages; i++) {",
        "		struct page *page = vm->pages[i];",
        "",
        "		BUG_ON(!page);",
        "		if (!(vm->flags & VM_MAP_PUT_PAGES))",
        "			mod_memcg_page_state(page, MEMCG_VMALLOC, -1);",
        "		/*",
        "		 * High-order allocs for huge vmallocs are split, so",
        "		 * can be freed as an array of order-0 allocations",
        "		 */",
        "		__free_page(page);",
        "		cond_resched();",
        "	}",
        "	if (!(vm->flags & VM_MAP_PUT_PAGES))",
        "		atomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);",
        "	kvfree(vm->pages);",
        "	kfree(vm);",
        "}",
        "EXPORT_SYMBOL(vfree);",
        "",
        "/**",
        " * vunmap - release virtual mapping obtained by vmap()",
        " * @addr:   memory base address",
        " *",
        " * Free the virtually contiguous memory area starting at @addr,",
        " * which was created from the page array passed to vmap().",
        " *",
        " * Must not be called in interrupt context.",
        " */",
        "void vunmap(const void *addr)",
        "{",
        "	struct vm_struct *vm;",
        "",
        "	BUG_ON(in_interrupt());",
        "	might_sleep();",
        "",
        "	if (!addr)",
        "		return;",
        "	vm = remove_vm_area(addr);",
        "	if (unlikely(!vm)) {",
        "		WARN(1, KERN_ERR \"Trying to vunmap() nonexistent vm area (%p)\\n\",",
        "				addr);",
        "		return;",
        "	}",
        "	kfree(vm);",
        "}",
        "EXPORT_SYMBOL(vunmap);",
        "",
        "/**",
        " * vmap - map an array of pages into virtually contiguous space",
        " * @pages: array of page pointers",
        " * @count: number of pages to map",
        " * @flags: vm_area->flags",
        " * @prot: page protection for the mapping",
        " *",
        " * Maps @count pages from @pages into contiguous kernel virtual space.",
        " * If @flags contains %VM_MAP_PUT_PAGES the ownership of the pages array itself",
        " * (which must be kmalloc or vmalloc memory) and one reference per pages in it",
        " * are transferred from the caller to vmap(), and will be freed / dropped when",
        " * vfree() is called on the return value.",
        " *",
        " * Return: the address of the area or %NULL on failure",
        " */",
        "void *vmap(struct page **pages, unsigned int count,",
        "	   unsigned long flags, pgprot_t prot)",
        "{",
        "	struct vm_struct *area;",
        "	unsigned long addr;",
        "	unsigned long size;		/* In bytes */",
        "",
        "	might_sleep();",
        "",
        "	if (WARN_ON_ONCE(flags & VM_FLUSH_RESET_PERMS))",
        "		return NULL;",
        "",
        "	/*",
        "	 * Your top guard is someone else's bottom guard. Not having a top",
        "	 * guard compromises someone else's mappings too.",
        "	 */",
        "	if (WARN_ON_ONCE(flags & VM_NO_GUARD))",
        "		flags &= ~VM_NO_GUARD;",
        "",
        "	if (count > totalram_pages())",
        "		return NULL;",
        "",
        "	size = (unsigned long)count << PAGE_SHIFT;",
        "	area = get_vm_area_caller(size, flags, __builtin_return_address(0));",
        "	if (!area)",
        "		return NULL;",
        "",
        "	addr = (unsigned long)area->addr;",
        "	if (vmap_pages_range(addr, addr + size, pgprot_nx(prot),",
        "				pages, PAGE_SHIFT) < 0) {",
        "		vunmap(area->addr);",
        "		return NULL;",
        "	}",
        "",
        "	if (flags & VM_MAP_PUT_PAGES) {",
        "		area->pages = pages;",
        "		area->nr_pages = count;",
        "	}",
        "	return area->addr;",
        "}",
        "EXPORT_SYMBOL(vmap);",
        "",
        "#ifdef CONFIG_VMAP_PFN",
        "struct vmap_pfn_data {",
        "	unsigned long	*pfns;",
        "	pgprot_t	prot;",
        "	unsigned int	idx;",
        "};",
        "",
        "static int vmap_pfn_apply(pte_t *pte, unsigned long addr, void *private)",
        "{",
        "	struct vmap_pfn_data *data = private;",
        "	unsigned long pfn = data->pfns[data->idx];",
        "	pte_t ptent;",
        "",
        "	if (WARN_ON_ONCE(pfn_valid(pfn)))",
        "		return -EINVAL;",
        "",
        "	ptent = pte_mkspecial(pfn_pte(pfn, data->prot));",
        "	set_pte_at(&init_mm, addr, pte, ptent);",
        "",
        "	data->idx++;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * vmap_pfn - map an array of PFNs into virtually contiguous space",
        " * @pfns: array of PFNs",
        " * @count: number of pages to map",
        " * @prot: page protection for the mapping",
        " *",
        " * Maps @count PFNs from @pfns into contiguous kernel virtual space and returns",
        " * the start address of the mapping.",
        " */",
        "void *vmap_pfn(unsigned long *pfns, unsigned int count, pgprot_t prot)",
        "{",
        "	struct vmap_pfn_data data = { .pfns = pfns, .prot = pgprot_nx(prot) };",
        "	struct vm_struct *area;",
        "",
        "	area = get_vm_area_caller(count * PAGE_SIZE, VM_IOREMAP,",
        "			__builtin_return_address(0));",
        "	if (!area)",
        "		return NULL;",
        "	if (apply_to_page_range(&init_mm, (unsigned long)area->addr,",
        "			count * PAGE_SIZE, vmap_pfn_apply, &data)) {",
        "		free_vm_area(area);",
        "		return NULL;",
        "	}",
        "",
        "	flush_cache_vmap((unsigned long)area->addr,",
        "			 (unsigned long)area->addr + count * PAGE_SIZE);",
        "",
        "	return area->addr;",
        "}",
        "EXPORT_SYMBOL_GPL(vmap_pfn);",
        "#endif /* CONFIG_VMAP_PFN */",
        "",
        "static inline unsigned int",
        "vm_area_alloc_pages(gfp_t gfp, int nid,",
        "		unsigned int order, unsigned int nr_pages, struct page **pages)",
        "{",
        "	unsigned int nr_allocated = 0;",
        "	struct page *page;",
        "	int i;",
        "",
        "	/*",
        "	 * For order-0 pages we make use of bulk allocator, if",
        "	 * the page array is partly or not at all populated due",
        "	 * to fails, fallback to a single page allocator that is",
        "	 * more permissive.",
        "	 */",
        "	if (!order) {",
        "		while (nr_allocated < nr_pages) {",
        "			unsigned int nr, nr_pages_request;",
        "",
        "			/*",
        "			 * A maximum allowed request is hard-coded and is 100",
        "			 * pages per call. That is done in order to prevent a",
        "			 * long preemption off scenario in the bulk-allocator",
        "			 * so the range is [1:100].",
        "			 */",
        "			nr_pages_request = min(100U, nr_pages - nr_allocated);",
        "",
        "			/* memory allocation should consider mempolicy, we can't",
        "			 * wrongly use nearest node when nid == NUMA_NO_NODE,",
        "			 * otherwise memory may be allocated in only one node,",
        "			 * but mempolicy wants to alloc memory by interleaving.",
        "			 */",
        "			if (IS_ENABLED(CONFIG_NUMA) && nid == NUMA_NO_NODE)",
        "				nr = alloc_pages_bulk_array_mempolicy_noprof(gfp,",
        "							nr_pages_request,",
        "							pages + nr_allocated);",
        "			else",
        "				nr = alloc_pages_bulk_array_node_noprof(gfp, nid,",
        "							nr_pages_request,",
        "							pages + nr_allocated);",
        "",
        "			nr_allocated += nr;",
        "			cond_resched();",
        "",
        "			/*",
        "			 * If zero or pages were obtained partly,",
        "			 * fallback to a single page allocator.",
        "			 */",
        "			if (nr != nr_pages_request)",
        "				break;",
        "		}",
        "	}",
        "",
        "	/* High-order pages or fallback path if \"bulk\" fails. */",
        "	while (nr_allocated < nr_pages) {",
        "		if (!(gfp & __GFP_NOFAIL) && fatal_signal_pending(current))",
        "			break;",
        "",
        "		if (nid == NUMA_NO_NODE)",
        "			page = alloc_pages_noprof(gfp, order);",
        "		else",
        "			page = alloc_pages_node_noprof(nid, gfp, order);",
        "",
        "		if (unlikely(!page))",
        "			break;",
        "",
        "		/*",
        "		 * High-order allocations must be able to be treated as",
        "		 * independent small pages by callers (as they can with",
        "		 * small-page vmallocs). Some drivers do their own refcounting",
        "		 * on vmalloc_to_page() pages, some use page->mapping,",
        "		 * page->lru, etc.",
        "		 */",
        "		if (order)",
        "			split_page(page, order);",
        "",
        "		/*",
        "		 * Careful, we allocate and map page-order pages, but",
        "		 * tracking is done per PAGE_SIZE page so as to keep the",
        "		 * vm_struct APIs independent of the physical/mapped size.",
        "		 */",
        "		for (i = 0; i < (1U << order); i++)",
        "			pages[nr_allocated + i] = page + i;",
        "",
        "		cond_resched();",
        "		nr_allocated += 1U << order;",
        "	}",
        "",
        "	return nr_allocated;",
        "}",
        "",
        "static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,",
        "				 pgprot_t prot, unsigned int page_shift,",
        "				 int node)",
        "{",
        "	const gfp_t nested_gfp = (gfp_mask & GFP_RECLAIM_MASK) | __GFP_ZERO;",
        "	bool nofail = gfp_mask & __GFP_NOFAIL;",
        "	unsigned long addr = (unsigned long)area->addr;",
        "	unsigned long size = get_vm_area_size(area);",
        "	unsigned long array_size;",
        "	unsigned int nr_small_pages = size >> PAGE_SHIFT;",
        "	unsigned int page_order;",
        "	unsigned int flags;",
        "	int ret;",
        "",
        "	array_size = (unsigned long)nr_small_pages * sizeof(struct page *);",
        "",
        "	if (!(gfp_mask & (GFP_DMA | GFP_DMA32)))",
        "		gfp_mask |= __GFP_HIGHMEM;",
        "",
        "	/* Please note that the recursion is strictly bounded. */",
        "	if (array_size > PAGE_SIZE) {",
        "		area->pages = __vmalloc_node_noprof(array_size, 1, nested_gfp, node,",
        "					area->caller);",
        "	} else {",
        "		area->pages = kmalloc_node_noprof(array_size, nested_gfp, node);",
        "	}",
        "",
        "	if (!area->pages) {",
        "		warn_alloc(gfp_mask, NULL,",
        "			\"vmalloc error: size %lu, failed to allocated page array size %lu\",",
        "			nr_small_pages * PAGE_SIZE, array_size);",
        "		free_vm_area(area);",
        "		return NULL;",
        "	}",
        "",
        "	set_vm_area_page_order(area, page_shift - PAGE_SHIFT);",
        "	page_order = vm_area_page_order(area);",
        "",
        "	/*",
        "	 * High-order nofail allocations are really expensive and",
        "	 * potentially dangerous (pre-mature OOM, disruptive reclaim",
        "	 * and compaction etc.",
        "	 *",
        "	 * Please note, the __vmalloc_node_range_noprof() falls-back",
        "	 * to order-0 pages if high-order attempt is unsuccessful.",
        "	 */",
        "	area->nr_pages = vm_area_alloc_pages((page_order ?",
        "		gfp_mask & ~__GFP_NOFAIL : gfp_mask) | __GFP_NOWARN,",
        "		node, page_order, nr_small_pages, area->pages);",
        "",
        "	atomic_long_add(area->nr_pages, &nr_vmalloc_pages);",
        "	if (gfp_mask & __GFP_ACCOUNT) {",
        "		int i;",
        "",
        "		for (i = 0; i < area->nr_pages; i++)",
        "			mod_memcg_page_state(area->pages[i], MEMCG_VMALLOC, 1);",
        "	}",
        "",
        "	/*",
        "	 * If not enough pages were obtained to accomplish an",
        "	 * allocation request, free them via vfree() if any.",
        "	 */",
        "	if (area->nr_pages != nr_small_pages) {",
        "		/*",
        "		 * vm_area_alloc_pages() can fail due to insufficient memory but",
        "		 * also:-",
        "		 *",
        "		 * - a pending fatal signal",
        "		 * - insufficient huge page-order pages",
        "		 *",
        "		 * Since we always retry allocations at order-0 in the huge page",
        "		 * case a warning for either is spurious.",
        "		 */",
        "		if (!fatal_signal_pending(current) && page_order == 0)",
        "			warn_alloc(gfp_mask, NULL,",
        "				\"vmalloc error: size %lu, failed to allocate pages\",",
        "				area->nr_pages * PAGE_SIZE);",
        "		goto fail;",
        "	}",
        "",
        "	/*",
        "	 * page tables allocations ignore external gfp mask, enforce it",
        "	 * by the scope API",
        "	 */",
        "	if ((gfp_mask & (__GFP_FS | __GFP_IO)) == __GFP_IO)",
        "		flags = memalloc_nofs_save();",
        "	else if ((gfp_mask & (__GFP_FS | __GFP_IO)) == 0)",
        "		flags = memalloc_noio_save();",
        "",
        "	do {",
        "		ret = vmap_pages_range(addr, addr + size, prot, area->pages,",
        "			page_shift);",
        "		if (nofail && (ret < 0))",
        "			schedule_timeout_uninterruptible(1);",
        "	} while (nofail && (ret < 0));",
        "",
        "	if ((gfp_mask & (__GFP_FS | __GFP_IO)) == __GFP_IO)",
        "		memalloc_nofs_restore(flags);",
        "	else if ((gfp_mask & (__GFP_FS | __GFP_IO)) == 0)",
        "		memalloc_noio_restore(flags);",
        "",
        "	if (ret < 0) {",
        "		warn_alloc(gfp_mask, NULL,",
        "			\"vmalloc error: size %lu, failed to map pages\",",
        "			area->nr_pages * PAGE_SIZE);",
        "		goto fail;",
        "	}",
        "",
        "	return area->addr;",
        "",
        "fail:",
        "	vfree(area->addr);",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * __vmalloc_node_range - allocate virtually contiguous memory",
        " * @size:		  allocation size",
        " * @align:		  desired alignment",
        " * @start:		  vm area range start",
        " * @end:		  vm area range end",
        " * @gfp_mask:		  flags for the page level allocator",
        " * @prot:		  protection mask for the allocated pages",
        " * @vm_flags:		  additional vm area flags (e.g. %VM_NO_GUARD)",
        " * @node:		  node to use for allocation or NUMA_NO_NODE",
        " * @caller:		  caller's return address",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator with @gfp_mask flags. Please note that the full set of gfp",
        " * flags are not supported. GFP_KERNEL, GFP_NOFS and GFP_NOIO are all",
        " * supported.",
        " * Zone modifiers are not supported. From the reclaim modifiers",
        " * __GFP_DIRECT_RECLAIM is required (aka GFP_NOWAIT is not supported)",
        " * and only __GFP_NOFAIL is supported (i.e. __GFP_NORETRY and",
        " * __GFP_RETRY_MAYFAIL are not supported).",
        " *",
        " * __GFP_NOWARN can be used to suppress failures messages.",
        " *",
        " * Map them into contiguous kernel virtual space, using a pagetable",
        " * protection of @prot.",
        " *",
        " * Return: the address of the area or %NULL on failure",
        " */",
        "void *__vmalloc_node_range_noprof(unsigned long size, unsigned long align,",
        "			unsigned long start, unsigned long end, gfp_t gfp_mask,",
        "			pgprot_t prot, unsigned long vm_flags, int node,",
        "			const void *caller)",
        "{",
        "	struct vm_struct *area;",
        "	void *ret;",
        "	kasan_vmalloc_flags_t kasan_flags = KASAN_VMALLOC_NONE;",
        "	unsigned long real_size = size;",
        "	unsigned long real_align = align;",
        "	unsigned int shift = PAGE_SHIFT;",
        "",
        "	if (WARN_ON_ONCE(!size))",
        "		return NULL;",
        "",
        "	if ((size >> PAGE_SHIFT) > totalram_pages()) {",
        "		warn_alloc(gfp_mask, NULL,",
        "			\"vmalloc error: size %lu, exceeds total pages\",",
        "			real_size);",
        "		return NULL;",
        "	}",
        "",
        "	if (vmap_allow_huge && (vm_flags & VM_ALLOW_HUGE_VMAP)) {",
        "		/*",
        "		 * Try huge pages. Only try for PAGE_KERNEL allocations,",
        "		 * others like modules don't yet expect huge pages in",
        "		 * their allocations due to apply_to_page_range not",
        "		 * supporting them.",
        "		 */",
        "",
        "		if (arch_vmap_pmd_supported(prot) && size >= PMD_SIZE)",
        "			shift = PMD_SHIFT;",
        "		else",
        "			shift = arch_vmap_pte_supported_shift(size);",
        "",
        "		align = max(real_align, 1UL << shift);",
        "		size = ALIGN(real_size, 1UL << shift);",
        "	}",
        "",
        "again:",
        "	area = __get_vm_area_node(real_size, align, shift, VM_ALLOC |",
        "				  VM_UNINITIALIZED | vm_flags, start, end, node,",
        "				  gfp_mask, caller);",
        "	if (!area) {",
        "		bool nofail = gfp_mask & __GFP_NOFAIL;",
        "		warn_alloc(gfp_mask, NULL,",
        "			\"vmalloc error: size %lu, vm_struct allocation failed%s\",",
        "			real_size, (nofail) ? \". Retrying.\" : \"\");",
        "		if (nofail) {",
        "			schedule_timeout_uninterruptible(1);",
        "			goto again;",
        "		}",
        "		goto fail;",
        "	}",
        "",
        "	/*",
        "	 * Prepare arguments for __vmalloc_area_node() and",
        "	 * kasan_unpoison_vmalloc().",
        "	 */",
        "	if (pgprot_val(prot) == pgprot_val(PAGE_KERNEL)) {",
        "		if (kasan_hw_tags_enabled()) {",
        "			/*",
        "			 * Modify protection bits to allow tagging.",
        "			 * This must be done before mapping.",
        "			 */",
        "			prot = arch_vmap_pgprot_tagged(prot);",
        "",
        "			/*",
        "			 * Skip page_alloc poisoning and zeroing for physical",
        "			 * pages backing VM_ALLOC mapping. Memory is instead",
        "			 * poisoned and zeroed by kasan_unpoison_vmalloc().",
        "			 */",
        "			gfp_mask |= __GFP_SKIP_KASAN | __GFP_SKIP_ZERO;",
        "		}",
        "",
        "		/* Take note that the mapping is PAGE_KERNEL. */",
        "		kasan_flags |= KASAN_VMALLOC_PROT_NORMAL;",
        "	}",
        "",
        "	/* Allocate physical pages and map them into vmalloc space. */",
        "	ret = __vmalloc_area_node(area, gfp_mask, prot, shift, node);",
        "	if (!ret)",
        "		goto fail;",
        "",
        "	/*",
        "	 * Mark the pages as accessible, now that they are mapped.",
        "	 * The condition for setting KASAN_VMALLOC_INIT should complement the",
        "	 * one in post_alloc_hook() with regards to the __GFP_SKIP_ZERO check",
        "	 * to make sure that memory is initialized under the same conditions.",
        "	 * Tag-based KASAN modes only assign tags to normal non-executable",
        "	 * allocations, see __kasan_unpoison_vmalloc().",
        "	 */",
        "	kasan_flags |= KASAN_VMALLOC_VM_ALLOC;",
        "	if (!want_init_on_free() && want_init_on_alloc(gfp_mask) &&",
        "	    (gfp_mask & __GFP_SKIP_ZERO))",
        "		kasan_flags |= KASAN_VMALLOC_INIT;",
        "	/* KASAN_VMALLOC_PROT_NORMAL already set if required. */",
        "	area->addr = kasan_unpoison_vmalloc(area->addr, real_size, kasan_flags);",
        "",
        "	/*",
        "	 * In this function, newly allocated vm_struct has VM_UNINITIALIZED",
        "	 * flag. It means that vm_struct is not fully initialized.",
        "	 * Now, it is fully initialized, so remove this flag here.",
        "	 */",
        "	clear_vm_uninitialized_flag(area);",
        "",
        "	size = PAGE_ALIGN(size);",
        "	if (!(vm_flags & VM_DEFER_KMEMLEAK))",
        "		kmemleak_vmalloc(area, size, gfp_mask);",
        "",
        "	return area->addr;",
        "",
        "fail:",
        "	if (shift > PAGE_SHIFT) {",
        "		shift = PAGE_SHIFT;",
        "		align = real_align;",
        "		size = real_size;",
        "		goto again;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * __vmalloc_node - allocate virtually contiguous memory",
        " * @size:	    allocation size",
        " * @align:	    desired alignment",
        " * @gfp_mask:	    flags for the page level allocator",
        " * @node:	    node to use for allocation or NUMA_NO_NODE",
        " * @caller:	    caller's return address",
        " *",
        " * Allocate enough pages to cover @size from the page level allocator with",
        " * @gfp_mask flags.  Map them into contiguous kernel virtual space.",
        " *",
        " * Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL",
        " * and __GFP_NOFAIL are not supported",
        " *",
        " * Any use of gfp flags outside of GFP_KERNEL should be consulted",
        " * with mm people.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *__vmalloc_node_noprof(unsigned long size, unsigned long align,",
        "			    gfp_t gfp_mask, int node, const void *caller)",
        "{",
        "	return __vmalloc_node_range_noprof(size, align, VMALLOC_START, VMALLOC_END,",
        "				gfp_mask, PAGE_KERNEL, 0, node, caller);",
        "}",
        "/*",
        " * This is only for performance analysis of vmalloc and stress purpose.",
        " * It is required by vmalloc test module, therefore do not use it other",
        " * than that.",
        " */",
        "#ifdef CONFIG_TEST_VMALLOC_MODULE",
        "EXPORT_SYMBOL_GPL(__vmalloc_node_noprof);",
        "#endif",
        "",
        "void *__vmalloc_noprof(unsigned long size, gfp_t gfp_mask)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, gfp_mask, NUMA_NO_NODE,",
        "				__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(__vmalloc_noprof);",
        "",
        "/**",
        " * vmalloc - allocate virtually contiguous memory",
        " * @size:    allocation size",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator and map them into contiguous kernel virtual space.",
        " *",
        " * For tight control over page level allocator and protection flags",
        " * use __vmalloc() instead.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_noprof(unsigned long size)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, GFP_KERNEL, NUMA_NO_NODE,",
        "				__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vmalloc_noprof);",
        "",
        "/**",
        " * vmalloc_huge - allocate virtually contiguous memory, allow huge pages",
        " * @size:      allocation size",
        " * @gfp_mask:  flags for the page level allocator",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator and map them into contiguous kernel virtual space.",
        " * If @size is greater than or equal to PMD_SIZE, allow using",
        " * huge pages for the memory",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_huge_noprof(unsigned long size, gfp_t gfp_mask)",
        "{",
        "	return __vmalloc_node_range_noprof(size, 1, VMALLOC_START, VMALLOC_END,",
        "				    gfp_mask, PAGE_KERNEL, VM_ALLOW_HUGE_VMAP,",
        "				    NUMA_NO_NODE, __builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL_GPL(vmalloc_huge_noprof);",
        "",
        "/**",
        " * vzalloc - allocate virtually contiguous memory with zero fill",
        " * @size:    allocation size",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator and map them into contiguous kernel virtual space.",
        " * The memory allocated is set to zero.",
        " *",
        " * For tight control over page level allocator and protection flags",
        " * use __vmalloc() instead.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vzalloc_noprof(unsigned long size)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, GFP_KERNEL | __GFP_ZERO, NUMA_NO_NODE,",
        "				__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vzalloc_noprof);",
        "",
        "/**",
        " * vmalloc_user - allocate zeroed virtually contiguous memory for userspace",
        " * @size: allocation size",
        " *",
        " * The resulting memory area is zeroed so it can be mapped to userspace",
        " * without leaking data.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_user_noprof(unsigned long size)",
        "{",
        "	return __vmalloc_node_range_noprof(size, SHMLBA,  VMALLOC_START, VMALLOC_END,",
        "				    GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL,",
        "				    VM_USERMAP, NUMA_NO_NODE,",
        "				    __builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vmalloc_user_noprof);",
        "",
        "/**",
        " * vmalloc_node - allocate memory on a specific node",
        " * @size:	  allocation size",
        " * @node:	  numa node",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator and map them into contiguous kernel virtual space.",
        " *",
        " * For tight control over page level allocator and protection flags",
        " * use __vmalloc() instead.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_node_noprof(unsigned long size, int node)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, GFP_KERNEL, node,",
        "			__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vmalloc_node_noprof);",
        "",
        "/**",
        " * vzalloc_node - allocate memory on a specific node with zero fill",
        " * @size:	allocation size",
        " * @node:	numa node",
        " *",
        " * Allocate enough pages to cover @size from the page level",
        " * allocator and map them into contiguous kernel virtual space.",
        " * The memory allocated is set to zero.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vzalloc_node_noprof(unsigned long size, int node)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, GFP_KERNEL | __GFP_ZERO, node,",
        "				__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vzalloc_node_noprof);",
        "",
        "/**",
        " * vrealloc - reallocate virtually contiguous memory; contents remain unchanged",
        " * @p: object to reallocate memory for",
        " * @size: the size to reallocate",
        " * @flags: the flags for the page level allocator",
        " *",
        " * If @p is %NULL, vrealloc() behaves exactly like vmalloc(). If @size is 0 and",
        " * @p is not a %NULL pointer, the object pointed to is freed.",
        " *",
        " * If __GFP_ZERO logic is requested, callers must ensure that, starting with the",
        " * initial memory allocation, every subsequent call to this API for the same",
        " * memory allocation is flagged with __GFP_ZERO. Otherwise, it is possible that",
        " * __GFP_ZERO is not fully honored by this API.",
        " *",
        " * In any case, the contents of the object pointed to are preserved up to the",
        " * lesser of the new and old sizes.",
        " *",
        " * This function must not be called concurrently with itself or vfree() for the",
        " * same memory allocation.",
        " *",
        " * Return: pointer to the allocated memory; %NULL if @size is zero or in case of",
        " *         failure",
        " */",
        "void *vrealloc_noprof(const void *p, size_t size, gfp_t flags)",
        "{",
        "	size_t old_size = 0;",
        "	void *n;",
        "",
        "	if (!size) {",
        "		vfree(p);",
        "		return NULL;",
        "	}",
        "",
        "	if (p) {",
        "		struct vm_struct *vm;",
        "",
        "		vm = find_vm_area(p);",
        "		if (unlikely(!vm)) {",
        "			WARN(1, \"Trying to vrealloc() nonexistent vm area (%p)\\n\", p);",
        "			return NULL;",
        "		}",
        "",
        "		old_size = get_vm_area_size(vm);",
        "	}",
        "",
        "	/*",
        "	 * TODO: Shrink the vm_area, i.e. unmap and free unused pages. What",
        "	 * would be a good heuristic for when to shrink the vm_area?",
        "	 */",
        "	if (size <= old_size) {",
        "		/* Zero out spare memory. */",
        "		if (want_init_on_alloc(flags))",
        "			memset((void *)p + size, 0, old_size - size);",
        "		kasan_poison_vmalloc(p + size, old_size - size);",
        "		kasan_unpoison_vmalloc(p, size, KASAN_VMALLOC_PROT_NORMAL);",
        "		return (void *)p;",
        "	}",
        "",
        "	/* TODO: Grow the vm_area, i.e. allocate and map additional pages. */",
        "	n = __vmalloc_noprof(size, flags);",
        "	if (!n)",
        "		return NULL;",
        "",
        "	if (p) {",
        "		memcpy(n, p, old_size);",
        "		vfree(p);",
        "	}",
        "",
        "	return n;",
        "}",
        "",
        "#if defined(CONFIG_64BIT) && defined(CONFIG_ZONE_DMA32)",
        "#define GFP_VMALLOC32 (GFP_DMA32 | GFP_KERNEL)",
        "#elif defined(CONFIG_64BIT) && defined(CONFIG_ZONE_DMA)",
        "#define GFP_VMALLOC32 (GFP_DMA | GFP_KERNEL)",
        "#else",
        "/*",
        " * 64b systems should always have either DMA or DMA32 zones. For others",
        " * GFP_DMA32 should do the right thing and use the normal zone.",
        " */",
        "#define GFP_VMALLOC32 (GFP_DMA32 | GFP_KERNEL)",
        "#endif",
        "",
        "/**",
        " * vmalloc_32 - allocate virtually contiguous memory (32bit addressable)",
        " * @size:	allocation size",
        " *",
        " * Allocate enough 32bit PA addressable pages to cover @size from the",
        " * page level allocator and map them into contiguous kernel virtual space.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_32_noprof(unsigned long size)",
        "{",
        "	return __vmalloc_node_noprof(size, 1, GFP_VMALLOC32, NUMA_NO_NODE,",
        "			__builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vmalloc_32_noprof);",
        "",
        "/**",
        " * vmalloc_32_user - allocate zeroed virtually contiguous 32bit memory",
        " * @size:	     allocation size",
        " *",
        " * The resulting memory area is 32bit addressable and zeroed so it can be",
        " * mapped to userspace without leaking data.",
        " *",
        " * Return: pointer to the allocated memory or %NULL on error",
        " */",
        "void *vmalloc_32_user_noprof(unsigned long size)",
        "{",
        "	return __vmalloc_node_range_noprof(size, SHMLBA,  VMALLOC_START, VMALLOC_END,",
        "				    GFP_VMALLOC32 | __GFP_ZERO, PAGE_KERNEL,",
        "				    VM_USERMAP, NUMA_NO_NODE,",
        "				    __builtin_return_address(0));",
        "}",
        "EXPORT_SYMBOL(vmalloc_32_user_noprof);",
        "",
        "/*",
        " * Atomically zero bytes in the iterator.",
        " *",
        " * Returns the number of zeroed bytes.",
        " */",
        "static size_t zero_iter(struct iov_iter *iter, size_t count)",
        "{",
        "	size_t remains = count;",
        "",
        "	while (remains > 0) {",
        "		size_t num, copied;",
        "",
        "		num = min_t(size_t, remains, PAGE_SIZE);",
        "		copied = copy_page_to_iter_nofault(ZERO_PAGE(0), 0, num, iter);",
        "		remains -= copied;",
        "",
        "		if (copied < num)",
        "			break;",
        "	}",
        "",
        "	return count - remains;",
        "}",
        "",
        "/*",
        " * small helper routine, copy contents to iter from addr.",
        " * If the page is not present, fill zero.",
        " *",
        " * Returns the number of copied bytes.",
        " */",
        "static size_t aligned_vread_iter(struct iov_iter *iter,",
        "				 const char *addr, size_t count)",
        "{",
        "	size_t remains = count;",
        "	struct page *page;",
        "",
        "	while (remains > 0) {",
        "		unsigned long offset, length;",
        "		size_t copied = 0;",
        "",
        "		offset = offset_in_page(addr);",
        "		length = PAGE_SIZE - offset;",
        "		if (length > remains)",
        "			length = remains;",
        "		page = vmalloc_to_page(addr);",
        "		/*",
        "		 * To do safe access to this _mapped_ area, we need lock. But",
        "		 * adding lock here means that we need to add overhead of",
        "		 * vmalloc()/vfree() calls for this _debug_ interface, rarely",
        "		 * used. Instead of that, we'll use an local mapping via",
        "		 * copy_page_to_iter_nofault() and accept a small overhead in",
        "		 * this access function.",
        "		 */",
        "		if (page)",
        "			copied = copy_page_to_iter_nofault(page, offset,",
        "							   length, iter);",
        "		else",
        "			copied = zero_iter(iter, length);",
        "",
        "		addr += copied;",
        "		remains -= copied;",
        "",
        "		if (copied != length)",
        "			break;",
        "	}",
        "",
        "	return count - remains;",
        "}",
        "",
        "/*",
        " * Read from a vm_map_ram region of memory.",
        " *",
        " * Returns the number of copied bytes.",
        " */",
        "static size_t vmap_ram_vread_iter(struct iov_iter *iter, const char *addr,",
        "				  size_t count, unsigned long flags)",
        "{",
        "	char *start;",
        "	struct vmap_block *vb;",
        "	struct xarray *xa;",
        "	unsigned long offset;",
        "	unsigned int rs, re;",
        "	size_t remains, n;",
        "",
        "	/*",
        "	 * If it's area created by vm_map_ram() interface directly, but",
        "	 * not further subdividing and delegating management to vmap_block,",
        "	 * handle it here.",
        "	 */",
        "	if (!(flags & VMAP_BLOCK))",
        "		return aligned_vread_iter(iter, addr, count);",
        "",
        "	remains = count;",
        "",
        "	/*",
        "	 * Area is split into regions and tracked with vmap_block, read out",
        "	 * each region and zero fill the hole between regions.",
        "	 */",
        "	xa = addr_to_vb_xa((unsigned long) addr);",
        "	vb = xa_load(xa, addr_to_vb_idx((unsigned long)addr));",
        "	if (!vb)",
        "		goto finished_zero;",
        "",
        "	spin_lock(&vb->lock);",
        "	if (bitmap_empty(vb->used_map, VMAP_BBMAP_BITS)) {",
        "		spin_unlock(&vb->lock);",
        "		goto finished_zero;",
        "	}",
        "",
        "	for_each_set_bitrange(rs, re, vb->used_map, VMAP_BBMAP_BITS) {",
        "		size_t copied;",
        "",
        "		if (remains == 0)",
        "			goto finished;",
        "",
        "		start = vmap_block_vaddr(vb->va->va_start, rs);",
        "",
        "		if (addr < start) {",
        "			size_t to_zero = min_t(size_t, start - addr, remains);",
        "			size_t zeroed = zero_iter(iter, to_zero);",
        "",
        "			addr += zeroed;",
        "			remains -= zeroed;",
        "",
        "			if (remains == 0 || zeroed != to_zero)",
        "				goto finished;",
        "		}",
        "",
        "		/*it could start reading from the middle of used region*/",
        "		offset = offset_in_page(addr);",
        "		n = ((re - rs + 1) << PAGE_SHIFT) - offset;",
        "		if (n > remains)",
        "			n = remains;",
        "",
        "		copied = aligned_vread_iter(iter, start + offset, n);",
        "",
        "		addr += copied;",
        "		remains -= copied;",
        "",
        "		if (copied != n)",
        "			goto finished;",
        "	}",
        "",
        "	spin_unlock(&vb->lock);",
        "",
        "finished_zero:",
        "	/* zero-fill the left dirty or free regions */",
        "	return count - remains + zero_iter(iter, remains);",
        "finished:",
        "	/* We couldn't copy/zero everything */",
        "	spin_unlock(&vb->lock);",
        "	return count - remains;",
        "}",
        "",
        "/**",
        " * vread_iter() - read vmalloc area in a safe way to an iterator.",
        " * @iter:         the iterator to which data should be written.",
        " * @addr:         vm address.",
        " * @count:        number of bytes to be read.",
        " *",
        " * This function checks that addr is a valid vmalloc'ed area, and",
        " * copy data from that area to a given buffer. If the given memory range",
        " * of [addr...addr+count) includes some valid address, data is copied to",
        " * proper area of @buf. If there are memory holes, they'll be zero-filled.",
        " * IOREMAP area is treated as memory hole and no copy is done.",
        " *",
        " * If [addr...addr+count) doesn't includes any intersects with alive",
        " * vm_struct area, returns 0. @buf should be kernel's buffer.",
        " *",
        " * Note: In usual ops, vread() is never necessary because the caller",
        " * should know vmalloc() area is valid and can use memcpy().",
        " * This is for routines which have to access vmalloc area without",
        " * any information, as /proc/kcore.",
        " *",
        " * Return: number of bytes for which addr and buf should be increased",
        " * (same number as @count) or %0 if [addr...addr+count) doesn't",
        " * include any intersection with valid vmalloc area",
        " */",
        "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	struct vm_struct *vm;",
        "	char *vaddr;",
        "	size_t n, size, flags, remains;",
        "	unsigned long next;",
        "",
        "	addr = kasan_reset_tag(addr);",
        "",
        "	/* Don't allow overflow */",
        "	if ((unsigned long) addr + count < count)",
        "		count = -(unsigned long) addr;",
        "",
        "	remains = count;",
        "",
        "	vn = find_vmap_area_exceed_addr_lock((unsigned long) addr, &va);",
        "	if (!vn)",
        "		goto finished_zero;",
        "",
        "	/* no intersects with alive vmap_area */",
        "	if ((unsigned long)addr + remains <= va->va_start)",
        "		goto finished_zero;",
        "",
        "	do {",
        "		size_t copied;",
        "",
        "		if (remains == 0)",
        "			goto finished;",
        "",
        "		vm = va->vm;",
        "		flags = va->flags & VMAP_FLAGS_MASK;",
        "		/*",
        "		 * VMAP_BLOCK indicates a sub-type of vm_map_ram area, need",
        "		 * be set together with VMAP_RAM.",
        "		 */",
        "		WARN_ON(flags == VMAP_BLOCK);",
        "",
        "		if (!vm && !flags)",
        "			goto next_va;",
        "",
        "		if (vm && (vm->flags & VM_UNINITIALIZED))",
        "			goto next_va;",
        "",
        "		/* Pair with smp_wmb() in clear_vm_uninitialized_flag() */",
        "		smp_rmb();",
        "",
        "		vaddr = (char *) va->va_start;",
        "		size = vm ? get_vm_area_size(vm) : va_size(va);",
        "",
        "		if (addr >= vaddr + size)",
        "			goto next_va;",
        "",
        "		if (addr < vaddr) {",
        "			size_t to_zero = min_t(size_t, vaddr - addr, remains);",
        "			size_t zeroed = zero_iter(iter, to_zero);",
        "",
        "			addr += zeroed;",
        "			remains -= zeroed;",
        "",
        "			if (remains == 0 || zeroed != to_zero)",
        "				goto finished;",
        "		}",
        "",
        "		n = vaddr + size - addr;",
        "		if (n > remains)",
        "			n = remains;",
        "",
        "		if (flags & VMAP_RAM)",
        "			copied = vmap_ram_vread_iter(iter, addr, n, flags);",
        "		else if (!(vm && (vm->flags & (VM_IOREMAP | VM_SPARSE))))",
        "			copied = aligned_vread_iter(iter, addr, n);",
        "		else /* IOREMAP | SPARSE area is treated as memory hole */",
        "			copied = zero_iter(iter, n);",
        "",
        "		addr += copied;",
        "		remains -= copied;",
        "",
        "		if (copied != n)",
        "			goto finished;",
        "",
        "	next_va:",
        "		next = va->va_end;",
        "		spin_unlock(&vn->busy.lock);",
        "	} while ((vn = find_vmap_area_exceed_addr_lock(next, &va)));",
        "",
        "finished_zero:",
        "	if (vn)",
        "		spin_unlock(&vn->busy.lock);",
        "",
        "	/* zero-fill memory holes */",
        "	return count - remains + zero_iter(iter, remains);",
        "finished:",
        "	/* Nothing remains, or We couldn't copy/zero everything. */",
        "	if (vn)",
        "		spin_unlock(&vn->busy.lock);",
        "",
        "	return count - remains;",
        "}",
        "",
        "/**",
        " * remap_vmalloc_range_partial - map vmalloc pages to userspace",
        " * @vma:		vma to cover",
        " * @uaddr:		target user address to start at",
        " * @kaddr:		virtual address of vmalloc kernel memory",
        " * @pgoff:		offset from @kaddr to start at",
        " * @size:		size of map area",
        " *",
        " * Returns:	0 for success, -Exxx on failure",
        " *",
        " * This function checks that @kaddr is a valid vmalloc'ed area,",
        " * and that it is big enough to cover the range starting at",
        " * @uaddr in @vma. Will return failure if that criteria isn't",
        " * met.",
        " *",
        " * Similar to remap_pfn_range() (see mm/memory.c)",
        " */",
        "int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,",
        "				void *kaddr, unsigned long pgoff,",
        "				unsigned long size)",
        "{",
        "	struct vm_struct *area;",
        "	unsigned long off;",
        "	unsigned long end_index;",
        "",
        "	if (check_shl_overflow(pgoff, PAGE_SHIFT, &off))",
        "		return -EINVAL;",
        "",
        "	size = PAGE_ALIGN(size);",
        "",
        "	if (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))",
        "		return -EINVAL;",
        "",
        "	area = find_vm_area(kaddr);",
        "	if (!area)",
        "		return -EINVAL;",
        "",
        "	if (!(area->flags & (VM_USERMAP | VM_DMA_COHERENT)))",
        "		return -EINVAL;",
        "",
        "	if (check_add_overflow(size, off, &end_index) ||",
        "	    end_index > get_vm_area_size(area))",
        "		return -EINVAL;",
        "	kaddr += off;",
        "",
        "	do {",
        "		struct page *page = vmalloc_to_page(kaddr);",
        "		int ret;",
        "",
        "		ret = vm_insert_page(vma, uaddr, page);",
        "		if (ret)",
        "			return ret;",
        "",
        "		uaddr += PAGE_SIZE;",
        "		kaddr += PAGE_SIZE;",
        "		size -= PAGE_SIZE;",
        "	} while (size > 0);",
        "",
        "	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * remap_vmalloc_range - map vmalloc pages to userspace",
        " * @vma:		vma to cover (map full range of vma)",
        " * @addr:		vmalloc memory",
        " * @pgoff:		number of pages into addr before first page to map",
        " *",
        " * Returns:	0 for success, -Exxx on failure",
        " *",
        " * This function checks that addr is a valid vmalloc'ed area, and",
        " * that it is big enough to cover the vma. Will return failure if",
        " * that criteria isn't met.",
        " *",
        " * Similar to remap_pfn_range() (see mm/memory.c)",
        " */",
        "int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,",
        "						unsigned long pgoff)",
        "{",
        "	return remap_vmalloc_range_partial(vma, vma->vm_start,",
        "					   addr, pgoff,",
        "					   vma->vm_end - vma->vm_start);",
        "}",
        "EXPORT_SYMBOL(remap_vmalloc_range);",
        "",
        "void free_vm_area(struct vm_struct *area)",
        "{",
        "	struct vm_struct *ret;",
        "	ret = remove_vm_area(area->addr);",
        "	BUG_ON(ret != area);",
        "	kfree(area);",
        "}",
        "EXPORT_SYMBOL_GPL(free_vm_area);",
        "",
        "#ifdef CONFIG_SMP",
        "static struct vmap_area *node_to_va(struct rb_node *n)",
        "{",
        "	return rb_entry_safe(n, struct vmap_area, rb_node);",
        "}",
        "",
        "/**",
        " * pvm_find_va_enclose_addr - find the vmap_area @addr belongs to",
        " * @addr: target address",
        " *",
        " * Returns: vmap_area if it is found. If there is no such area",
        " *   the first highest(reverse order) vmap_area is returned",
        " *   i.e. va->va_start < addr && va->va_end < addr or NULL",
        " *   if there are no any areas before @addr.",
        " */",
        "static struct vmap_area *",
        "pvm_find_va_enclose_addr(unsigned long addr)",
        "{",
        "	struct vmap_area *va, *tmp;",
        "	struct rb_node *n;",
        "",
        "	n = free_vmap_area_root.rb_node;",
        "	va = NULL;",
        "",
        "	while (n) {",
        "		tmp = rb_entry(n, struct vmap_area, rb_node);",
        "		if (tmp->va_start <= addr) {",
        "			va = tmp;",
        "			if (tmp->va_end >= addr)",
        "				break;",
        "",
        "			n = n->rb_right;",
        "		} else {",
        "			n = n->rb_left;",
        "		}",
        "	}",
        "",
        "	return va;",
        "}",
        "",
        "/**",
        " * pvm_determine_end_from_reverse - find the highest aligned address",
        " * of free block below VMALLOC_END",
        " * @va:",
        " *   in - the VA we start the search(reverse order);",
        " *   out - the VA with the highest aligned end address.",
        " * @align: alignment for required highest address",
        " *",
        " * Returns: determined end address within vmap_area",
        " */",
        "static unsigned long",
        "pvm_determine_end_from_reverse(struct vmap_area **va, unsigned long align)",
        "{",
        "	unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);",
        "	unsigned long addr;",
        "",
        "	if (likely(*va)) {",
        "		list_for_each_entry_from_reverse((*va),",
        "				&free_vmap_area_list, list) {",
        "			addr = min((*va)->va_end & ~(align - 1), vmalloc_end);",
        "			if ((*va)->va_start < addr)",
        "				return addr;",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * pcpu_get_vm_areas - allocate vmalloc areas for percpu allocator",
        " * @offsets: array containing offset of each area",
        " * @sizes: array containing size of each area",
        " * @nr_vms: the number of areas to allocate",
        " * @align: alignment, all entries in @offsets and @sizes must be aligned to this",
        " *",
        " * Returns: kmalloc'd vm_struct pointer array pointing to allocated",
        " *	    vm_structs on success, %NULL on failure",
        " *",
        " * Percpu allocator wants to use congruent vm areas so that it can",
        " * maintain the offsets among percpu areas.  This function allocates",
        " * congruent vmalloc areas for it with GFP_KERNEL.  These areas tend to",
        " * be scattered pretty far, distance between two areas easily going up",
        " * to gigabytes.  To avoid interacting with regular vmallocs, these",
        " * areas are allocated from top.",
        " *",
        " * Despite its complicated look, this allocator is rather simple. It",
        " * does everything top-down and scans free blocks from the end looking",
        " * for matching base. While scanning, if any of the areas do not fit the",
        " * base address is pulled down to fit the area. Scanning is repeated till",
        " * all the areas fit and then all necessary data structures are inserted",
        " * and the result is returned.",
        " */",
        "struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,",
        "				     const size_t *sizes, int nr_vms,",
        "				     size_t align)",
        "{",
        "	const unsigned long vmalloc_start = ALIGN(VMALLOC_START, align);",
        "	const unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);",
        "	struct vmap_area **vas, *va;",
        "	struct vm_struct **vms;",
        "	int area, area2, last_area, term_area;",
        "	unsigned long base, start, size, end, last_end, orig_start, orig_end;",
        "	bool purged = false;",
        "",
        "	/* verify parameters and allocate data structures */",
        "	BUG_ON(offset_in_page(align) || !is_power_of_2(align));",
        "	for (last_area = 0, area = 0; area < nr_vms; area++) {",
        "		start = offsets[area];",
        "		end = start + sizes[area];",
        "",
        "		/* is everything aligned properly? */",
        "		BUG_ON(!IS_ALIGNED(offsets[area], align));",
        "		BUG_ON(!IS_ALIGNED(sizes[area], align));",
        "",
        "		/* detect the area with the highest address */",
        "		if (start > offsets[last_area])",
        "			last_area = area;",
        "",
        "		for (area2 = area + 1; area2 < nr_vms; area2++) {",
        "			unsigned long start2 = offsets[area2];",
        "			unsigned long end2 = start2 + sizes[area2];",
        "",
        "			BUG_ON(start2 < end && start < end2);",
        "		}",
        "	}",
        "	last_end = offsets[last_area] + sizes[last_area];",
        "",
        "	if (vmalloc_end - vmalloc_start < last_end) {",
        "		WARN_ON(true);",
        "		return NULL;",
        "	}",
        "",
        "	vms = kcalloc(nr_vms, sizeof(vms[0]), GFP_KERNEL);",
        "	vas = kcalloc(nr_vms, sizeof(vas[0]), GFP_KERNEL);",
        "	if (!vas || !vms)",
        "		goto err_free2;",
        "",
        "	for (area = 0; area < nr_vms; area++) {",
        "		vas[area] = kmem_cache_zalloc(vmap_area_cachep, GFP_KERNEL);",
        "		vms[area] = kzalloc(sizeof(struct vm_struct), GFP_KERNEL);",
        "		if (!vas[area] || !vms[area])",
        "			goto err_free;",
        "	}",
        "retry:",
        "	spin_lock(&free_vmap_area_lock);",
        "",
        "	/* start scanning - we scan from the top, begin with the last area */",
        "	area = term_area = last_area;",
        "	start = offsets[area];",
        "	end = start + sizes[area];",
        "",
        "	va = pvm_find_va_enclose_addr(vmalloc_end);",
        "	base = pvm_determine_end_from_reverse(&va, align) - end;",
        "",
        "	while (true) {",
        "		/*",
        "		 * base might have underflowed, add last_end before",
        "		 * comparing.",
        "		 */",
        "		if (base + last_end < vmalloc_start + last_end)",
        "			goto overflow;",
        "",
        "		/*",
        "		 * Fitting base has not been found.",
        "		 */",
        "		if (va == NULL)",
        "			goto overflow;",
        "",
        "		/*",
        "		 * If required width exceeds current VA block, move",
        "		 * base downwards and then recheck.",
        "		 */",
        "		if (base + end > va->va_end) {",
        "			base = pvm_determine_end_from_reverse(&va, align) - end;",
        "			term_area = area;",
        "			continue;",
        "		}",
        "",
        "		/*",
        "		 * If this VA does not fit, move base downwards and recheck.",
        "		 */",
        "		if (base + start < va->va_start) {",
        "			va = node_to_va(rb_prev(&va->rb_node));",
        "			base = pvm_determine_end_from_reverse(&va, align) - end;",
        "			term_area = area;",
        "			continue;",
        "		}",
        "",
        "		/*",
        "		 * This area fits, move on to the previous one.  If",
        "		 * the previous one is the terminal one, we're done.",
        "		 */",
        "		area = (area + nr_vms - 1) % nr_vms;",
        "		if (area == term_area)",
        "			break;",
        "",
        "		start = offsets[area];",
        "		end = start + sizes[area];",
        "		va = pvm_find_va_enclose_addr(base + end);",
        "	}",
        "",
        "	/* we've found a fitting base, insert all va's */",
        "	for (area = 0; area < nr_vms; area++) {",
        "		int ret;",
        "",
        "		start = base + offsets[area];",
        "		size = sizes[area];",
        "",
        "		va = pvm_find_va_enclose_addr(start);",
        "		if (WARN_ON_ONCE(va == NULL))",
        "			/* It is a BUG(), but trigger recovery instead. */",
        "			goto recovery;",
        "",
        "		ret = va_clip(&free_vmap_area_root,",
        "			&free_vmap_area_list, va, start, size);",
        "		if (WARN_ON_ONCE(unlikely(ret)))",
        "			/* It is a BUG(), but trigger recovery instead. */",
        "			goto recovery;",
        "",
        "		/* Allocated area. */",
        "		va = vas[area];",
        "		va->va_start = start;",
        "		va->va_end = start + size;",
        "	}",
        "",
        "	spin_unlock(&free_vmap_area_lock);",
        "",
        "	/* populate the kasan shadow space */",
        "	for (area = 0; area < nr_vms; area++) {",
        "		if (kasan_populate_vmalloc(vas[area]->va_start, sizes[area]))",
        "			goto err_free_shadow;",
        "	}",
        "",
        "	/* insert all vm's */",
        "	for (area = 0; area < nr_vms; area++) {",
        "		struct vmap_node *vn = addr_to_node(vas[area]->va_start);",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		insert_vmap_area(vas[area], &vn->busy.root, &vn->busy.head);",
        "		setup_vmalloc_vm(vms[area], vas[area], VM_ALLOC,",
        "				 pcpu_get_vm_areas);",
        "		spin_unlock(&vn->busy.lock);",
        "	}",
        "",
        "	/*",
        "	 * Mark allocated areas as accessible. Do it now as a best-effort",
        "	 * approach, as they can be mapped outside of vmalloc code.",
        "	 * With hardware tag-based KASAN, marking is skipped for",
        "	 * non-VM_ALLOC mappings, see __kasan_unpoison_vmalloc().",
        "	 */",
        "	for (area = 0; area < nr_vms; area++)",
        "		vms[area]->addr = kasan_unpoison_vmalloc(vms[area]->addr,",
        "				vms[area]->size, KASAN_VMALLOC_PROT_NORMAL);",
        "",
        "	kfree(vas);",
        "	return vms;",
        "",
        "recovery:",
        "	/*",
        "	 * Remove previously allocated areas. There is no",
        "	 * need in removing these areas from the busy tree,",
        "	 * because they are inserted only on the final step",
        "	 * and when pcpu_get_vm_areas() is success.",
        "	 */",
        "	while (area--) {",
        "		orig_start = vas[area]->va_start;",
        "		orig_end = vas[area]->va_end;",
        "		va = merge_or_add_vmap_area_augment(vas[area], &free_vmap_area_root,",
        "				&free_vmap_area_list);",
        "		if (va)",
        "			kasan_release_vmalloc(orig_start, orig_end,",
        "				va->va_start, va->va_end,",
        "				KASAN_VMALLOC_PAGE_RANGE | KASAN_VMALLOC_TLB_FLUSH);",
        "		vas[area] = NULL;",
        "	}",
        "",
        "overflow:",
        "	spin_unlock(&free_vmap_area_lock);",
        "	if (!purged) {",
        "		reclaim_and_purge_vmap_areas();",
        "		purged = true;",
        "",
        "		/* Before \"retry\", check if we recover. */",
        "		for (area = 0; area < nr_vms; area++) {",
        "			if (vas[area])",
        "				continue;",
        "",
        "			vas[area] = kmem_cache_zalloc(",
        "				vmap_area_cachep, GFP_KERNEL);",
        "			if (!vas[area])",
        "				goto err_free;",
        "		}",
        "",
        "		goto retry;",
        "	}",
        "",
        "err_free:",
        "	for (area = 0; area < nr_vms; area++) {",
        "		if (vas[area])",
        "			kmem_cache_free(vmap_area_cachep, vas[area]);",
        "",
        "		kfree(vms[area]);",
        "	}",
        "err_free2:",
        "	kfree(vas);",
        "	kfree(vms);",
        "	return NULL;",
        "",
        "err_free_shadow:",
        "	spin_lock(&free_vmap_area_lock);",
        "	/*",
        "	 * We release all the vmalloc shadows, even the ones for regions that",
        "	 * hadn't been successfully added. This relies on kasan_release_vmalloc",
        "	 * being able to tolerate this case.",
        "	 */",
        "	for (area = 0; area < nr_vms; area++) {",
        "		orig_start = vas[area]->va_start;",
        "		orig_end = vas[area]->va_end;",
        "		va = merge_or_add_vmap_area_augment(vas[area], &free_vmap_area_root,",
        "				&free_vmap_area_list);",
        "		if (va)",
        "			kasan_release_vmalloc(orig_start, orig_end,",
        "				va->va_start, va->va_end,",
        "				KASAN_VMALLOC_PAGE_RANGE | KASAN_VMALLOC_TLB_FLUSH);",
        "		vas[area] = NULL;",
        "		kfree(vms[area]);",
        "	}",
        "	spin_unlock(&free_vmap_area_lock);",
        "	kfree(vas);",
        "	kfree(vms);",
        "	return NULL;",
        "}",
        "",
        "/**",
        " * pcpu_free_vm_areas - free vmalloc areas for percpu allocator",
        " * @vms: vm_struct pointer array returned by pcpu_get_vm_areas()",
        " * @nr_vms: the number of allocated areas",
        " *",
        " * Free vm_structs and the array allocated by pcpu_get_vm_areas().",
        " */",
        "void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < nr_vms; i++)",
        "		free_vm_area(vms[i]);",
        "	kfree(vms);",
        "}",
        "#endif	/* CONFIG_SMP */",
        "",
        "#ifdef CONFIG_PRINTK",
        "bool vmalloc_dump_obj(void *object)",
        "{",
        "	const void *caller;",
        "	struct vm_struct *vm;",
        "	struct vmap_area *va;",
        "	struct vmap_node *vn;",
        "	unsigned long addr;",
        "	unsigned int nr_pages;",
        "",
        "	addr = PAGE_ALIGN((unsigned long) object);",
        "	vn = addr_to_node(addr);",
        "",
        "	if (!spin_trylock(&vn->busy.lock))",
        "		return false;",
        "",
        "	va = __find_vmap_area(addr, &vn->busy.root);",
        "	if (!va || !va->vm) {",
        "		spin_unlock(&vn->busy.lock);",
        "		return false;",
        "	}",
        "",
        "	vm = va->vm;",
        "	addr = (unsigned long) vm->addr;",
        "	caller = vm->caller;",
        "	nr_pages = vm->nr_pages;",
        "	spin_unlock(&vn->busy.lock);",
        "",
        "	pr_cont(\" %u-page vmalloc region starting at %#lx allocated at %pS\\n\",",
        "		nr_pages, addr, caller);",
        "",
        "	return true;",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_PROC_FS",
        "static void show_numa_info(struct seq_file *m, struct vm_struct *v)",
        "{",
        "	if (IS_ENABLED(CONFIG_NUMA)) {",
        "		unsigned int nr, *counters = m->private;",
        "		unsigned int step = 1U << vm_area_page_order(v);",
        "",
        "		if (!counters)",
        "			return;",
        "",
        "		if (v->flags & VM_UNINITIALIZED)",
        "			return;",
        "		/* Pair with smp_wmb() in clear_vm_uninitialized_flag() */",
        "		smp_rmb();",
        "",
        "		memset(counters, 0, nr_node_ids * sizeof(unsigned int));",
        "",
        "		for (nr = 0; nr < v->nr_pages; nr += step)",
        "			counters[page_to_nid(v->pages[nr])] += step;",
        "		for_each_node_state(nr, N_HIGH_MEMORY)",
        "			if (counters[nr])",
        "				seq_printf(m, \" N%u=%u\", nr, counters[nr]);",
        "	}",
        "}",
        "",
        "static void show_purge_info(struct seq_file *m)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	int i;",
        "",
        "	for (i = 0; i < nr_vmap_nodes; i++) {",
        "		vn = &vmap_nodes[i];",
        "",
        "		spin_lock(&vn->lazy.lock);",
        "		list_for_each_entry(va, &vn->lazy.head, list) {",
        "			seq_printf(m, \"0x%pK-0x%pK %7ld unpurged vm_area\\n\",",
        "				(void *)va->va_start, (void *)va->va_end,",
        "				va_size(va));",
        "		}",
        "		spin_unlock(&vn->lazy.lock);",
        "	}",
        "}",
        "",
        "static int vmalloc_info_show(struct seq_file *m, void *p)",
        "{",
        "	struct vmap_node *vn;",
        "	struct vmap_area *va;",
        "	struct vm_struct *v;",
        "	int i;",
        "",
        "	for (i = 0; i < nr_vmap_nodes; i++) {",
        "		vn = &vmap_nodes[i];",
        "",
        "		spin_lock(&vn->busy.lock);",
        "		list_for_each_entry(va, &vn->busy.head, list) {",
        "			if (!va->vm) {",
        "				if (va->flags & VMAP_RAM)",
        "					seq_printf(m, \"0x%pK-0x%pK %7ld vm_map_ram\\n\",",
        "						(void *)va->va_start, (void *)va->va_end,",
        "						va_size(va));",
        "",
        "				continue;",
        "			}",
        "",
        "			v = va->vm;",
        "",
        "			seq_printf(m, \"0x%pK-0x%pK %7ld\",",
        "				v->addr, v->addr + v->size, v->size);",
        "",
        "			if (v->caller)",
        "				seq_printf(m, \" %pS\", v->caller);",
        "",
        "			if (v->nr_pages)",
        "				seq_printf(m, \" pages=%d\", v->nr_pages);",
        "",
        "			if (v->phys_addr)",
        "				seq_printf(m, \" phys=%pa\", &v->phys_addr);",
        "",
        "			if (v->flags & VM_IOREMAP)",
        "				seq_puts(m, \" ioremap\");",
        "",
        "			if (v->flags & VM_SPARSE)",
        "				seq_puts(m, \" sparse\");",
        "",
        "			if (v->flags & VM_ALLOC)",
        "				seq_puts(m, \" vmalloc\");",
        "",
        "			if (v->flags & VM_MAP)",
        "				seq_puts(m, \" vmap\");",
        "",
        "			if (v->flags & VM_USERMAP)",
        "				seq_puts(m, \" user\");",
        "",
        "			if (v->flags & VM_DMA_COHERENT)",
        "				seq_puts(m, \" dma-coherent\");",
        "",
        "			if (is_vmalloc_addr(v->pages))",
        "				seq_puts(m, \" vpages\");",
        "",
        "			show_numa_info(m, v);",
        "			seq_putc(m, '\\n');",
        "		}",
        "		spin_unlock(&vn->busy.lock);",
        "	}",
        "",
        "	/*",
        "	 * As a final step, dump \"unpurged\" areas.",
        "	 */",
        "	show_purge_info(m);",
        "	return 0;",
        "}",
        "",
        "static int __init proc_vmalloc_init(void)",
        "{",
        "	void *priv_data = NULL;",
        "",
        "	if (IS_ENABLED(CONFIG_NUMA))",
        "		priv_data = kmalloc(nr_node_ids * sizeof(unsigned int), GFP_KERNEL);",
        "",
        "	proc_create_single_data(\"vmallocinfo\",",
        "		0400, NULL, vmalloc_info_show, priv_data);",
        "",
        "	return 0;",
        "}",
        "module_init(proc_vmalloc_init);",
        "",
        "#endif",
        "",
        "static void __init vmap_init_free_space(void)",
        "{",
        "	unsigned long vmap_start = 1;",
        "	const unsigned long vmap_end = ULONG_MAX;",
        "	struct vmap_area *free;",
        "	struct vm_struct *busy;",
        "",
        "	/*",
        "	 *     B     F     B     B     B     F",
        "	 * -|-----|.....|-----|-----|-----|.....|-",
        "	 *  |           The KVA space           |",
        "	 *  |<--------------------------------->|",
        "	 */",
        "	for (busy = vmlist; busy; busy = busy->next) {",
        "		if ((unsigned long) busy->addr - vmap_start > 0) {",
        "			free = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);",
        "			if (!WARN_ON_ONCE(!free)) {",
        "				free->va_start = vmap_start;",
        "				free->va_end = (unsigned long) busy->addr;",
        "",
        "				insert_vmap_area_augment(free, NULL,",
        "					&free_vmap_area_root,",
        "						&free_vmap_area_list);",
        "			}",
        "		}",
        "",
        "		vmap_start = (unsigned long) busy->addr + busy->size;",
        "	}",
        "",
        "	if (vmap_end - vmap_start > 0) {",
        "		free = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);",
        "		if (!WARN_ON_ONCE(!free)) {",
        "			free->va_start = vmap_start;",
        "			free->va_end = vmap_end;",
        "",
        "			insert_vmap_area_augment(free, NULL,",
        "				&free_vmap_area_root,",
        "					&free_vmap_area_list);",
        "		}",
        "	}",
        "}",
        "",
        "static void vmap_init_nodes(void)",
        "{",
        "	struct vmap_node *vn;",
        "	int i, n;",
        "",
        "#if BITS_PER_LONG == 64",
        "	/*",
        "	 * A high threshold of max nodes is fixed and bound to 128,",
        "	 * thus a scale factor is 1 for systems where number of cores",
        "	 * are less or equal to specified threshold.",
        "	 *",
        "	 * As for NUMA-aware notes. For bigger systems, for example",
        "	 * NUMA with multi-sockets, where we can end-up with thousands",
        "	 * of cores in total, a \"sub-numa-clustering\" should be added.",
        "	 *",
        "	 * In this case a NUMA domain is considered as a single entity",
        "	 * with dedicated sub-nodes in it which describe one group or",
        "	 * set of cores. Therefore a per-domain purging is supposed to",
        "	 * be added as well as a per-domain balancing.",
        "	 */",
        "	n = clamp_t(unsigned int, num_possible_cpus(), 1, 128);",
        "",
        "	if (n > 1) {",
        "		vn = kmalloc_array(n, sizeof(*vn), GFP_NOWAIT | __GFP_NOWARN);",
        "		if (vn) {",
        "			/* Node partition is 16 pages. */",
        "			vmap_zone_size = (1 << 4) * PAGE_SIZE;",
        "			nr_vmap_nodes = n;",
        "			vmap_nodes = vn;",
        "		} else {",
        "			pr_err(\"Failed to allocate an array. Disable a node layer\\n\");",
        "		}",
        "	}",
        "#endif",
        "",
        "	for (n = 0; n < nr_vmap_nodes; n++) {",
        "		vn = &vmap_nodes[n];",
        "		vn->busy.root = RB_ROOT;",
        "		INIT_LIST_HEAD(&vn->busy.head);",
        "		spin_lock_init(&vn->busy.lock);",
        "",
        "		vn->lazy.root = RB_ROOT;",
        "		INIT_LIST_HEAD(&vn->lazy.head);",
        "		spin_lock_init(&vn->lazy.lock);",
        "",
        "		for (i = 0; i < MAX_VA_SIZE_PAGES; i++) {",
        "			INIT_LIST_HEAD(&vn->pool[i].head);",
        "			WRITE_ONCE(vn->pool[i].len, 0);",
        "		}",
        "",
        "		spin_lock_init(&vn->pool_lock);",
        "	}",
        "}",
        "",
        "static unsigned long",
        "vmap_node_shrink_count(struct shrinker *shrink, struct shrink_control *sc)",
        "{",
        "	unsigned long count;",
        "	struct vmap_node *vn;",
        "	int i, j;",
        "",
        "	for (count = 0, i = 0; i < nr_vmap_nodes; i++) {",
        "		vn = &vmap_nodes[i];",
        "",
        "		for (j = 0; j < MAX_VA_SIZE_PAGES; j++)",
        "			count += READ_ONCE(vn->pool[j].len);",
        "	}",
        "",
        "	return count ? count : SHRINK_EMPTY;",
        "}",
        "",
        "static unsigned long",
        "vmap_node_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < nr_vmap_nodes; i++)",
        "		decay_va_pool_node(&vmap_nodes[i], true);",
        "",
        "	return SHRINK_STOP;",
        "}",
        "",
        "void __init vmalloc_init(void)",
        "{",
        "	struct shrinker *vmap_node_shrinker;",
        "	struct vmap_area *va;",
        "	struct vmap_node *vn;",
        "	struct vm_struct *tmp;",
        "	int i;",
        "",
        "	/*",
        "	 * Create the cache for vmap_area objects.",
        "	 */",
        "	vmap_area_cachep = KMEM_CACHE(vmap_area, SLAB_PANIC);",
        "",
        "	for_each_possible_cpu(i) {",
        "		struct vmap_block_queue *vbq;",
        "		struct vfree_deferred *p;",
        "",
        "		vbq = &per_cpu(vmap_block_queue, i);",
        "		spin_lock_init(&vbq->lock);",
        "		INIT_LIST_HEAD(&vbq->free);",
        "		p = &per_cpu(vfree_deferred, i);",
        "		init_llist_head(&p->list);",
        "		INIT_WORK(&p->wq, delayed_vfree_work);",
        "		xa_init(&vbq->vmap_blocks);",
        "	}",
        "",
        "	/*",
        "	 * Setup nodes before importing vmlist.",
        "	 */",
        "	vmap_init_nodes();",
        "",
        "	/* Import existing vmlist entries. */",
        "	for (tmp = vmlist; tmp; tmp = tmp->next) {",
        "		va = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);",
        "		if (WARN_ON_ONCE(!va))",
        "			continue;",
        "",
        "		va->va_start = (unsigned long)tmp->addr;",
        "		va->va_end = va->va_start + tmp->size;",
        "		va->vm = tmp;",
        "",
        "		vn = addr_to_node(va->va_start);",
        "		insert_vmap_area(va, &vn->busy.root, &vn->busy.head);",
        "	}",
        "",
        "	/*",
        "	 * Now we can initialize a free vmap space.",
        "	 */",
        "	vmap_init_free_space();",
        "	vmap_initialized = true;",
        "",
        "	vmap_node_shrinker = shrinker_alloc(0, \"vmap-node\");",
        "	if (!vmap_node_shrinker) {",
        "		pr_err(\"Failed to allocate vmap-node shrinker!\\n\");",
        "		return;",
        "	}",
        "",
        "	vmap_node_shrinker->count_objects = vmap_node_shrink_count;",
        "	vmap_node_shrinker->scan_objects = vmap_node_shrink_scan;",
        "	shrinker_register(vmap_node_shrinker);",
        "}"
    ]
  },
  "include_linux_list_h": {
    path: "include/linux/list.h",
    covered: [982, 1026],
    totalLines: 1205,
    coveredCount: 2,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_LIST_H",
        "#define _LINUX_LIST_H",
        "",
        "#include <linux/container_of.h>",
        "#include <linux/types.h>",
        "#include <linux/stddef.h>",
        "#include <linux/poison.h>",
        "#include <linux/const.h>",
        "",
        "#include <asm/barrier.h>",
        "",
        "/*",
        " * Circular doubly linked list implementation.",
        " *",
        " * Some of the internal functions (\"__xxx\") are useful when",
        " * manipulating whole lists rather than single entries, as",
        " * sometimes we already know the next/prev entries and we can",
        " * generate better code by using them directly rather than",
        " * using the generic single-entry routines.",
        " */",
        "",
        "#define LIST_HEAD_INIT(name) { &(name), &(name) }",
        "",
        "#define LIST_HEAD(name) \\",
        "	struct list_head name = LIST_HEAD_INIT(name)",
        "",
        "/**",
        " * INIT_LIST_HEAD - Initialize a list_head structure",
        " * @list: list_head structure to be initialized.",
        " *",
        " * Initializes the list_head to point to itself.  If it is a list header,",
        " * the result is an empty list.",
        " */",
        "static inline void INIT_LIST_HEAD(struct list_head *list)",
        "{",
        "	WRITE_ONCE(list->next, list);",
        "	WRITE_ONCE(list->prev, list);",
        "}",
        "",
        "#ifdef CONFIG_LIST_HARDENED",
        "",
        "#ifdef CONFIG_DEBUG_LIST",
        "# define __list_valid_slowpath",
        "#else",
        "# define __list_valid_slowpath __cold __preserve_most",
        "#endif",
        "",
        "/*",
        " * Performs the full set of list corruption checks before __list_add().",
        " * On list corruption reports a warning, and returns false.",
        " */",
        "extern bool __list_valid_slowpath __list_add_valid_or_report(struct list_head *new,",
        "							     struct list_head *prev,",
        "							     struct list_head *next);",
        "",
        "/*",
        " * Performs list corruption checks before __list_add(). Returns false if a",
        " * corruption is detected, true otherwise.",
        " *",
        " * With CONFIG_LIST_HARDENED only, performs minimal list integrity checking",
        " * inline to catch non-faulting corruptions, and only if a corruption is",
        " * detected calls the reporting function __list_add_valid_or_report().",
        " */",
        "static __always_inline bool __list_add_valid(struct list_head *new,",
        "					     struct list_head *prev,",
        "					     struct list_head *next)",
        "{",
        "	bool ret = true;",
        "",
        "	if (!IS_ENABLED(CONFIG_DEBUG_LIST)) {",
        "		/*",
        "		 * With the hardening version, elide checking if next and prev",
        "		 * are NULL, since the immediate dereference of them below would",
        "		 * result in a fault if NULL.",
        "		 *",
        "		 * With the reduced set of checks, we can afford to inline the",
        "		 * checks, which also gives the compiler a chance to elide some",
        "		 * of them completely if they can be proven at compile-time. If",
        "		 * one of the pre-conditions does not hold, the slow-path will",
        "		 * show a report which pre-condition failed.",
        "		 */",
        "		if (likely(next->prev == prev && prev->next == next && new != prev && new != next))",
        "			return true;",
        "		ret = false;",
        "	}",
        "",
        "	ret &= __list_add_valid_or_report(new, prev, next);",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Performs the full set of list corruption checks before __list_del_entry().",
        " * On list corruption reports a warning, and returns false.",
        " */",
        "extern bool __list_valid_slowpath __list_del_entry_valid_or_report(struct list_head *entry);",
        "",
        "/*",
        " * Performs list corruption checks before __list_del_entry(). Returns false if a",
        " * corruption is detected, true otherwise.",
        " *",
        " * With CONFIG_LIST_HARDENED only, performs minimal list integrity checking",
        " * inline to catch non-faulting corruptions, and only if a corruption is",
        " * detected calls the reporting function __list_del_entry_valid_or_report().",
        " */",
        "static __always_inline bool __list_del_entry_valid(struct list_head *entry)",
        "{",
        "	bool ret = true;",
        "",
        "	if (!IS_ENABLED(CONFIG_DEBUG_LIST)) {",
        "		struct list_head *prev = entry->prev;",
        "		struct list_head *next = entry->next;",
        "",
        "		/*",
        "		 * With the hardening version, elide checking if next and prev",
        "		 * are NULL, LIST_POISON1 or LIST_POISON2, since the immediate",
        "		 * dereference of them below would result in a fault.",
        "		 */",
        "		if (likely(prev->next == entry && next->prev == entry))",
        "			return true;",
        "		ret = false;",
        "	}",
        "",
        "	ret &= __list_del_entry_valid_or_report(entry);",
        "	return ret;",
        "}",
        "#else",
        "static inline bool __list_add_valid(struct list_head *new,",
        "				struct list_head *prev,",
        "				struct list_head *next)",
        "{",
        "	return true;",
        "}",
        "static inline bool __list_del_entry_valid(struct list_head *entry)",
        "{",
        "	return true;",
        "}",
        "#endif",
        "",
        "/*",
        " * Insert a new entry between two known consecutive entries.",
        " *",
        " * This is only for internal list manipulation where we know",
        " * the prev/next entries already!",
        " */",
        "static inline void __list_add(struct list_head *new,",
        "			      struct list_head *prev,",
        "			      struct list_head *next)",
        "{",
        "	if (!__list_add_valid(new, prev, next))",
        "		return;",
        "",
        "	next->prev = new;",
        "	new->next = next;",
        "	new->prev = prev;",
        "	WRITE_ONCE(prev->next, new);",
        "}",
        "",
        "/**",
        " * list_add - add a new entry",
        " * @new: new entry to be added",
        " * @head: list head to add it after",
        " *",
        " * Insert a new entry after the specified head.",
        " * This is good for implementing stacks.",
        " */",
        "static inline void list_add(struct list_head *new, struct list_head *head)",
        "{",
        "	__list_add(new, head, head->next);",
        "}",
        "",
        "",
        "/**",
        " * list_add_tail - add a new entry",
        " * @new: new entry to be added",
        " * @head: list head to add it before",
        " *",
        " * Insert a new entry before the specified head.",
        " * This is useful for implementing queues.",
        " */",
        "static inline void list_add_tail(struct list_head *new, struct list_head *head)",
        "{",
        "	__list_add(new, head->prev, head);",
        "}",
        "",
        "/*",
        " * Delete a list entry by making the prev/next entries",
        " * point to each other.",
        " *",
        " * This is only for internal list manipulation where we know",
        " * the prev/next entries already!",
        " */",
        "static inline void __list_del(struct list_head * prev, struct list_head * next)",
        "{",
        "	next->prev = prev;",
        "	WRITE_ONCE(prev->next, next);",
        "}",
        "",
        "/*",
        " * Delete a list entry and clear the 'prev' pointer.",
        " *",
        " * This is a special-purpose list clearing method used in the networking code",
        " * for lists allocated as per-cpu, where we don't want to incur the extra",
        " * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this",
        " * needs to check the node 'prev' pointer instead of calling list_empty().",
        " */",
        "static inline void __list_del_clearprev(struct list_head *entry)",
        "{",
        "	__list_del(entry->prev, entry->next);",
        "	entry->prev = NULL;",
        "}",
        "",
        "static inline void __list_del_entry(struct list_head *entry)",
        "{",
        "	if (!__list_del_entry_valid(entry))",
        "		return;",
        "",
        "	__list_del(entry->prev, entry->next);",
        "}",
        "",
        "/**",
        " * list_del - deletes entry from list.",
        " * @entry: the element to delete from the list.",
        " * Note: list_empty() on entry does not return true after this, the entry is",
        " * in an undefined state.",
        " */",
        "static inline void list_del(struct list_head *entry)",
        "{",
        "	__list_del_entry(entry);",
        "	entry->next = LIST_POISON1;",
        "	entry->prev = LIST_POISON2;",
        "}",
        "",
        "/**",
        " * list_replace - replace old entry by new one",
        " * @old : the element to be replaced",
        " * @new : the new element to insert",
        " *",
        " * If @old was empty, it will be overwritten.",
        " */",
        "static inline void list_replace(struct list_head *old,",
        "				struct list_head *new)",
        "{",
        "	new->next = old->next;",
        "	new->next->prev = new;",
        "	new->prev = old->prev;",
        "	new->prev->next = new;",
        "}",
        "",
        "/**",
        " * list_replace_init - replace old entry by new one and initialize the old one",
        " * @old : the element to be replaced",
        " * @new : the new element to insert",
        " *",
        " * If @old was empty, it will be overwritten.",
        " */",
        "static inline void list_replace_init(struct list_head *old,",
        "				     struct list_head *new)",
        "{",
        "	list_replace(old, new);",
        "	INIT_LIST_HEAD(old);",
        "}",
        "",
        "/**",
        " * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position",
        " * @entry1: the location to place entry2",
        " * @entry2: the location to place entry1",
        " */",
        "static inline void list_swap(struct list_head *entry1,",
        "			     struct list_head *entry2)",
        "{",
        "	struct list_head *pos = entry2->prev;",
        "",
        "	list_del(entry2);",
        "	list_replace(entry1, entry2);",
        "	if (pos == entry1)",
        "		pos = entry2;",
        "	list_add(entry1, pos);",
        "}",
        "",
        "/**",
        " * list_del_init - deletes entry from list and reinitialize it.",
        " * @entry: the element to delete from the list.",
        " */",
        "static inline void list_del_init(struct list_head *entry)",
        "{",
        "	__list_del_entry(entry);",
        "	INIT_LIST_HEAD(entry);",
        "}",
        "",
        "/**",
        " * list_move - delete from one list and add as another's head",
        " * @list: the entry to move",
        " * @head: the head that will precede our entry",
        " */",
        "static inline void list_move(struct list_head *list, struct list_head *head)",
        "{",
        "	__list_del_entry(list);",
        "	list_add(list, head);",
        "}",
        "",
        "/**",
        " * list_move_tail - delete from one list and add as another's tail",
        " * @list: the entry to move",
        " * @head: the head that will follow our entry",
        " */",
        "static inline void list_move_tail(struct list_head *list,",
        "				  struct list_head *head)",
        "{",
        "	__list_del_entry(list);",
        "	list_add_tail(list, head);",
        "}",
        "",
        "/**",
        " * list_bulk_move_tail - move a subsection of a list to its tail",
        " * @head: the head that will follow our entry",
        " * @first: first entry to move",
        " * @last: last entry to move, can be the same as first",
        " *",
        " * Move all entries between @first and including @last before @head.",
        " * All three entries must belong to the same linked list.",
        " */",
        "static inline void list_bulk_move_tail(struct list_head *head,",
        "				       struct list_head *first,",
        "				       struct list_head *last)",
        "{",
        "	first->prev->next = last->next;",
        "	last->next->prev = first->prev;",
        "",
        "	head->prev->next = first;",
        "	first->prev = head->prev;",
        "",
        "	last->next = head;",
        "	head->prev = last;",
        "}",
        "",
        "/**",
        " * list_is_first -- tests whether @list is the first entry in list @head",
        " * @list: the entry to test",
        " * @head: the head of the list",
        " */",
        "static inline int list_is_first(const struct list_head *list, const struct list_head *head)",
        "{",
        "	return list->prev == head;",
        "}",
        "",
        "/**",
        " * list_is_last - tests whether @list is the last entry in list @head",
        " * @list: the entry to test",
        " * @head: the head of the list",
        " */",
        "static inline int list_is_last(const struct list_head *list, const struct list_head *head)",
        "{",
        "	return list->next == head;",
        "}",
        "",
        "/**",
        " * list_is_head - tests whether @list is the list @head",
        " * @list: the entry to test",
        " * @head: the head of the list",
        " */",
        "static inline int list_is_head(const struct list_head *list, const struct list_head *head)",
        "{",
        "	return list == head;",
        "}",
        "",
        "/**",
        " * list_empty - tests whether a list is empty",
        " * @head: the list to test.",
        " */",
        "static inline int list_empty(const struct list_head *head)",
        "{",
        "	return READ_ONCE(head->next) == head;",
        "}",
        "",
        "/**",
        " * list_del_init_careful - deletes entry from list and reinitialize it.",
        " * @entry: the element to delete from the list.",
        " *",
        " * This is the same as list_del_init(), except designed to be used",
        " * together with list_empty_careful() in a way to guarantee ordering",
        " * of other memory operations.",
        " *",
        " * Any memory operations done before a list_del_init_careful() are",
        " * guaranteed to be visible after a list_empty_careful() test.",
        " */",
        "static inline void list_del_init_careful(struct list_head *entry)",
        "{",
        "	__list_del_entry(entry);",
        "	WRITE_ONCE(entry->prev, entry);",
        "	smp_store_release(&entry->next, entry);",
        "}",
        "",
        "/**",
        " * list_empty_careful - tests whether a list is empty and not being modified",
        " * @head: the list to test",
        " *",
        " * Description:",
        " * tests whether a list is empty _and_ checks that no other CPU might be",
        " * in the process of modifying either member (next or prev)",
        " *",
        " * NOTE: using list_empty_careful() without synchronization",
        " * can only be safe if the only activity that can happen",
        " * to the list entry is list_del_init(). Eg. it cannot be used",
        " * if another CPU could re-list_add() it.",
        " */",
        "static inline int list_empty_careful(const struct list_head *head)",
        "{",
        "	struct list_head *next = smp_load_acquire(&head->next);",
        "	return list_is_head(next, head) && (next == READ_ONCE(head->prev));",
        "}",
        "",
        "/**",
        " * list_rotate_left - rotate the list to the left",
        " * @head: the head of the list",
        " */",
        "static inline void list_rotate_left(struct list_head *head)",
        "{",
        "	struct list_head *first;",
        "",
        "	if (!list_empty(head)) {",
        "		first = head->next;",
        "		list_move_tail(first, head);",
        "	}",
        "}",
        "",
        "/**",
        " * list_rotate_to_front() - Rotate list to specific item.",
        " * @list: The desired new front of the list.",
        " * @head: The head of the list.",
        " *",
        " * Rotates list so that @list becomes the new front of the list.",
        " */",
        "static inline void list_rotate_to_front(struct list_head *list,",
        "					struct list_head *head)",
        "{",
        "	/*",
        "	 * Deletes the list head from the list denoted by @head and",
        "	 * places it as the tail of @list, this effectively rotates the",
        "	 * list so that @list is at the front.",
        "	 */",
        "	list_move_tail(head, list);",
        "}",
        "",
        "/**",
        " * list_is_singular - tests whether a list has just one entry.",
        " * @head: the list to test.",
        " */",
        "static inline int list_is_singular(const struct list_head *head)",
        "{",
        "	return !list_empty(head) && (head->next == head->prev);",
        "}",
        "",
        "static inline void __list_cut_position(struct list_head *list,",
        "		struct list_head *head, struct list_head *entry)",
        "{",
        "	struct list_head *new_first = entry->next;",
        "	list->next = head->next;",
        "	list->next->prev = list;",
        "	list->prev = entry;",
        "	entry->next = list;",
        "	head->next = new_first;",
        "	new_first->prev = head;",
        "}",
        "",
        "/**",
        " * list_cut_position - cut a list into two",
        " * @list: a new list to add all removed entries",
        " * @head: a list with entries",
        " * @entry: an entry within head, could be the head itself",
        " *	and if so we won't cut the list",
        " *",
        " * This helper moves the initial part of @head, up to and",
        " * including @entry, from @head to @list. You should",
        " * pass on @entry an element you know is on @head. @list",
        " * should be an empty list or a list you do not care about",
        " * losing its data.",
        " *",
        " */",
        "static inline void list_cut_position(struct list_head *list,",
        "		struct list_head *head, struct list_head *entry)",
        "{",
        "	if (list_empty(head))",
        "		return;",
        "	if (list_is_singular(head) && !list_is_head(entry, head) && (entry != head->next))",
        "		return;",
        "	if (list_is_head(entry, head))",
        "		INIT_LIST_HEAD(list);",
        "	else",
        "		__list_cut_position(list, head, entry);",
        "}",
        "",
        "/**",
        " * list_cut_before - cut a list into two, before given entry",
        " * @list: a new list to add all removed entries",
        " * @head: a list with entries",
        " * @entry: an entry within head, could be the head itself",
        " *",
        " * This helper moves the initial part of @head, up to but",
        " * excluding @entry, from @head to @list.  You should pass",
        " * in @entry an element you know is on @head.  @list should",
        " * be an empty list or a list you do not care about losing",
        " * its data.",
        " * If @entry == @head, all entries on @head are moved to",
        " * @list.",
        " */",
        "static inline void list_cut_before(struct list_head *list,",
        "				   struct list_head *head,",
        "				   struct list_head *entry)",
        "{",
        "	if (head->next == entry) {",
        "		INIT_LIST_HEAD(list);",
        "		return;",
        "	}",
        "	list->next = head->next;",
        "	list->next->prev = list;",
        "	list->prev = entry->prev;",
        "	list->prev->next = list;",
        "	head->next = entry;",
        "	entry->prev = head;",
        "}",
        "",
        "static inline void __list_splice(const struct list_head *list,",
        "				 struct list_head *prev,",
        "				 struct list_head *next)",
        "{",
        "	struct list_head *first = list->next;",
        "	struct list_head *last = list->prev;",
        "",
        "	first->prev = prev;",
        "	prev->next = first;",
        "",
        "	last->next = next;",
        "	next->prev = last;",
        "}",
        "",
        "/**",
        " * list_splice - join two lists, this is designed for stacks",
        " * @list: the new list to add.",
        " * @head: the place to add it in the first list.",
        " */",
        "static inline void list_splice(const struct list_head *list,",
        "				struct list_head *head)",
        "{",
        "	if (!list_empty(list))",
        "		__list_splice(list, head, head->next);",
        "}",
        "",
        "/**",
        " * list_splice_tail - join two lists, each list being a queue",
        " * @list: the new list to add.",
        " * @head: the place to add it in the first list.",
        " */",
        "static inline void list_splice_tail(struct list_head *list,",
        "				struct list_head *head)",
        "{",
        "	if (!list_empty(list))",
        "		__list_splice(list, head->prev, head);",
        "}",
        "",
        "/**",
        " * list_splice_init - join two lists and reinitialise the emptied list.",
        " * @list: the new list to add.",
        " * @head: the place to add it in the first list.",
        " *",
        " * The list at @list is reinitialised",
        " */",
        "static inline void list_splice_init(struct list_head *list,",
        "				    struct list_head *head)",
        "{",
        "	if (!list_empty(list)) {",
        "		__list_splice(list, head, head->next);",
        "		INIT_LIST_HEAD(list);",
        "	}",
        "}",
        "",
        "/**",
        " * list_splice_tail_init - join two lists and reinitialise the emptied list",
        " * @list: the new list to add.",
        " * @head: the place to add it in the first list.",
        " *",
        " * Each of the lists is a queue.",
        " * The list at @list is reinitialised",
        " */",
        "static inline void list_splice_tail_init(struct list_head *list,",
        "					 struct list_head *head)",
        "{",
        "	if (!list_empty(list)) {",
        "		__list_splice(list, head->prev, head);",
        "		INIT_LIST_HEAD(list);",
        "	}",
        "}",
        "",
        "/**",
        " * list_entry - get the struct for this entry",
        " * @ptr:	the &struct list_head pointer.",
        " * @type:	the type of the struct this is embedded in.",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_entry(ptr, type, member) \\",
        "	container_of(ptr, type, member)",
        "",
        "/**",
        " * list_first_entry - get the first element from a list",
        " * @ptr:	the list head to take the element from.",
        " * @type:	the type of the struct this is embedded in.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Note, that list is expected to be not empty.",
        " */",
        "#define list_first_entry(ptr, type, member) \\",
        "	list_entry((ptr)->next, type, member)",
        "",
        "/**",
        " * list_last_entry - get the last element from a list",
        " * @ptr:	the list head to take the element from.",
        " * @type:	the type of the struct this is embedded in.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Note, that list is expected to be not empty.",
        " */",
        "#define list_last_entry(ptr, type, member) \\",
        "	list_entry((ptr)->prev, type, member)",
        "",
        "/**",
        " * list_first_entry_or_null - get the first element from a list",
        " * @ptr:	the list head to take the element from.",
        " * @type:	the type of the struct this is embedded in.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Note that if the list is empty, it returns NULL.",
        " */",
        "#define list_first_entry_or_null(ptr, type, member) ({ \\",
        "	struct list_head *head__ = (ptr); \\",
        "	struct list_head *pos__ = READ_ONCE(head__->next); \\",
        "	pos__ != head__ ? list_entry(pos__, type, member) : NULL; \\",
        "})",
        "",
        "/**",
        " * list_next_entry - get the next element in list",
        " * @pos:	the type * to cursor",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_next_entry(pos, member) \\",
        "	list_entry((pos)->member.next, typeof(*(pos)), member)",
        "",
        "/**",
        " * list_next_entry_circular - get the next element in list",
        " * @pos:	the type * to cursor.",
        " * @head:	the list head to take the element from.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Wraparound if pos is the last element (return the first element).",
        " * Note, that list is expected to be not empty.",
        " */",
        "#define list_next_entry_circular(pos, head, member) \\",
        "	(list_is_last(&(pos)->member, head) ? \\",
        "	list_first_entry(head, typeof(*(pos)), member) : list_next_entry(pos, member))",
        "",
        "/**",
        " * list_prev_entry - get the prev element in list",
        " * @pos:	the type * to cursor",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_prev_entry(pos, member) \\",
        "	list_entry((pos)->member.prev, typeof(*(pos)), member)",
        "",
        "/**",
        " * list_prev_entry_circular - get the prev element in list",
        " * @pos:	the type * to cursor.",
        " * @head:	the list head to take the element from.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Wraparound if pos is the first element (return the last element).",
        " * Note, that list is expected to be not empty.",
        " */",
        "#define list_prev_entry_circular(pos, head, member) \\",
        "	(list_is_first(&(pos)->member, head) ? \\",
        "	list_last_entry(head, typeof(*(pos)), member) : list_prev_entry(pos, member))",
        "",
        "/**",
        " * list_for_each	-	iterate over a list",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @head:	the head for your list.",
        " */",
        "#define list_for_each(pos, head) \\",
        "	for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)",
        "",
        "/**",
        " * list_for_each_rcu - Iterate over a list in an RCU-safe fashion",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @head:	the head for your list.",
        " */",
        "#define list_for_each_rcu(pos, head)		  \\",
        "	for (pos = rcu_dereference((head)->next); \\",
        "	     !list_is_head(pos, (head)); \\",
        "	     pos = rcu_dereference(pos->next))",
        "",
        "/**",
        " * list_for_each_continue - continue iteration over a list",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @head:	the head for your list.",
        " *",
        " * Continue to iterate over a list, continuing after the current position.",
        " */",
        "#define list_for_each_continue(pos, head) \\",
        "	for (pos = pos->next; !list_is_head(pos, (head)); pos = pos->next)",
        "",
        "/**",
        " * list_for_each_prev	-	iterate over a list backwards",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @head:	the head for your list.",
        " */",
        "#define list_for_each_prev(pos, head) \\",
        "	for (pos = (head)->prev; !list_is_head(pos, (head)); pos = pos->prev)",
        "",
        "/**",
        " * list_for_each_safe - iterate over a list safe against removal of list entry",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @n:		another &struct list_head to use as temporary storage",
        " * @head:	the head for your list.",
        " */",
        "#define list_for_each_safe(pos, n, head) \\",
        "	for (pos = (head)->next, n = pos->next; \\",
        "	     !list_is_head(pos, (head)); \\",
        "	     pos = n, n = pos->next)",
        "",
        "/**",
        " * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry",
        " * @pos:	the &struct list_head to use as a loop cursor.",
        " * @n:		another &struct list_head to use as temporary storage",
        " * @head:	the head for your list.",
        " */",
        "#define list_for_each_prev_safe(pos, n, head) \\",
        "	for (pos = (head)->prev, n = pos->prev; \\",
        "	     !list_is_head(pos, (head)); \\",
        "	     pos = n, n = pos->prev)",
        "",
        "/**",
        " * list_count_nodes - count nodes in the list",
        " * @head:	the head for your list.",
        " */",
        "static inline size_t list_count_nodes(struct list_head *head)",
        "{",
        "	struct list_head *pos;",
        "	size_t count = 0;",
        "",
        "	list_for_each(pos, head)",
        "		count++;",
        "",
        "	return count;",
        "}",
        "",
        "/**",
        " * list_entry_is_head - test if the entry points to the head of the list",
        " * @pos:	the type * to cursor",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_entry_is_head(pos, head, member)				\\",
        "	list_is_head(&pos->member, (head))",
        "",
        "/**",
        " * list_for_each_entry	-	iterate over list of given type",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_for_each_entry(pos, head, member)				\\",
        "	for (pos = list_first_entry(head, typeof(*pos), member);	\\",
        "	     !list_entry_is_head(pos, head, member);			\\",
        "	     pos = list_next_entry(pos, member))",
        "",
        "/**",
        " * list_for_each_entry_reverse - iterate backwards over list of given type.",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_for_each_entry_reverse(pos, head, member)			\\",
        "	for (pos = list_last_entry(head, typeof(*pos), member);		\\",
        "	     !list_entry_is_head(pos, head, member); 			\\",
        "	     pos = list_prev_entry(pos, member))",
        "",
        "/**",
        " * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()",
        " * @pos:	the type * to use as a start point",
        " * @head:	the head of the list",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Prepares a pos entry for use as a start point in list_for_each_entry_continue().",
        " */",
        "#define list_prepare_entry(pos, head, member) \\",
        "	((pos) ? : list_entry(head, typeof(*pos), member))",
        "",
        "/**",
        " * list_for_each_entry_continue - continue iteration over list of given type",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Continue to iterate over list of given type, continuing after",
        " * the current position.",
        " */",
        "#define list_for_each_entry_continue(pos, head, member) 		\\",
        "	for (pos = list_next_entry(pos, member);			\\",
        "	     !list_entry_is_head(pos, head, member);			\\",
        "	     pos = list_next_entry(pos, member))",
        "",
        "/**",
        " * list_for_each_entry_continue_reverse - iterate backwards from the given point",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Start to iterate over list of given type backwards, continuing after",
        " * the current position.",
        " */",
        "#define list_for_each_entry_continue_reverse(pos, head, member)		\\",
        "	for (pos = list_prev_entry(pos, member);			\\",
        "	     !list_entry_is_head(pos, head, member);			\\",
        "	     pos = list_prev_entry(pos, member))",
        "",
        "/**",
        " * list_for_each_entry_from - iterate over list of given type from the current point",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Iterate over list of given type, continuing from current position.",
        " */",
        "#define list_for_each_entry_from(pos, head, member) 			\\",
        "	for (; !list_entry_is_head(pos, head, member);			\\",
        "	     pos = list_next_entry(pos, member))",
        "",
        "/**",
        " * list_for_each_entry_from_reverse - iterate backwards over list of given type",
        " *                                    from the current point",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Iterate backwards over list of given type, continuing from current position.",
        " */",
        "#define list_for_each_entry_from_reverse(pos, head, member)		\\",
        "	for (; !list_entry_is_head(pos, head, member);			\\",
        "	     pos = list_prev_entry(pos, member))",
        "",
        "/**",
        " * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry",
        " * @pos:	the type * to use as a loop cursor.",
        " * @n:		another type * to use as temporary storage",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " */",
        "#define list_for_each_entry_safe(pos, n, head, member)			\\",
        "	for (pos = list_first_entry(head, typeof(*pos), member),	\\",
        "		n = list_next_entry(pos, member);			\\",
        "	     !list_entry_is_head(pos, head, member); 			\\",
        "	     pos = n, n = list_next_entry(n, member))",
        "",
        "/**",
        " * list_for_each_entry_safe_continue - continue list iteration safe against removal",
        " * @pos:	the type * to use as a loop cursor.",
        " * @n:		another type * to use as temporary storage",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Iterate over list of given type, continuing after current point,",
        " * safe against removal of list entry.",
        " */",
        "#define list_for_each_entry_safe_continue(pos, n, head, member) 		\\",
        "	for (pos = list_next_entry(pos, member), 				\\",
        "		n = list_next_entry(pos, member);				\\",
        "	     !list_entry_is_head(pos, head, member);				\\",
        "	     pos = n, n = list_next_entry(n, member))",
        "",
        "/**",
        " * list_for_each_entry_safe_from - iterate over list from current point safe against removal",
        " * @pos:	the type * to use as a loop cursor.",
        " * @n:		another type * to use as temporary storage",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Iterate over list of given type from current point, safe against",
        " * removal of list entry.",
        " */",
        "#define list_for_each_entry_safe_from(pos, n, head, member) 			\\",
        "	for (n = list_next_entry(pos, member);					\\",
        "	     !list_entry_is_head(pos, head, member);				\\",
        "	     pos = n, n = list_next_entry(n, member))",
        "",
        "/**",
        " * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal",
        " * @pos:	the type * to use as a loop cursor.",
        " * @n:		another type * to use as temporary storage",
        " * @head:	the head for your list.",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * Iterate backwards over list of given type, safe against removal",
        " * of list entry.",
        " */",
        "#define list_for_each_entry_safe_reverse(pos, n, head, member)		\\",
        "	for (pos = list_last_entry(head, typeof(*pos), member),		\\",
        "		n = list_prev_entry(pos, member);			\\",
        "	     !list_entry_is_head(pos, head, member); 			\\",
        "	     pos = n, n = list_prev_entry(n, member))",
        "",
        "/**",
        " * list_safe_reset_next - reset a stale list_for_each_entry_safe loop",
        " * @pos:	the loop cursor used in the list_for_each_entry_safe loop",
        " * @n:		temporary storage used in list_for_each_entry_safe",
        " * @member:	the name of the list_head within the struct.",
        " *",
        " * list_safe_reset_next is not safe to use in general if the list may be",
        " * modified concurrently (eg. the lock is dropped in the loop body). An",
        " * exception to this is if the cursor element (pos) is pinned in the list,",
        " * and list_safe_reset_next is called after re-taking the lock and before",
        " * completing the current iteration of the loop body.",
        " */",
        "#define list_safe_reset_next(pos, n, member)				\\",
        "	n = list_next_entry(pos, member)",
        "",
        "/*",
        " * Double linked lists with a single pointer list head.",
        " * Mostly useful for hash tables where the two pointer list head is",
        " * too wasteful.",
        " * You lose the ability to access the tail in O(1).",
        " */",
        "",
        "#define HLIST_HEAD_INIT { .first = NULL }",
        "#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }",
        "#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)",
        "static inline void INIT_HLIST_NODE(struct hlist_node *h)",
        "{",
        "	h->next = NULL;",
        "	h->pprev = NULL;",
        "}",
        "",
        "/**",
        " * hlist_unhashed - Has node been removed from list and reinitialized?",
        " * @h: Node to be checked",
        " *",
        " * Not that not all removal functions will leave a node in unhashed",
        " * state.  For example, hlist_nulls_del_init_rcu() does leave the",
        " * node in unhashed state, but hlist_nulls_del() does not.",
        " */",
        "static inline int hlist_unhashed(const struct hlist_node *h)",
        "{",
        "	return !h->pprev;",
        "}",
        "",
        "/**",
        " * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use",
        " * @h: Node to be checked",
        " *",
        " * This variant of hlist_unhashed() must be used in lockless contexts",
        " * to avoid potential load-tearing.  The READ_ONCE() is paired with the",
        " * various WRITE_ONCE() in hlist helpers that are defined below.",
        " */",
        "static inline int hlist_unhashed_lockless(const struct hlist_node *h)",
        "{",
        "	return !READ_ONCE(h->pprev);",
        "}",
        "",
        "/**",
        " * hlist_empty - Is the specified hlist_head structure an empty hlist?",
        " * @h: Structure to check.",
        " */",
        "static inline int hlist_empty(const struct hlist_head *h)",
        "{",
        "	return !READ_ONCE(h->first);",
        "}",
        "",
        "static inline void __hlist_del(struct hlist_node *n)",
        "{",
        "	struct hlist_node *next = n->next;",
        "	struct hlist_node **pprev = n->pprev;",
        "",
        "	WRITE_ONCE(*pprev, next);",
        "	if (next)",
        "		WRITE_ONCE(next->pprev, pprev);",
        "}",
        "",
        "/**",
        " * hlist_del - Delete the specified hlist_node from its list",
        " * @n: Node to delete.",
        " *",
        " * Note that this function leaves the node in hashed state.  Use",
        " * hlist_del_init() or similar instead to unhash @n.",
        " */",
        "static inline void hlist_del(struct hlist_node *n)",
        "{",
        "	__hlist_del(n);",
        "	n->next = LIST_POISON1;",
        "	n->pprev = LIST_POISON2;",
        "}",
        "",
        "/**",
        " * hlist_del_init - Delete the specified hlist_node from its list and initialize",
        " * @n: Node to delete.",
        " *",
        " * Note that this function leaves the node in unhashed state.",
        " */",
        "static inline void hlist_del_init(struct hlist_node *n)",
        "{",
        "	if (!hlist_unhashed(n)) {",
        "		__hlist_del(n);",
        "		INIT_HLIST_NODE(n);",
        "	}",
        "}",
        "",
        "/**",
        " * hlist_add_head - add a new entry at the beginning of the hlist",
        " * @n: new entry to be added",
        " * @h: hlist head to add it after",
        " *",
        " * Insert a new entry after the specified head.",
        " * This is good for implementing stacks.",
        " */",
        "static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)",
        "{",
        "	struct hlist_node *first = h->first;",
        "	WRITE_ONCE(n->next, first);",
        "	if (first)",
        "		WRITE_ONCE(first->pprev, &n->next);",
        "	WRITE_ONCE(h->first, n);",
        "	WRITE_ONCE(n->pprev, &h->first);",
        "}",
        "",
        "/**",
        " * hlist_add_before - add a new entry before the one specified",
        " * @n: new entry to be added",
        " * @next: hlist node to add it before, which must be non-NULL",
        " */",
        "static inline void hlist_add_before(struct hlist_node *n,",
        "				    struct hlist_node *next)",
        "{",
        "	WRITE_ONCE(n->pprev, next->pprev);",
        "	WRITE_ONCE(n->next, next);",
        "	WRITE_ONCE(next->pprev, &n->next);",
        "	WRITE_ONCE(*(n->pprev), n);",
        "}",
        "",
        "/**",
        " * hlist_add_behind - add a new entry after the one specified",
        " * @n: new entry to be added",
        " * @prev: hlist node to add it after, which must be non-NULL",
        " */",
        "static inline void hlist_add_behind(struct hlist_node *n,",
        "				    struct hlist_node *prev)",
        "{",
        "	WRITE_ONCE(n->next, prev->next);",
        "	WRITE_ONCE(prev->next, n);",
        "	WRITE_ONCE(n->pprev, &prev->next);",
        "",
        "	if (n->next)",
        "		WRITE_ONCE(n->next->pprev, &n->next);",
        "}",
        "",
        "/**",
        " * hlist_add_fake - create a fake hlist consisting of a single headless node",
        " * @n: Node to make a fake list out of",
        " *",
        " * This makes @n appear to be its own predecessor on a headless hlist.",
        " * The point of this is to allow things like hlist_del() to work correctly",
        " * in cases where there is no list.",
        " */",
        "static inline void hlist_add_fake(struct hlist_node *n)",
        "{",
        "	n->pprev = &n->next;",
        "}",
        "",
        "/**",
        " * hlist_fake: Is this node a fake hlist?",
        " * @h: Node to check for being a self-referential fake hlist.",
        " */",
        "static inline bool hlist_fake(struct hlist_node *h)",
        "{",
        "	return h->pprev == &h->next;",
        "}",
        "",
        "/**",
        " * hlist_is_singular_node - is node the only element of the specified hlist?",
        " * @n: Node to check for singularity.",
        " * @h: Header for potentially singular list.",
        " *",
        " * Check whether the node is the only node of the head without",
        " * accessing head, thus avoiding unnecessary cache misses.",
        " */",
        "static inline bool",
        "hlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)",
        "{",
        "	return !n->next && n->pprev == &h->first;",
        "}",
        "",
        "/**",
        " * hlist_move_list - Move an hlist",
        " * @old: hlist_head for old list.",
        " * @new: hlist_head for new list.",
        " *",
        " * Move a list from one list head to another. Fixup the pprev",
        " * reference of the first entry if it exists.",
        " */",
        "static inline void hlist_move_list(struct hlist_head *old,",
        "				   struct hlist_head *new)",
        "{",
        "	new->first = old->first;",
        "	if (new->first)",
        "		new->first->pprev = &new->first;",
        "	old->first = NULL;",
        "}",
        "",
        "/**",
        " * hlist_splice_init() - move all entries from one list to another",
        " * @from: hlist_head from which entries will be moved",
        " * @last: last entry on the @from list",
        " * @to:   hlist_head to which entries will be moved",
        " *",
        " * @to can be empty, @from must contain at least @last.",
        " */",
        "static inline void hlist_splice_init(struct hlist_head *from,",
        "				     struct hlist_node *last,",
        "				     struct hlist_head *to)",
        "{",
        "	if (to->first)",
        "		to->first->pprev = &last->next;",
        "	last->next = to->first;",
        "	to->first = from->first;",
        "	from->first->pprev = &to->first;",
        "	from->first = NULL;",
        "}",
        "",
        "#define hlist_entry(ptr, type, member) container_of(ptr,type,member)",
        "",
        "#define hlist_for_each(pos, head) \\",
        "	for (pos = (head)->first; pos ; pos = pos->next)",
        "",
        "#define hlist_for_each_safe(pos, n, head) \\",
        "	for (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\",
        "	     pos = n)",
        "",
        "#define hlist_entry_safe(ptr, type, member) \\",
        "	({ typeof(ptr) ____ptr = (ptr); \\",
        "	   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\",
        "	})",
        "",
        "/**",
        " * hlist_for_each_entry	- iterate over list of given type",
        " * @pos:	the type * to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the hlist_node within the struct.",
        " */",
        "#define hlist_for_each_entry(pos, head, member)				\\",
        "	for (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\",
        "	     pos;							\\",
        "	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))",
        "",
        "/**",
        " * hlist_for_each_entry_continue - iterate over a hlist continuing after current point",
        " * @pos:	the type * to use as a loop cursor.",
        " * @member:	the name of the hlist_node within the struct.",
        " */",
        "#define hlist_for_each_entry_continue(pos, member)			\\",
        "	for (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\",
        "	     pos;							\\",
        "	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))",
        "",
        "/**",
        " * hlist_for_each_entry_from - iterate over a hlist continuing from current point",
        " * @pos:	the type * to use as a loop cursor.",
        " * @member:	the name of the hlist_node within the struct.",
        " */",
        "#define hlist_for_each_entry_from(pos, member)				\\",
        "	for (; pos;							\\",
        "	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))",
        "",
        "/**",
        " * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry",
        " * @pos:	the type * to use as a loop cursor.",
        " * @n:		a &struct hlist_node to use as temporary storage",
        " * @head:	the head for your list.",
        " * @member:	the name of the hlist_node within the struct.",
        " */",
        "#define hlist_for_each_entry_safe(pos, n, head, member) 		\\",
        "	for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\",
        "	     pos && ({ n = pos->member.next; 1; });			\\",
        "	     pos = hlist_entry_safe(n, typeof(*pos), member))",
        "",
        "/**",
        " * hlist_count_nodes - count nodes in the hlist",
        " * @head:	the head for your hlist.",
        " */",
        "static inline size_t hlist_count_nodes(struct hlist_head *head)",
        "{",
        "	struct hlist_node *pos;",
        "	size_t count = 0;",
        "",
        "	hlist_for_each(pos, head)",
        "		count++;",
        "",
        "	return count;",
        "}",
        "",
        "#endif"
    ]
  },
  "kernel_time_timekeeping_c": {
    path: "kernel/time/timekeeping.c",
    covered: [802, 856, 335, 813, 854, 858, 817, 330, 845, 808, 811, 855, 815, 851],
    totalLines: 2636,
    coveredCount: 14,
    coveragePct: 0.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/*",
        " *  Kernel timekeeping code and accessor functions. Based on code from",
        " *  timer.c, moved in commit 8524070b7982.",
        " */",
        "#include <linux/timekeeper_internal.h>",
        "#include <linux/module.h>",
        "#include <linux/interrupt.h>",
        "#include <linux/percpu.h>",
        "#include <linux/init.h>",
        "#include <linux/mm.h>",
        "#include <linux/nmi.h>",
        "#include <linux/sched.h>",
        "#include <linux/sched/loadavg.h>",
        "#include <linux/sched/clock.h>",
        "#include <linux/syscore_ops.h>",
        "#include <linux/clocksource.h>",
        "#include <linux/jiffies.h>",
        "#include <linux/time.h>",
        "#include <linux/timex.h>",
        "#include <linux/tick.h>",
        "#include <linux/stop_machine.h>",
        "#include <linux/pvclock_gtod.h>",
        "#include <linux/compiler.h>",
        "#include <linux/audit.h>",
        "#include <linux/random.h>",
        "",
        "#include \"tick-internal.h\"",
        "#include \"ntp_internal.h\"",
        "#include \"timekeeping_internal.h\"",
        "",
        "#define TK_CLEAR_NTP		(1 << 0)",
        "#define TK_CLOCK_WAS_SET	(1 << 1)",
        "",
        "#define TK_UPDATE_ALL		(TK_CLEAR_NTP | TK_CLOCK_WAS_SET)",
        "",
        "enum timekeeping_adv_mode {",
        "	/* Update timekeeper when a tick has passed */",
        "	TK_ADV_TICK,",
        "",
        "	/* Update timekeeper on a direct frequency change */",
        "	TK_ADV_FREQ",
        "};",
        "",
        "/*",
        " * The most important data for readout fits into a single 64 byte",
        " * cache line.",
        " */",
        "struct tk_data {",
        "	seqcount_raw_spinlock_t	seq;",
        "	struct timekeeper	timekeeper;",
        "	struct timekeeper	shadow_timekeeper;",
        "	raw_spinlock_t		lock;",
        "} ____cacheline_aligned;",
        "",
        "static struct tk_data tk_core;",
        "",
        "/* flag for if timekeeping is suspended */",
        "int __read_mostly timekeeping_suspended;",
        "",
        "/**",
        " * struct tk_fast - NMI safe timekeeper",
        " * @seq:	Sequence counter for protecting updates. The lowest bit",
        " *		is the index for the tk_read_base array",
        " * @base:	tk_read_base array. Access is indexed by the lowest bit of",
        " *		@seq.",
        " *",
        " * See @update_fast_timekeeper() below.",
        " */",
        "struct tk_fast {",
        "	seqcount_latch_t	seq;",
        "	struct tk_read_base	base[2];",
        "};",
        "",
        "/* Suspend-time cycles value for halted fast timekeeper. */",
        "static u64 cycles_at_suspend;",
        "",
        "static u64 dummy_clock_read(struct clocksource *cs)",
        "{",
        "	if (timekeeping_suspended)",
        "		return cycles_at_suspend;",
        "	return local_clock();",
        "}",
        "",
        "static struct clocksource dummy_clock = {",
        "	.read = dummy_clock_read,",
        "};",
        "",
        "/*",
        " * Boot time initialization which allows local_clock() to be utilized",
        " * during early boot when clocksources are not available. local_clock()",
        " * returns nanoseconds already so no conversion is required, hence mult=1",
        " * and shift=0. When the first proper clocksource is installed then",
        " * the fast time keepers are updated with the correct values.",
        " */",
        "#define FAST_TK_INIT						\\",
        "	{							\\",
        "		.clock		= &dummy_clock,			\\",
        "		.mask		= CLOCKSOURCE_MASK(64),		\\",
        "		.mult		= 1,				\\",
        "		.shift		= 0,				\\",
        "	}",
        "",
        "static struct tk_fast tk_fast_mono ____cacheline_aligned = {",
        "	.seq     = SEQCNT_LATCH_ZERO(tk_fast_mono.seq),",
        "	.base[0] = FAST_TK_INIT,",
        "	.base[1] = FAST_TK_INIT,",
        "};",
        "",
        "static struct tk_fast tk_fast_raw  ____cacheline_aligned = {",
        "	.seq     = SEQCNT_LATCH_ZERO(tk_fast_raw.seq),",
        "	.base[0] = FAST_TK_INIT,",
        "	.base[1] = FAST_TK_INIT,",
        "};",
        "",
        "unsigned long timekeeper_lock_irqsave(void)",
        "{",
        "	unsigned long flags;",
        "",
        "	raw_spin_lock_irqsave(&tk_core.lock, flags);",
        "	return flags;",
        "}",
        "",
        "void timekeeper_unlock_irqrestore(unsigned long flags)",
        "{",
        "	raw_spin_unlock_irqrestore(&tk_core.lock, flags);",
        "}",
        "",
        "/*",
        " * Multigrain timestamps require tracking the latest fine-grained timestamp",
        " * that has been issued, and never returning a coarse-grained timestamp that is",
        " * earlier than that value.",
        " *",
        " * mg_floor represents the latest fine-grained time that has been handed out as",
        " * a file timestamp on the system. This is tracked as a monotonic ktime_t, and",
        " * converted to a realtime clock value on an as-needed basis.",
        " *",
        " * Maintaining mg_floor ensures the multigrain interfaces never issue a",
        " * timestamp earlier than one that has been previously issued.",
        " *",
        " * The exception to this rule is when there is a backward realtime clock jump. If",
        " * such an event occurs, a timestamp can appear to be earlier than a previous one.",
        " */",
        "static __cacheline_aligned_in_smp atomic64_t mg_floor;",
        "",
        "static inline void tk_normalize_xtime(struct timekeeper *tk)",
        "{",
        "	while (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {",
        "		tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;",
        "		tk->xtime_sec++;",
        "	}",
        "	while (tk->tkr_raw.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_raw.shift)) {",
        "		tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;",
        "		tk->raw_sec++;",
        "	}",
        "}",
        "",
        "static inline struct timespec64 tk_xtime(const struct timekeeper *tk)",
        "{",
        "	struct timespec64 ts;",
        "",
        "	ts.tv_sec = tk->xtime_sec;",
        "	ts.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);",
        "	return ts;",
        "}",
        "",
        "static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)",
        "{",
        "	tk->xtime_sec = ts->tv_sec;",
        "	tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;",
        "}",
        "",
        "static void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)",
        "{",
        "	tk->xtime_sec += ts->tv_sec;",
        "	tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;",
        "	tk_normalize_xtime(tk);",
        "}",
        "",
        "static void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)",
        "{",
        "	struct timespec64 tmp;",
        "",
        "	/*",
        "	 * Verify consistency of: offset_real = -wall_to_monotonic",
        "	 * before modifying anything",
        "	 */",
        "	set_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,",
        "					-tk->wall_to_monotonic.tv_nsec);",
        "	WARN_ON_ONCE(tk->offs_real != timespec64_to_ktime(tmp));",
        "	tk->wall_to_monotonic = wtm;",
        "	set_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);",
        "	/* Paired with READ_ONCE() in ktime_mono_to_any() */",
        "	WRITE_ONCE(tk->offs_real, timespec64_to_ktime(tmp));",
        "	WRITE_ONCE(tk->offs_tai, ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0)));",
        "}",
        "",
        "static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)",
        "{",
        "	/* Paired with READ_ONCE() in ktime_mono_to_any() */",
        "	WRITE_ONCE(tk->offs_boot, ktime_add(tk->offs_boot, delta));",
        "	/*",
        "	 * Timespec representation for VDSO update to avoid 64bit division",
        "	 * on every update.",
        "	 */",
        "	tk->monotonic_to_boot = ktime_to_timespec64(tk->offs_boot);",
        "}",
        "",
        "/*",
        " * tk_clock_read - atomic clocksource read() helper",
        " *",
        " * This helper is necessary to use in the read paths because, while the",
        " * seqcount ensures we don't return a bad value while structures are updated,",
        " * it doesn't protect from potential crashes. There is the possibility that",
        " * the tkr's clocksource may change between the read reference, and the",
        " * clock reference passed to the read function.  This can cause crashes if",
        " * the wrong clocksource is passed to the wrong read function.",
        " * This isn't necessary to use when holding the tk_core.lock or doing",
        " * a read of the fast-timekeeper tkrs (which is protected by its own locking",
        " * and update logic).",
        " */",
        "static inline u64 tk_clock_read(const struct tk_read_base *tkr)",
        "{",
        "	struct clocksource *clock = READ_ONCE(tkr->clock);",
        "",
        "	return clock->read(clock);",
        "}",
        "",
        "/**",
        " * tk_setup_internals - Set up internals to use clocksource clock.",
        " *",
        " * @tk:		The target timekeeper to setup.",
        " * @clock:		Pointer to clocksource.",
        " *",
        " * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment",
        " * pair and interval request.",
        " *",
        " * Unless you're the timekeeping code, you should not be using this!",
        " */",
        "static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)",
        "{",
        "	u64 interval;",
        "	u64 tmp, ntpinterval;",
        "	struct clocksource *old_clock;",
        "",
        "	++tk->cs_was_changed_seq;",
        "	old_clock = tk->tkr_mono.clock;",
        "	tk->tkr_mono.clock = clock;",
        "	tk->tkr_mono.mask = clock->mask;",
        "	tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);",
        "",
        "	tk->tkr_raw.clock = clock;",
        "	tk->tkr_raw.mask = clock->mask;",
        "	tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;",
        "",
        "	/* Do the ns -> cycle conversion first, using original mult */",
        "	tmp = NTP_INTERVAL_LENGTH;",
        "	tmp <<= clock->shift;",
        "	ntpinterval = tmp;",
        "	tmp += clock->mult/2;",
        "	do_div(tmp, clock->mult);",
        "	if (tmp == 0)",
        "		tmp = 1;",
        "",
        "	interval = (u64) tmp;",
        "	tk->cycle_interval = interval;",
        "",
        "	/* Go back from cycles -> shifted ns */",
        "	tk->xtime_interval = interval * clock->mult;",
        "	tk->xtime_remainder = ntpinterval - tk->xtime_interval;",
        "	tk->raw_interval = interval * clock->mult;",
        "",
        "	 /* if changing clocks, convert xtime_nsec shift units */",
        "	if (old_clock) {",
        "		int shift_change = clock->shift - old_clock->shift;",
        "		if (shift_change < 0) {",
        "			tk->tkr_mono.xtime_nsec >>= -shift_change;",
        "			tk->tkr_raw.xtime_nsec >>= -shift_change;",
        "		} else {",
        "			tk->tkr_mono.xtime_nsec <<= shift_change;",
        "			tk->tkr_raw.xtime_nsec <<= shift_change;",
        "		}",
        "	}",
        "",
        "	tk->tkr_mono.shift = clock->shift;",
        "	tk->tkr_raw.shift = clock->shift;",
        "",
        "	tk->ntp_error = 0;",
        "	tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;",
        "	tk->ntp_tick = ntpinterval << tk->ntp_error_shift;",
        "",
        "	/*",
        "	 * The timekeeper keeps its own mult values for the currently",
        "	 * active clocksource. These value will be adjusted via NTP",
        "	 * to counteract clock drifting.",
        "	 */",
        "	tk->tkr_mono.mult = clock->mult;",
        "	tk->tkr_raw.mult = clock->mult;",
        "	tk->ntp_err_mult = 0;",
        "	tk->skip_second_overflow = 0;",
        "}",
        "",
        "/* Timekeeper helper functions. */",
        "static noinline u64 delta_to_ns_safe(const struct tk_read_base *tkr, u64 delta)",
        "{",
        "	return mul_u64_u32_add_u64_shr(delta, tkr->mult, tkr->xtime_nsec, tkr->shift);",
        "}",
        "",
        "static inline u64 timekeeping_cycles_to_ns(const struct tk_read_base *tkr, u64 cycles)",
        "{",
        "	/* Calculate the delta since the last update_wall_time() */",
        "	u64 mask = tkr->mask, delta = (cycles - tkr->cycle_last) & mask;",
        "",
        "	/*",
        "	 * This detects both negative motion and the case where the delta",
        "	 * overflows the multiplication with tkr->mult.",
        "	 */",
        "	if (unlikely(delta > tkr->clock->max_cycles)) {",
        "		/*",
        "		 * Handle clocksource inconsistency between CPUs to prevent",
        "		 * time from going backwards by checking for the MSB of the",
        "		 * mask being set in the delta.",
        "		 */",
        "		if (delta & ~(mask >> 1))",
        "			return tkr->xtime_nsec >> tkr->shift;",
        "",
        "		return delta_to_ns_safe(tkr, delta);",
        "	}",
        "",
        "	return ((delta * tkr->mult) + tkr->xtime_nsec) >> tkr->shift;",
        "}",
        "",
        "static __always_inline u64 timekeeping_get_ns(const struct tk_read_base *tkr)",
        "{",
        "	return timekeeping_cycles_to_ns(tkr, tk_clock_read(tkr));",
        "}",
        "",
        "/**",
        " * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.",
        " * @tkr: Timekeeping readout base from which we take the update",
        " * @tkf: Pointer to NMI safe timekeeper",
        " *",
        " * We want to use this from any context including NMI and tracing /",
        " * instrumenting the timekeeping code itself.",
        " *",
        " * Employ the latch technique; see @write_seqcount_latch.",
        " *",
        " * So if a NMI hits the update of base[0] then it will use base[1]",
        " * which is still consistent. In the worst case this can result is a",
        " * slightly wrong timestamp (a few nanoseconds). See",
        " * @ktime_get_mono_fast_ns.",
        " */",
        "static void update_fast_timekeeper(const struct tk_read_base *tkr,",
        "				   struct tk_fast *tkf)",
        "{",
        "	struct tk_read_base *base = tkf->base;",
        "",
        "	/* Force readers off to base[1] */",
        "	write_seqcount_latch_begin(&tkf->seq);",
        "",
        "	/* Update base[0] */",
        "	memcpy(base, tkr, sizeof(*base));",
        "",
        "	/* Force readers back to base[0] */",
        "	write_seqcount_latch(&tkf->seq);",
        "",
        "	/* Update base[1] */",
        "	memcpy(base + 1, base, sizeof(*base));",
        "",
        "	write_seqcount_latch_end(&tkf->seq);",
        "}",
        "",
        "static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)",
        "{",
        "	struct tk_read_base *tkr;",
        "	unsigned int seq;",
        "	u64 now;",
        "",
        "	do {",
        "		seq = read_seqcount_latch(&tkf->seq);",
        "		tkr = tkf->base + (seq & 0x01);",
        "		now = ktime_to_ns(tkr->base);",
        "		now += timekeeping_get_ns(tkr);",
        "	} while (read_seqcount_latch_retry(&tkf->seq, seq));",
        "",
        "	return now;",
        "}",
        "",
        "/**",
        " * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic",
        " *",
        " * This timestamp is not guaranteed to be monotonic across an update.",
        " * The timestamp is calculated by:",
        " *",
        " *	now = base_mono + clock_delta * slope",
        " *",
        " * So if the update lowers the slope, readers who are forced to the",
        " * not yet updated second array are still using the old steeper slope.",
        " *",
        " * tmono",
        " * ^",
        " * |    o  n",
        " * |   o n",
        " * |  u",
        " * | o",
        " * |o",
        " * |12345678---> reader order",
        " *",
        " * o = old slope",
        " * u = update",
        " * n = new slope",
        " *",
        " * So reader 6 will observe time going backwards versus reader 5.",
        " *",
        " * While other CPUs are likely to be able to observe that, the only way",
        " * for a CPU local observation is when an NMI hits in the middle of",
        " * the update. Timestamps taken from that NMI context might be ahead",
        " * of the following timestamps. Callers need to be aware of that and",
        " * deal with it.",
        " */",
        "u64 notrace ktime_get_mono_fast_ns(void)",
        "{",
        "	return __ktime_get_fast_ns(&tk_fast_mono);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);",
        "",
        "/**",
        " * ktime_get_raw_fast_ns - Fast NMI safe access to clock monotonic raw",
        " *",
        " * Contrary to ktime_get_mono_fast_ns() this is always correct because the",
        " * conversion factor is not affected by NTP/PTP correction.",
        " */",
        "u64 notrace ktime_get_raw_fast_ns(void)",
        "{",
        "	return __ktime_get_fast_ns(&tk_fast_raw);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);",
        "",
        "/**",
        " * ktime_get_boot_fast_ns - NMI safe and fast access to boot clock.",
        " *",
        " * To keep it NMI safe since we're accessing from tracing, we're not using a",
        " * separate timekeeper with updates to monotonic clock and boot offset",
        " * protected with seqcounts. This has the following minor side effects:",
        " *",
        " * (1) Its possible that a timestamp be taken after the boot offset is updated",
        " * but before the timekeeper is updated. If this happens, the new boot offset",
        " * is added to the old timekeeping making the clock appear to update slightly",
        " * earlier:",
        " *    CPU 0                                        CPU 1",
        " *    timekeeping_inject_sleeptime64()",
        " *    __timekeeping_inject_sleeptime(tk, delta);",
        " *                                                 timestamp();",
        " *    timekeeping_update_staged(tkd, TK_CLEAR_NTP...);",
        " *",
        " * (2) On 32-bit systems, the 64-bit boot offset (tk->offs_boot) may be",
        " * partially updated.  Since the tk->offs_boot update is a rare event, this",
        " * should be a rare occurrence which postprocessing should be able to handle.",
        " *",
        " * The caveats vs. timestamp ordering as documented for ktime_get_mono_fast_ns()",
        " * apply as well.",
        " */",
        "u64 notrace ktime_get_boot_fast_ns(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	return (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_boot)));",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_boot_fast_ns);",
        "",
        "/**",
        " * ktime_get_tai_fast_ns - NMI safe and fast access to tai clock.",
        " *",
        " * The same limitations as described for ktime_get_boot_fast_ns() apply. The",
        " * mono time and the TAI offset are not read atomically which may yield wrong",
        " * readouts. However, an update of the TAI offset is an rare event e.g., caused",
        " * by settime or adjtimex with an offset. The user of this function has to deal",
        " * with the possibility of wrong timestamps in post processing.",
        " */",
        "u64 notrace ktime_get_tai_fast_ns(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	return (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_tai)));",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_tai_fast_ns);",
        "",
        "static __always_inline u64 __ktime_get_real_fast(struct tk_fast *tkf, u64 *mono)",
        "{",
        "	struct tk_read_base *tkr;",
        "	u64 basem, baser, delta;",
        "	unsigned int seq;",
        "",
        "	do {",
        "		seq = raw_read_seqcount_latch(&tkf->seq);",
        "		tkr = tkf->base + (seq & 0x01);",
        "		basem = ktime_to_ns(tkr->base);",
        "		baser = ktime_to_ns(tkr->base_real);",
        "		delta = timekeeping_get_ns(tkr);",
        "	} while (raw_read_seqcount_latch_retry(&tkf->seq, seq));",
        "",
        "	if (mono)",
        "		*mono = basem + delta;",
        "	return baser + delta;",
        "}",
        "",
        "/**",
        " * ktime_get_real_fast_ns: - NMI safe and fast access to clock realtime.",
        " *",
        " * See ktime_get_mono_fast_ns() for documentation of the time stamp ordering.",
        " */",
        "u64 ktime_get_real_fast_ns(void)",
        "{",
        "	return __ktime_get_real_fast(&tk_fast_mono, NULL);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_real_fast_ns);",
        "",
        "/**",
        " * ktime_get_fast_timestamps: - NMI safe timestamps",
        " * @snapshot:	Pointer to timestamp storage",
        " *",
        " * Stores clock monotonic, boottime and realtime timestamps.",
        " *",
        " * Boot time is a racy access on 32bit systems if the sleep time injection",
        " * happens late during resume and not in timekeeping_resume(). That could",
        " * be avoided by expanding struct tk_read_base with boot offset for 32bit",
        " * and adding more overhead to the update. As this is a hard to observe",
        " * once per resume event which can be filtered with reasonable effort using",
        " * the accurate mono/real timestamps, it's probably not worth the trouble.",
        " *",
        " * Aside of that it might be possible on 32 and 64 bit to observe the",
        " * following when the sleep time injection happens late:",
        " *",
        " * CPU 0				CPU 1",
        " * timekeeping_resume()",
        " * ktime_get_fast_timestamps()",
        " *	mono, real = __ktime_get_real_fast()",
        " *					inject_sleep_time()",
        " *					   update boot offset",
        " *	boot = mono + bootoffset;",
        " *",
        " * That means that boot time already has the sleep time adjustment, but",
        " * real time does not. On the next readout both are in sync again.",
        " *",
        " * Preventing this for 64bit is not really feasible without destroying the",
        " * careful cache layout of the timekeeper because the sequence count and",
        " * struct tk_read_base would then need two cache lines instead of one.",
        " *",
        " * Access to the time keeper clock source is disabled across the innermost",
        " * steps of suspend/resume. The accessors still work, but the timestamps",
        " * are frozen until time keeping is resumed which happens very early.",
        " *",
        " * For regular suspend/resume there is no observable difference vs. sched",
        " * clock, but it might affect some of the nasty low level debug printks.",
        " *",
        " * OTOH, access to sched clock is not guaranteed across suspend/resume on",
        " * all systems either so it depends on the hardware in use.",
        " *",
        " * If that turns out to be a real problem then this could be mitigated by",
        " * using sched clock in a similar way as during early boot. But it's not as",
        " * trivial as on early boot because it needs some careful protection",
        " * against the clock monotonic timestamp jumping backwards on resume.",
        " */",
        "void ktime_get_fast_timestamps(struct ktime_timestamps *snapshot)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	snapshot->real = __ktime_get_real_fast(&tk_fast_mono, &snapshot->mono);",
        "	snapshot->boot = snapshot->mono + ktime_to_ns(data_race(tk->offs_boot));",
        "}",
        "",
        "/**",
        " * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.",
        " * @tk: Timekeeper to snapshot.",
        " *",
        " * It generally is unsafe to access the clocksource after timekeeping has been",
        " * suspended, so take a snapshot of the readout base of @tk and use it as the",
        " * fast timekeeper's readout base while suspended.  It will return the same",
        " * number of cycles every time until timekeeping is resumed at which time the",
        " * proper readout base for the fast timekeeper will be restored automatically.",
        " */",
        "static void halt_fast_timekeeper(const struct timekeeper *tk)",
        "{",
        "	static struct tk_read_base tkr_dummy;",
        "	const struct tk_read_base *tkr = &tk->tkr_mono;",
        "",
        "	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));",
        "	cycles_at_suspend = tk_clock_read(tkr);",
        "	tkr_dummy.clock = &dummy_clock;",
        "	tkr_dummy.base_real = tkr->base + tk->offs_real;",
        "	update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);",
        "",
        "	tkr = &tk->tkr_raw;",
        "	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));",
        "	tkr_dummy.clock = &dummy_clock;",
        "	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);",
        "}",
        "",
        "static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);",
        "",
        "static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)",
        "{",
        "	raw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);",
        "}",
        "",
        "/**",
        " * pvclock_gtod_register_notifier - register a pvclock timedata update listener",
        " * @nb: Pointer to the notifier block to register",
        " */",
        "int pvclock_gtod_register_notifier(struct notifier_block *nb)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	int ret;",
        "",
        "	guard(raw_spinlock_irqsave)(&tk_core.lock);",
        "	ret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);",
        "	update_pvclock_gtod(tk, true);",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(pvclock_gtod_register_notifier);",
        "",
        "/**",
        " * pvclock_gtod_unregister_notifier - unregister a pvclock",
        " * timedata update listener",
        " * @nb: Pointer to the notifier block to unregister",
        " */",
        "int pvclock_gtod_unregister_notifier(struct notifier_block *nb)",
        "{",
        "	guard(raw_spinlock_irqsave)(&tk_core.lock);",
        "	return raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);",
        "}",
        "EXPORT_SYMBOL_GPL(pvclock_gtod_unregister_notifier);",
        "",
        "/*",
        " * tk_update_leap_state - helper to update the next_leap_ktime",
        " */",
        "static inline void tk_update_leap_state(struct timekeeper *tk)",
        "{",
        "	tk->next_leap_ktime = ntp_get_next_leap();",
        "	if (tk->next_leap_ktime != KTIME_MAX)",
        "		/* Convert to monotonic time */",
        "		tk->next_leap_ktime = ktime_sub(tk->next_leap_ktime, tk->offs_real);",
        "}",
        "",
        "/*",
        " * Leap state update for both shadow and the real timekeeper",
        " * Separate to spare a full memcpy() of the timekeeper.",
        " */",
        "static void tk_update_leap_state_all(struct tk_data *tkd)",
        "{",
        "	write_seqcount_begin(&tkd->seq);",
        "	tk_update_leap_state(&tkd->shadow_timekeeper);",
        "	tkd->timekeeper.next_leap_ktime = tkd->shadow_timekeeper.next_leap_ktime;",
        "	write_seqcount_end(&tkd->seq);",
        "}",
        "",
        "/*",
        " * Update the ktime_t based scalar nsec members of the timekeeper",
        " */",
        "static inline void tk_update_ktime_data(struct timekeeper *tk)",
        "{",
        "	u64 seconds;",
        "	u32 nsec;",
        "",
        "	/*",
        "	 * The xtime based monotonic readout is:",
        "	 *	nsec = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec + now();",
        "	 * The ktime based monotonic readout is:",
        "	 *	nsec = base_mono + now();",
        "	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec",
        "	 */",
        "	seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);",
        "	nsec = (u32) tk->wall_to_monotonic.tv_nsec;",
        "	tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);",
        "",
        "	/*",
        "	 * The sum of the nanoseconds portions of xtime and",
        "	 * wall_to_monotonic can be greater/equal one second. Take",
        "	 * this into account before updating tk->ktime_sec.",
        "	 */",
        "	nsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);",
        "	if (nsec >= NSEC_PER_SEC)",
        "		seconds++;",
        "	tk->ktime_sec = seconds;",
        "",
        "	/* Update the monotonic raw base */",
        "	tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);",
        "}",
        "",
        "/*",
        " * Restore the shadow timekeeper from the real timekeeper.",
        " */",
        "static void timekeeping_restore_shadow(struct tk_data *tkd)",
        "{",
        "	lockdep_assert_held(&tkd->lock);",
        "	memcpy(&tkd->shadow_timekeeper, &tkd->timekeeper, sizeof(tkd->timekeeper));",
        "}",
        "",
        "static void timekeeping_update_from_shadow(struct tk_data *tkd, unsigned int action)",
        "{",
        "	struct timekeeper *tk = &tk_core.shadow_timekeeper;",
        "",
        "	lockdep_assert_held(&tkd->lock);",
        "",
        "	/*",
        "	 * Block out readers before running the updates below because that",
        "	 * updates VDSO and other time related infrastructure. Not blocking",
        "	 * the readers might let a reader see time going backwards when",
        "	 * reading from the VDSO after the VDSO update and then reading in",
        "	 * the kernel from the timekeeper before that got updated.",
        "	 */",
        "	write_seqcount_begin(&tkd->seq);",
        "",
        "	if (action & TK_CLEAR_NTP) {",
        "		tk->ntp_error = 0;",
        "		ntp_clear();",
        "	}",
        "",
        "	tk_update_leap_state(tk);",
        "	tk_update_ktime_data(tk);",
        "",
        "	update_vsyscall(tk);",
        "	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);",
        "",
        "	tk->tkr_mono.base_real = tk->tkr_mono.base + tk->offs_real;",
        "	update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);",
        "	update_fast_timekeeper(&tk->tkr_raw,  &tk_fast_raw);",
        "",
        "	if (action & TK_CLOCK_WAS_SET)",
        "		tk->clock_was_set_seq++;",
        "",
        "	/*",
        "	 * Update the real timekeeper.",
        "	 *",
        "	 * We could avoid this memcpy() by switching pointers, but that has",
        "	 * the downside that the reader side does not longer benefit from",
        "	 * the cacheline optimized data layout of the timekeeper and requires",
        "	 * another indirection.",
        "	 */",
        "	memcpy(&tkd->timekeeper, tk, sizeof(*tk));",
        "	write_seqcount_end(&tkd->seq);",
        "}",
        "",
        "/**",
        " * timekeeping_forward_now - update clock to the current time",
        " * @tk:		Pointer to the timekeeper to update",
        " *",
        " * Forward the current clock to update its state since the last call to",
        " * update_wall_time(). This is useful before significant clock changes,",
        " * as it avoids having to deal with this time offset explicitly.",
        " */",
        "static void timekeeping_forward_now(struct timekeeper *tk)",
        "{",
        "	u64 cycle_now, delta;",
        "",
        "	cycle_now = tk_clock_read(&tk->tkr_mono);",
        "	delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask,",
        "				  tk->tkr_mono.clock->max_raw_delta);",
        "	tk->tkr_mono.cycle_last = cycle_now;",
        "	tk->tkr_raw.cycle_last  = cycle_now;",
        "",
        "	while (delta > 0) {",
        "		u64 max = tk->tkr_mono.clock->max_cycles;",
        "		u64 incr = delta < max ? delta : max;",
        "",
        "		tk->tkr_mono.xtime_nsec += incr * tk->tkr_mono.mult;",
        "		tk->tkr_raw.xtime_nsec += incr * tk->tkr_raw.mult;",
        "		tk_normalize_xtime(tk);",
        "		delta -= incr;",
        "	}",
        "}",
        "",
        "/**",
        " * ktime_get_real_ts64 - Returns the time of day in a timespec64.",
        " * @ts:		pointer to the timespec to be set",
        " *",
        " * Returns the time of day in a timespec64 (WARN if suspended).",
        " */",
        "void ktime_get_real_ts64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	u64 nsecs;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		ts->tv_sec = tk->xtime_sec;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_mono);",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	ts->tv_nsec = 0;",
        "	timespec64_add_ns(ts, nsecs);",
        "}",
        "EXPORT_SYMBOL(ktime_get_real_ts64);",
        "",
        "ktime_t ktime_get(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base;",
        "	u64 nsecs;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		base = tk->tkr_mono.base;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_mono);",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ktime_add_ns(base, nsecs);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get);",
        "",
        "u32 ktime_get_resolution_ns(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	u32 nsecs;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		nsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return nsecs;",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_resolution_ns);",
        "",
        "static ktime_t *offsets[TK_OFFS_MAX] = {",
        "	[TK_OFFS_REAL]	= &tk_core.timekeeper.offs_real,",
        "	[TK_OFFS_BOOT]	= &tk_core.timekeeper.offs_boot,",
        "	[TK_OFFS_TAI]	= &tk_core.timekeeper.offs_tai,",
        "};",
        "",
        "ktime_t ktime_get_with_offset(enum tk_offsets offs)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base, *offset = offsets[offs];",
        "	u64 nsecs;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		base = ktime_add(tk->tkr_mono.base, *offset);",
        "		nsecs = timekeeping_get_ns(&tk->tkr_mono);",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ktime_add_ns(base, nsecs);",
        "",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_with_offset);",
        "",
        "ktime_t ktime_get_coarse_with_offset(enum tk_offsets offs)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base, *offset = offsets[offs];",
        "	u64 nsecs;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		base = ktime_add(tk->tkr_mono.base, *offset);",
        "		nsecs = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ktime_add_ns(base, nsecs);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_coarse_with_offset);",
        "",
        "/**",
        " * ktime_mono_to_any() - convert monotonic time to any other time",
        " * @tmono:	time to convert.",
        " * @offs:	which offset to use",
        " */",
        "ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)",
        "{",
        "	ktime_t *offset = offsets[offs];",
        "	unsigned int seq;",
        "	ktime_t tconv;",
        "",
        "	if (IS_ENABLED(CONFIG_64BIT)) {",
        "		/*",
        "		 * Paired with WRITE_ONCE()s in tk_set_wall_to_mono() and",
        "		 * tk_update_sleep_time().",
        "		 */",
        "		return ktime_add(tmono, READ_ONCE(*offset));",
        "	}",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		tconv = ktime_add(tmono, *offset);",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return tconv;",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_mono_to_any);",
        "",
        "/**",
        " * ktime_get_raw - Returns the raw monotonic time in ktime_t format",
        " */",
        "ktime_t ktime_get_raw(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base;",
        "	u64 nsecs;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		base = tk->tkr_raw.base;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_raw);",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ktime_add_ns(base, nsecs);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_raw);",
        "",
        "/**",
        " * ktime_get_ts64 - get the monotonic clock in timespec64 format",
        " * @ts:		pointer to timespec variable",
        " *",
        " * The function calculates the monotonic clock from the realtime",
        " * clock and the wall_to_monotonic offset and stores the result",
        " * in normalized timespec64 format in the variable pointed to by @ts.",
        " */",
        "void ktime_get_ts64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	struct timespec64 tomono;",
        "	unsigned int seq;",
        "	u64 nsec;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		ts->tv_sec = tk->xtime_sec;",
        "		nsec = timekeeping_get_ns(&tk->tkr_mono);",
        "		tomono = tk->wall_to_monotonic;",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	ts->tv_sec += tomono.tv_sec;",
        "	ts->tv_nsec = 0;",
        "	timespec64_add_ns(ts, nsec + tomono.tv_nsec);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_ts64);",
        "",
        "/**",
        " * ktime_get_seconds - Get the seconds portion of CLOCK_MONOTONIC",
        " *",
        " * Returns the seconds portion of CLOCK_MONOTONIC with a single non",
        " * serialized read. tk->ktime_sec is of type 'unsigned long' so this",
        " * works on both 32 and 64 bit systems. On 32 bit systems the readout",
        " * covers ~136 years of uptime which should be enough to prevent",
        " * premature wrap arounds.",
        " */",
        "time64_t ktime_get_seconds(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	WARN_ON(timekeeping_suspended);",
        "	return tk->ktime_sec;",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_seconds);",
        "",
        "/**",
        " * ktime_get_real_seconds - Get the seconds portion of CLOCK_REALTIME",
        " *",
        " * Returns the wall clock seconds since 1970.",
        " *",
        " * For 64bit systems the fast access to tk->xtime_sec is preserved. On",
        " * 32bit systems the access must be protected with the sequence",
        " * counter to provide \"atomic\" access to the 64bit tk->xtime_sec",
        " * value.",
        " */",
        "time64_t ktime_get_real_seconds(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	time64_t seconds;",
        "	unsigned int seq;",
        "",
        "	if (IS_ENABLED(CONFIG_64BIT))",
        "		return tk->xtime_sec;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		seconds = tk->xtime_sec;",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return seconds;",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_real_seconds);",
        "",
        "/**",
        " * __ktime_get_real_seconds - The same as ktime_get_real_seconds",
        " * but without the sequence counter protect. This internal function",
        " * is called just when timekeeping lock is already held.",
        " */",
        "noinstr time64_t __ktime_get_real_seconds(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	return tk->xtime_sec;",
        "}",
        "",
        "/**",
        " * ktime_get_snapshot - snapshots the realtime/monotonic raw clocks with counter",
        " * @systime_snapshot:	pointer to struct receiving the system time snapshot",
        " */",
        "void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base_raw;",
        "	ktime_t base_real;",
        "	ktime_t base_boot;",
        "	u64 nsec_raw;",
        "	u64 nsec_real;",
        "	u64 now;",
        "",
        "	WARN_ON_ONCE(timekeeping_suspended);",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		now = tk_clock_read(&tk->tkr_mono);",
        "		systime_snapshot->cs_id = tk->tkr_mono.clock->id;",
        "		systime_snapshot->cs_was_changed_seq = tk->cs_was_changed_seq;",
        "		systime_snapshot->clock_was_set_seq = tk->clock_was_set_seq;",
        "		base_real = ktime_add(tk->tkr_mono.base,",
        "				      tk_core.timekeeper.offs_real);",
        "		base_boot = ktime_add(tk->tkr_mono.base,",
        "				      tk_core.timekeeper.offs_boot);",
        "		base_raw = tk->tkr_raw.base;",
        "		nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);",
        "		nsec_raw  = timekeeping_cycles_to_ns(&tk->tkr_raw, now);",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	systime_snapshot->cycles = now;",
        "	systime_snapshot->real = ktime_add_ns(base_real, nsec_real);",
        "	systime_snapshot->boot = ktime_add_ns(base_boot, nsec_real);",
        "	systime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_get_snapshot);",
        "",
        "/* Scale base by mult/div checking for overflow */",
        "static int scale64_check_overflow(u64 mult, u64 div, u64 *base)",
        "{",
        "	u64 tmp, rem;",
        "",
        "	tmp = div64_u64_rem(*base, div, &rem);",
        "",
        "	if (((int)sizeof(u64)*8 - fls64(mult) < fls64(tmp)) ||",
        "	    ((int)sizeof(u64)*8 - fls64(mult) < fls64(rem)))",
        "		return -EOVERFLOW;",
        "	tmp *= mult;",
        "",
        "	rem = div64_u64(rem * mult, div);",
        "	*base = tmp + rem;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * adjust_historical_crosststamp - adjust crosstimestamp previous to current interval",
        " * @history:			Snapshot representing start of history",
        " * @partial_history_cycles:	Cycle offset into history (fractional part)",
        " * @total_history_cycles:	Total history length in cycles",
        " * @discontinuity:		True indicates clock was set on history period",
        " * @ts:				Cross timestamp that should be adjusted using",
        " *	partial/total ratio",
        " *",
        " * Helper function used by get_device_system_crosststamp() to correct the",
        " * crosstimestamp corresponding to the start of the current interval to the",
        " * system counter value (timestamp point) provided by the driver. The",
        " * total_history_* quantities are the total history starting at the provided",
        " * reference point and ending at the start of the current interval. The cycle",
        " * count between the driver timestamp point and the start of the current",
        " * interval is partial_history_cycles.",
        " */",
        "static int adjust_historical_crosststamp(struct system_time_snapshot *history,",
        "					 u64 partial_history_cycles,",
        "					 u64 total_history_cycles,",
        "					 bool discontinuity,",
        "					 struct system_device_crosststamp *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	u64 corr_raw, corr_real;",
        "	bool interp_forward;",
        "	int ret;",
        "",
        "	if (total_history_cycles == 0 || partial_history_cycles == 0)",
        "		return 0;",
        "",
        "	/* Interpolate shortest distance from beginning or end of history */",
        "	interp_forward = partial_history_cycles > total_history_cycles / 2;",
        "	partial_history_cycles = interp_forward ?",
        "		total_history_cycles - partial_history_cycles :",
        "		partial_history_cycles;",
        "",
        "	/*",
        "	 * Scale the monotonic raw time delta by:",
        "	 *	partial_history_cycles / total_history_cycles",
        "	 */",
        "	corr_raw = (u64)ktime_to_ns(",
        "		ktime_sub(ts->sys_monoraw, history->raw));",
        "	ret = scale64_check_overflow(partial_history_cycles,",
        "				     total_history_cycles, &corr_raw);",
        "	if (ret)",
        "		return ret;",
        "",
        "	/*",
        "	 * If there is a discontinuity in the history, scale monotonic raw",
        "	 *	correction by:",
        "	 *	mult(real)/mult(raw) yielding the realtime correction",
        "	 * Otherwise, calculate the realtime correction similar to monotonic",
        "	 *	raw calculation",
        "	 */",
        "	if (discontinuity) {",
        "		corr_real = mul_u64_u32_div",
        "			(corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);",
        "	} else {",
        "		corr_real = (u64)ktime_to_ns(",
        "			ktime_sub(ts->sys_realtime, history->real));",
        "		ret = scale64_check_overflow(partial_history_cycles,",
        "					     total_history_cycles, &corr_real);",
        "		if (ret)",
        "			return ret;",
        "	}",
        "",
        "	/* Fixup monotonic raw and real time time values */",
        "	if (interp_forward) {",
        "		ts->sys_monoraw = ktime_add_ns(history->raw, corr_raw);",
        "		ts->sys_realtime = ktime_add_ns(history->real, corr_real);",
        "	} else {",
        "		ts->sys_monoraw = ktime_sub_ns(ts->sys_monoraw, corr_raw);",
        "		ts->sys_realtime = ktime_sub_ns(ts->sys_realtime, corr_real);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * timestamp_in_interval - true if ts is chronologically in [start, end]",
        " *",
        " * True if ts occurs chronologically at or after start, and before or at end.",
        " */",
        "static bool timestamp_in_interval(u64 start, u64 end, u64 ts)",
        "{",
        "	if (ts >= start && ts <= end)",
        "		return true;",
        "	if (start > end && (ts >= start || ts <= end))",
        "		return true;",
        "	return false;",
        "}",
        "",
        "static bool convert_clock(u64 *val, u32 numerator, u32 denominator)",
        "{",
        "	u64 rem, res;",
        "",
        "	if (!numerator || !denominator)",
        "		return false;",
        "",
        "	res = div64_u64_rem(*val, denominator, &rem) * numerator;",
        "	*val = res + div_u64(rem * numerator, denominator);",
        "	return true;",
        "}",
        "",
        "static bool convert_base_to_cs(struct system_counterval_t *scv)",
        "{",
        "	struct clocksource *cs = tk_core.timekeeper.tkr_mono.clock;",
        "	struct clocksource_base *base;",
        "	u32 num, den;",
        "",
        "	/* The timestamp was taken from the time keeper clock source */",
        "	if (cs->id == scv->cs_id)",
        "		return true;",
        "",
        "	/*",
        "	 * Check whether cs_id matches the base clock. Prevent the compiler from",
        "	 * re-evaluating @base as the clocksource might change concurrently.",
        "	 */",
        "	base = READ_ONCE(cs->base);",
        "	if (!base || base->id != scv->cs_id)",
        "		return false;",
        "",
        "	num = scv->use_nsecs ? cs->freq_khz : base->numerator;",
        "	den = scv->use_nsecs ? USEC_PER_SEC : base->denominator;",
        "",
        "	if (!convert_clock(&scv->cycles, num, den))",
        "		return false;",
        "",
        "	scv->cycles += base->offset;",
        "	return true;",
        "}",
        "",
        "static bool convert_cs_to_base(u64 *cycles, enum clocksource_ids base_id)",
        "{",
        "	struct clocksource *cs = tk_core.timekeeper.tkr_mono.clock;",
        "	struct clocksource_base *base;",
        "",
        "	/*",
        "	 * Check whether base_id matches the base clock. Prevent the compiler from",
        "	 * re-evaluating @base as the clocksource might change concurrently.",
        "	 */",
        "	base = READ_ONCE(cs->base);",
        "	if (!base || base->id != base_id)",
        "		return false;",
        "",
        "	*cycles -= base->offset;",
        "	if (!convert_clock(cycles, base->denominator, base->numerator))",
        "		return false;",
        "	return true;",
        "}",
        "",
        "static bool convert_ns_to_cs(u64 *delta)",
        "{",
        "	struct tk_read_base *tkr = &tk_core.timekeeper.tkr_mono;",
        "",
        "	if (BITS_TO_BYTES(fls64(*delta) + tkr->shift) >= sizeof(*delta))",
        "		return false;",
        "",
        "	*delta = div_u64((*delta << tkr->shift) - tkr->xtime_nsec, tkr->mult);",
        "	return true;",
        "}",
        "",
        "/**",
        " * ktime_real_to_base_clock() - Convert CLOCK_REALTIME timestamp to a base clock timestamp",
        " * @treal:	CLOCK_REALTIME timestamp to convert",
        " * @base_id:	base clocksource id",
        " * @cycles:	pointer to store the converted base clock timestamp",
        " *",
        " * Converts a supplied, future realtime clock value to the corresponding base clock value.",
        " *",
        " * Return:  true if the conversion is successful, false otherwise.",
        " */",
        "bool ktime_real_to_base_clock(ktime_t treal, enum clocksource_ids base_id, u64 *cycles)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	u64 delta;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		if ((u64)treal < tk->tkr_mono.base_real)",
        "			return false;",
        "		delta = (u64)treal - tk->tkr_mono.base_real;",
        "		if (!convert_ns_to_cs(&delta))",
        "			return false;",
        "		*cycles = tk->tkr_mono.cycle_last + delta;",
        "		if (!convert_cs_to_base(cycles, base_id))",
        "			return false;",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return true;",
        "}",
        "EXPORT_SYMBOL_GPL(ktime_real_to_base_clock);",
        "",
        "/**",
        " * get_device_system_crosststamp - Synchronously capture system/device timestamp",
        " * @get_time_fn:	Callback to get simultaneous device time and",
        " *	system counter from the device driver",
        " * @ctx:		Context passed to get_time_fn()",
        " * @history_begin:	Historical reference point used to interpolate system",
        " *	time when counter provided by the driver is before the current interval",
        " * @xtstamp:		Receives simultaneously captured system and device time",
        " *",
        " * Reads a timestamp from a device and correlates it to system time",
        " */",
        "int get_device_system_crosststamp(int (*get_time_fn)",
        "				  (ktime_t *device_time,",
        "				   struct system_counterval_t *sys_counterval,",
        "				   void *ctx),",
        "				  void *ctx,",
        "				  struct system_time_snapshot *history_begin,",
        "				  struct system_device_crosststamp *xtstamp)",
        "{",
        "	struct system_counterval_t system_counterval;",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	u64 cycles, now, interval_start;",
        "	unsigned int clock_was_set_seq = 0;",
        "	ktime_t base_real, base_raw;",
        "	u64 nsec_real, nsec_raw;",
        "	u8 cs_was_changed_seq;",
        "	unsigned int seq;",
        "	bool do_interp;",
        "	int ret;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		/*",
        "		 * Try to synchronously capture device time and a system",
        "		 * counter value calling back into the device driver",
        "		 */",
        "		ret = get_time_fn(&xtstamp->device, &system_counterval, ctx);",
        "		if (ret)",
        "			return ret;",
        "",
        "		/*",
        "		 * Verify that the clocksource ID associated with the captured",
        "		 * system counter value is the same as for the currently",
        "		 * installed timekeeper clocksource",
        "		 */",
        "		if (system_counterval.cs_id == CSID_GENERIC ||",
        "		    !convert_base_to_cs(&system_counterval))",
        "			return -ENODEV;",
        "		cycles = system_counterval.cycles;",
        "",
        "		/*",
        "		 * Check whether the system counter value provided by the",
        "		 * device driver is on the current timekeeping interval.",
        "		 */",
        "		now = tk_clock_read(&tk->tkr_mono);",
        "		interval_start = tk->tkr_mono.cycle_last;",
        "		if (!timestamp_in_interval(interval_start, now, cycles)) {",
        "			clock_was_set_seq = tk->clock_was_set_seq;",
        "			cs_was_changed_seq = tk->cs_was_changed_seq;",
        "			cycles = interval_start;",
        "			do_interp = true;",
        "		} else {",
        "			do_interp = false;",
        "		}",
        "",
        "		base_real = ktime_add(tk->tkr_mono.base,",
        "				      tk_core.timekeeper.offs_real);",
        "		base_raw = tk->tkr_raw.base;",
        "",
        "		nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, cycles);",
        "		nsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw, cycles);",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	xtstamp->sys_realtime = ktime_add_ns(base_real, nsec_real);",
        "	xtstamp->sys_monoraw = ktime_add_ns(base_raw, nsec_raw);",
        "",
        "	/*",
        "	 * Interpolate if necessary, adjusting back from the start of the",
        "	 * current interval",
        "	 */",
        "	if (do_interp) {",
        "		u64 partial_history_cycles, total_history_cycles;",
        "		bool discontinuity;",
        "",
        "		/*",
        "		 * Check that the counter value is not before the provided",
        "		 * history reference and that the history doesn't cross a",
        "		 * clocksource change",
        "		 */",
        "		if (!history_begin ||",
        "		    !timestamp_in_interval(history_begin->cycles,",
        "					   cycles, system_counterval.cycles) ||",
        "		    history_begin->cs_was_changed_seq != cs_was_changed_seq)",
        "			return -EINVAL;",
        "		partial_history_cycles = cycles - system_counterval.cycles;",
        "		total_history_cycles = cycles - history_begin->cycles;",
        "		discontinuity =",
        "			history_begin->clock_was_set_seq != clock_was_set_seq;",
        "",
        "		ret = adjust_historical_crosststamp(history_begin,",
        "						    partial_history_cycles,",
        "						    total_history_cycles,",
        "						    discontinuity, xtstamp);",
        "		if (ret)",
        "			return ret;",
        "	}",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL_GPL(get_device_system_crosststamp);",
        "",
        "/**",
        " * timekeeping_clocksource_has_base - Check whether the current clocksource",
        " *				      is based on given a base clock",
        " * @id:		base clocksource ID",
        " *",
        " * Note:	The return value is a snapshot which can become invalid right",
        " *		after the function returns.",
        " *",
        " * Return:	true if the timekeeper clocksource has a base clock with @id,",
        " *		false otherwise",
        " */",
        "bool timekeeping_clocksource_has_base(enum clocksource_ids id)",
        "{",
        "	/*",
        "	 * This is a snapshot, so no point in using the sequence",
        "	 * count. Just prevent the compiler from re-evaluating @base as the",
        "	 * clocksource might change concurrently.",
        "	 */",
        "	struct clocksource_base *base = READ_ONCE(tk_core.timekeeper.tkr_mono.clock->base);",
        "",
        "	return base ? base->id == id : false;",
        "}",
        "EXPORT_SYMBOL_GPL(timekeeping_clocksource_has_base);",
        "",
        "/**",
        " * do_settimeofday64 - Sets the time of day.",
        " * @ts:     pointer to the timespec64 variable containing the new time",
        " *",
        " * Sets the time of day to the new time and update NTP and notify hrtimers",
        " */",
        "int do_settimeofday64(const struct timespec64 *ts)",
        "{",
        "	struct timespec64 ts_delta, xt;",
        "",
        "	if (!timespec64_valid_settod(ts))",
        "		return -EINVAL;",
        "",
        "	scoped_guard (raw_spinlock_irqsave, &tk_core.lock) {",
        "		struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "",
        "		timekeeping_forward_now(tks);",
        "",
        "		xt = tk_xtime(tks);",
        "		ts_delta = timespec64_sub(*ts, xt);",
        "",
        "		if (timespec64_compare(&tks->wall_to_monotonic, &ts_delta) > 0) {",
        "			timekeeping_restore_shadow(&tk_core);",
        "			return -EINVAL;",
        "		}",
        "",
        "		tk_set_wall_to_mono(tks, timespec64_sub(tks->wall_to_monotonic, ts_delta));",
        "		tk_set_xtime(tks, ts);",
        "		timekeeping_update_from_shadow(&tk_core, TK_UPDATE_ALL);",
        "	}",
        "",
        "	/* Signal hrtimers about time change */",
        "	clock_was_set(CLOCK_SET_WALL);",
        "",
        "	audit_tk_injoffset(ts_delta);",
        "	add_device_randomness(ts, sizeof(*ts));",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(do_settimeofday64);",
        "",
        "/**",
        " * timekeeping_inject_offset - Adds or subtracts from the current time.",
        " * @ts:		Pointer to the timespec variable containing the offset",
        " *",
        " * Adds or subtracts an offset value from the current time.",
        " */",
        "static int timekeeping_inject_offset(const struct timespec64 *ts)",
        "{",
        "	if (ts->tv_nsec < 0 || ts->tv_nsec >= NSEC_PER_SEC)",
        "		return -EINVAL;",
        "",
        "	scoped_guard (raw_spinlock_irqsave, &tk_core.lock) {",
        "		struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "		struct timespec64 tmp;",
        "",
        "		timekeeping_forward_now(tks);",
        "",
        "		/* Make sure the proposed value is valid */",
        "		tmp = timespec64_add(tk_xtime(tks), *ts);",
        "		if (timespec64_compare(&tks->wall_to_monotonic, ts) > 0 ||",
        "		    !timespec64_valid_settod(&tmp)) {",
        "			timekeeping_restore_shadow(&tk_core);",
        "			return -EINVAL;",
        "		}",
        "",
        "		tk_xtime_add(tks, ts);",
        "		tk_set_wall_to_mono(tks, timespec64_sub(tks->wall_to_monotonic, *ts));",
        "		timekeeping_update_from_shadow(&tk_core, TK_UPDATE_ALL);",
        "	}",
        "",
        "	/* Signal hrtimers about time change */",
        "	clock_was_set(CLOCK_SET_WALL);",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Indicates if there is an offset between the system clock and the hardware",
        " * clock/persistent clock/rtc.",
        " */",
        "int persistent_clock_is_local;",
        "",
        "/*",
        " * Adjust the time obtained from the CMOS to be UTC time instead of",
        " * local time.",
        " *",
        " * This is ugly, but preferable to the alternatives.  Otherwise we",
        " * would either need to write a program to do it in /etc/rc (and risk",
        " * confusion if the program gets run more than once; it would also be",
        " * hard to make the program warp the clock precisely n hours)  or",
        " * compile in the timezone information into the kernel.  Bad, bad....",
        " *",
        " *						- TYT, 1992-01-01",
        " *",
        " * The best thing to do is to keep the CMOS clock in universal time (UTC)",
        " * as real UNIX machines always do it. This avoids all headaches about",
        " * daylight saving times and warping kernel clocks.",
        " */",
        "void timekeeping_warp_clock(void)",
        "{",
        "	if (sys_tz.tz_minuteswest != 0) {",
        "		struct timespec64 adjust;",
        "",
        "		persistent_clock_is_local = 1;",
        "		adjust.tv_sec = sys_tz.tz_minuteswest * 60;",
        "		adjust.tv_nsec = 0;",
        "		timekeeping_inject_offset(&adjust);",
        "	}",
        "}",
        "",
        "/*",
        " * __timekeeping_set_tai_offset - Sets the TAI offset from UTC and monotonic",
        " */",
        "static void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)",
        "{",
        "	tk->tai_offset = tai_offset;",
        "	tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));",
        "}",
        "",
        "/*",
        " * change_clocksource - Swaps clocksources if a new one is available",
        " *",
        " * Accumulates current time interval and initializes new clocksource",
        " */",
        "static int change_clocksource(void *data)",
        "{",
        "	struct clocksource *new = data, *old = NULL;",
        "",
        "	/*",
        "	 * If the clocksource is in a module, get a module reference.",
        "	 * Succeeds for built-in code (owner == NULL) as well. Abort if the",
        "	 * reference can't be acquired.",
        "	 */",
        "	if (!try_module_get(new->owner))",
        "		return 0;",
        "",
        "	/* Abort if the device can't be enabled */",
        "	if (new->enable && new->enable(new) != 0) {",
        "		module_put(new->owner);",
        "		return 0;",
        "	}",
        "",
        "	scoped_guard (raw_spinlock_irqsave, &tk_core.lock) {",
        "		struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "",
        "		timekeeping_forward_now(tks);",
        "		old = tks->tkr_mono.clock;",
        "		tk_setup_internals(tks, new);",
        "		timekeeping_update_from_shadow(&tk_core, TK_UPDATE_ALL);",
        "	}",
        "",
        "	if (old) {",
        "		if (old->disable)",
        "			old->disable(old);",
        "		module_put(old->owner);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * timekeeping_notify - Install a new clock source",
        " * @clock:		pointer to the clock source",
        " *",
        " * This function is called from clocksource.c after a new, better clock",
        " * source has been registered. The caller holds the clocksource_mutex.",
        " */",
        "int timekeeping_notify(struct clocksource *clock)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "",
        "	if (tk->tkr_mono.clock == clock)",
        "		return 0;",
        "	stop_machine(change_clocksource, clock, NULL);",
        "	tick_clock_notify();",
        "	return tk->tkr_mono.clock == clock ? 0 : -1;",
        "}",
        "",
        "/**",
        " * ktime_get_raw_ts64 - Returns the raw monotonic time in a timespec",
        " * @ts:		pointer to the timespec64 to be set",
        " *",
        " * Returns the raw monotonic time (completely un-modified by ntp)",
        " */",
        "void ktime_get_raw_ts64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	u64 nsecs;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		ts->tv_sec = tk->raw_sec;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_raw);",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	ts->tv_nsec = 0;",
        "	timespec64_add_ns(ts, nsecs);",
        "}",
        "EXPORT_SYMBOL(ktime_get_raw_ts64);",
        "",
        "",
        "/**",
        " * timekeeping_valid_for_hres - Check if timekeeping is suitable for hres",
        " */",
        "int timekeeping_valid_for_hres(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	int ret;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		ret = tk->tkr_mono.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " * timekeeping_max_deferment - Returns max time the clocksource can be deferred",
        " */",
        "u64 timekeeping_max_deferment(void)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	u64 ret;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		ret = tk->tkr_mono.clock->max_idle_ns;",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " * read_persistent_clock64 -  Return time from the persistent clock.",
        " * @ts: Pointer to the storage for the readout value",
        " *",
        " * Weak dummy function for arches that do not yet support it.",
        " * Reads the time from the battery backed persistent clock.",
        " * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported.",
        " *",
        " *  XXX - Do be sure to remove it once all arches implement it.",
        " */",
        "void __weak read_persistent_clock64(struct timespec64 *ts)",
        "{",
        "	ts->tv_sec = 0;",
        "	ts->tv_nsec = 0;",
        "}",
        "",
        "/**",
        " * read_persistent_wall_and_boot_offset - Read persistent clock, and also offset",
        " *                                        from the boot.",
        " * @wall_time:	  current time as returned by persistent clock",
        " * @boot_offset:  offset that is defined as wall_time - boot_time",
        " *",
        " * Weak dummy function for arches that do not yet support it.",
        " *",
        " * The default function calculates offset based on the current value of",
        " * local_clock(). This way architectures that support sched_clock() but don't",
        " * support dedicated boot time clock will provide the best estimate of the",
        " * boot time.",
        " */",
        "void __weak __init",
        "read_persistent_wall_and_boot_offset(struct timespec64 *wall_time,",
        "				     struct timespec64 *boot_offset)",
        "{",
        "	read_persistent_clock64(wall_time);",
        "	*boot_offset = ns_to_timespec64(local_clock());",
        "}",
        "",
        "static __init void tkd_basic_setup(struct tk_data *tkd)",
        "{",
        "	raw_spin_lock_init(&tkd->lock);",
        "	seqcount_raw_spinlock_init(&tkd->seq, &tkd->lock);",
        "}",
        "",
        "/*",
        " * Flag reflecting whether timekeeping_resume() has injected sleeptime.",
        " *",
        " * The flag starts of false and is only set when a suspend reaches",
        " * timekeeping_suspend(), timekeeping_resume() sets it to false when the",
        " * timekeeper clocksource is not stopping across suspend and has been",
        " * used to update sleep time. If the timekeeper clocksource has stopped",
        " * then the flag stays true and is used by the RTC resume code to decide",
        " * whether sleeptime must be injected and if so the flag gets false then.",
        " *",
        " * If a suspend fails before reaching timekeeping_resume() then the flag",
        " * stays false and prevents erroneous sleeptime injection.",
        " */",
        "static bool suspend_timing_needed;",
        "",
        "/* Flag for if there is a persistent clock on this platform */",
        "static bool persistent_clock_exists;",
        "",
        "/*",
        " * timekeeping_init - Initializes the clocksource and common timekeeping values",
        " */",
        "void __init timekeeping_init(void)",
        "{",
        "	struct timespec64 wall_time, boot_offset, wall_to_mono;",
        "	struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "	struct clocksource *clock;",
        "",
        "	tkd_basic_setup(&tk_core);",
        "",
        "	read_persistent_wall_and_boot_offset(&wall_time, &boot_offset);",
        "	if (timespec64_valid_settod(&wall_time) &&",
        "	    timespec64_to_ns(&wall_time) > 0) {",
        "		persistent_clock_exists = true;",
        "	} else if (timespec64_to_ns(&wall_time) != 0) {",
        "		pr_warn(\"Persistent clock returned invalid value\");",
        "		wall_time = (struct timespec64){0};",
        "	}",
        "",
        "	if (timespec64_compare(&wall_time, &boot_offset) < 0)",
        "		boot_offset = (struct timespec64){0};",
        "",
        "	/*",
        "	 * We want set wall_to_mono, so the following is true:",
        "	 * wall time + wall_to_mono = boot time",
        "	 */",
        "	wall_to_mono = timespec64_sub(boot_offset, wall_time);",
        "",
        "	guard(raw_spinlock_irqsave)(&tk_core.lock);",
        "",
        "	ntp_init();",
        "",
        "	clock = clocksource_default_clock();",
        "	if (clock->enable)",
        "		clock->enable(clock);",
        "	tk_setup_internals(tks, clock);",
        "",
        "	tk_set_xtime(tks, &wall_time);",
        "	tks->raw_sec = 0;",
        "",
        "	tk_set_wall_to_mono(tks, wall_to_mono);",
        "",
        "	timekeeping_update_from_shadow(&tk_core, TK_CLOCK_WAS_SET);",
        "}",
        "",
        "/* time in seconds when suspend began for persistent clock */",
        "static struct timespec64 timekeeping_suspend_time;",
        "",
        "/**",
        " * __timekeeping_inject_sleeptime - Internal function to add sleep interval",
        " * @tk:		Pointer to the timekeeper to be updated",
        " * @delta:	Pointer to the delta value in timespec64 format",
        " *",
        " * Takes a timespec offset measuring a suspend interval and properly",
        " * adds the sleep offset to the timekeeping variables.",
        " */",
        "static void __timekeeping_inject_sleeptime(struct timekeeper *tk,",
        "					   const struct timespec64 *delta)",
        "{",
        "	if (!timespec64_valid_strict(delta)) {",
        "		printk_deferred(KERN_WARNING",
        "				\"__timekeeping_inject_sleeptime: Invalid \"",
        "				\"sleep delta value!\\n\");",
        "		return;",
        "	}",
        "	tk_xtime_add(tk, delta);",
        "	tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));",
        "	tk_update_sleep_time(tk, timespec64_to_ktime(*delta));",
        "	tk_debug_account_sleep_time(delta);",
        "}",
        "",
        "#if defined(CONFIG_PM_SLEEP) && defined(CONFIG_RTC_HCTOSYS_DEVICE)",
        "/*",
        " * We have three kinds of time sources to use for sleep time",
        " * injection, the preference order is:",
        " * 1) non-stop clocksource",
        " * 2) persistent clock (ie: RTC accessible when irqs are off)",
        " * 3) RTC",
        " *",
        " * 1) and 2) are used by timekeeping, 3) by RTC subsystem.",
        " * If system has neither 1) nor 2), 3) will be used finally.",
        " *",
        " *",
        " * If timekeeping has injected sleeptime via either 1) or 2),",
        " * 3) becomes needless, so in this case we don't need to call",
        " * rtc_resume(), and this is what timekeeping_rtc_skipresume()",
        " * means.",
        " */",
        "bool timekeeping_rtc_skipresume(void)",
        "{",
        "	return !suspend_timing_needed;",
        "}",
        "",
        "/*",
        " * 1) can be determined whether to use or not only when doing",
        " * timekeeping_resume() which is invoked after rtc_suspend(),",
        " * so we can't skip rtc_suspend() surely if system has 1).",
        " *",
        " * But if system has 2), 2) will definitely be used, so in this",
        " * case we don't need to call rtc_suspend(), and this is what",
        " * timekeeping_rtc_skipsuspend() means.",
        " */",
        "bool timekeeping_rtc_skipsuspend(void)",
        "{",
        "	return persistent_clock_exists;",
        "}",
        "",
        "/**",
        " * timekeeping_inject_sleeptime64 - Adds suspend interval to timeekeeping values",
        " * @delta: pointer to a timespec64 delta value",
        " *",
        " * This hook is for architectures that cannot support read_persistent_clock64",
        " * because their RTC/persistent clock is only accessible when irqs are enabled.",
        " * and also don't have an effective nonstop clocksource.",
        " *",
        " * This function should only be called by rtc_resume(), and allows",
        " * a suspend offset to be injected into the timekeeping values.",
        " */",
        "void timekeeping_inject_sleeptime64(const struct timespec64 *delta)",
        "{",
        "	scoped_guard(raw_spinlock_irqsave, &tk_core.lock) {",
        "		struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "",
        "		suspend_timing_needed = false;",
        "		timekeeping_forward_now(tks);",
        "		__timekeeping_inject_sleeptime(tks, delta);",
        "		timekeeping_update_from_shadow(&tk_core, TK_UPDATE_ALL);",
        "	}",
        "",
        "	/* Signal hrtimers about time change */",
        "	clock_was_set(CLOCK_SET_WALL | CLOCK_SET_BOOT);",
        "}",
        "#endif",
        "",
        "/**",
        " * timekeeping_resume - Resumes the generic timekeeping subsystem.",
        " */",
        "void timekeeping_resume(void)",
        "{",
        "	struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "	struct clocksource *clock = tks->tkr_mono.clock;",
        "	struct timespec64 ts_new, ts_delta;",
        "	bool inject_sleeptime = false;",
        "	u64 cycle_now, nsec;",
        "	unsigned long flags;",
        "",
        "	read_persistent_clock64(&ts_new);",
        "",
        "	clockevents_resume();",
        "	clocksource_resume();",
        "",
        "	raw_spin_lock_irqsave(&tk_core.lock, flags);",
        "",
        "	/*",
        "	 * After system resumes, we need to calculate the suspended time and",
        "	 * compensate it for the OS time. There are 3 sources that could be",
        "	 * used: Nonstop clocksource during suspend, persistent clock and rtc",
        "	 * device.",
        "	 *",
        "	 * One specific platform may have 1 or 2 or all of them, and the",
        "	 * preference will be:",
        "	 *	suspend-nonstop clocksource -> persistent clock -> rtc",
        "	 * The less preferred source will only be tried if there is no better",
        "	 * usable source. The rtc part is handled separately in rtc core code.",
        "	 */",
        "	cycle_now = tk_clock_read(&tks->tkr_mono);",
        "	nsec = clocksource_stop_suspend_timing(clock, cycle_now);",
        "	if (nsec > 0) {",
        "		ts_delta = ns_to_timespec64(nsec);",
        "		inject_sleeptime = true;",
        "	} else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {",
        "		ts_delta = timespec64_sub(ts_new, timekeeping_suspend_time);",
        "		inject_sleeptime = true;",
        "	}",
        "",
        "	if (inject_sleeptime) {",
        "		suspend_timing_needed = false;",
        "		__timekeeping_inject_sleeptime(tks, &ts_delta);",
        "	}",
        "",
        "	/* Re-base the last cycle value */",
        "	tks->tkr_mono.cycle_last = cycle_now;",
        "	tks->tkr_raw.cycle_last  = cycle_now;",
        "",
        "	tks->ntp_error = 0;",
        "	timekeeping_suspended = 0;",
        "	timekeeping_update_from_shadow(&tk_core, TK_CLOCK_WAS_SET);",
        "	raw_spin_unlock_irqrestore(&tk_core.lock, flags);",
        "",
        "	touch_softlockup_watchdog();",
        "",
        "	/* Resume the clockevent device(s) and hrtimers */",
        "	tick_resume();",
        "	/* Notify timerfd as resume is equivalent to clock_was_set() */",
        "	timerfd_resume();",
        "}",
        "",
        "int timekeeping_suspend(void)",
        "{",
        "	struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "	struct timespec64 delta, delta_delta;",
        "	static struct timespec64 old_delta;",
        "	struct clocksource *curr_clock;",
        "	unsigned long flags;",
        "	u64 cycle_now;",
        "",
        "	read_persistent_clock64(&timekeeping_suspend_time);",
        "",
        "	/*",
        "	 * On some systems the persistent_clock can not be detected at",
        "	 * timekeeping_init by its return value, so if we see a valid",
        "	 * value returned, update the persistent_clock_exists flag.",
        "	 */",
        "	if (timekeeping_suspend_time.tv_sec || timekeeping_suspend_time.tv_nsec)",
        "		persistent_clock_exists = true;",
        "",
        "	suspend_timing_needed = true;",
        "",
        "	raw_spin_lock_irqsave(&tk_core.lock, flags);",
        "	timekeeping_forward_now(tks);",
        "	timekeeping_suspended = 1;",
        "",
        "	/*",
        "	 * Since we've called forward_now, cycle_last stores the value",
        "	 * just read from the current clocksource. Save this to potentially",
        "	 * use in suspend timing.",
        "	 */",
        "	curr_clock = tks->tkr_mono.clock;",
        "	cycle_now = tks->tkr_mono.cycle_last;",
        "	clocksource_start_suspend_timing(curr_clock, cycle_now);",
        "",
        "	if (persistent_clock_exists) {",
        "		/*",
        "		 * To avoid drift caused by repeated suspend/resumes,",
        "		 * which each can add ~1 second drift error,",
        "		 * try to compensate so the difference in system time",
        "		 * and persistent_clock time stays close to constant.",
        "		 */",
        "		delta = timespec64_sub(tk_xtime(tks), timekeeping_suspend_time);",
        "		delta_delta = timespec64_sub(delta, old_delta);",
        "		if (abs(delta_delta.tv_sec) >= 2) {",
        "			/*",
        "			 * if delta_delta is too large, assume time correction",
        "			 * has occurred and set old_delta to the current delta.",
        "			 */",
        "			old_delta = delta;",
        "		} else {",
        "			/* Otherwise try to adjust old_system to compensate */",
        "			timekeeping_suspend_time =",
        "				timespec64_add(timekeeping_suspend_time, delta_delta);",
        "		}",
        "	}",
        "",
        "	timekeeping_update_from_shadow(&tk_core, 0);",
        "	halt_fast_timekeeper(tks);",
        "	raw_spin_unlock_irqrestore(&tk_core.lock, flags);",
        "",
        "	tick_suspend();",
        "	clocksource_suspend();",
        "	clockevents_suspend();",
        "",
        "	return 0;",
        "}",
        "",
        "/* sysfs resume/suspend bits for timekeeping */",
        "static struct syscore_ops timekeeping_syscore_ops = {",
        "	.resume		= timekeeping_resume,",
        "	.suspend	= timekeeping_suspend,",
        "};",
        "",
        "static int __init timekeeping_init_ops(void)",
        "{",
        "	register_syscore_ops(&timekeeping_syscore_ops);",
        "	return 0;",
        "}",
        "device_initcall(timekeeping_init_ops);",
        "",
        "/*",
        " * Apply a multiplier adjustment to the timekeeper",
        " */",
        "static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,",
        "							 s64 offset,",
        "							 s32 mult_adj)",
        "{",
        "	s64 interval = tk->cycle_interval;",
        "",
        "	if (mult_adj == 0) {",
        "		return;",
        "	} else if (mult_adj == -1) {",
        "		interval = -interval;",
        "		offset = -offset;",
        "	} else if (mult_adj != 1) {",
        "		interval *= mult_adj;",
        "		offset *= mult_adj;",
        "	}",
        "",
        "	/*",
        "	 * So the following can be confusing.",
        "	 *",
        "	 * To keep things simple, lets assume mult_adj == 1 for now.",
        "	 *",
        "	 * When mult_adj != 1, remember that the interval and offset values",
        "	 * have been appropriately scaled so the math is the same.",
        "	 *",
        "	 * The basic idea here is that we're increasing the multiplier",
        "	 * by one, this causes the xtime_interval to be incremented by",
        "	 * one cycle_interval. This is because:",
        "	 *	xtime_interval = cycle_interval * mult",
        "	 * So if mult is being incremented by one:",
        "	 *	xtime_interval = cycle_interval * (mult + 1)",
        "	 * Its the same as:",
        "	 *	xtime_interval = (cycle_interval * mult) + cycle_interval",
        "	 * Which can be shortened to:",
        "	 *	xtime_interval += cycle_interval",
        "	 *",
        "	 * So offset stores the non-accumulated cycles. Thus the current",
        "	 * time (in shifted nanoseconds) is:",
        "	 *	now = (offset * adj) + xtime_nsec",
        "	 * Now, even though we're adjusting the clock frequency, we have",
        "	 * to keep time consistent. In other words, we can't jump back",
        "	 * in time, and we also want to avoid jumping forward in time.",
        "	 *",
        "	 * So given the same offset value, we need the time to be the same",
        "	 * both before and after the freq adjustment.",
        "	 *	now = (offset * adj_1) + xtime_nsec_1",
        "	 *	now = (offset * adj_2) + xtime_nsec_2",
        "	 * So:",
        "	 *	(offset * adj_1) + xtime_nsec_1 =",
        "	 *		(offset * adj_2) + xtime_nsec_2",
        "	 * And we know:",
        "	 *	adj_2 = adj_1 + 1",
        "	 * So:",
        "	 *	(offset * adj_1) + xtime_nsec_1 =",
        "	 *		(offset * (adj_1+1)) + xtime_nsec_2",
        "	 *	(offset * adj_1) + xtime_nsec_1 =",
        "	 *		(offset * adj_1) + offset + xtime_nsec_2",
        "	 * Canceling the sides:",
        "	 *	xtime_nsec_1 = offset + xtime_nsec_2",
        "	 * Which gives us:",
        "	 *	xtime_nsec_2 = xtime_nsec_1 - offset",
        "	 * Which simplifies to:",
        "	 *	xtime_nsec -= offset",
        "	 */",
        "	if ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {",
        "		/* NTP adjustment caused clocksource mult overflow */",
        "		WARN_ON_ONCE(1);",
        "		return;",
        "	}",
        "",
        "	tk->tkr_mono.mult += mult_adj;",
        "	tk->xtime_interval += interval;",
        "	tk->tkr_mono.xtime_nsec -= offset;",
        "}",
        "",
        "/*",
        " * Adjust the timekeeper's multiplier to the correct frequency",
        " * and also to reduce the accumulated error value.",
        " */",
        "static void timekeeping_adjust(struct timekeeper *tk, s64 offset)",
        "{",
        "	u64 ntp_tl = ntp_tick_length();",
        "	u32 mult;",
        "",
        "	/*",
        "	 * Determine the multiplier from the current NTP tick length.",
        "	 * Avoid expensive division when the tick length doesn't change.",
        "	 */",
        "	if (likely(tk->ntp_tick == ntp_tl)) {",
        "		mult = tk->tkr_mono.mult - tk->ntp_err_mult;",
        "	} else {",
        "		tk->ntp_tick = ntp_tl;",
        "		mult = div64_u64((tk->ntp_tick >> tk->ntp_error_shift) -",
        "				 tk->xtime_remainder, tk->cycle_interval);",
        "	}",
        "",
        "	/*",
        "	 * If the clock is behind the NTP time, increase the multiplier by 1",
        "	 * to catch up with it. If it's ahead and there was a remainder in the",
        "	 * tick division, the clock will slow down. Otherwise it will stay",
        "	 * ahead until the tick length changes to a non-divisible value.",
        "	 */",
        "	tk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;",
        "	mult += tk->ntp_err_mult;",
        "",
        "	timekeeping_apply_adjustment(tk, offset, mult - tk->tkr_mono.mult);",
        "",
        "	if (unlikely(tk->tkr_mono.clock->maxadj &&",
        "		(abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)",
        "			> tk->tkr_mono.clock->maxadj))) {",
        "		printk_once(KERN_WARNING",
        "			\"Adjusting %s more than 11%% (%ld vs %ld)\\n\",",
        "			tk->tkr_mono.clock->name, (long)tk->tkr_mono.mult,",
        "			(long)tk->tkr_mono.clock->mult + tk->tkr_mono.clock->maxadj);",
        "	}",
        "",
        "	/*",
        "	 * It may be possible that when we entered this function, xtime_nsec",
        "	 * was very small.  Further, if we're slightly speeding the clocksource",
        "	 * in the code above, its possible the required corrective factor to",
        "	 * xtime_nsec could cause it to underflow.",
        "	 *",
        "	 * Now, since we have already accumulated the second and the NTP",
        "	 * subsystem has been notified via second_overflow(), we need to skip",
        "	 * the next update.",
        "	 */",
        "	if (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {",
        "		tk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC <<",
        "							tk->tkr_mono.shift;",
        "		tk->xtime_sec--;",
        "		tk->skip_second_overflow = 1;",
        "	}",
        "}",
        "",
        "/*",
        " * accumulate_nsecs_to_secs - Accumulates nsecs into secs",
        " *",
        " * Helper function that accumulates the nsecs greater than a second",
        " * from the xtime_nsec field to the xtime_secs field.",
        " * It also calls into the NTP code to handle leapsecond processing.",
        " */",
        "static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)",
        "{",
        "	u64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;",
        "	unsigned int clock_set = 0;",
        "",
        "	while (tk->tkr_mono.xtime_nsec >= nsecps) {",
        "		int leap;",
        "",
        "		tk->tkr_mono.xtime_nsec -= nsecps;",
        "		tk->xtime_sec++;",
        "",
        "		/*",
        "		 * Skip NTP update if this second was accumulated before,",
        "		 * i.e. xtime_nsec underflowed in timekeeping_adjust()",
        "		 */",
        "		if (unlikely(tk->skip_second_overflow)) {",
        "			tk->skip_second_overflow = 0;",
        "			continue;",
        "		}",
        "",
        "		/* Figure out if its a leap sec and apply if needed */",
        "		leap = second_overflow(tk->xtime_sec);",
        "		if (unlikely(leap)) {",
        "			struct timespec64 ts;",
        "",
        "			tk->xtime_sec += leap;",
        "",
        "			ts.tv_sec = leap;",
        "			ts.tv_nsec = 0;",
        "			tk_set_wall_to_mono(tk,",
        "				timespec64_sub(tk->wall_to_monotonic, ts));",
        "",
        "			__timekeeping_set_tai_offset(tk, tk->tai_offset - leap);",
        "",
        "			clock_set = TK_CLOCK_WAS_SET;",
        "		}",
        "	}",
        "	return clock_set;",
        "}",
        "",
        "/*",
        " * logarithmic_accumulation - shifted accumulation of cycles",
        " *",
        " * This functions accumulates a shifted interval of cycles into",
        " * a shifted interval nanoseconds. Allows for O(log) accumulation",
        " * loop.",
        " *",
        " * Returns the unconsumed cycles.",
        " */",
        "static u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,",
        "				    u32 shift, unsigned int *clock_set)",
        "{",
        "	u64 interval = tk->cycle_interval << shift;",
        "	u64 snsec_per_sec;",
        "",
        "	/* If the offset is smaller than a shifted interval, do nothing */",
        "	if (offset < interval)",
        "		return offset;",
        "",
        "	/* Accumulate one shifted interval */",
        "	offset -= interval;",
        "	tk->tkr_mono.cycle_last += interval;",
        "	tk->tkr_raw.cycle_last  += interval;",
        "",
        "	tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;",
        "	*clock_set |= accumulate_nsecs_to_secs(tk);",
        "",
        "	/* Accumulate raw time */",
        "	tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;",
        "	snsec_per_sec = (u64)NSEC_PER_SEC << tk->tkr_raw.shift;",
        "	while (tk->tkr_raw.xtime_nsec >= snsec_per_sec) {",
        "		tk->tkr_raw.xtime_nsec -= snsec_per_sec;",
        "		tk->raw_sec++;",
        "	}",
        "",
        "	/* Accumulate error between NTP and clock interval */",
        "	tk->ntp_error += tk->ntp_tick << shift;",
        "	tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<",
        "						(tk->ntp_error_shift + shift);",
        "",
        "	return offset;",
        "}",
        "",
        "/*",
        " * timekeeping_advance - Updates the timekeeper to the current time and",
        " * current NTP tick length",
        " */",
        "static bool timekeeping_advance(enum timekeeping_adv_mode mode)",
        "{",
        "	struct timekeeper *tk = &tk_core.shadow_timekeeper;",
        "	struct timekeeper *real_tk = &tk_core.timekeeper;",
        "	unsigned int clock_set = 0;",
        "	int shift = 0, maxshift;",
        "	u64 offset;",
        "",
        "	guard(raw_spinlock_irqsave)(&tk_core.lock);",
        "",
        "	/* Make sure we're fully resumed: */",
        "	if (unlikely(timekeeping_suspended))",
        "		return false;",
        "",
        "	offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),",
        "				   tk->tkr_mono.cycle_last, tk->tkr_mono.mask,",
        "				   tk->tkr_mono.clock->max_raw_delta);",
        "",
        "	/* Check if there's really nothing to do */",
        "	if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)",
        "		return false;",
        "",
        "	/*",
        "	 * With NO_HZ we may have to accumulate many cycle_intervals",
        "	 * (think \"ticks\") worth of time at once. To do this efficiently,",
        "	 * we calculate the largest doubling multiple of cycle_intervals",
        "	 * that is smaller than the offset.  We then accumulate that",
        "	 * chunk in one go, and then try to consume the next smaller",
        "	 * doubled multiple.",
        "	 */",
        "	shift = ilog2(offset) - ilog2(tk->cycle_interval);",
        "	shift = max(0, shift);",
        "	/* Bound shift to one less than what overflows tick_length */",
        "	maxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;",
        "	shift = min(shift, maxshift);",
        "	while (offset >= tk->cycle_interval) {",
        "		offset = logarithmic_accumulation(tk, offset, shift, &clock_set);",
        "		if (offset < tk->cycle_interval<<shift)",
        "			shift--;",
        "	}",
        "",
        "	/* Adjust the multiplier to correct NTP error */",
        "	timekeeping_adjust(tk, offset);",
        "",
        "	/*",
        "	 * Finally, make sure that after the rounding",
        "	 * xtime_nsec isn't larger than NSEC_PER_SEC",
        "	 */",
        "	clock_set |= accumulate_nsecs_to_secs(tk);",
        "",
        "	timekeeping_update_from_shadow(&tk_core, clock_set);",
        "",
        "	return !!clock_set;",
        "}",
        "",
        "/**",
        " * update_wall_time - Uses the current clocksource to increment the wall time",
        " *",
        " */",
        "void update_wall_time(void)",
        "{",
        "	if (timekeeping_advance(TK_ADV_TICK))",
        "		clock_was_set_delayed();",
        "}",
        "",
        "/**",
        " * getboottime64 - Return the real time of system boot.",
        " * @ts:		pointer to the timespec64 to be set",
        " *",
        " * Returns the wall-time of boot in a timespec64.",
        " *",
        " * This is based on the wall_to_monotonic offset and the total suspend",
        " * time. Calls to settimeofday will affect the value returned (which",
        " * basically means that however wrong your real time clock is at boot time,",
        " * you get the right time here).",
        " */",
        "void getboottime64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	ktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);",
        "",
        "	*ts = ktime_to_timespec64(t);",
        "}",
        "EXPORT_SYMBOL_GPL(getboottime64);",
        "",
        "void ktime_get_coarse_real_ts64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		*ts = tk_xtime(tk);",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "}",
        "EXPORT_SYMBOL(ktime_get_coarse_real_ts64);",
        "",
        "/**",
        " * ktime_get_coarse_real_ts64_mg - return latter of coarse grained time or floor",
        " * @ts:		timespec64 to be filled",
        " *",
        " * Fetch the global mg_floor value, convert it to realtime and compare it",
        " * to the current coarse-grained time. Fill @ts with whichever is",
        " * latest. Note that this is a filesystem-specific interface and should be",
        " * avoided outside of that context.",
        " */",
        "void ktime_get_coarse_real_ts64_mg(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	u64 floor = atomic64_read(&mg_floor);",
        "	ktime_t f_real, offset, coarse;",
        "	unsigned int seq;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "		*ts = tk_xtime(tk);",
        "		offset = tk_core.timekeeper.offs_real;",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	coarse = timespec64_to_ktime(*ts);",
        "	f_real = ktime_add(floor, offset);",
        "	if (ktime_after(f_real, coarse))",
        "		*ts = ktime_to_timespec64(f_real);",
        "}",
        "",
        "/**",
        " * ktime_get_real_ts64_mg - attempt to update floor value and return result",
        " * @ts:		pointer to the timespec to be set",
        " *",
        " * Get a monotonic fine-grained time value and attempt to swap it into",
        " * mg_floor. If that succeeds then accept the new floor value. If it fails",
        " * then another task raced in during the interim time and updated the",
        " * floor.  Since any update to the floor must be later than the previous",
        " * floor, either outcome is acceptable.",
        " *",
        " * Typically this will be called after calling ktime_get_coarse_real_ts64_mg(),",
        " * and determining that the resulting coarse-grained timestamp did not effect",
        " * a change in ctime. Any more recent floor value would effect a change to",
        " * ctime, so there is no need to retry the atomic64_try_cmpxchg() on failure.",
        " *",
        " * @ts will be filled with the latest floor value, regardless of the outcome of",
        " * the cmpxchg. Note that this is a filesystem specific interface and should be",
        " * avoided outside of that context.",
        " */",
        "void ktime_get_real_ts64_mg(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	ktime_t old = atomic64_read(&mg_floor);",
        "	ktime_t offset, mono;",
        "	unsigned int seq;",
        "	u64 nsecs;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		ts->tv_sec = tk->xtime_sec;",
        "		mono = tk->tkr_mono.base;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_mono);",
        "		offset = tk_core.timekeeper.offs_real;",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	mono = ktime_add_ns(mono, nsecs);",
        "",
        "	/*",
        "	 * Attempt to update the floor with the new time value. As any",
        "	 * update must be later then the existing floor, and would effect",
        "	 * a change to ctime from the perspective of the current task,",
        "	 * accept the resulting floor value regardless of the outcome of",
        "	 * the swap.",
        "	 */",
        "	if (atomic64_try_cmpxchg(&mg_floor, &old, mono)) {",
        "		ts->tv_nsec = 0;",
        "		timespec64_add_ns(ts, nsecs);",
        "		timekeeping_inc_mg_floor_swaps();",
        "	} else {",
        "		/*",
        "		 * Another task changed mg_floor since \"old\" was fetched.",
        "		 * \"old\" has been updated with the latest value of \"mg_floor\".",
        "		 * That value is newer than the previous floor value, which",
        "		 * is enough to effect a change to ctime. Accept it.",
        "		 */",
        "		*ts = ktime_to_timespec64(ktime_add(old, offset));",
        "	}",
        "}",
        "",
        "void ktime_get_coarse_ts64(struct timespec64 *ts)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	struct timespec64 now, mono;",
        "	unsigned int seq;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		now = tk_xtime(tk);",
        "		mono = tk->wall_to_monotonic;",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	set_normalized_timespec64(ts, now.tv_sec + mono.tv_sec,",
        "				now.tv_nsec + mono.tv_nsec);",
        "}",
        "EXPORT_SYMBOL(ktime_get_coarse_ts64);",
        "",
        "/*",
        " * Must hold jiffies_lock",
        " */",
        "void do_timer(unsigned long ticks)",
        "{",
        "	jiffies_64 += ticks;",
        "	calc_global_load();",
        "}",
        "",
        "/**",
        " * ktime_get_update_offsets_now - hrtimer helper",
        " * @cwsseq:	pointer to check and store the clock was set sequence number",
        " * @offs_real:	pointer to storage for monotonic -> realtime offset",
        " * @offs_boot:	pointer to storage for monotonic -> boottime offset",
        " * @offs_tai:	pointer to storage for monotonic -> clock tai offset",
        " *",
        " * Returns current monotonic time and updates the offsets if the",
        " * sequence number in @cwsseq and timekeeper.clock_was_set_seq are",
        " * different.",
        " *",
        " * Called from hrtimer_interrupt() or retrigger_next_event()",
        " */",
        "ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,",
        "				     ktime_t *offs_boot, ktime_t *offs_tai)",
        "{",
        "	struct timekeeper *tk = &tk_core.timekeeper;",
        "	unsigned int seq;",
        "	ktime_t base;",
        "	u64 nsecs;",
        "",
        "	do {",
        "		seq = read_seqcount_begin(&tk_core.seq);",
        "",
        "		base = tk->tkr_mono.base;",
        "		nsecs = timekeeping_get_ns(&tk->tkr_mono);",
        "		base = ktime_add_ns(base, nsecs);",
        "",
        "		if (*cwsseq != tk->clock_was_set_seq) {",
        "			*cwsseq = tk->clock_was_set_seq;",
        "			*offs_real = tk->offs_real;",
        "			*offs_boot = tk->offs_boot;",
        "			*offs_tai = tk->offs_tai;",
        "		}",
        "",
        "		/* Handle leapsecond insertion adjustments */",
        "		if (unlikely(base >= tk->next_leap_ktime))",
        "			*offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));",
        "",
        "	} while (read_seqcount_retry(&tk_core.seq, seq));",
        "",
        "	return base;",
        "}",
        "",
        "/*",
        " * timekeeping_validate_timex - Ensures the timex is ok for use in do_adjtimex",
        " */",
        "static int timekeeping_validate_timex(const struct __kernel_timex *txc)",
        "{",
        "	if (txc->modes & ADJ_ADJTIME) {",
        "		/* singleshot must not be used with any other mode bits */",
        "		if (!(txc->modes & ADJ_OFFSET_SINGLESHOT))",
        "			return -EINVAL;",
        "		if (!(txc->modes & ADJ_OFFSET_READONLY) &&",
        "		    !capable(CAP_SYS_TIME))",
        "			return -EPERM;",
        "	} else {",
        "		/* In order to modify anything, you gotta be super-user! */",
        "		if (txc->modes && !capable(CAP_SYS_TIME))",
        "			return -EPERM;",
        "		/*",
        "		 * if the quartz is off by more than 10% then",
        "		 * something is VERY wrong!",
        "		 */",
        "		if (txc->modes & ADJ_TICK &&",
        "		    (txc->tick <  900000/USER_HZ ||",
        "		     txc->tick > 1100000/USER_HZ))",
        "			return -EINVAL;",
        "	}",
        "",
        "	if (txc->modes & ADJ_SETOFFSET) {",
        "		/* In order to inject time, you gotta be super-user! */",
        "		if (!capable(CAP_SYS_TIME))",
        "			return -EPERM;",
        "",
        "		/*",
        "		 * Validate if a timespec/timeval used to inject a time",
        "		 * offset is valid.  Offsets can be positive or negative, so",
        "		 * we don't check tv_sec. The value of the timeval/timespec",
        "		 * is the sum of its fields,but *NOTE*:",
        "		 * The field tv_usec/tv_nsec must always be non-negative and",
        "		 * we can't have more nanoseconds/microseconds than a second.",
        "		 */",
        "		if (txc->time.tv_usec < 0)",
        "			return -EINVAL;",
        "",
        "		if (txc->modes & ADJ_NANO) {",
        "			if (txc->time.tv_usec >= NSEC_PER_SEC)",
        "				return -EINVAL;",
        "		} else {",
        "			if (txc->time.tv_usec >= USEC_PER_SEC)",
        "				return -EINVAL;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * Check for potential multiplication overflows that can",
        "	 * only happen on 64-bit systems:",
        "	 */",
        "	if ((txc->modes & ADJ_FREQUENCY) && (BITS_PER_LONG == 64)) {",
        "		if (LLONG_MIN / PPM_SCALE > txc->freq)",
        "			return -EINVAL;",
        "		if (LLONG_MAX / PPM_SCALE < txc->freq)",
        "			return -EINVAL;",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * random_get_entropy_fallback - Returns the raw clock source value,",
        " * used by random.c for platforms with no valid random_get_entropy().",
        " */",
        "unsigned long random_get_entropy_fallback(void)",
        "{",
        "	struct tk_read_base *tkr = &tk_core.timekeeper.tkr_mono;",
        "	struct clocksource *clock = READ_ONCE(tkr->clock);",
        "",
        "	if (unlikely(timekeeping_suspended || !clock))",
        "		return 0;",
        "	return clock->read(clock);",
        "}",
        "EXPORT_SYMBOL_GPL(random_get_entropy_fallback);",
        "",
        "/**",
        " * do_adjtimex() - Accessor function to NTP __do_adjtimex function",
        " * @txc:	Pointer to kernel_timex structure containing NTP parameters",
        " */",
        "int do_adjtimex(struct __kernel_timex *txc)",
        "{",
        "	struct audit_ntp_data ad;",
        "	bool offset_set = false;",
        "	bool clock_set = false;",
        "	struct timespec64 ts;",
        "	int ret;",
        "",
        "	/* Validate the data before disabling interrupts */",
        "	ret = timekeeping_validate_timex(txc);",
        "	if (ret)",
        "		return ret;",
        "	add_device_randomness(txc, sizeof(*txc));",
        "",
        "	if (txc->modes & ADJ_SETOFFSET) {",
        "		struct timespec64 delta;",
        "",
        "		delta.tv_sec  = txc->time.tv_sec;",
        "		delta.tv_nsec = txc->time.tv_usec;",
        "		if (!(txc->modes & ADJ_NANO))",
        "			delta.tv_nsec *= 1000;",
        "		ret = timekeeping_inject_offset(&delta);",
        "		if (ret)",
        "			return ret;",
        "",
        "		offset_set = delta.tv_sec != 0;",
        "		audit_tk_injoffset(delta);",
        "	}",
        "",
        "	audit_ntp_init(&ad);",
        "",
        "	ktime_get_real_ts64(&ts);",
        "	add_device_randomness(&ts, sizeof(ts));",
        "",
        "	scoped_guard (raw_spinlock_irqsave, &tk_core.lock) {",
        "		struct timekeeper *tks = &tk_core.shadow_timekeeper;",
        "		s32 orig_tai, tai;",
        "",
        "		orig_tai = tai = tks->tai_offset;",
        "		ret = __do_adjtimex(txc, &ts, &tai, &ad);",
        "",
        "		if (tai != orig_tai) {",
        "			__timekeeping_set_tai_offset(tks, tai);",
        "			timekeeping_update_from_shadow(&tk_core, TK_CLOCK_WAS_SET);",
        "			clock_set = true;",
        "		} else {",
        "			tk_update_leap_state_all(&tk_core);",
        "		}",
        "	}",
        "",
        "	audit_ntp_log(&ad);",
        "",
        "	/* Update the multiplier immediately if frequency was set directly */",
        "	if (txc->modes & (ADJ_FREQUENCY | ADJ_TICK))",
        "		clock_set |= timekeeping_advance(TK_ADV_FREQ);",
        "",
        "	if (clock_set)",
        "		clock_was_set(CLOCK_SET_WALL);",
        "",
        "	ntp_notify_cmos_timer(offset_set);",
        "",
        "	return ret;",
        "}",
        "",
        "#ifdef CONFIG_NTP_PPS",
        "/**",
        " * hardpps() - Accessor function to NTP __hardpps function",
        " * @phase_ts:	Pointer to timespec64 structure representing phase timestamp",
        " * @raw_ts:	Pointer to timespec64 structure representing raw timestamp",
        " */",
        "void hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)",
        "{",
        "	guard(raw_spinlock_irqsave)(&tk_core.lock);",
        "	__hardpps(phase_ts, raw_ts);",
        "}",
        "EXPORT_SYMBOL(hardpps);",
        "#endif /* CONFIG_NTP_PPS */"
    ]
  },
  "lib_errseq_c": {
    path: "lib/errseq.c",
    covered: [124],
    totalLines: 207,
    coveredCount: 1,
    coveragePct: 0.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "#include <linux/err.h>",
        "#include <linux/bug.h>",
        "#include <linux/atomic.h>",
        "#include <linux/errseq.h>",
        "#include <linux/log2.h>",
        "",
        "/*",
        " * An errseq_t is a way of recording errors in one place, and allowing any",
        " * number of \"subscribers\" to tell whether it has changed since a previous",
        " * point where it was sampled.",
        " *",
        " * It's implemented as an unsigned 32-bit value. The low order bits are",
        " * designated to hold an error code (between 0 and -MAX_ERRNO). The upper bits",
        " * are used as a counter. This is done with atomics instead of locking so that",
        " * these functions can be called from any context.",
        " *",
        " * The general idea is for consumers to sample an errseq_t value. That value",
        " * can later be used to tell whether any new errors have occurred since that",
        " * sampling was done.",
        " *",
        " * Note that there is a risk of collisions if new errors are being recorded",
        " * frequently, since we have so few bits to use as a counter.",
        " *",
        " * To mitigate this, one bit is used as a flag to tell whether the value has",
        " * been sampled since a new value was recorded. That allows us to avoid bumping",
        " * the counter if no one has sampled it since the last time an error was",
        " * recorded.",
        " *",
        " * A new errseq_t should always be zeroed out.  A errseq_t value of all zeroes",
        " * is the special (but common) case where there has never been an error. An all",
        " * zero value thus serves as the \"epoch\" if one wishes to know whether there",
        " * has ever been an error set since it was first initialized.",
        " */",
        "",
        "/* The low bits are designated for error code (max of MAX_ERRNO) */",
        "#define ERRSEQ_SHIFT		ilog2(MAX_ERRNO + 1)",
        "",
        "/* This bit is used as a flag to indicate whether the value has been seen */",
        "#define ERRSEQ_SEEN		(1 << ERRSEQ_SHIFT)",
        "",
        "/* The lowest bit of the counter */",
        "#define ERRSEQ_CTR_INC		(1 << (ERRSEQ_SHIFT + 1))",
        "",
        "/**",
        " * errseq_set - set a errseq_t for later reporting",
        " * @eseq: errseq_t field that should be set",
        " * @err: error to set (must be between -1 and -MAX_ERRNO)",
        " *",
        " * This function sets the error in @eseq, and increments the sequence counter",
        " * if the last sequence was sampled at some point in the past.",
        " *",
        " * Any error set will always overwrite an existing error.",
        " *",
        " * Return: The previous value, primarily for debugging purposes. The",
        " * return value should not be used as a previously sampled value in later",
        " * calls as it will not have the SEEN flag set.",
        " */",
        "errseq_t errseq_set(errseq_t *eseq, int err)",
        "{",
        "	errseq_t cur, old;",
        "",
        "	/* MAX_ERRNO must be able to serve as a mask */",
        "	BUILD_BUG_ON_NOT_POWER_OF_2(MAX_ERRNO + 1);",
        "",
        "	/*",
        "	 * Ensure the error code actually fits where we want it to go. If it",
        "	 * doesn't then just throw a warning and don't record anything. We",
        "	 * also don't accept zero here as that would effectively clear a",
        "	 * previous error.",
        "	 */",
        "	old = READ_ONCE(*eseq);",
        "",
        "	if (WARN(unlikely(err == 0 || (unsigned int)-err > MAX_ERRNO),",
        "				\"err = %d\\n\", err))",
        "		return old;",
        "",
        "	for (;;) {",
        "		errseq_t new;",
        "",
        "		/* Clear out error bits and set new error */",
        "		new = (old & ~(MAX_ERRNO|ERRSEQ_SEEN)) | -err;",
        "",
        "		/* Only increment if someone has looked at it */",
        "		if (old & ERRSEQ_SEEN)",
        "			new += ERRSEQ_CTR_INC;",
        "",
        "		/* If there would be no change, then call it done */",
        "		if (new == old) {",
        "			cur = new;",
        "			break;",
        "		}",
        "",
        "		/* Try to swap the new value into place */",
        "		cur = cmpxchg(eseq, old, new);",
        "",
        "		/*",
        "		 * Call it success if we did the swap or someone else beat us",
        "		 * to it for the same value.",
        "		 */",
        "		if (likely(cur == old || cur == new))",
        "			break;",
        "",
        "		/* Raced with an update, try again */",
        "		old = cur;",
        "	}",
        "	return cur;",
        "}",
        "EXPORT_SYMBOL(errseq_set);",
        "",
        "/**",
        " * errseq_sample() - Grab current errseq_t value.",
        " * @eseq: Pointer to errseq_t to be sampled.",
        " *",
        " * This function allows callers to initialise their errseq_t variable.",
        " * If the error has been \"seen\", new callers will not see an old error.",
        " * If there is an unseen error in @eseq, the caller of this function will",
        " * see it the next time it checks for an error.",
        " *",
        " * Context: Any context.",
        " * Return: The current errseq value.",
        " */",
        "errseq_t errseq_sample(errseq_t *eseq)",
        "{",
        "	errseq_t old = READ_ONCE(*eseq);",
        "",
        "	/* If nobody has seen this error yet, then we can be the first. */",
        "	if (!(old & ERRSEQ_SEEN))",
        "		old = 0;",
        "	return old;",
        "}",
        "EXPORT_SYMBOL(errseq_sample);",
        "",
        "/**",
        " * errseq_check() - Has an error occurred since a particular sample point?",
        " * @eseq: Pointer to errseq_t value to be checked.",
        " * @since: Previously-sampled errseq_t from which to check.",
        " *",
        " * Grab the value that eseq points to, and see if it has changed @since",
        " * the given value was sampled. The @since value is not advanced, so there",
        " * is no need to mark the value as seen.",
        " *",
        " * Return: The latest error set in the errseq_t or 0 if it hasn't changed.",
        " */",
        "int errseq_check(errseq_t *eseq, errseq_t since)",
        "{",
        "	errseq_t cur = READ_ONCE(*eseq);",
        "",
        "	if (likely(cur == since))",
        "		return 0;",
        "	return -(cur & MAX_ERRNO);",
        "}",
        "EXPORT_SYMBOL(errseq_check);",
        "",
        "/**",
        " * errseq_check_and_advance() - Check an errseq_t and advance to current value.",
        " * @eseq: Pointer to value being checked and reported.",
        " * @since: Pointer to previously-sampled errseq_t to check against and advance.",
        " *",
        " * Grab the eseq value, and see whether it matches the value that @since",
        " * points to. If it does, then just return 0.",
        " *",
        " * If it doesn't, then the value has changed. Set the \"seen\" flag, and try to",
        " * swap it into place as the new eseq value. Then, set that value as the new",
        " * \"since\" value, and return whatever the error portion is set to.",
        " *",
        " * Note that no locking is provided here for concurrent updates to the \"since\"",
        " * value. The caller must provide that if necessary. Because of this, callers",
        " * may want to do a lockless errseq_check before taking the lock and calling",
        " * this.",
        " *",
        " * Return: Negative errno if one has been stored, or 0 if no new error has",
        " * occurred.",
        " */",
        "int errseq_check_and_advance(errseq_t *eseq, errseq_t *since)",
        "{",
        "	int err = 0;",
        "	errseq_t old, new;",
        "",
        "	/*",
        "	 * Most callers will want to use the inline wrapper to check this,",
        "	 * so that the common case of no error is handled without needing",
        "	 * to take the lock that protects the \"since\" value.",
        "	 */",
        "	old = READ_ONCE(*eseq);",
        "	if (old != *since) {",
        "		/*",
        "		 * Set the flag and try to swap it into place if it has",
        "		 * changed.",
        "		 *",
        "		 * We don't care about the outcome of the swap here. If the",
        "		 * swap doesn't occur, then it has either been updated by a",
        "		 * writer who is altering the value in some way (updating",
        "		 * counter or resetting the error), or another reader who is",
        "		 * just setting the \"seen\" flag. Either outcome is OK, and we",
        "		 * can advance \"since\" and return an error based on what we",
        "		 * have.",
        "		 */",
        "		new = old | ERRSEQ_SEEN;",
        "		if (new != old)",
        "			cmpxchg(eseq, old, new);",
        "		*since = new;",
        "		err = -(new & MAX_ERRNO);",
        "	}",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(errseq_check_and_advance);"
    ]
  },
  "fs_namei_c": {
    path: "fs/namei.c",
    covered: [1572, 1950, 755, 852, 926, 283, 2432, 1621, 680, 1955, 2481, 2013, 296, 2467, 293, 1793, 747, 2439, 1991, 956, 402, 2487, 2608, 401, 1804, 1716, 1952, 2434, 1962, 2410, 811, 748, 667, 2517, 404, 588, 2670, 840, 723, 2403, 2579, 2633, 2623, 2106, 569, 2669, 282, 2510, 162, 154, 2454, 849, 943, 1025, 790, 732, 210, 2571, 2664, 173, 2462, 2309, 1947, 592, 2065, 545, 2636, 1820, 523, 1639, 2104, 1740, 596, 1989, 1015, 1013, 2414, 2096, 984, 691, 2607, 2072, 2657, 1619, 2672, 471, 156, 2557, 2430, 2069, 2627, 1792, 2554, 1728, 2661, 2113, 201, 2593, 960, 624, 1016, 465, 2518, 2116, 1794, 2447, 2009, 693, 2412, 195, 2542, 2643, 754, 759, 797, 2666, 670, 2477, 678, 2058, 2558, 2662, 2111, 752, 2634, 2078, 998, 135, 130, 1028, 3068, 954, 2499, 2059, 549, 2425, 572, 1767],
    totalLines: 5455,
    coveredCount: 138,
    coveragePct: 2.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/*",
        " *  linux/fs/namei.c",
        " *",
        " *  Copyright (C) 1991, 1992  Linus Torvalds",
        " */",
        "",
        "/*",
        " * Some corrections by tytso.",
        " */",
        "",
        "/* [Feb 1997 T. Schoebel-Theuer] Complete rewrite of the pathname",
        " * lookup logic.",
        " */",
        "/* [Feb-Apr 2000, AV] Rewrite to the new namespace architecture.",
        " */",
        "",
        "#include <linux/init.h>",
        "#include <linux/export.h>",
        "#include <linux/slab.h>",
        "#include <linux/wordpart.h>",
        "#include <linux/fs.h>",
        "#include <linux/filelock.h>",
        "#include <linux/namei.h>",
        "#include <linux/pagemap.h>",
        "#include <linux/sched/mm.h>",
        "#include <linux/fsnotify.h>",
        "#include <linux/personality.h>",
        "#include <linux/security.h>",
        "#include <linux/syscalls.h>",
        "#include <linux/mount.h>",
        "#include <linux/audit.h>",
        "#include <linux/capability.h>",
        "#include <linux/file.h>",
        "#include <linux/fcntl.h>",
        "#include <linux/device_cgroup.h>",
        "#include <linux/fs_struct.h>",
        "#include <linux/posix_acl.h>",
        "#include <linux/hash.h>",
        "#include <linux/bitops.h>",
        "#include <linux/init_task.h>",
        "#include <linux/uaccess.h>",
        "",
        "#include \"internal.h\"",
        "#include \"mount.h\"",
        "",
        "/* [Feb-1997 T. Schoebel-Theuer]",
        " * Fundamental changes in the pathname lookup mechanisms (namei)",
        " * were necessary because of omirr.  The reason is that omirr needs",
        " * to know the _real_ pathname, not the user-supplied one, in case",
        " * of symlinks (and also when transname replacements occur).",
        " *",
        " * The new code replaces the old recursive symlink resolution with",
        " * an iterative one (in case of non-nested symlink chains).  It does",
        " * this with calls to <fs>_follow_link().",
        " * As a side effect, dir_namei(), _namei() and follow_link() are now ",
        " * replaced with a single function lookup_dentry() that can handle all ",
        " * the special cases of the former code.",
        " *",
        " * With the new dcache, the pathname is stored at each inode, at least as",
        " * long as the refcount of the inode is positive.  As a side effect, the",
        " * size of the dcache depends on the inode cache and thus is dynamic.",
        " *",
        " * [29-Apr-1998 C. Scott Ananian] Updated above description of symlink",
        " * resolution to correspond with current state of the code.",
        " *",
        " * Note that the symlink resolution is not *completely* iterative.",
        " * There is still a significant amount of tail- and mid- recursion in",
        " * the algorithm.  Also, note that <fs>_readlink() is not used in",
        " * lookup_dentry(): lookup_dentry() on the result of <fs>_readlink()",
        " * may return different results than <fs>_follow_link().  Many virtual",
        " * filesystems (including /proc) exhibit this behavior.",
        " */",
        "",
        "/* [24-Feb-97 T. Schoebel-Theuer] Side effects caused by new implementation:",
        " * New symlink semantics: when open() is called with flags O_CREAT | O_EXCL",
        " * and the name already exists in form of a symlink, try to create the new",
        " * name indicated by the symlink. The old code always complained that the",
        " * name already exists, due to not following the symlink even if its target",
        " * is nonexistent.  The new semantics affects also mknod() and link() when",
        " * the name is a symlink pointing to a non-existent name.",
        " *",
        " * I don't know which semantics is the right one, since I have no access",
        " * to standards. But I found by trial that HP-UX 9.0 has the full \"new\"",
        " * semantics implemented, while SunOS 4.1.1 and Solaris (SunOS 5.4) have the",
        " * \"old\" one. Personally, I think the new semantics is much more logical.",
        " * Note that \"ln old new\" where \"new\" is a symlink pointing to a non-existing",
        " * file does succeed in both HP-UX and SunOs, but not in Solaris",
        " * and in the old Linux semantics.",
        " */",
        "",
        "/* [16-Dec-97 Kevin Buhr] For security reasons, we change some symlink",
        " * semantics.  See the comments in \"open_namei\" and \"do_link\" below.",
        " *",
        " * [10-Sep-98 Alan Modra] Another symlink change.",
        " */",
        "",
        "/* [Feb-Apr 2000 AV] Complete rewrite. Rules for symlinks:",
        " *	inside the path - always follow.",
        " *	in the last component in creation/removal/renaming - never follow.",
        " *	if LOOKUP_FOLLOW passed - follow.",
        " *	if the pathname has trailing slashes - follow.",
        " *	otherwise - don't follow.",
        " * (applied in that order).",
        " *",
        " * [Jun 2000 AV] Inconsistent behaviour of open() in case if flags==O_CREAT",
        " * restored for 2.4. This is the last surviving part of old 4.2BSD bug.",
        " * During the 2.4 we need to fix the userland stuff depending on it -",
        " * hopefully we will be able to get rid of that wart in 2.5. So far only",
        " * XEmacs seems to be relying on it...",
        " */",
        "/*",
        " * [Sep 2001 AV] Single-semaphore locking scheme (kudos to David Holland)",
        " * implemented.  Let's see if raised priority of ->s_vfs_rename_mutex gives",
        " * any extra contention...",
        " */",
        "",
        "/* In order to reduce some races, while at the same time doing additional",
        " * checking and hopefully speeding things up, we copy filenames to the",
        " * kernel data space before using them..",
        " *",
        " * POSIX.1 2.4: an empty pathname is invalid (ENOENT).",
        " * PATH_MAX includes the nul terminator --RR.",
        " */",
        "",
        "#define EMBEDDED_NAME_MAX	(PATH_MAX - offsetof(struct filename, iname))",
        "",
        "struct filename *",
        "getname_flags(const char __user *filename, int flags)",
        "{",
        "	struct filename *result;",
        "	char *kname;",
        "	int len;",
        "",
        "	result = audit_reusename(filename);",
        "	if (result)",
        "		return result;",
        "",
        "	result = __getname();",
        "	if (unlikely(!result))",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	/*",
        "	 * First, try to embed the struct filename inside the names_cache",
        "	 * allocation",
        "	 */",
        "	kname = (char *)result->iname;",
        "	result->name = kname;",
        "",
        "	len = strncpy_from_user(kname, filename, EMBEDDED_NAME_MAX);",
        "	/*",
        "	 * Handle both empty path and copy failure in one go.",
        "	 */",
        "	if (unlikely(len <= 0)) {",
        "		if (unlikely(len < 0)) {",
        "			__putname(result);",
        "			return ERR_PTR(len);",
        "		}",
        "",
        "		/* The empty path is special. */",
        "		if (!(flags & LOOKUP_EMPTY)) {",
        "			__putname(result);",
        "			return ERR_PTR(-ENOENT);",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * Uh-oh. We have a name that's approaching PATH_MAX. Allocate a",
        "	 * separate struct filename so we can dedicate the entire",
        "	 * names_cache allocation for the pathname, and re-do the copy from",
        "	 * userland.",
        "	 */",
        "	if (unlikely(len == EMBEDDED_NAME_MAX)) {",
        "		const size_t size = offsetof(struct filename, iname[1]);",
        "		kname = (char *)result;",
        "",
        "		/*",
        "		 * size is chosen that way we to guarantee that",
        "		 * result->iname[0] is within the same object and that",
        "		 * kname can't be equal to result->iname, no matter what.",
        "		 */",
        "		result = kzalloc(size, GFP_KERNEL);",
        "		if (unlikely(!result)) {",
        "			__putname(kname);",
        "			return ERR_PTR(-ENOMEM);",
        "		}",
        "		result->name = kname;",
        "		len = strncpy_from_user(kname, filename, PATH_MAX);",
        "		if (unlikely(len < 0)) {",
        "			__putname(kname);",
        "			kfree(result);",
        "			return ERR_PTR(len);",
        "		}",
        "		/* The empty path is special. */",
        "		if (unlikely(!len) && !(flags & LOOKUP_EMPTY)) {",
        "			__putname(kname);",
        "			kfree(result);",
        "			return ERR_PTR(-ENOENT);",
        "		}",
        "		if (unlikely(len == PATH_MAX)) {",
        "			__putname(kname);",
        "			kfree(result);",
        "			return ERR_PTR(-ENAMETOOLONG);",
        "		}",
        "	}",
        "",
        "	atomic_set(&result->refcnt, 1);",
        "	result->uptr = filename;",
        "	result->aname = NULL;",
        "	audit_getname(result);",
        "	return result;",
        "}",
        "",
        "struct filename *getname_uflags(const char __user *filename, int uflags)",
        "{",
        "	int flags = (uflags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;",
        "",
        "	return getname_flags(filename, flags);",
        "}",
        "",
        "struct filename *getname(const char __user * filename)",
        "{",
        "	return getname_flags(filename, 0);",
        "}",
        "",
        "struct filename *__getname_maybe_null(const char __user *pathname)",
        "{",
        "	struct filename *name;",
        "	char c;",
        "",
        "	/* try to save on allocations; loss on um, though */",
        "	if (get_user(c, pathname))",
        "		return ERR_PTR(-EFAULT);",
        "	if (!c)",
        "		return NULL;",
        "",
        "	name = getname_flags(pathname, LOOKUP_EMPTY);",
        "	if (!IS_ERR(name) && !(name->name[0])) {",
        "		putname(name);",
        "		name = NULL;",
        "	}",
        "	return name;",
        "}",
        "",
        "struct filename *getname_kernel(const char * filename)",
        "{",
        "	struct filename *result;",
        "	int len = strlen(filename) + 1;",
        "",
        "	result = __getname();",
        "	if (unlikely(!result))",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	if (len <= EMBEDDED_NAME_MAX) {",
        "		result->name = (char *)result->iname;",
        "	} else if (len <= PATH_MAX) {",
        "		const size_t size = offsetof(struct filename, iname[1]);",
        "		struct filename *tmp;",
        "",
        "		tmp = kmalloc(size, GFP_KERNEL);",
        "		if (unlikely(!tmp)) {",
        "			__putname(result);",
        "			return ERR_PTR(-ENOMEM);",
        "		}",
        "		tmp->name = (char *)result;",
        "		result = tmp;",
        "	} else {",
        "		__putname(result);",
        "		return ERR_PTR(-ENAMETOOLONG);",
        "	}",
        "	memcpy((char *)result->name, filename, len);",
        "	result->uptr = NULL;",
        "	result->aname = NULL;",
        "	atomic_set(&result->refcnt, 1);",
        "	audit_getname(result);",
        "",
        "	return result;",
        "}",
        "EXPORT_SYMBOL(getname_kernel);",
        "",
        "void putname(struct filename *name)",
        "{",
        "	if (IS_ERR_OR_NULL(name))",
        "		return;",
        "",
        "	if (WARN_ON_ONCE(!atomic_read(&name->refcnt)))",
        "		return;",
        "",
        "	if (!atomic_dec_and_test(&name->refcnt))",
        "		return;",
        "",
        "	if (name->name != name->iname) {",
        "		__putname(name->name);",
        "		kfree(name);",
        "	} else",
        "		__putname(name);",
        "}",
        "EXPORT_SYMBOL(putname);",
        "",
        "/**",
        " * check_acl - perform ACL permission checking",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @inode:	inode to check permissions on",
        " * @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)",
        " *",
        " * This function performs the ACL permission checking. Since this function",
        " * retrieve POSIX acls it needs to know whether it is called from a blocking or",
        " * non-blocking context and thus cares about the MAY_NOT_BLOCK bit.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "static int check_acl(struct mnt_idmap *idmap,",
        "		     struct inode *inode, int mask)",
        "{",
        "#ifdef CONFIG_FS_POSIX_ACL",
        "	struct posix_acl *acl;",
        "",
        "	if (mask & MAY_NOT_BLOCK) {",
        "		acl = get_cached_acl_rcu(inode, ACL_TYPE_ACCESS);",
        "	        if (!acl)",
        "	                return -EAGAIN;",
        "		/* no ->get_inode_acl() calls in RCU mode... */",
        "		if (is_uncached_acl(acl))",
        "			return -ECHILD;",
        "	        return posix_acl_permission(idmap, inode, acl, mask);",
        "	}",
        "",
        "	acl = get_inode_acl(inode, ACL_TYPE_ACCESS);",
        "	if (IS_ERR(acl))",
        "		return PTR_ERR(acl);",
        "	if (acl) {",
        "	        int error = posix_acl_permission(idmap, inode, acl, mask);",
        "	        posix_acl_release(acl);",
        "	        return error;",
        "	}",
        "#endif",
        "",
        "	return -EAGAIN;",
        "}",
        "",
        "/*",
        " * Very quick optimistic \"we know we have no ACL's\" check.",
        " *",
        " * Note that this is purely for ACL_TYPE_ACCESS, and purely",
        " * for the \"we have cached that there are no ACLs\" case.",
        " *",
        " * If this returns true, we know there are no ACLs. But if",
        " * it returns false, we might still not have ACLs (it could",
        " * be the is_uncached_acl() case).",
        " */",
        "static inline bool no_acl_inode(struct inode *inode)",
        "{",
        "#ifdef CONFIG_FS_POSIX_ACL",
        "	return likely(!READ_ONCE(inode->i_acl));",
        "#else",
        "	return true;",
        "#endif",
        "}",
        "",
        "/**",
        " * acl_permission_check - perform basic UNIX permission checking",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @inode:	inode to check permissions on",
        " * @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)",
        " *",
        " * This function performs the basic UNIX permission checking. Since this",
        " * function may retrieve POSIX acls it needs to know whether it is called from a",
        " * blocking or non-blocking context and thus cares about the MAY_NOT_BLOCK bit.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "static int acl_permission_check(struct mnt_idmap *idmap,",
        "				struct inode *inode, int mask)",
        "{",
        "	unsigned int mode = inode->i_mode;",
        "	vfsuid_t vfsuid;",
        "",
        "	/*",
        "	 * Common cheap case: everybody has the requested",
        "	 * rights, and there are no ACLs to check. No need",
        "	 * to do any owner/group checks in that case.",
        "	 *",
        "	 *  - 'mask&7' is the requested permission bit set",
        "	 *  - multiplying by 0111 spreads them out to all of ugo",
        "	 *  - '& ~mode' looks for missing inode permission bits",
        "	 *  - the '!' is for \"no missing permissions\"",
        "	 *",
        "	 * After that, we just need to check that there are no",
        "	 * ACL's on the inode - do the 'IS_POSIXACL()' check last",
        "	 * because it will dereference the ->i_sb pointer and we",
        "	 * want to avoid that if at all possible.",
        "	 */",
        "	if (!((mask & 7) * 0111 & ~mode)) {",
        "		if (no_acl_inode(inode))",
        "			return 0;",
        "		if (!IS_POSIXACL(inode))",
        "			return 0;",
        "	}",
        "",
        "	/* Are we the owner? If so, ACL's don't matter */",
        "	vfsuid = i_uid_into_vfsuid(idmap, inode);",
        "	if (likely(vfsuid_eq_kuid(vfsuid, current_fsuid()))) {",
        "		mask &= 7;",
        "		mode >>= 6;",
        "		return (mask & ~mode) ? -EACCES : 0;",
        "	}",
        "",
        "	/* Do we have ACL's? */",
        "	if (IS_POSIXACL(inode) && (mode & S_IRWXG)) {",
        "		int error = check_acl(idmap, inode, mask);",
        "		if (error != -EAGAIN)",
        "			return error;",
        "	}",
        "",
        "	/* Only RWX matters for group/other mode bits */",
        "	mask &= 7;",
        "",
        "	/*",
        "	 * Are the group permissions different from",
        "	 * the other permissions in the bits we care",
        "	 * about? Need to check group ownership if so.",
        "	 */",
        "	if (mask & (mode ^ (mode >> 3))) {",
        "		vfsgid_t vfsgid = i_gid_into_vfsgid(idmap, inode);",
        "		if (vfsgid_in_group_p(vfsgid))",
        "			mode >>= 3;",
        "	}",
        "",
        "	/* Bits in 'mode' clear that we require? */",
        "	return (mask & ~mode) ? -EACCES : 0;",
        "}",
        "",
        "/**",
        " * generic_permission -  check for access rights on a Posix-like filesystem",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @inode:	inode to check access rights for",
        " * @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC,",
        " *		%MAY_NOT_BLOCK ...)",
        " *",
        " * Used to check for read/write/execute permissions on a file.",
        " * We use \"fsuid\" for this, letting us set arbitrary permissions",
        " * for filesystem access without changing the \"normal\" uids which",
        " * are used for other things.",
        " *",
        " * generic_permission is rcu-walk aware. It returns -ECHILD in case an rcu-walk",
        " * request cannot be satisfied (eg. requires blocking or too much complexity).",
        " * It would then be called again in ref-walk mode.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int generic_permission(struct mnt_idmap *idmap, struct inode *inode,",
        "		       int mask)",
        "{",
        "	int ret;",
        "",
        "	/*",
        "	 * Do the basic permission checks.",
        "	 */",
        "	ret = acl_permission_check(idmap, inode, mask);",
        "	if (ret != -EACCES)",
        "		return ret;",
        "",
        "	if (S_ISDIR(inode->i_mode)) {",
        "		/* DACs are overridable for directories */",
        "		if (!(mask & MAY_WRITE))",
        "			if (capable_wrt_inode_uidgid(idmap, inode,",
        "						     CAP_DAC_READ_SEARCH))",
        "				return 0;",
        "		if (capable_wrt_inode_uidgid(idmap, inode,",
        "					     CAP_DAC_OVERRIDE))",
        "			return 0;",
        "		return -EACCES;",
        "	}",
        "",
        "	/*",
        "	 * Searching includes executable on directories, else just read.",
        "	 */",
        "	mask &= MAY_READ | MAY_WRITE | MAY_EXEC;",
        "	if (mask == MAY_READ)",
        "		if (capable_wrt_inode_uidgid(idmap, inode,",
        "					     CAP_DAC_READ_SEARCH))",
        "			return 0;",
        "	/*",
        "	 * Read/write DACs are always overridable.",
        "	 * Executable DACs are overridable when there is",
        "	 * at least one exec bit set.",
        "	 */",
        "	if (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))",
        "		if (capable_wrt_inode_uidgid(idmap, inode,",
        "					     CAP_DAC_OVERRIDE))",
        "			return 0;",
        "",
        "	return -EACCES;",
        "}",
        "EXPORT_SYMBOL(generic_permission);",
        "",
        "/**",
        " * do_inode_permission - UNIX permission checking",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @inode:	inode to check permissions on",
        " * @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)",
        " *",
        " * We _really_ want to just do \"generic_permission()\" without",
        " * even looking at the inode->i_op values. So we keep a cache",
        " * flag in inode->i_opflags, that says \"this has not special",
        " * permission function, use the fast case\".",
        " */",
        "static inline int do_inode_permission(struct mnt_idmap *idmap,",
        "				      struct inode *inode, int mask)",
        "{",
        "	if (unlikely(!(inode->i_opflags & IOP_FASTPERM))) {",
        "		if (likely(inode->i_op->permission))",
        "			return inode->i_op->permission(idmap, inode, mask);",
        "",
        "		/* This gets set once for the inode lifetime */",
        "		spin_lock(&inode->i_lock);",
        "		inode->i_opflags |= IOP_FASTPERM;",
        "		spin_unlock(&inode->i_lock);",
        "	}",
        "	return generic_permission(idmap, inode, mask);",
        "}",
        "",
        "/**",
        " * sb_permission - Check superblock-level permissions",
        " * @sb: Superblock of inode to check permission on",
        " * @inode: Inode to check permission on",
        " * @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)",
        " *",
        " * Separate out file-system wide checks from inode-specific permission checks.",
        " */",
        "static int sb_permission(struct super_block *sb, struct inode *inode, int mask)",
        "{",
        "	if (unlikely(mask & MAY_WRITE)) {",
        "		umode_t mode = inode->i_mode;",
        "",
        "		/* Nobody gets write access to a read-only fs. */",
        "		if (sb_rdonly(sb) && (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)))",
        "			return -EROFS;",
        "	}",
        "	return 0;",
        "}",
        "",
        "/**",
        " * inode_permission - Check for access rights to a given inode",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @inode:	Inode to check permission on",
        " * @mask:	Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)",
        " *",
        " * Check for read/write/execute permissions on an inode.  We use fs[ug]id for",
        " * this, letting us set arbitrary permissions for filesystem access without",
        " * changing the \"normal\" UIDs which are used for other things.",
        " *",
        " * When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.",
        " */",
        "int inode_permission(struct mnt_idmap *idmap,",
        "		     struct inode *inode, int mask)",
        "{",
        "	int retval;",
        "",
        "	retval = sb_permission(inode->i_sb, inode, mask);",
        "	if (retval)",
        "		return retval;",
        "",
        "	if (unlikely(mask & MAY_WRITE)) {",
        "		/*",
        "		 * Nobody gets write access to an immutable file.",
        "		 */",
        "		if (IS_IMMUTABLE(inode))",
        "			return -EPERM;",
        "",
        "		/*",
        "		 * Updating mtime will likely cause i_uid and i_gid to be",
        "		 * written back improperly if their true value is unknown",
        "		 * to the vfs.",
        "		 */",
        "		if (HAS_UNMAPPED_ID(idmap, inode))",
        "			return -EACCES;",
        "	}",
        "",
        "	retval = do_inode_permission(idmap, inode, mask);",
        "	if (retval)",
        "		return retval;",
        "",
        "	retval = devcgroup_inode_permission(inode, mask);",
        "	if (retval)",
        "		return retval;",
        "",
        "	return security_inode_permission(inode, mask);",
        "}",
        "EXPORT_SYMBOL(inode_permission);",
        "",
        "/**",
        " * path_get - get a reference to a path",
        " * @path: path to get the reference to",
        " *",
        " * Given a path increment the reference count to the dentry and the vfsmount.",
        " */",
        "void path_get(const struct path *path)",
        "{",
        "	mntget(path->mnt);",
        "	dget(path->dentry);",
        "}",
        "EXPORT_SYMBOL(path_get);",
        "",
        "/**",
        " * path_put - put a reference to a path",
        " * @path: path to put the reference to",
        " *",
        " * Given a path decrement the reference count to the dentry and the vfsmount.",
        " */",
        "void path_put(const struct path *path)",
        "{",
        "	dput(path->dentry);",
        "	mntput(path->mnt);",
        "}",
        "EXPORT_SYMBOL(path_put);",
        "",
        "#define EMBEDDED_LEVELS 2",
        "struct nameidata {",
        "	struct path	path;",
        "	struct qstr	last;",
        "	struct path	root;",
        "	struct inode	*inode; /* path.dentry.d_inode */",
        "	unsigned int	flags, state;",
        "	unsigned	seq, next_seq, m_seq, r_seq;",
        "	int		last_type;",
        "	unsigned	depth;",
        "	int		total_link_count;",
        "	struct saved {",
        "		struct path link;",
        "		struct delayed_call done;",
        "		const char *name;",
        "		unsigned seq;",
        "	} *stack, internal[EMBEDDED_LEVELS];",
        "	struct filename	*name;",
        "	const char *pathname;",
        "	struct nameidata *saved;",
        "	unsigned	root_seq;",
        "	int		dfd;",
        "	vfsuid_t	dir_vfsuid;",
        "	umode_t		dir_mode;",
        "} __randomize_layout;",
        "",
        "#define ND_ROOT_PRESET 1",
        "#define ND_ROOT_GRABBED 2",
        "#define ND_JUMPED 4",
        "",
        "static void __set_nameidata(struct nameidata *p, int dfd, struct filename *name)",
        "{",
        "	struct nameidata *old = current->nameidata;",
        "	p->stack = p->internal;",
        "	p->depth = 0;",
        "	p->dfd = dfd;",
        "	p->name = name;",
        "	p->pathname = likely(name) ? name->name : \"\";",
        "	p->path.mnt = NULL;",
        "	p->path.dentry = NULL;",
        "	p->total_link_count = old ? old->total_link_count : 0;",
        "	p->saved = old;",
        "	current->nameidata = p;",
        "}",
        "",
        "static inline void set_nameidata(struct nameidata *p, int dfd, struct filename *name,",
        "			  const struct path *root)",
        "{",
        "	__set_nameidata(p, dfd, name);",
        "	p->state = 0;",
        "	if (unlikely(root)) {",
        "		p->state = ND_ROOT_PRESET;",
        "		p->root = *root;",
        "	}",
        "}",
        "",
        "static void restore_nameidata(void)",
        "{",
        "	struct nameidata *now = current->nameidata, *old = now->saved;",
        "",
        "	current->nameidata = old;",
        "	if (old)",
        "		old->total_link_count = now->total_link_count;",
        "	if (now->stack != now->internal)",
        "		kfree(now->stack);",
        "}",
        "",
        "static bool nd_alloc_stack(struct nameidata *nd)",
        "{",
        "	struct saved *p;",
        "",
        "	p= kmalloc_array(MAXSYMLINKS, sizeof(struct saved),",
        "			 nd->flags & LOOKUP_RCU ? GFP_ATOMIC : GFP_KERNEL);",
        "	if (unlikely(!p))",
        "		return false;",
        "	memcpy(p, nd->internal, sizeof(nd->internal));",
        "	nd->stack = p;",
        "	return true;",
        "}",
        "",
        "/**",
        " * path_connected - Verify that a dentry is below mnt.mnt_root",
        " * @mnt: The mountpoint to check.",
        " * @dentry: The dentry to check.",
        " *",
        " * Rename can sometimes move a file or directory outside of a bind",
        " * mount, path_connected allows those cases to be detected.",
        " */",
        "static bool path_connected(struct vfsmount *mnt, struct dentry *dentry)",
        "{",
        "	struct super_block *sb = mnt->mnt_sb;",
        "",
        "	/* Bind mounts can have disconnected paths */",
        "	if (mnt->mnt_root == sb->s_root)",
        "		return true;",
        "",
        "	return is_subdir(dentry, mnt->mnt_root);",
        "}",
        "",
        "static void drop_links(struct nameidata *nd)",
        "{",
        "	int i = nd->depth;",
        "	while (i--) {",
        "		struct saved *last = nd->stack + i;",
        "		do_delayed_call(&last->done);",
        "		clear_delayed_call(&last->done);",
        "	}",
        "}",
        "",
        "static void leave_rcu(struct nameidata *nd)",
        "{",
        "	nd->flags &= ~LOOKUP_RCU;",
        "	nd->seq = nd->next_seq = 0;",
        "	rcu_read_unlock();",
        "}",
        "",
        "static void terminate_walk(struct nameidata *nd)",
        "{",
        "	drop_links(nd);",
        "	if (!(nd->flags & LOOKUP_RCU)) {",
        "		int i;",
        "		path_put(&nd->path);",
        "		for (i = 0; i < nd->depth; i++)",
        "			path_put(&nd->stack[i].link);",
        "		if (nd->state & ND_ROOT_GRABBED) {",
        "			path_put(&nd->root);",
        "			nd->state &= ~ND_ROOT_GRABBED;",
        "		}",
        "	} else {",
        "		leave_rcu(nd);",
        "	}",
        "	nd->depth = 0;",
        "	nd->path.mnt = NULL;",
        "	nd->path.dentry = NULL;",
        "}",
        "",
        "/* path_put is needed afterwards regardless of success or failure */",
        "static bool __legitimize_path(struct path *path, unsigned seq, unsigned mseq)",
        "{",
        "	int res = __legitimize_mnt(path->mnt, mseq);",
        "	if (unlikely(res)) {",
        "		if (res > 0)",
        "			path->mnt = NULL;",
        "		path->dentry = NULL;",
        "		return false;",
        "	}",
        "	if (unlikely(!lockref_get_not_dead(&path->dentry->d_lockref))) {",
        "		path->dentry = NULL;",
        "		return false;",
        "	}",
        "	return !read_seqcount_retry(&path->dentry->d_seq, seq);",
        "}",
        "",
        "static inline bool legitimize_path(struct nameidata *nd,",
        "			    struct path *path, unsigned seq)",
        "{",
        "	return __legitimize_path(path, seq, nd->m_seq);",
        "}",
        "",
        "static bool legitimize_links(struct nameidata *nd)",
        "{",
        "	int i;",
        "	if (unlikely(nd->flags & LOOKUP_CACHED)) {",
        "		drop_links(nd);",
        "		nd->depth = 0;",
        "		return false;",
        "	}",
        "	for (i = 0; i < nd->depth; i++) {",
        "		struct saved *last = nd->stack + i;",
        "		if (unlikely(!legitimize_path(nd, &last->link, last->seq))) {",
        "			drop_links(nd);",
        "			nd->depth = i + 1;",
        "			return false;",
        "		}",
        "	}",
        "	return true;",
        "}",
        "",
        "static bool legitimize_root(struct nameidata *nd)",
        "{",
        "	/* Nothing to do if nd->root is zero or is managed by the VFS user. */",
        "	if (!nd->root.mnt || (nd->state & ND_ROOT_PRESET))",
        "		return true;",
        "	nd->state |= ND_ROOT_GRABBED;",
        "	return legitimize_path(nd, &nd->root, nd->root_seq);",
        "}",
        "",
        "/*",
        " * Path walking has 2 modes, rcu-walk and ref-walk (see",
        " * Documentation/filesystems/path-lookup.txt).  In situations when we can't",
        " * continue in RCU mode, we attempt to drop out of rcu-walk mode and grab",
        " * normal reference counts on dentries and vfsmounts to transition to ref-walk",
        " * mode.  Refcounts are grabbed at the last known good point before rcu-walk",
        " * got stuck, so ref-walk may continue from there. If this is not successful",
        " * (eg. a seqcount has changed), then failure is returned and it's up to caller",
        " * to restart the path walk from the beginning in ref-walk mode.",
        " */",
        "",
        "/**",
        " * try_to_unlazy - try to switch to ref-walk mode.",
        " * @nd: nameidata pathwalk data",
        " * Returns: true on success, false on failure",
        " *",
        " * try_to_unlazy attempts to legitimize the current nd->path and nd->root",
        " * for ref-walk mode.",
        " * Must be called from rcu-walk context.",
        " * Nothing should touch nameidata between try_to_unlazy() failure and",
        " * terminate_walk().",
        " */",
        "static bool try_to_unlazy(struct nameidata *nd)",
        "{",
        "	struct dentry *parent = nd->path.dentry;",
        "",
        "	BUG_ON(!(nd->flags & LOOKUP_RCU));",
        "",
        "	if (unlikely(!legitimize_links(nd)))",
        "		goto out1;",
        "	if (unlikely(!legitimize_path(nd, &nd->path, nd->seq)))",
        "		goto out;",
        "	if (unlikely(!legitimize_root(nd)))",
        "		goto out;",
        "	leave_rcu(nd);",
        "	BUG_ON(nd->inode != parent->d_inode);",
        "	return true;",
        "",
        "out1:",
        "	nd->path.mnt = NULL;",
        "	nd->path.dentry = NULL;",
        "out:",
        "	leave_rcu(nd);",
        "	return false;",
        "}",
        "",
        "/**",
        " * try_to_unlazy_next - try to switch to ref-walk mode.",
        " * @nd: nameidata pathwalk data",
        " * @dentry: next dentry to step into",
        " * Returns: true on success, false on failure",
        " *",
        " * Similar to try_to_unlazy(), but here we have the next dentry already",
        " * picked by rcu-walk and want to legitimize that in addition to the current",
        " * nd->path and nd->root for ref-walk mode.  Must be called from rcu-walk context.",
        " * Nothing should touch nameidata between try_to_unlazy_next() failure and",
        " * terminate_walk().",
        " */",
        "static bool try_to_unlazy_next(struct nameidata *nd, struct dentry *dentry)",
        "{",
        "	int res;",
        "	BUG_ON(!(nd->flags & LOOKUP_RCU));",
        "",
        "	if (unlikely(!legitimize_links(nd)))",
        "		goto out2;",
        "	res = __legitimize_mnt(nd->path.mnt, nd->m_seq);",
        "	if (unlikely(res)) {",
        "		if (res > 0)",
        "			goto out2;",
        "		goto out1;",
        "	}",
        "	if (unlikely(!lockref_get_not_dead(&nd->path.dentry->d_lockref)))",
        "		goto out1;",
        "",
        "	/*",
        "	 * We need to move both the parent and the dentry from the RCU domain",
        "	 * to be properly refcounted. And the sequence number in the dentry",
        "	 * validates *both* dentry counters, since we checked the sequence",
        "	 * number of the parent after we got the child sequence number. So we",
        "	 * know the parent must still be valid if the child sequence number is",
        "	 */",
        "	if (unlikely(!lockref_get_not_dead(&dentry->d_lockref)))",
        "		goto out;",
        "	if (read_seqcount_retry(&dentry->d_seq, nd->next_seq))",
        "		goto out_dput;",
        "	/*",
        "	 * Sequence counts matched. Now make sure that the root is",
        "	 * still valid and get it if required.",
        "	 */",
        "	if (unlikely(!legitimize_root(nd)))",
        "		goto out_dput;",
        "	leave_rcu(nd);",
        "	return true;",
        "",
        "out2:",
        "	nd->path.mnt = NULL;",
        "out1:",
        "	nd->path.dentry = NULL;",
        "out:",
        "	leave_rcu(nd);",
        "	return false;",
        "out_dput:",
        "	leave_rcu(nd);",
        "	dput(dentry);",
        "	return false;",
        "}",
        "",
        "static inline int d_revalidate(struct dentry *dentry, unsigned int flags)",
        "{",
        "	if (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE))",
        "		return dentry->d_op->d_revalidate(dentry, flags);",
        "	else",
        "		return 1;",
        "}",
        "",
        "/**",
        " * complete_walk - successful completion of path walk",
        " * @nd:  pointer nameidata",
        " *",
        " * If we had been in RCU mode, drop out of it and legitimize nd->path.",
        " * Revalidate the final result, unless we'd already done that during",
        " * the path walk or the filesystem doesn't ask for it.  Return 0 on",
        " * success, -error on failure.  In case of failure caller does not",
        " * need to drop nd->path.",
        " */",
        "static int complete_walk(struct nameidata *nd)",
        "{",
        "	struct dentry *dentry = nd->path.dentry;",
        "	int status;",
        "",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		/*",
        "		 * We don't want to zero nd->root for scoped-lookups or",
        "		 * externally-managed nd->root.",
        "		 */",
        "		if (!(nd->state & ND_ROOT_PRESET))",
        "			if (!(nd->flags & LOOKUP_IS_SCOPED))",
        "				nd->root.mnt = NULL;",
        "		nd->flags &= ~LOOKUP_CACHED;",
        "		if (!try_to_unlazy(nd))",
        "			return -ECHILD;",
        "	}",
        "",
        "	if (unlikely(nd->flags & LOOKUP_IS_SCOPED)) {",
        "		/*",
        "		 * While the guarantee of LOOKUP_IS_SCOPED is (roughly) \"don't",
        "		 * ever step outside the root during lookup\" and should already",
        "		 * be guaranteed by the rest of namei, we want to avoid a namei",
        "		 * BUG resulting in userspace being given a path that was not",
        "		 * scoped within the root at some point during the lookup.",
        "		 *",
        "		 * So, do a final sanity-check to make sure that in the",
        "		 * worst-case scenario (a complete bypass of LOOKUP_IS_SCOPED)",
        "		 * we won't silently return an fd completely outside of the",
        "		 * requested root to userspace.",
        "		 *",
        "		 * Userspace could move the path outside the root after this",
        "		 * check, but as discussed elsewhere this is not a concern (the",
        "		 * resolved file was inside the root at some point).",
        "		 */",
        "		if (!path_is_under(&nd->path, &nd->root))",
        "			return -EXDEV;",
        "	}",
        "",
        "	if (likely(!(nd->state & ND_JUMPED)))",
        "		return 0;",
        "",
        "	if (likely(!(dentry->d_flags & DCACHE_OP_WEAK_REVALIDATE)))",
        "		return 0;",
        "",
        "	status = dentry->d_op->d_weak_revalidate(dentry, nd->flags);",
        "	if (status > 0)",
        "		return 0;",
        "",
        "	if (!status)",
        "		status = -ESTALE;",
        "",
        "	return status;",
        "}",
        "",
        "static int set_root(struct nameidata *nd)",
        "{",
        "	struct fs_struct *fs = current->fs;",
        "",
        "	/*",
        "	 * Jumping to the real root in a scoped-lookup is a BUG in namei, but we",
        "	 * still have to ensure it doesn't happen because it will cause a breakout",
        "	 * from the dirfd.",
        "	 */",
        "	if (WARN_ON(nd->flags & LOOKUP_IS_SCOPED))",
        "		return -ENOTRECOVERABLE;",
        "",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		unsigned seq;",
        "",
        "		do {",
        "			seq = read_seqcount_begin(&fs->seq);",
        "			nd->root = fs->root;",
        "			nd->root_seq = __read_seqcount_begin(&nd->root.dentry->d_seq);",
        "		} while (read_seqcount_retry(&fs->seq, seq));",
        "	} else {",
        "		get_fs_root(fs, &nd->root);",
        "		nd->state |= ND_ROOT_GRABBED;",
        "	}",
        "	return 0;",
        "}",
        "",
        "static int nd_jump_root(struct nameidata *nd)",
        "{",
        "	if (unlikely(nd->flags & LOOKUP_BENEATH))",
        "		return -EXDEV;",
        "	if (unlikely(nd->flags & LOOKUP_NO_XDEV)) {",
        "		/* Absolute path arguments to path_init() are allowed. */",
        "		if (nd->path.mnt != NULL && nd->path.mnt != nd->root.mnt)",
        "			return -EXDEV;",
        "	}",
        "	if (!nd->root.mnt) {",
        "		int error = set_root(nd);",
        "		if (error)",
        "			return error;",
        "	}",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		struct dentry *d;",
        "		nd->path = nd->root;",
        "		d = nd->path.dentry;",
        "		nd->inode = d->d_inode;",
        "		nd->seq = nd->root_seq;",
        "		if (read_seqcount_retry(&d->d_seq, nd->seq))",
        "			return -ECHILD;",
        "	} else {",
        "		path_put(&nd->path);",
        "		nd->path = nd->root;",
        "		path_get(&nd->path);",
        "		nd->inode = nd->path.dentry->d_inode;",
        "	}",
        "	nd->state |= ND_JUMPED;",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Helper to directly jump to a known parsed path from ->get_link,",
        " * caller must have taken a reference to path beforehand.",
        " */",
        "int nd_jump_link(const struct path *path)",
        "{",
        "	int error = -ELOOP;",
        "	struct nameidata *nd = current->nameidata;",
        "",
        "	if (unlikely(nd->flags & LOOKUP_NO_MAGICLINKS))",
        "		goto err;",
        "",
        "	error = -EXDEV;",
        "	if (unlikely(nd->flags & LOOKUP_NO_XDEV)) {",
        "		if (nd->path.mnt != path->mnt)",
        "			goto err;",
        "	}",
        "	/* Not currently safe for scoped-lookups. */",
        "	if (unlikely(nd->flags & LOOKUP_IS_SCOPED))",
        "		goto err;",
        "",
        "	path_put(&nd->path);",
        "	nd->path = *path;",
        "	nd->inode = nd->path.dentry->d_inode;",
        "	nd->state |= ND_JUMPED;",
        "	return 0;",
        "",
        "err:",
        "	path_put(path);",
        "	return error;",
        "}",
        "",
        "static inline void put_link(struct nameidata *nd)",
        "{",
        "	struct saved *last = nd->stack + --nd->depth;",
        "	do_delayed_call(&last->done);",
        "	if (!(nd->flags & LOOKUP_RCU))",
        "		path_put(&last->link);",
        "}",
        "",
        "static int sysctl_protected_symlinks __read_mostly;",
        "static int sysctl_protected_hardlinks __read_mostly;",
        "static int sysctl_protected_fifos __read_mostly;",
        "static int sysctl_protected_regular __read_mostly;",
        "",
        "#ifdef CONFIG_SYSCTL",
        "static struct ctl_table namei_sysctls[] = {",
        "	{",
        "		.procname	= \"protected_symlinks\",",
        "		.data		= &sysctl_protected_symlinks,",
        "		.maxlen		= sizeof(int),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_dointvec_minmax,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_ONE,",
        "	},",
        "	{",
        "		.procname	= \"protected_hardlinks\",",
        "		.data		= &sysctl_protected_hardlinks,",
        "		.maxlen		= sizeof(int),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_dointvec_minmax,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_ONE,",
        "	},",
        "	{",
        "		.procname	= \"protected_fifos\",",
        "		.data		= &sysctl_protected_fifos,",
        "		.maxlen		= sizeof(int),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_dointvec_minmax,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_TWO,",
        "	},",
        "	{",
        "		.procname	= \"protected_regular\",",
        "		.data		= &sysctl_protected_regular,",
        "		.maxlen		= sizeof(int),",
        "		.mode		= 0644,",
        "		.proc_handler	= proc_dointvec_minmax,",
        "		.extra1		= SYSCTL_ZERO,",
        "		.extra2		= SYSCTL_TWO,",
        "	},",
        "};",
        "",
        "static int __init init_fs_namei_sysctls(void)",
        "{",
        "	register_sysctl_init(\"fs\", namei_sysctls);",
        "	return 0;",
        "}",
        "fs_initcall(init_fs_namei_sysctls);",
        "",
        "#endif /* CONFIG_SYSCTL */",
        "",
        "/**",
        " * may_follow_link - Check symlink following for unsafe situations",
        " * @nd: nameidata pathwalk data",
        " * @inode: Used for idmapping.",
        " *",
        " * In the case of the sysctl_protected_symlinks sysctl being enabled,",
        " * CAP_DAC_OVERRIDE needs to be specifically ignored if the symlink is",
        " * in a sticky world-writable directory. This is to protect privileged",
        " * processes from failing races against path names that may change out",
        " * from under them by way of other users creating malicious symlinks.",
        " * It will permit symlinks to be followed only when outside a sticky",
        " * world-writable directory, or when the uid of the symlink and follower",
        " * match, or when the directory owner matches the symlink's owner.",
        " *",
        " * Returns 0 if following the symlink is allowed, -ve on error.",
        " */",
        "static inline int may_follow_link(struct nameidata *nd, const struct inode *inode)",
        "{",
        "	struct mnt_idmap *idmap;",
        "	vfsuid_t vfsuid;",
        "",
        "	if (!sysctl_protected_symlinks)",
        "		return 0;",
        "",
        "	idmap = mnt_idmap(nd->path.mnt);",
        "	vfsuid = i_uid_into_vfsuid(idmap, inode);",
        "	/* Allowed if owner and follower match. */",
        "	if (vfsuid_eq_kuid(vfsuid, current_fsuid()))",
        "		return 0;",
        "",
        "	/* Allowed if parent directory not sticky and world-writable. */",
        "	if ((nd->dir_mode & (S_ISVTX|S_IWOTH)) != (S_ISVTX|S_IWOTH))",
        "		return 0;",
        "",
        "	/* Allowed if parent directory and link owner match. */",
        "	if (vfsuid_valid(nd->dir_vfsuid) && vfsuid_eq(nd->dir_vfsuid, vfsuid))",
        "		return 0;",
        "",
        "	if (nd->flags & LOOKUP_RCU)",
        "		return -ECHILD;",
        "",
        "	audit_inode(nd->name, nd->stack[0].link.dentry, 0);",
        "	audit_log_path_denied(AUDIT_ANOM_LINK, \"follow_link\");",
        "	return -EACCES;",
        "}",
        "",
        "/**",
        " * safe_hardlink_source - Check for safe hardlink conditions",
        " * @idmap: idmap of the mount the inode was found from",
        " * @inode: the source inode to hardlink from",
        " *",
        " * Return false if at least one of the following conditions:",
        " *    - inode is not a regular file",
        " *    - inode is setuid",
        " *    - inode is setgid and group-exec",
        " *    - access failure for read and write",
        " *",
        " * Otherwise returns true.",
        " */",
        "static bool safe_hardlink_source(struct mnt_idmap *idmap,",
        "				 struct inode *inode)",
        "{",
        "	umode_t mode = inode->i_mode;",
        "",
        "	/* Special files should not get pinned to the filesystem. */",
        "	if (!S_ISREG(mode))",
        "		return false;",
        "",
        "	/* Setuid files should not get pinned to the filesystem. */",
        "	if (mode & S_ISUID)",
        "		return false;",
        "",
        "	/* Executable setgid files should not get pinned to the filesystem. */",
        "	if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP))",
        "		return false;",
        "",
        "	/* Hardlinking to unreadable or unwritable sources is dangerous. */",
        "	if (inode_permission(idmap, inode, MAY_READ | MAY_WRITE))",
        "		return false;",
        "",
        "	return true;",
        "}",
        "",
        "/**",
        " * may_linkat - Check permissions for creating a hardlink",
        " * @idmap: idmap of the mount the inode was found from",
        " * @link:  the source to hardlink from",
        " *",
        " * Block hardlink when all of:",
        " *  - sysctl_protected_hardlinks enabled",
        " *  - fsuid does not match inode",
        " *  - hardlink source is unsafe (see safe_hardlink_source() above)",
        " *  - not CAP_FOWNER in a namespace with the inode owner uid mapped",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " *",
        " * Returns 0 if successful, -ve on error.",
        " */",
        "int may_linkat(struct mnt_idmap *idmap, const struct path *link)",
        "{",
        "	struct inode *inode = link->dentry->d_inode;",
        "",
        "	/* Inode writeback is not safe when the uid or gid are invalid. */",
        "	if (!vfsuid_valid(i_uid_into_vfsuid(idmap, inode)) ||",
        "	    !vfsgid_valid(i_gid_into_vfsgid(idmap, inode)))",
        "		return -EOVERFLOW;",
        "",
        "	if (!sysctl_protected_hardlinks)",
        "		return 0;",
        "",
        "	/* Source inode owner (or CAP_FOWNER) can hardlink all they like,",
        "	 * otherwise, it must be a safe source.",
        "	 */",
        "	if (safe_hardlink_source(idmap, inode) ||",
        "	    inode_owner_or_capable(idmap, inode))",
        "		return 0;",
        "",
        "	audit_log_path_denied(AUDIT_ANOM_LINK, \"linkat\");",
        "	return -EPERM;",
        "}",
        "",
        "/**",
        " * may_create_in_sticky - Check whether an O_CREAT open in a sticky directory",
        " *			  should be allowed, or not, on files that already",
        " *			  exist.",
        " * @idmap: idmap of the mount the inode was found from",
        " * @nd: nameidata pathwalk data",
        " * @inode: the inode of the file to open",
        " *",
        " * Block an O_CREAT open of a FIFO (or a regular file) when:",
        " *   - sysctl_protected_fifos (or sysctl_protected_regular) is enabled",
        " *   - the file already exists",
        " *   - we are in a sticky directory",
        " *   - we don't own the file",
        " *   - the owner of the directory doesn't own the file",
        " *   - the directory is world writable",
        " * If the sysctl_protected_fifos (or sysctl_protected_regular) is set to 2",
        " * the directory doesn't have to be world writable: being group writable will",
        " * be enough.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " *",
        " * Returns 0 if the open is allowed, -ve on error.",
        " */",
        "static int may_create_in_sticky(struct mnt_idmap *idmap, struct nameidata *nd,",
        "				struct inode *const inode)",
        "{",
        "	umode_t dir_mode = nd->dir_mode;",
        "	vfsuid_t dir_vfsuid = nd->dir_vfsuid, i_vfsuid;",
        "",
        "	if (likely(!(dir_mode & S_ISVTX)))",
        "		return 0;",
        "",
        "	if (S_ISREG(inode->i_mode) && !sysctl_protected_regular)",
        "		return 0;",
        "",
        "	if (S_ISFIFO(inode->i_mode) && !sysctl_protected_fifos)",
        "		return 0;",
        "",
        "	i_vfsuid = i_uid_into_vfsuid(idmap, inode);",
        "",
        "	if (vfsuid_eq(i_vfsuid, dir_vfsuid))",
        "		return 0;",
        "",
        "	if (vfsuid_eq_kuid(i_vfsuid, current_fsuid()))",
        "		return 0;",
        "",
        "	if (likely(dir_mode & 0002)) {",
        "		audit_log_path_denied(AUDIT_ANOM_CREAT, \"sticky_create\");",
        "		return -EACCES;",
        "	}",
        "",
        "	if (dir_mode & 0020) {",
        "		if (sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) {",
        "			audit_log_path_denied(AUDIT_ANOM_CREAT,",
        "					      \"sticky_create_fifo\");",
        "			return -EACCES;",
        "		}",
        "",
        "		if (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode)) {",
        "			audit_log_path_denied(AUDIT_ANOM_CREAT,",
        "					      \"sticky_create_regular\");",
        "			return -EACCES;",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * follow_up - Find the mountpoint of path's vfsmount",
        " *",
        " * Given a path, find the mountpoint of its source file system.",
        " * Replace @path with the path of the mountpoint in the parent mount.",
        " * Up is towards /.",
        " *",
        " * Return 1 if we went up a level and 0 if we were already at the",
        " * root.",
        " */",
        "int follow_up(struct path *path)",
        "{",
        "	struct mount *mnt = real_mount(path->mnt);",
        "	struct mount *parent;",
        "	struct dentry *mountpoint;",
        "",
        "	read_seqlock_excl(&mount_lock);",
        "	parent = mnt->mnt_parent;",
        "	if (parent == mnt) {",
        "		read_sequnlock_excl(&mount_lock);",
        "		return 0;",
        "	}",
        "	mntget(&parent->mnt);",
        "	mountpoint = dget(mnt->mnt_mountpoint);",
        "	read_sequnlock_excl(&mount_lock);",
        "	dput(path->dentry);",
        "	path->dentry = mountpoint;",
        "	mntput(path->mnt);",
        "	path->mnt = &parent->mnt;",
        "	return 1;",
        "}",
        "EXPORT_SYMBOL(follow_up);",
        "",
        "static bool choose_mountpoint_rcu(struct mount *m, const struct path *root,",
        "				  struct path *path, unsigned *seqp)",
        "{",
        "	while (mnt_has_parent(m)) {",
        "		struct dentry *mountpoint = m->mnt_mountpoint;",
        "",
        "		m = m->mnt_parent;",
        "		if (unlikely(root->dentry == mountpoint &&",
        "			     root->mnt == &m->mnt))",
        "			break;",
        "		if (mountpoint != m->mnt.mnt_root) {",
        "			path->mnt = &m->mnt;",
        "			path->dentry = mountpoint;",
        "			*seqp = read_seqcount_begin(&mountpoint->d_seq);",
        "			return true;",
        "		}",
        "	}",
        "	return false;",
        "}",
        "",
        "static bool choose_mountpoint(struct mount *m, const struct path *root,",
        "			      struct path *path)",
        "{",
        "	bool found;",
        "",
        "	rcu_read_lock();",
        "	while (1) {",
        "		unsigned seq, mseq = read_seqbegin(&mount_lock);",
        "",
        "		found = choose_mountpoint_rcu(m, root, path, &seq);",
        "		if (unlikely(!found)) {",
        "			if (!read_seqretry(&mount_lock, mseq))",
        "				break;",
        "		} else {",
        "			if (likely(__legitimize_path(path, seq, mseq)))",
        "				break;",
        "			rcu_read_unlock();",
        "			path_put(path);",
        "			rcu_read_lock();",
        "		}",
        "	}",
        "	rcu_read_unlock();",
        "	return found;",
        "}",
        "",
        "/*",
        " * Perform an automount",
        " * - return -EISDIR to tell follow_managed() to stop and return the path we",
        " *   were called with.",
        " */",
        "static int follow_automount(struct path *path, int *count, unsigned lookup_flags)",
        "{",
        "	struct dentry *dentry = path->dentry;",
        "",
        "	/* We don't want to mount if someone's just doing a stat -",
        "	 * unless they're stat'ing a directory and appended a '/' to",
        "	 * the name.",
        "	 *",
        "	 * We do, however, want to mount if someone wants to open or",
        "	 * create a file of any type under the mountpoint, wants to",
        "	 * traverse through the mountpoint or wants to open the",
        "	 * mounted directory.  Also, autofs may mark negative dentries",
        "	 * as being automount points.  These will need the attentions",
        "	 * of the daemon to instantiate them before they can be used.",
        "	 */",
        "	if (!(lookup_flags & (LOOKUP_PARENT | LOOKUP_DIRECTORY |",
        "			   LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_AUTOMOUNT)) &&",
        "	    dentry->d_inode)",
        "		return -EISDIR;",
        "",
        "	if (count && (*count)++ >= MAXSYMLINKS)",
        "		return -ELOOP;",
        "",
        "	return finish_automount(dentry->d_op->d_automount(path), path);",
        "}",
        "",
        "/*",
        " * mount traversal - out-of-line part.  One note on ->d_flags accesses -",
        " * dentries are pinned but not locked here, so negative dentry can go",
        " * positive right under us.  Use of smp_load_acquire() provides a barrier",
        " * sufficient for ->d_inode and ->d_flags consistency.",
        " */",
        "static int __traverse_mounts(struct path *path, unsigned flags, bool *jumped,",
        "			     int *count, unsigned lookup_flags)",
        "{",
        "	struct vfsmount *mnt = path->mnt;",
        "	bool need_mntput = false;",
        "	int ret = 0;",
        "",
        "	while (flags & DCACHE_MANAGED_DENTRY) {",
        "		/* Allow the filesystem to manage the transit without i_mutex",
        "		 * being held. */",
        "		if (flags & DCACHE_MANAGE_TRANSIT) {",
        "			ret = path->dentry->d_op->d_manage(path, false);",
        "			flags = smp_load_acquire(&path->dentry->d_flags);",
        "			if (ret < 0)",
        "				break;",
        "		}",
        "",
        "		if (flags & DCACHE_MOUNTED) {	// something's mounted on it..",
        "			struct vfsmount *mounted = lookup_mnt(path);",
        "			if (mounted) {		// ... in our namespace",
        "				dput(path->dentry);",
        "				if (need_mntput)",
        "					mntput(path->mnt);",
        "				path->mnt = mounted;",
        "				path->dentry = dget(mounted->mnt_root);",
        "				// here we know it's positive",
        "				flags = path->dentry->d_flags;",
        "				need_mntput = true;",
        "				continue;",
        "			}",
        "		}",
        "",
        "		if (!(flags & DCACHE_NEED_AUTOMOUNT))",
        "			break;",
        "",
        "		// uncovered automount point",
        "		ret = follow_automount(path, count, lookup_flags);",
        "		flags = smp_load_acquire(&path->dentry->d_flags);",
        "		if (ret < 0)",
        "			break;",
        "	}",
        "",
        "	if (ret == -EISDIR)",
        "		ret = 0;",
        "	// possible if you race with several mount --move",
        "	if (need_mntput && path->mnt == mnt)",
        "		mntput(path->mnt);",
        "	if (!ret && unlikely(d_flags_negative(flags)))",
        "		ret = -ENOENT;",
        "	*jumped = need_mntput;",
        "	return ret;",
        "}",
        "",
        "static inline int traverse_mounts(struct path *path, bool *jumped,",
        "				  int *count, unsigned lookup_flags)",
        "{",
        "	unsigned flags = smp_load_acquire(&path->dentry->d_flags);",
        "",
        "	/* fastpath */",
        "	if (likely(!(flags & DCACHE_MANAGED_DENTRY))) {",
        "		*jumped = false;",
        "		if (unlikely(d_flags_negative(flags)))",
        "			return -ENOENT;",
        "		return 0;",
        "	}",
        "	return __traverse_mounts(path, flags, jumped, count, lookup_flags);",
        "}",
        "",
        "int follow_down_one(struct path *path)",
        "{",
        "	struct vfsmount *mounted;",
        "",
        "	mounted = lookup_mnt(path);",
        "	if (mounted) {",
        "		dput(path->dentry);",
        "		mntput(path->mnt);",
        "		path->mnt = mounted;",
        "		path->dentry = dget(mounted->mnt_root);",
        "		return 1;",
        "	}",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(follow_down_one);",
        "",
        "/*",
        " * Follow down to the covering mount currently visible to userspace.  At each",
        " * point, the filesystem owning that dentry may be queried as to whether the",
        " * caller is permitted to proceed or not.",
        " */",
        "int follow_down(struct path *path, unsigned int flags)",
        "{",
        "	struct vfsmount *mnt = path->mnt;",
        "	bool jumped;",
        "	int ret = traverse_mounts(path, &jumped, NULL, flags);",
        "",
        "	if (path->mnt != mnt)",
        "		mntput(mnt);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(follow_down);",
        "",
        "/*",
        " * Try to skip to top of mountpoint pile in rcuwalk mode.  Fail if",
        " * we meet a managed dentry that would need blocking.",
        " */",
        "static bool __follow_mount_rcu(struct nameidata *nd, struct path *path)",
        "{",
        "	struct dentry *dentry = path->dentry;",
        "	unsigned int flags = dentry->d_flags;",
        "",
        "	if (likely(!(flags & DCACHE_MANAGED_DENTRY)))",
        "		return true;",
        "",
        "	if (unlikely(nd->flags & LOOKUP_NO_XDEV))",
        "		return false;",
        "",
        "	for (;;) {",
        "		/*",
        "		 * Don't forget we might have a non-mountpoint managed dentry",
        "		 * that wants to block transit.",
        "		 */",
        "		if (unlikely(flags & DCACHE_MANAGE_TRANSIT)) {",
        "			int res = dentry->d_op->d_manage(path, true);",
        "			if (res)",
        "				return res == -EISDIR;",
        "			flags = dentry->d_flags;",
        "		}",
        "",
        "		if (flags & DCACHE_MOUNTED) {",
        "			struct mount *mounted = __lookup_mnt(path->mnt, dentry);",
        "			if (mounted) {",
        "				path->mnt = &mounted->mnt;",
        "				dentry = path->dentry = mounted->mnt.mnt_root;",
        "				nd->state |= ND_JUMPED;",
        "				nd->next_seq = read_seqcount_begin(&dentry->d_seq);",
        "				flags = dentry->d_flags;",
        "				// makes sure that non-RCU pathwalk could reach",
        "				// this state.",
        "				if (read_seqretry(&mount_lock, nd->m_seq))",
        "					return false;",
        "				continue;",
        "			}",
        "			if (read_seqretry(&mount_lock, nd->m_seq))",
        "				return false;",
        "		}",
        "		return !(flags & DCACHE_NEED_AUTOMOUNT);",
        "	}",
        "}",
        "",
        "static inline int handle_mounts(struct nameidata *nd, struct dentry *dentry,",
        "			  struct path *path)",
        "{",
        "	bool jumped;",
        "	int ret;",
        "",
        "	path->mnt = nd->path.mnt;",
        "	path->dentry = dentry;",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		unsigned int seq = nd->next_seq;",
        "		if (likely(__follow_mount_rcu(nd, path)))",
        "			return 0;",
        "		// *path and nd->next_seq might've been clobbered",
        "		path->mnt = nd->path.mnt;",
        "		path->dentry = dentry;",
        "		nd->next_seq = seq;",
        "		if (!try_to_unlazy_next(nd, dentry))",
        "			return -ECHILD;",
        "	}",
        "	ret = traverse_mounts(path, &jumped, &nd->total_link_count, nd->flags);",
        "	if (jumped) {",
        "		if (unlikely(nd->flags & LOOKUP_NO_XDEV))",
        "			ret = -EXDEV;",
        "		else",
        "			nd->state |= ND_JUMPED;",
        "	}",
        "	if (unlikely(ret)) {",
        "		dput(path->dentry);",
        "		if (path->mnt != nd->path.mnt)",
        "			mntput(path->mnt);",
        "	}",
        "	return ret;",
        "}",
        "",
        "/*",
        " * This looks up the name in dcache and possibly revalidates the found dentry.",
        " * NULL is returned if the dentry does not exist in the cache.",
        " */",
        "static struct dentry *lookup_dcache(const struct qstr *name,",
        "				    struct dentry *dir,",
        "				    unsigned int flags)",
        "{",
        "	struct dentry *dentry = d_lookup(dir, name);",
        "	if (dentry) {",
        "		int error = d_revalidate(dentry, flags);",
        "		if (unlikely(error <= 0)) {",
        "			if (!error)",
        "				d_invalidate(dentry);",
        "			dput(dentry);",
        "			return ERR_PTR(error);",
        "		}",
        "	}",
        "	return dentry;",
        "}",
        "",
        "/*",
        " * Parent directory has inode locked exclusive.  This is one",
        " * and only case when ->lookup() gets called on non in-lookup",
        " * dentries - as the matter of fact, this only gets called",
        " * when directory is guaranteed to have no in-lookup children",
        " * at all.",
        " */",
        "struct dentry *lookup_one_qstr_excl(const struct qstr *name,",
        "				    struct dentry *base,",
        "				    unsigned int flags)",
        "{",
        "	struct dentry *dentry = lookup_dcache(name, base, flags);",
        "	struct dentry *old;",
        "	struct inode *dir = base->d_inode;",
        "",
        "	if (dentry)",
        "		return dentry;",
        "",
        "	/* Don't create child dentry for a dead directory. */",
        "	if (unlikely(IS_DEADDIR(dir)))",
        "		return ERR_PTR(-ENOENT);",
        "",
        "	dentry = d_alloc(base, name);",
        "	if (unlikely(!dentry))",
        "		return ERR_PTR(-ENOMEM);",
        "",
        "	old = dir->i_op->lookup(dir, dentry, flags);",
        "	if (unlikely(old)) {",
        "		dput(dentry);",
        "		dentry = old;",
        "	}",
        "	return dentry;",
        "}",
        "EXPORT_SYMBOL(lookup_one_qstr_excl);",
        "",
        "/**",
        " * lookup_fast - do fast lockless (but racy) lookup of a dentry",
        " * @nd: current nameidata",
        " *",
        " * Do a fast, but racy lookup in the dcache for the given dentry, and",
        " * revalidate it. Returns a valid dentry pointer or NULL if one wasn't",
        " * found. On error, an ERR_PTR will be returned.",
        " *",
        " * If this function returns a valid dentry and the walk is no longer",
        " * lazy, the dentry will carry a reference that must later be put. If",
        " * RCU mode is still in force, then this is not the case and the dentry",
        " * must be legitimized before use. If this returns NULL, then the walk",
        " * will no longer be in RCU mode.",
        " */",
        "static struct dentry *lookup_fast(struct nameidata *nd)",
        "{",
        "	struct dentry *dentry, *parent = nd->path.dentry;",
        "	int status = 1;",
        "",
        "	/*",
        "	 * Rename seqlock is not required here because in the off chance",
        "	 * of a false negative due to a concurrent rename, the caller is",
        "	 * going to fall back to non-racy lookup.",
        "	 */",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		dentry = __d_lookup_rcu(parent, &nd->last, &nd->next_seq);",
        "		if (unlikely(!dentry)) {",
        "			if (!try_to_unlazy(nd))",
        "				return ERR_PTR(-ECHILD);",
        "			return NULL;",
        "		}",
        "",
        "		/*",
        "		 * This sequence count validates that the parent had no",
        "		 * changes while we did the lookup of the dentry above.",
        "		 */",
        "		if (read_seqcount_retry(&parent->d_seq, nd->seq))",
        "			return ERR_PTR(-ECHILD);",
        "",
        "		status = d_revalidate(dentry, nd->flags);",
        "		if (likely(status > 0))",
        "			return dentry;",
        "		if (!try_to_unlazy_next(nd, dentry))",
        "			return ERR_PTR(-ECHILD);",
        "		if (status == -ECHILD)",
        "			/* we'd been told to redo it in non-rcu mode */",
        "			status = d_revalidate(dentry, nd->flags);",
        "	} else {",
        "		dentry = __d_lookup(parent, &nd->last);",
        "		if (unlikely(!dentry))",
        "			return NULL;",
        "		status = d_revalidate(dentry, nd->flags);",
        "	}",
        "	if (unlikely(status <= 0)) {",
        "		if (!status)",
        "			d_invalidate(dentry);",
        "		dput(dentry);",
        "		return ERR_PTR(status);",
        "	}",
        "	return dentry;",
        "}",
        "",
        "/* Fast lookup failed, do it the slow way */",
        "static struct dentry *__lookup_slow(const struct qstr *name,",
        "				    struct dentry *dir,",
        "				    unsigned int flags)",
        "{",
        "	struct dentry *dentry, *old;",
        "	struct inode *inode = dir->d_inode;",
        "	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);",
        "",
        "	/* Don't go there if it's already dead */",
        "	if (unlikely(IS_DEADDIR(inode)))",
        "		return ERR_PTR(-ENOENT);",
        "again:",
        "	dentry = d_alloc_parallel(dir, name, &wq);",
        "	if (IS_ERR(dentry))",
        "		return dentry;",
        "	if (unlikely(!d_in_lookup(dentry))) {",
        "		int error = d_revalidate(dentry, flags);",
        "		if (unlikely(error <= 0)) {",
        "			if (!error) {",
        "				d_invalidate(dentry);",
        "				dput(dentry);",
        "				goto again;",
        "			}",
        "			dput(dentry);",
        "			dentry = ERR_PTR(error);",
        "		}",
        "	} else {",
        "		old = inode->i_op->lookup(inode, dentry, flags);",
        "		d_lookup_done(dentry);",
        "		if (unlikely(old)) {",
        "			dput(dentry);",
        "			dentry = old;",
        "		}",
        "	}",
        "	return dentry;",
        "}",
        "",
        "static struct dentry *lookup_slow(const struct qstr *name,",
        "				  struct dentry *dir,",
        "				  unsigned int flags)",
        "{",
        "	struct inode *inode = dir->d_inode;",
        "	struct dentry *res;",
        "	inode_lock_shared(inode);",
        "	res = __lookup_slow(name, dir, flags);",
        "	inode_unlock_shared(inode);",
        "	return res;",
        "}",
        "",
        "static inline int may_lookup(struct mnt_idmap *idmap,",
        "			     struct nameidata *restrict nd)",
        "{",
        "	int err, mask;",
        "",
        "	mask = nd->flags & LOOKUP_RCU ? MAY_NOT_BLOCK : 0;",
        "	err = inode_permission(idmap, nd->inode, mask | MAY_EXEC);",
        "	if (likely(!err))",
        "		return 0;",
        "",
        "	// If we failed, and we weren't in LOOKUP_RCU, it's final",
        "	if (!(nd->flags & LOOKUP_RCU))",
        "		return err;",
        "",
        "	// Drop out of RCU mode to make sure it wasn't transient",
        "	if (!try_to_unlazy(nd))",
        "		return -ECHILD;	// redo it all non-lazy",
        "",
        "	if (err != -ECHILD)	// hard error",
        "		return err;",
        "",
        "	return inode_permission(idmap, nd->inode, MAY_EXEC);",
        "}",
        "",
        "static int reserve_stack(struct nameidata *nd, struct path *link)",
        "{",
        "	if (unlikely(nd->total_link_count++ >= MAXSYMLINKS))",
        "		return -ELOOP;",
        "",
        "	if (likely(nd->depth != EMBEDDED_LEVELS))",
        "		return 0;",
        "	if (likely(nd->stack != nd->internal))",
        "		return 0;",
        "	if (likely(nd_alloc_stack(nd)))",
        "		return 0;",
        "",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		// we need to grab link before we do unlazy.  And we can't skip",
        "		// unlazy even if we fail to grab the link - cleanup needs it",
        "		bool grabbed_link = legitimize_path(nd, link, nd->next_seq);",
        "",
        "		if (!try_to_unlazy(nd) || !grabbed_link)",
        "			return -ECHILD;",
        "",
        "		if (nd_alloc_stack(nd))",
        "			return 0;",
        "	}",
        "	return -ENOMEM;",
        "}",
        "",
        "enum {WALK_TRAILING = 1, WALK_MORE = 2, WALK_NOFOLLOW = 4};",
        "",
        "static const char *pick_link(struct nameidata *nd, struct path *link,",
        "		     struct inode *inode, int flags)",
        "{",
        "	struct saved *last;",
        "	const char *res;",
        "	int error = reserve_stack(nd, link);",
        "",
        "	if (unlikely(error)) {",
        "		if (!(nd->flags & LOOKUP_RCU))",
        "			path_put(link);",
        "		return ERR_PTR(error);",
        "	}",
        "	last = nd->stack + nd->depth++;",
        "	last->link = *link;",
        "	clear_delayed_call(&last->done);",
        "	last->seq = nd->next_seq;",
        "",
        "	if (flags & WALK_TRAILING) {",
        "		error = may_follow_link(nd, inode);",
        "		if (unlikely(error))",
        "			return ERR_PTR(error);",
        "	}",
        "",
        "	if (unlikely(nd->flags & LOOKUP_NO_SYMLINKS) ||",
        "			unlikely(link->mnt->mnt_flags & MNT_NOSYMFOLLOW))",
        "		return ERR_PTR(-ELOOP);",
        "",
        "	if (!(nd->flags & LOOKUP_RCU)) {",
        "		touch_atime(&last->link);",
        "		cond_resched();",
        "	} else if (atime_needs_update(&last->link, inode)) {",
        "		if (!try_to_unlazy(nd))",
        "			return ERR_PTR(-ECHILD);",
        "		touch_atime(&last->link);",
        "	}",
        "",
        "	error = security_inode_follow_link(link->dentry, inode,",
        "					   nd->flags & LOOKUP_RCU);",
        "	if (unlikely(error))",
        "		return ERR_PTR(error);",
        "",
        "	res = READ_ONCE(inode->i_link);",
        "	if (!res) {",
        "		const char * (*get)(struct dentry *, struct inode *,",
        "				struct delayed_call *);",
        "		get = inode->i_op->get_link;",
        "		if (nd->flags & LOOKUP_RCU) {",
        "			res = get(NULL, inode, &last->done);",
        "			if (res == ERR_PTR(-ECHILD) && try_to_unlazy(nd))",
        "				res = get(link->dentry, inode, &last->done);",
        "		} else {",
        "			res = get(link->dentry, inode, &last->done);",
        "		}",
        "		if (!res)",
        "			goto all_done;",
        "		if (IS_ERR(res))",
        "			return res;",
        "	}",
        "	if (*res == '/') {",
        "		error = nd_jump_root(nd);",
        "		if (unlikely(error))",
        "			return ERR_PTR(error);",
        "		while (unlikely(*++res == '/'))",
        "			;",
        "	}",
        "	if (*res)",
        "		return res;",
        "all_done: // pure jump",
        "	put_link(nd);",
        "	return NULL;",
        "}",
        "",
        "/*",
        " * Do we need to follow links? We _really_ want to be able",
        " * to do this check without having to look at inode->i_op,",
        " * so we keep a cache of \"no, this doesn't need follow_link\"",
        " * for the common case.",
        " *",
        " * NOTE: dentry must be what nd->next_seq had been sampled from.",
        " */",
        "static const char *step_into(struct nameidata *nd, int flags,",
        "		     struct dentry *dentry)",
        "{",
        "	struct path path;",
        "	struct inode *inode;",
        "	int err = handle_mounts(nd, dentry, &path);",
        "",
        "	if (err < 0)",
        "		return ERR_PTR(err);",
        "	inode = path.dentry->d_inode;",
        "	if (likely(!d_is_symlink(path.dentry)) ||",
        "	   ((flags & WALK_TRAILING) && !(nd->flags & LOOKUP_FOLLOW)) ||",
        "	   (flags & WALK_NOFOLLOW)) {",
        "		/* not a symlink or should not follow */",
        "		if (nd->flags & LOOKUP_RCU) {",
        "			if (read_seqcount_retry(&path.dentry->d_seq, nd->next_seq))",
        "				return ERR_PTR(-ECHILD);",
        "			if (unlikely(!inode))",
        "				return ERR_PTR(-ENOENT);",
        "		} else {",
        "			dput(nd->path.dentry);",
        "			if (nd->path.mnt != path.mnt)",
        "				mntput(nd->path.mnt);",
        "		}",
        "		nd->path = path;",
        "		nd->inode = inode;",
        "		nd->seq = nd->next_seq;",
        "		return NULL;",
        "	}",
        "	if (nd->flags & LOOKUP_RCU) {",
        "		/* make sure that d_is_symlink above matches inode */",
        "		if (read_seqcount_retry(&path.dentry->d_seq, nd->next_seq))",
        "			return ERR_PTR(-ECHILD);",
        "	} else {",
        "		if (path.mnt == nd->path.mnt)",
        "			mntget(path.mnt);",
        "	}",
        "	return pick_link(nd, &path, inode, flags);",
        "}",
        "",
        "static struct dentry *follow_dotdot_rcu(struct nameidata *nd)",
        "{",
        "	struct dentry *parent, *old;",
        "",
        "	if (path_equal(&nd->path, &nd->root))",
        "		goto in_root;",
        "	if (unlikely(nd->path.dentry == nd->path.mnt->mnt_root)) {",
        "		struct path path;",
        "		unsigned seq;",
        "		if (!choose_mountpoint_rcu(real_mount(nd->path.mnt),",
        "					   &nd->root, &path, &seq))",
        "			goto in_root;",
        "		if (unlikely(nd->flags & LOOKUP_NO_XDEV))",
        "			return ERR_PTR(-ECHILD);",
        "		nd->path = path;",
        "		nd->inode = path.dentry->d_inode;",
        "		nd->seq = seq;",
        "		// makes sure that non-RCU pathwalk could reach this state",
        "		if (read_seqretry(&mount_lock, nd->m_seq))",
        "			return ERR_PTR(-ECHILD);",
        "		/* we know that mountpoint was pinned */",
        "	}",
        "	old = nd->path.dentry;",
        "	parent = old->d_parent;",
        "	nd->next_seq = read_seqcount_begin(&parent->d_seq);",
        "	// makes sure that non-RCU pathwalk could reach this state",
        "	if (read_seqcount_retry(&old->d_seq, nd->seq))",
        "		return ERR_PTR(-ECHILD);",
        "	if (unlikely(!path_connected(nd->path.mnt, parent)))",
        "		return ERR_PTR(-ECHILD);",
        "	return parent;",
        "in_root:",
        "	if (read_seqretry(&mount_lock, nd->m_seq))",
        "		return ERR_PTR(-ECHILD);",
        "	if (unlikely(nd->flags & LOOKUP_BENEATH))",
        "		return ERR_PTR(-ECHILD);",
        "	nd->next_seq = nd->seq;",
        "	return nd->path.dentry;",
        "}",
        "",
        "static struct dentry *follow_dotdot(struct nameidata *nd)",
        "{",
        "	struct dentry *parent;",
        "",
        "	if (path_equal(&nd->path, &nd->root))",
        "		goto in_root;",
        "	if (unlikely(nd->path.dentry == nd->path.mnt->mnt_root)) {",
        "		struct path path;",
        "",
        "		if (!choose_mountpoint(real_mount(nd->path.mnt),",
        "				       &nd->root, &path))",
        "			goto in_root;",
        "		path_put(&nd->path);",
        "		nd->path = path;",
        "		nd->inode = path.dentry->d_inode;",
        "		if (unlikely(nd->flags & LOOKUP_NO_XDEV))",
        "			return ERR_PTR(-EXDEV);",
        "	}",
        "	/* rare case of legitimate dget_parent()... */",
        "	parent = dget_parent(nd->path.dentry);",
        "	if (unlikely(!path_connected(nd->path.mnt, parent))) {",
        "		dput(parent);",
        "		return ERR_PTR(-ENOENT);",
        "	}",
        "	return parent;",
        "",
        "in_root:",
        "	if (unlikely(nd->flags & LOOKUP_BENEATH))",
        "		return ERR_PTR(-EXDEV);",
        "	return dget(nd->path.dentry);",
        "}",
        "",
        "static const char *handle_dots(struct nameidata *nd, int type)",
        "{",
        "	if (type == LAST_DOTDOT) {",
        "		const char *error = NULL;",
        "		struct dentry *parent;",
        "",
        "		if (!nd->root.mnt) {",
        "			error = ERR_PTR(set_root(nd));",
        "			if (error)",
        "				return error;",
        "		}",
        "		if (nd->flags & LOOKUP_RCU)",
        "			parent = follow_dotdot_rcu(nd);",
        "		else",
        "			parent = follow_dotdot(nd);",
        "		if (IS_ERR(parent))",
        "			return ERR_CAST(parent);",
        "		error = step_into(nd, WALK_NOFOLLOW, parent);",
        "		if (unlikely(error))",
        "			return error;",
        "",
        "		if (unlikely(nd->flags & LOOKUP_IS_SCOPED)) {",
        "			/*",
        "			 * If there was a racing rename or mount along our",
        "			 * path, then we can't be sure that \"..\" hasn't jumped",
        "			 * above nd->root (and so userspace should retry or use",
        "			 * some fallback).",
        "			 */",
        "			smp_rmb();",
        "			if (__read_seqcount_retry(&mount_lock.seqcount, nd->m_seq))",
        "				return ERR_PTR(-EAGAIN);",
        "			if (__read_seqcount_retry(&rename_lock.seqcount, nd->r_seq))",
        "				return ERR_PTR(-EAGAIN);",
        "		}",
        "	}",
        "	return NULL;",
        "}",
        "",
        "static const char *walk_component(struct nameidata *nd, int flags)",
        "{",
        "	struct dentry *dentry;",
        "	/*",
        "	 * \".\" and \"..\" are special - \"..\" especially so because it has",
        "	 * to be able to know about the current root directory and",
        "	 * parent relationships.",
        "	 */",
        "	if (unlikely(nd->last_type != LAST_NORM)) {",
        "		if (!(flags & WALK_MORE) && nd->depth)",
        "			put_link(nd);",
        "		return handle_dots(nd, nd->last_type);",
        "	}",
        "	dentry = lookup_fast(nd);",
        "	if (IS_ERR(dentry))",
        "		return ERR_CAST(dentry);",
        "	if (unlikely(!dentry)) {",
        "		dentry = lookup_slow(&nd->last, nd->path.dentry, nd->flags);",
        "		if (IS_ERR(dentry))",
        "			return ERR_CAST(dentry);",
        "	}",
        "	if (!(flags & WALK_MORE) && nd->depth)",
        "		put_link(nd);",
        "	return step_into(nd, flags, dentry);",
        "}",
        "",
        "/*",
        " * We can do the critical dentry name comparison and hashing",
        " * operations one word at a time, but we are limited to:",
        " *",
        " * - Architectures with fast unaligned word accesses. We could",
        " *   do a \"get_unaligned()\" if this helps and is sufficiently",
        " *   fast.",
        " *",
        " * - non-CONFIG_DEBUG_PAGEALLOC configurations (so that we",
        " *   do not trap on the (extremely unlikely) case of a page",
        " *   crossing operation.",
        " *",
        " * - Furthermore, we need an efficient 64-bit compile for the",
        " *   64-bit case in order to generate the \"number of bytes in",
        " *   the final mask\". Again, that could be replaced with a",
        " *   efficient population count instruction or similar.",
        " */",
        "#ifdef CONFIG_DCACHE_WORD_ACCESS",
        "",
        "#include <asm/word-at-a-time.h>",
        "",
        "#ifdef HASH_MIX",
        "",
        "/* Architecture provides HASH_MIX and fold_hash() in <asm/hash.h> */",
        "",
        "#elif defined(CONFIG_64BIT)",
        "/*",
        " * Register pressure in the mixing function is an issue, particularly",
        " * on 32-bit x86, but almost any function requires one state value and",
        " * one temporary.  Instead, use a function designed for two state values",
        " * and no temporaries.",
        " *",
        " * This function cannot create a collision in only two iterations, so",
        " * we have two iterations to achieve avalanche.  In those two iterations,",
        " * we have six layers of mixing, which is enough to spread one bit's",
        " * influence out to 2^6 = 64 state bits.",
        " *",
        " * Rotate constants are scored by considering either 64 one-bit input",
        " * deltas or 64*63/2 = 2016 two-bit input deltas, and finding the",
        " * probability of that delta causing a change to each of the 128 output",
        " * bits, using a sample of random initial states.",
        " *",
        " * The Shannon entropy of the computed probabilities is then summed",
        " * to produce a score.  Ideally, any input change has a 50% chance of",
        " * toggling any given output bit.",
        " *",
        " * Mixing scores (in bits) for (12,45):",
        " * Input delta: 1-bit      2-bit",
        " * 1 round:     713.3    42542.6",
        " * 2 rounds:   2753.7   140389.8",
        " * 3 rounds:   5954.1   233458.2",
        " * 4 rounds:   7862.6   256672.2",
        " * Perfect:    8192     258048",
        " *            (64*128) (64*63/2 * 128)",
        " */",
        "#define HASH_MIX(x, y, a)	\\",
        "	(	x ^= (a),	\\",
        "	y ^= x,	x = rol64(x,12),\\",
        "	x += y,	y = rol64(y,45),\\",
        "	y *= 9			)",
        "",
        "/*",
        " * Fold two longs into one 32-bit hash value.  This must be fast, but",
        " * latency isn't quite as critical, as there is a fair bit of additional",
        " * work done before the hash value is used.",
        " */",
        "static inline unsigned int fold_hash(unsigned long x, unsigned long y)",
        "{",
        "	y ^= x * GOLDEN_RATIO_64;",
        "	y *= GOLDEN_RATIO_64;",
        "	return y >> 32;",
        "}",
        "",
        "#else	/* 32-bit case */",
        "",
        "/*",
        " * Mixing scores (in bits) for (7,20):",
        " * Input delta: 1-bit      2-bit",
        " * 1 round:     330.3     9201.6",
        " * 2 rounds:   1246.4    25475.4",
        " * 3 rounds:   1907.1    31295.1",
        " * 4 rounds:   2042.3    31718.6",
        " * Perfect:    2048      31744",
        " *            (32*64)   (32*31/2 * 64)",
        " */",
        "#define HASH_MIX(x, y, a)	\\",
        "	(	x ^= (a),	\\",
        "	y ^= x,	x = rol32(x, 7),\\",
        "	x += y,	y = rol32(y,20),\\",
        "	y *= 9			)",
        "",
        "static inline unsigned int fold_hash(unsigned long x, unsigned long y)",
        "{",
        "	/* Use arch-optimized multiply if one exists */",
        "	return __hash_32(y ^ __hash_32(x));",
        "}",
        "",
        "#endif",
        "",
        "/*",
        " * Return the hash of a string of known length.  This is carfully",
        " * designed to match hash_name(), which is the more critical function.",
        " * In particular, we must end by hashing a final word containing 0..7",
        " * payload bytes, to match the way that hash_name() iterates until it",
        " * finds the delimiter after the name.",
        " */",
        "unsigned int full_name_hash(const void *salt, const char *name, unsigned int len)",
        "{",
        "	unsigned long a, x = 0, y = (unsigned long)salt;",
        "",
        "	for (;;) {",
        "		if (!len)",
        "			goto done;",
        "		a = load_unaligned_zeropad(name);",
        "		if (len < sizeof(unsigned long))",
        "			break;",
        "		HASH_MIX(x, y, a);",
        "		name += sizeof(unsigned long);",
        "		len -= sizeof(unsigned long);",
        "	}",
        "	x ^= a & bytemask_from_count(len);",
        "done:",
        "	return fold_hash(x, y);",
        "}",
        "EXPORT_SYMBOL(full_name_hash);",
        "",
        "/* Return the \"hash_len\" (hash and length) of a null-terminated string */",
        "u64 hashlen_string(const void *salt, const char *name)",
        "{",
        "	unsigned long a = 0, x = 0, y = (unsigned long)salt;",
        "	unsigned long adata, mask, len;",
        "	const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;",
        "",
        "	len = 0;",
        "	goto inside;",
        "",
        "	do {",
        "		HASH_MIX(x, y, a);",
        "		len += sizeof(unsigned long);",
        "inside:",
        "		a = load_unaligned_zeropad(name+len);",
        "	} while (!has_zero(a, &adata, &constants));",
        "",
        "	adata = prep_zero_mask(a, adata, &constants);",
        "	mask = create_zero_mask(adata);",
        "	x ^= a & zero_bytemask(mask);",
        "",
        "	return hashlen_create(fold_hash(x, y), len + find_zero(mask));",
        "}",
        "EXPORT_SYMBOL(hashlen_string);",
        "",
        "/*",
        " * Calculate the length and hash of the path component, and",
        " * return the length as the result.",
        " */",
        "static inline const char *hash_name(struct nameidata *nd,",
        "				    const char *name,",
        "				    unsigned long *lastword)",
        "{",
        "	unsigned long a, b, x, y = (unsigned long)nd->path.dentry;",
        "	unsigned long adata, bdata, mask, len;",
        "	const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;",
        "",
        "	/*",
        "	 * The first iteration is special, because it can result in",
        "	 * '.' and '..' and has no mixing other than the final fold.",
        "	 */",
        "	a = load_unaligned_zeropad(name);",
        "	b = a ^ REPEAT_BYTE('/');",
        "	if (has_zero(a, &adata, &constants) | has_zero(b, &bdata, &constants)) {",
        "		adata = prep_zero_mask(a, adata, &constants);",
        "		bdata = prep_zero_mask(b, bdata, &constants);",
        "		mask = create_zero_mask(adata | bdata);",
        "		a &= zero_bytemask(mask);",
        "		*lastword = a;",
        "		len = find_zero(mask);",
        "		nd->last.hash = fold_hash(a, y);",
        "		nd->last.len = len;",
        "		return name + len;",
        "	}",
        "",
        "	len = 0;",
        "	x = 0;",
        "	do {",
        "		HASH_MIX(x, y, a);",
        "		len += sizeof(unsigned long);",
        "		a = load_unaligned_zeropad(name+len);",
        "		b = a ^ REPEAT_BYTE('/');",
        "	} while (!(has_zero(a, &adata, &constants) | has_zero(b, &bdata, &constants)));",
        "",
        "	adata = prep_zero_mask(a, adata, &constants);",
        "	bdata = prep_zero_mask(b, bdata, &constants);",
        "	mask = create_zero_mask(adata | bdata);",
        "	a &= zero_bytemask(mask);",
        "	x ^= a;",
        "	len += find_zero(mask);",
        "	*lastword = 0;		// Multi-word components cannot be DOT or DOTDOT",
        "",
        "	nd->last.hash = fold_hash(x, y);",
        "	nd->last.len = len;",
        "	return name + len;",
        "}",
        "",
        "/*",
        " * Note that the 'last' word is always zero-masked, but",
        " * was loaded as a possibly big-endian word.",
        " */",
        "#ifdef __BIG_ENDIAN",
        "  #define LAST_WORD_IS_DOT	(0x2eul << (BITS_PER_LONG-8))",
        "  #define LAST_WORD_IS_DOTDOT	(0x2e2eul << (BITS_PER_LONG-16))",
        "#endif",
        "",
        "#else	/* !CONFIG_DCACHE_WORD_ACCESS: Slow, byte-at-a-time version */",
        "",
        "/* Return the hash of a string of known length */",
        "unsigned int full_name_hash(const void *salt, const char *name, unsigned int len)",
        "{",
        "	unsigned long hash = init_name_hash(salt);",
        "	while (len--)",
        "		hash = partial_name_hash((unsigned char)*name++, hash);",
        "	return end_name_hash(hash);",
        "}",
        "EXPORT_SYMBOL(full_name_hash);",
        "",
        "/* Return the \"hash_len\" (hash and length) of a null-terminated string */",
        "u64 hashlen_string(const void *salt, const char *name)",
        "{",
        "	unsigned long hash = init_name_hash(salt);",
        "	unsigned long len = 0, c;",
        "",
        "	c = (unsigned char)*name;",
        "	while (c) {",
        "		len++;",
        "		hash = partial_name_hash(c, hash);",
        "		c = (unsigned char)name[len];",
        "	}",
        "	return hashlen_create(end_name_hash(hash), len);",
        "}",
        "EXPORT_SYMBOL(hashlen_string);",
        "",
        "/*",
        " * We know there's a real path component here of at least",
        " * one character.",
        " */",
        "static inline const char *hash_name(struct nameidata *nd, const char *name, unsigned long *lastword)",
        "{",
        "	unsigned long hash = init_name_hash(nd->path.dentry);",
        "	unsigned long len = 0, c, last = 0;",
        "",
        "	c = (unsigned char)*name;",
        "	do {",
        "		last = (last << 8) + c;",
        "		len++;",
        "		hash = partial_name_hash(c, hash);",
        "		c = (unsigned char)name[len];",
        "	} while (c && c != '/');",
        "",
        "	// This is reliable for DOT or DOTDOT, since the component",
        "	// cannot contain NUL characters - top bits being zero means",
        "	// we cannot have had any other pathnames.",
        "	*lastword = last;",
        "	nd->last.hash = end_name_hash(hash);",
        "	nd->last.len = len;",
        "	return name + len;",
        "}",
        "",
        "#endif",
        "",
        "#ifndef LAST_WORD_IS_DOT",
        "  #define LAST_WORD_IS_DOT	0x2e",
        "  #define LAST_WORD_IS_DOTDOT	0x2e2e",
        "#endif",
        "",
        "/*",
        " * Name resolution.",
        " * This is the basic name resolution function, turning a pathname into",
        " * the final dentry. We expect 'base' to be positive and a directory.",
        " *",
        " * Returns 0 and nd will have valid dentry and mnt on success.",
        " * Returns error and drops reference to input namei data on failure.",
        " */",
        "static int link_path_walk(const char *name, struct nameidata *nd)",
        "{",
        "	int depth = 0; // depth <= nd->depth",
        "	int err;",
        "",
        "	nd->last_type = LAST_ROOT;",
        "	nd->flags |= LOOKUP_PARENT;",
        "	if (IS_ERR(name))",
        "		return PTR_ERR(name);",
        "	while (*name=='/')",
        "		name++;",
        "	if (!*name) {",
        "		nd->dir_mode = 0; // short-circuit the 'hardening' idiocy",
        "		return 0;",
        "	}",
        "",
        "	/* At this point we know we have a real path component. */",
        "	for(;;) {",
        "		struct mnt_idmap *idmap;",
        "		const char *link;",
        "		unsigned long lastword;",
        "",
        "		idmap = mnt_idmap(nd->path.mnt);",
        "		err = may_lookup(idmap, nd);",
        "		if (err)",
        "			return err;",
        "",
        "		nd->last.name = name;",
        "		name = hash_name(nd, name, &lastword);",
        "",
        "		switch(lastword) {",
        "		case LAST_WORD_IS_DOTDOT:",
        "			nd->last_type = LAST_DOTDOT;",
        "			nd->state |= ND_JUMPED;",
        "			break;",
        "",
        "		case LAST_WORD_IS_DOT:",
        "			nd->last_type = LAST_DOT;",
        "			break;",
        "",
        "		default:",
        "			nd->last_type = LAST_NORM;",
        "			nd->state &= ~ND_JUMPED;",
        "",
        "			struct dentry *parent = nd->path.dentry;",
        "			if (unlikely(parent->d_flags & DCACHE_OP_HASH)) {",
        "				err = parent->d_op->d_hash(parent, &nd->last);",
        "				if (err < 0)",
        "					return err;",
        "			}",
        "		}",
        "",
        "		if (!*name)",
        "			goto OK;",
        "		/*",
        "		 * If it wasn't NUL, we know it was '/'. Skip that",
        "		 * slash, and continue until no more slashes.",
        "		 */",
        "		do {",
        "			name++;",
        "		} while (unlikely(*name == '/'));",
        "		if (unlikely(!*name)) {",
        "OK:",
        "			/* pathname or trailing symlink, done */",
        "			if (!depth) {",
        "				nd->dir_vfsuid = i_uid_into_vfsuid(idmap, nd->inode);",
        "				nd->dir_mode = nd->inode->i_mode;",
        "				nd->flags &= ~LOOKUP_PARENT;",
        "				return 0;",
        "			}",
        "			/* last component of nested symlink */",
        "			name = nd->stack[--depth].name;",
        "			link = walk_component(nd, 0);",
        "		} else {",
        "			/* not the last component */",
        "			link = walk_component(nd, WALK_MORE);",
        "		}",
        "		if (unlikely(link)) {",
        "			if (IS_ERR(link))",
        "				return PTR_ERR(link);",
        "			/* a symlink to follow */",
        "			nd->stack[depth++].name = name;",
        "			name = link;",
        "			continue;",
        "		}",
        "		if (unlikely(!d_can_lookup(nd->path.dentry))) {",
        "			if (nd->flags & LOOKUP_RCU) {",
        "				if (!try_to_unlazy(nd))",
        "					return -ECHILD;",
        "			}",
        "			return -ENOTDIR;",
        "		}",
        "	}",
        "}",
        "",
        "/* must be paired with terminate_walk() */",
        "static const char *path_init(struct nameidata *nd, unsigned flags)",
        "{",
        "	int error;",
        "	const char *s = nd->pathname;",
        "",
        "	/* LOOKUP_CACHED requires RCU, ask caller to retry */",
        "	if ((flags & (LOOKUP_RCU | LOOKUP_CACHED)) == LOOKUP_CACHED)",
        "		return ERR_PTR(-EAGAIN);",
        "",
        "	if (!*s)",
        "		flags &= ~LOOKUP_RCU;",
        "	if (flags & LOOKUP_RCU)",
        "		rcu_read_lock();",
        "	else",
        "		nd->seq = nd->next_seq = 0;",
        "",
        "	nd->flags = flags;",
        "	nd->state |= ND_JUMPED;",
        "",
        "	nd->m_seq = __read_seqcount_begin(&mount_lock.seqcount);",
        "	nd->r_seq = __read_seqcount_begin(&rename_lock.seqcount);",
        "	smp_rmb();",
        "",
        "	if (nd->state & ND_ROOT_PRESET) {",
        "		struct dentry *root = nd->root.dentry;",
        "		struct inode *inode = root->d_inode;",
        "		if (*s && unlikely(!d_can_lookup(root)))",
        "			return ERR_PTR(-ENOTDIR);",
        "		nd->path = nd->root;",
        "		nd->inode = inode;",
        "		if (flags & LOOKUP_RCU) {",
        "			nd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);",
        "			nd->root_seq = nd->seq;",
        "		} else {",
        "			path_get(&nd->path);",
        "		}",
        "		return s;",
        "	}",
        "",
        "	nd->root.mnt = NULL;",
        "",
        "	/* Absolute pathname -- fetch the root (LOOKUP_IN_ROOT uses nd->dfd). */",
        "	if (*s == '/' && !(flags & LOOKUP_IN_ROOT)) {",
        "		error = nd_jump_root(nd);",
        "		if (unlikely(error))",
        "			return ERR_PTR(error);",
        "		return s;",
        "	}",
        "",
        "	/* Relative pathname -- get the starting-point it is relative to. */",
        "	if (nd->dfd == AT_FDCWD) {",
        "		if (flags & LOOKUP_RCU) {",
        "			struct fs_struct *fs = current->fs;",
        "			unsigned seq;",
        "",
        "			do {",
        "				seq = read_seqcount_begin(&fs->seq);",
        "				nd->path = fs->pwd;",
        "				nd->inode = nd->path.dentry->d_inode;",
        "				nd->seq = __read_seqcount_begin(&nd->path.dentry->d_seq);",
        "			} while (read_seqcount_retry(&fs->seq, seq));",
        "		} else {",
        "			get_fs_pwd(current->fs, &nd->path);",
        "			nd->inode = nd->path.dentry->d_inode;",
        "		}",
        "	} else {",
        "		/* Caller must check execute permissions on the starting path component */",
        "		CLASS(fd_raw, f)(nd->dfd);",
        "		struct dentry *dentry;",
        "",
        "		if (fd_empty(f))",
        "			return ERR_PTR(-EBADF);",
        "",
        "		if (flags & LOOKUP_LINKAT_EMPTY) {",
        "			if (fd_file(f)->f_cred != current_cred() &&",
        "			    !ns_capable(fd_file(f)->f_cred->user_ns, CAP_DAC_READ_SEARCH))",
        "				return ERR_PTR(-ENOENT);",
        "		}",
        "",
        "		dentry = fd_file(f)->f_path.dentry;",
        "",
        "		if (*s && unlikely(!d_can_lookup(dentry)))",
        "			return ERR_PTR(-ENOTDIR);",
        "",
        "		nd->path = fd_file(f)->f_path;",
        "		if (flags & LOOKUP_RCU) {",
        "			nd->inode = nd->path.dentry->d_inode;",
        "			nd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);",
        "		} else {",
        "			path_get(&nd->path);",
        "			nd->inode = nd->path.dentry->d_inode;",
        "		}",
        "	}",
        "",
        "	/* For scoped-lookups we need to set the root to the dirfd as well. */",
        "	if (flags & LOOKUP_IS_SCOPED) {",
        "		nd->root = nd->path;",
        "		if (flags & LOOKUP_RCU) {",
        "			nd->root_seq = nd->seq;",
        "		} else {",
        "			path_get(&nd->root);",
        "			nd->state |= ND_ROOT_GRABBED;",
        "		}",
        "	}",
        "	return s;",
        "}",
        "",
        "static inline const char *lookup_last(struct nameidata *nd)",
        "{",
        "	if (nd->last_type == LAST_NORM && nd->last.name[nd->last.len])",
        "		nd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;",
        "",
        "	return walk_component(nd, WALK_TRAILING);",
        "}",
        "",
        "static int handle_lookup_down(struct nameidata *nd)",
        "{",
        "	if (!(nd->flags & LOOKUP_RCU))",
        "		dget(nd->path.dentry);",
        "	nd->next_seq = nd->seq;",
        "	return PTR_ERR(step_into(nd, WALK_NOFOLLOW, nd->path.dentry));",
        "}",
        "",
        "/* Returns 0 and nd will be valid on success; Returns error, otherwise. */",
        "static int path_lookupat(struct nameidata *nd, unsigned flags, struct path *path)",
        "{",
        "	const char *s = path_init(nd, flags);",
        "	int err;",
        "",
        "	if (unlikely(flags & LOOKUP_DOWN) && !IS_ERR(s)) {",
        "		err = handle_lookup_down(nd);",
        "		if (unlikely(err < 0))",
        "			s = ERR_PTR(err);",
        "	}",
        "",
        "	while (!(err = link_path_walk(s, nd)) &&",
        "	       (s = lookup_last(nd)) != NULL)",
        "		;",
        "	if (!err && unlikely(nd->flags & LOOKUP_MOUNTPOINT)) {",
        "		err = handle_lookup_down(nd);",
        "		nd->state &= ~ND_JUMPED; // no d_weak_revalidate(), please...",
        "	}",
        "	if (!err)",
        "		err = complete_walk(nd);",
        "",
        "	if (!err && nd->flags & LOOKUP_DIRECTORY)",
        "		if (!d_can_lookup(nd->path.dentry))",
        "			err = -ENOTDIR;",
        "	if (!err) {",
        "		*path = nd->path;",
        "		nd->path.mnt = NULL;",
        "		nd->path.dentry = NULL;",
        "	}",
        "	terminate_walk(nd);",
        "	return err;",
        "}",
        "",
        "int filename_lookup(int dfd, struct filename *name, unsigned flags,",
        "		    struct path *path, struct path *root)",
        "{",
        "	int retval;",
        "	struct nameidata nd;",
        "	if (IS_ERR(name))",
        "		return PTR_ERR(name);",
        "	set_nameidata(&nd, dfd, name, root);",
        "	retval = path_lookupat(&nd, flags | LOOKUP_RCU, path);",
        "	if (unlikely(retval == -ECHILD))",
        "		retval = path_lookupat(&nd, flags, path);",
        "	if (unlikely(retval == -ESTALE))",
        "		retval = path_lookupat(&nd, flags | LOOKUP_REVAL, path);",
        "",
        "	if (likely(!retval))",
        "		audit_inode(name, path->dentry,",
        "			    flags & LOOKUP_MOUNTPOINT ? AUDIT_INODE_NOEVAL : 0);",
        "	restore_nameidata();",
        "	return retval;",
        "}",
        "",
        "/* Returns 0 and nd will be valid on success; Returns error, otherwise. */",
        "static int path_parentat(struct nameidata *nd, unsigned flags,",
        "				struct path *parent)",
        "{",
        "	const char *s = path_init(nd, flags);",
        "	int err = link_path_walk(s, nd);",
        "	if (!err)",
        "		err = complete_walk(nd);",
        "	if (!err) {",
        "		*parent = nd->path;",
        "		nd->path.mnt = NULL;",
        "		nd->path.dentry = NULL;",
        "	}",
        "	terminate_walk(nd);",
        "	return err;",
        "}",
        "",
        "/* Note: this does not consume \"name\" */",
        "static int __filename_parentat(int dfd, struct filename *name,",
        "			       unsigned int flags, struct path *parent,",
        "			       struct qstr *last, int *type,",
        "			       const struct path *root)",
        "{",
        "	int retval;",
        "	struct nameidata nd;",
        "",
        "	if (IS_ERR(name))",
        "		return PTR_ERR(name);",
        "	set_nameidata(&nd, dfd, name, root);",
        "	retval = path_parentat(&nd, flags | LOOKUP_RCU, parent);",
        "	if (unlikely(retval == -ECHILD))",
        "		retval = path_parentat(&nd, flags, parent);",
        "	if (unlikely(retval == -ESTALE))",
        "		retval = path_parentat(&nd, flags | LOOKUP_REVAL, parent);",
        "	if (likely(!retval)) {",
        "		*last = nd.last;",
        "		*type = nd.last_type;",
        "		audit_inode(name, parent->dentry, AUDIT_INODE_PARENT);",
        "	}",
        "	restore_nameidata();",
        "	return retval;",
        "}",
        "",
        "static int filename_parentat(int dfd, struct filename *name,",
        "			     unsigned int flags, struct path *parent,",
        "			     struct qstr *last, int *type)",
        "{",
        "	return __filename_parentat(dfd, name, flags, parent, last, type, NULL);",
        "}",
        "",
        "/* does lookup, returns the object with parent locked */",
        "static struct dentry *__kern_path_locked(int dfd, struct filename *name, struct path *path)",
        "{",
        "	struct dentry *d;",
        "	struct qstr last;",
        "	int type, error;",
        "",
        "	error = filename_parentat(dfd, name, 0, path, &last, &type);",
        "	if (error)",
        "		return ERR_PTR(error);",
        "	if (unlikely(type != LAST_NORM)) {",
        "		path_put(path);",
        "		return ERR_PTR(-EINVAL);",
        "	}",
        "	inode_lock_nested(path->dentry->d_inode, I_MUTEX_PARENT);",
        "	d = lookup_one_qstr_excl(&last, path->dentry, 0);",
        "	if (IS_ERR(d)) {",
        "		inode_unlock(path->dentry->d_inode);",
        "		path_put(path);",
        "	}",
        "	return d;",
        "}",
        "",
        "struct dentry *kern_path_locked(const char *name, struct path *path)",
        "{",
        "	struct filename *filename = getname_kernel(name);",
        "	struct dentry *res = __kern_path_locked(AT_FDCWD, filename, path);",
        "",
        "	putname(filename);",
        "	return res;",
        "}",
        "",
        "struct dentry *user_path_locked_at(int dfd, const char __user *name, struct path *path)",
        "{",
        "	struct filename *filename = getname(name);",
        "	struct dentry *res = __kern_path_locked(dfd, filename, path);",
        "",
        "	putname(filename);",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(user_path_locked_at);",
        "",
        "int kern_path(const char *name, unsigned int flags, struct path *path)",
        "{",
        "	struct filename *filename = getname_kernel(name);",
        "	int ret = filename_lookup(AT_FDCWD, filename, flags, path, NULL);",
        "",
        "	putname(filename);",
        "	return ret;",
        "",
        "}",
        "EXPORT_SYMBOL(kern_path);",
        "",
        "/**",
        " * vfs_path_parent_lookup - lookup a parent path relative to a dentry-vfsmount pair",
        " * @filename: filename structure",
        " * @flags: lookup flags",
        " * @parent: pointer to struct path to fill",
        " * @last: last component",
        " * @type: type of the last component",
        " * @root: pointer to struct path of the base directory",
        " */",
        "int vfs_path_parent_lookup(struct filename *filename, unsigned int flags,",
        "			   struct path *parent, struct qstr *last, int *type,",
        "			   const struct path *root)",
        "{",
        "	return  __filename_parentat(AT_FDCWD, filename, flags, parent, last,",
        "				    type, root);",
        "}",
        "EXPORT_SYMBOL(vfs_path_parent_lookup);",
        "",
        "/**",
        " * vfs_path_lookup - lookup a file path relative to a dentry-vfsmount pair",
        " * @dentry:  pointer to dentry of the base directory",
        " * @mnt: pointer to vfs mount of the base directory",
        " * @name: pointer to file name",
        " * @flags: lookup flags",
        " * @path: pointer to struct path to fill",
        " */",
        "int vfs_path_lookup(struct dentry *dentry, struct vfsmount *mnt,",
        "		    const char *name, unsigned int flags,",
        "		    struct path *path)",
        "{",
        "	struct filename *filename;",
        "	struct path root = {.mnt = mnt, .dentry = dentry};",
        "	int ret;",
        "",
        "	filename = getname_kernel(name);",
        "	/* the first argument of filename_lookup() is ignored with root */",
        "	ret = filename_lookup(AT_FDCWD, filename, flags, path, &root);",
        "	putname(filename);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(vfs_path_lookup);",
        "",
        "static int lookup_one_common(struct mnt_idmap *idmap,",
        "			     const char *name, struct dentry *base, int len,",
        "			     struct qstr *this)",
        "{",
        "	this->name = name;",
        "	this->len = len;",
        "	this->hash = full_name_hash(base, name, len);",
        "	if (!len)",
        "		return -EACCES;",
        "",
        "	if (is_dot_dotdot(name, len))",
        "		return -EACCES;",
        "",
        "	while (len--) {",
        "		unsigned int c = *(const unsigned char *)name++;",
        "		if (c == '/' || c == '\\0')",
        "			return -EACCES;",
        "	}",
        "	/*",
        "	 * See if the low-level filesystem might want",
        "	 * to use its own hash..",
        "	 */",
        "	if (base->d_flags & DCACHE_OP_HASH) {",
        "		int err = base->d_op->d_hash(base, this);",
        "		if (err < 0)",
        "			return err;",
        "	}",
        "",
        "	return inode_permission(idmap, base->d_inode, MAY_EXEC);",
        "}",
        "",
        "/**",
        " * try_lookup_one_len - filesystem helper to lookup single pathname component",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * Look up a dentry by name in the dcache, returning NULL if it does not",
        " * currently exist.  The function does not try to create a dentry.",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * The caller must hold base->i_mutex.",
        " */",
        "struct dentry *try_lookup_one_len(const char *name, struct dentry *base, int len)",
        "{",
        "	struct qstr this;",
        "	int err;",
        "",
        "	WARN_ON_ONCE(!inode_is_locked(base->d_inode));",
        "",
        "	err = lookup_one_common(&nop_mnt_idmap, name, base, len, &this);",
        "	if (err)",
        "		return ERR_PTR(err);",
        "",
        "	return lookup_dcache(&this, base, 0);",
        "}",
        "EXPORT_SYMBOL(try_lookup_one_len);",
        "",
        "/**",
        " * lookup_one_len - filesystem helper to lookup single pathname component",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * The caller must hold base->i_mutex.",
        " */",
        "struct dentry *lookup_one_len(const char *name, struct dentry *base, int len)",
        "{",
        "	struct dentry *dentry;",
        "	struct qstr this;",
        "	int err;",
        "",
        "	WARN_ON_ONCE(!inode_is_locked(base->d_inode));",
        "",
        "	err = lookup_one_common(&nop_mnt_idmap, name, base, len, &this);",
        "	if (err)",
        "		return ERR_PTR(err);",
        "",
        "	dentry = lookup_dcache(&this, base, 0);",
        "	return dentry ? dentry : __lookup_slow(&this, base, 0);",
        "}",
        "EXPORT_SYMBOL(lookup_one_len);",
        "",
        "/**",
        " * lookup_one - filesystem helper to lookup single pathname component",
        " * @idmap:	idmap of the mount the lookup is performed from",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * The caller must hold base->i_mutex.",
        " */",
        "struct dentry *lookup_one(struct mnt_idmap *idmap, const char *name,",
        "			  struct dentry *base, int len)",
        "{",
        "	struct dentry *dentry;",
        "	struct qstr this;",
        "	int err;",
        "",
        "	WARN_ON_ONCE(!inode_is_locked(base->d_inode));",
        "",
        "	err = lookup_one_common(idmap, name, base, len, &this);",
        "	if (err)",
        "		return ERR_PTR(err);",
        "",
        "	dentry = lookup_dcache(&this, base, 0);",
        "	return dentry ? dentry : __lookup_slow(&this, base, 0);",
        "}",
        "EXPORT_SYMBOL(lookup_one);",
        "",
        "/**",
        " * lookup_one_unlocked - filesystem helper to lookup single pathname component",
        " * @idmap:	idmap of the mount the lookup is performed from",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * Unlike lookup_one_len, it should be called without the parent",
        " * i_mutex held, and will take the i_mutex itself if necessary.",
        " */",
        "struct dentry *lookup_one_unlocked(struct mnt_idmap *idmap,",
        "				   const char *name, struct dentry *base,",
        "				   int len)",
        "{",
        "	struct qstr this;",
        "	int err;",
        "	struct dentry *ret;",
        "",
        "	err = lookup_one_common(idmap, name, base, len, &this);",
        "	if (err)",
        "		return ERR_PTR(err);",
        "",
        "	ret = lookup_dcache(&this, base, 0);",
        "	if (!ret)",
        "		ret = lookup_slow(&this, base, 0);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(lookup_one_unlocked);",
        "",
        "/**",
        " * lookup_one_positive_unlocked - filesystem helper to lookup single",
        " *				  pathname component",
        " * @idmap:	idmap of the mount the lookup is performed from",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * This helper will yield ERR_PTR(-ENOENT) on negatives. The helper returns",
        " * known positive or ERR_PTR(). This is what most of the users want.",
        " *",
        " * Note that pinned negative with unlocked parent _can_ become positive at any",
        " * time, so callers of lookup_one_unlocked() need to be very careful; pinned",
        " * positives have >d_inode stable, so this one avoids such problems.",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * The helper should be called without i_mutex held.",
        " */",
        "struct dentry *lookup_one_positive_unlocked(struct mnt_idmap *idmap,",
        "					    const char *name,",
        "					    struct dentry *base, int len)",
        "{",
        "	struct dentry *ret = lookup_one_unlocked(idmap, name, base, len);",
        "",
        "	if (!IS_ERR(ret) && d_flags_negative(smp_load_acquire(&ret->d_flags))) {",
        "		dput(ret);",
        "		ret = ERR_PTR(-ENOENT);",
        "	}",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(lookup_one_positive_unlocked);",
        "",
        "/**",
        " * lookup_one_len_unlocked - filesystem helper to lookup single pathname component",
        " * @name:	pathname component to lookup",
        " * @base:	base directory to lookup from",
        " * @len:	maximum length @len should be interpreted to",
        " *",
        " * Note that this routine is purely a helper for filesystem usage and should",
        " * not be called by generic code.",
        " *",
        " * Unlike lookup_one_len, it should be called without the parent",
        " * i_mutex held, and will take the i_mutex itself if necessary.",
        " */",
        "struct dentry *lookup_one_len_unlocked(const char *name,",
        "				       struct dentry *base, int len)",
        "{",
        "	return lookup_one_unlocked(&nop_mnt_idmap, name, base, len);",
        "}",
        "EXPORT_SYMBOL(lookup_one_len_unlocked);",
        "",
        "/*",
        " * Like lookup_one_len_unlocked(), except that it yields ERR_PTR(-ENOENT)",
        " * on negatives.  Returns known positive or ERR_PTR(); that's what",
        " * most of the users want.  Note that pinned negative with unlocked parent",
        " * _can_ become positive at any time, so callers of lookup_one_len_unlocked()",
        " * need to be very careful; pinned positives have ->d_inode stable, so",
        " * this one avoids such problems.",
        " */",
        "struct dentry *lookup_positive_unlocked(const char *name,",
        "				       struct dentry *base, int len)",
        "{",
        "	return lookup_one_positive_unlocked(&nop_mnt_idmap, name, base, len);",
        "}",
        "EXPORT_SYMBOL(lookup_positive_unlocked);",
        "",
        "#ifdef CONFIG_UNIX98_PTYS",
        "int path_pts(struct path *path)",
        "{",
        "	/* Find something mounted on \"pts\" in the same directory as",
        "	 * the input path.",
        "	 */",
        "	struct dentry *parent = dget_parent(path->dentry);",
        "	struct dentry *child;",
        "	struct qstr this = QSTR_INIT(\"pts\", 3);",
        "",
        "	if (unlikely(!path_connected(path->mnt, parent))) {",
        "		dput(parent);",
        "		return -ENOENT;",
        "	}",
        "	dput(path->dentry);",
        "	path->dentry = parent;",
        "	child = d_hash_and_lookup(parent, &this);",
        "	if (IS_ERR_OR_NULL(child))",
        "		return -ENOENT;",
        "",
        "	path->dentry = child;",
        "	dput(parent);",
        "	follow_down(path, 0);",
        "	return 0;",
        "}",
        "#endif",
        "",
        "int user_path_at(int dfd, const char __user *name, unsigned flags,",
        "		 struct path *path)",
        "{",
        "	struct filename *filename = getname_flags(name, flags);",
        "	int ret = filename_lookup(dfd, filename, flags, path, NULL);",
        "",
        "	putname(filename);",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(user_path_at);",
        "",
        "int __check_sticky(struct mnt_idmap *idmap, struct inode *dir,",
        "		   struct inode *inode)",
        "{",
        "	kuid_t fsuid = current_fsuid();",
        "",
        "	if (vfsuid_eq_kuid(i_uid_into_vfsuid(idmap, inode), fsuid))",
        "		return 0;",
        "	if (vfsuid_eq_kuid(i_uid_into_vfsuid(idmap, dir), fsuid))",
        "		return 0;",
        "	return !capable_wrt_inode_uidgid(idmap, inode, CAP_FOWNER);",
        "}",
        "EXPORT_SYMBOL(__check_sticky);",
        "",
        "/*",
        " *	Check whether we can remove a link victim from directory dir, check",
        " *  whether the type of victim is right.",
        " *  1. We can't do it if dir is read-only (done in permission())",
        " *  2. We should have write and exec permissions on dir",
        " *  3. We can't remove anything from append-only dir",
        " *  4. We can't do anything with immutable dir (done in permission())",
        " *  5. If the sticky bit on dir is set we should either",
        " *	a. be owner of dir, or",
        " *	b. be owner of victim, or",
        " *	c. have CAP_FOWNER capability",
        " *  6. If the victim is append-only or immutable we can't do antyhing with",
        " *     links pointing to it.",
        " *  7. If the victim has an unknown uid or gid we can't change the inode.",
        " *  8. If we were asked to remove a directory and victim isn't one - ENOTDIR.",
        " *  9. If we were asked to remove a non-directory and victim isn't one - EISDIR.",
        " * 10. We can't remove a root or mountpoint.",
        " * 11. We don't allow removal of NFS sillyrenamed files; it's handled by",
        " *     nfs_async_unlink().",
        " */",
        "static int may_delete(struct mnt_idmap *idmap, struct inode *dir,",
        "		      struct dentry *victim, bool isdir)",
        "{",
        "	struct inode *inode = d_backing_inode(victim);",
        "	int error;",
        "",
        "	if (d_is_negative(victim))",
        "		return -ENOENT;",
        "	BUG_ON(!inode);",
        "",
        "	BUG_ON(victim->d_parent->d_inode != dir);",
        "",
        "	/* Inode writeback is not safe when the uid or gid are invalid. */",
        "	if (!vfsuid_valid(i_uid_into_vfsuid(idmap, inode)) ||",
        "	    !vfsgid_valid(i_gid_into_vfsgid(idmap, inode)))",
        "		return -EOVERFLOW;",
        "",
        "	audit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);",
        "",
        "	error = inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);",
        "	if (error)",
        "		return error;",
        "	if (IS_APPEND(dir))",
        "		return -EPERM;",
        "",
        "	if (check_sticky(idmap, dir, inode) || IS_APPEND(inode) ||",
        "	    IS_IMMUTABLE(inode) || IS_SWAPFILE(inode) ||",
        "	    HAS_UNMAPPED_ID(idmap, inode))",
        "		return -EPERM;",
        "	if (isdir) {",
        "		if (!d_is_dir(victim))",
        "			return -ENOTDIR;",
        "		if (IS_ROOT(victim))",
        "			return -EBUSY;",
        "	} else if (d_is_dir(victim))",
        "		return -EISDIR;",
        "	if (IS_DEADDIR(dir))",
        "		return -ENOENT;",
        "	if (victim->d_flags & DCACHE_NFSFS_RENAMED)",
        "		return -EBUSY;",
        "	return 0;",
        "}",
        "",
        "/*	Check whether we can create an object with dentry child in directory",
        " *  dir.",
        " *  1. We can't do it if child already exists (open has special treatment for",
        " *     this case, but since we are inlined it's OK)",
        " *  2. We can't do it if dir is read-only (done in permission())",
        " *  3. We can't do it if the fs can't represent the fsuid or fsgid.",
        " *  4. We should have write and exec permissions on dir",
        " *  5. We can't do it if dir is immutable (done in permission())",
        " */",
        "static inline int may_create(struct mnt_idmap *idmap,",
        "			     struct inode *dir, struct dentry *child)",
        "{",
        "	audit_inode_child(dir, child, AUDIT_TYPE_CHILD_CREATE);",
        "	if (child->d_inode)",
        "		return -EEXIST;",
        "	if (IS_DEADDIR(dir))",
        "		return -ENOENT;",
        "	if (!fsuidgid_has_mapping(dir->i_sb, idmap))",
        "		return -EOVERFLOW;",
        "",
        "	return inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);",
        "}",
        "",
        "// p1 != p2, both are on the same filesystem, ->s_vfs_rename_mutex is held",
        "static struct dentry *lock_two_directories(struct dentry *p1, struct dentry *p2)",
        "{",
        "	struct dentry *p = p1, *q = p2, *r;",
        "",
        "	while ((r = p->d_parent) != p2 && r != p)",
        "		p = r;",
        "	if (r == p2) {",
        "		// p is a child of p2 and an ancestor of p1 or p1 itself",
        "		inode_lock_nested(p2->d_inode, I_MUTEX_PARENT);",
        "		inode_lock_nested(p1->d_inode, I_MUTEX_PARENT2);",
        "		return p;",
        "	}",
        "	// p is the root of connected component that contains p1",
        "	// p2 does not occur on the path from p to p1",
        "	while ((r = q->d_parent) != p1 && r != p && r != q)",
        "		q = r;",
        "	if (r == p1) {",
        "		// q is a child of p1 and an ancestor of p2 or p2 itself",
        "		inode_lock_nested(p1->d_inode, I_MUTEX_PARENT);",
        "		inode_lock_nested(p2->d_inode, I_MUTEX_PARENT2);",
        "		return q;",
        "	} else if (likely(r == p)) {",
        "		// both p2 and p1 are descendents of p",
        "		inode_lock_nested(p1->d_inode, I_MUTEX_PARENT);",
        "		inode_lock_nested(p2->d_inode, I_MUTEX_PARENT2);",
        "		return NULL;",
        "	} else { // no common ancestor at the time we'd been called",
        "		mutex_unlock(&p1->d_sb->s_vfs_rename_mutex);",
        "		return ERR_PTR(-EXDEV);",
        "	}",
        "}",
        "",
        "/*",
        " * p1 and p2 should be directories on the same fs.",
        " */",
        "struct dentry *lock_rename(struct dentry *p1, struct dentry *p2)",
        "{",
        "	if (p1 == p2) {",
        "		inode_lock_nested(p1->d_inode, I_MUTEX_PARENT);",
        "		return NULL;",
        "	}",
        "",
        "	mutex_lock(&p1->d_sb->s_vfs_rename_mutex);",
        "	return lock_two_directories(p1, p2);",
        "}",
        "EXPORT_SYMBOL(lock_rename);",
        "",
        "/*",
        " * c1 and p2 should be on the same fs.",
        " */",
        "struct dentry *lock_rename_child(struct dentry *c1, struct dentry *p2)",
        "{",
        "	if (READ_ONCE(c1->d_parent) == p2) {",
        "		/*",
        "		 * hopefully won't need to touch ->s_vfs_rename_mutex at all.",
        "		 */",
        "		inode_lock_nested(p2->d_inode, I_MUTEX_PARENT);",
        "		/*",
        "		 * now that p2 is locked, nobody can move in or out of it,",
        "		 * so the test below is safe.",
        "		 */",
        "		if (likely(c1->d_parent == p2))",
        "			return NULL;",
        "",
        "		/*",
        "		 * c1 got moved out of p2 while we'd been taking locks;",
        "		 * unlock and fall back to slow case.",
        "		 */",
        "		inode_unlock(p2->d_inode);",
        "	}",
        "",
        "	mutex_lock(&c1->d_sb->s_vfs_rename_mutex);",
        "	/*",
        "	 * nobody can move out of any directories on this fs.",
        "	 */",
        "	if (likely(c1->d_parent != p2))",
        "		return lock_two_directories(c1->d_parent, p2);",
        "",
        "	/*",
        "	 * c1 got moved into p2 while we were taking locks;",
        "	 * we need p2 locked and ->s_vfs_rename_mutex unlocked,",
        "	 * for consistency with lock_rename().",
        "	 */",
        "	inode_lock_nested(p2->d_inode, I_MUTEX_PARENT);",
        "	mutex_unlock(&c1->d_sb->s_vfs_rename_mutex);",
        "	return NULL;",
        "}",
        "EXPORT_SYMBOL(lock_rename_child);",
        "",
        "void unlock_rename(struct dentry *p1, struct dentry *p2)",
        "{",
        "	inode_unlock(p1->d_inode);",
        "	if (p1 != p2) {",
        "		inode_unlock(p2->d_inode);",
        "		mutex_unlock(&p1->d_sb->s_vfs_rename_mutex);",
        "	}",
        "}",
        "EXPORT_SYMBOL(unlock_rename);",
        "",
        "/**",
        " * vfs_prepare_mode - prepare the mode to be used for a new inode",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	parent directory of the new inode",
        " * @mode:	mode of the new inode",
        " * @mask_perms:	allowed permission by the vfs",
        " * @type:	type of file to be created",
        " *",
        " * This helper consolidates and enforces vfs restrictions on the @mode of a new",
        " * object to be created.",
        " *",
        " * Umask stripping depends on whether the filesystem supports POSIX ACLs (see",
        " * the kernel documentation for mode_strip_umask()). Moving umask stripping",
        " * after setgid stripping allows the same ordering for both non-POSIX ACL and",
        " * POSIX ACL supporting filesystems.",
        " *",
        " * Note that it's currently valid for @type to be 0 if a directory is created.",
        " * Filesystems raise that flag individually and we need to check whether each",
        " * filesystem can deal with receiving S_IFDIR from the vfs before we enforce a",
        " * non-zero type.",
        " *",
        " * Returns: mode to be passed to the filesystem",
        " */",
        "static inline umode_t vfs_prepare_mode(struct mnt_idmap *idmap,",
        "				       const struct inode *dir, umode_t mode,",
        "				       umode_t mask_perms, umode_t type)",
        "{",
        "	mode = mode_strip_sgid(idmap, dir, mode);",
        "	mode = mode_strip_umask(dir, mode);",
        "",
        "	/*",
        "	 * Apply the vfs mandated allowed permission mask and set the type of",
        "	 * file to be created before we call into the filesystem.",
        "	 */",
        "	mode &= (mask_perms & ~S_IFMT);",
        "	mode |= (type & S_IFMT);",
        "",
        "	return mode;",
        "}",
        "",
        "/**",
        " * vfs_create - create new file",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	inode of the parent directory",
        " * @dentry:	dentry of the child file",
        " * @mode:	mode of the child file",
        " * @want_excl:	whether the file must not yet exist",
        " *",
        " * Create a new file.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_create(struct mnt_idmap *idmap, struct inode *dir,",
        "	       struct dentry *dentry, umode_t mode, bool want_excl)",
        "{",
        "	int error;",
        "",
        "	error = may_create(idmap, dir, dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	if (!dir->i_op->create)",
        "		return -EACCES;	/* shouldn't it be ENOSYS? */",
        "",
        "	mode = vfs_prepare_mode(idmap, dir, mode, S_IALLUGO, S_IFREG);",
        "	error = security_inode_create(dir, dentry, mode);",
        "	if (error)",
        "		return error;",
        "	error = dir->i_op->create(idmap, dir, dentry, mode, want_excl);",
        "	if (!error)",
        "		fsnotify_create(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_create);",
        "",
        "int vfs_mkobj(struct dentry *dentry, umode_t mode,",
        "		int (*f)(struct dentry *, umode_t, void *),",
        "		void *arg)",
        "{",
        "	struct inode *dir = dentry->d_parent->d_inode;",
        "	int error = may_create(&nop_mnt_idmap, dir, dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	mode &= S_IALLUGO;",
        "	mode |= S_IFREG;",
        "	error = security_inode_create(dir, dentry, mode);",
        "	if (error)",
        "		return error;",
        "	error = f(dentry, mode, arg);",
        "	if (!error)",
        "		fsnotify_create(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_mkobj);",
        "",
        "bool may_open_dev(const struct path *path)",
        "{",
        "	return !(path->mnt->mnt_flags & MNT_NODEV) &&",
        "		!(path->mnt->mnt_sb->s_iflags & SB_I_NODEV);",
        "}",
        "",
        "static int may_open(struct mnt_idmap *idmap, const struct path *path,",
        "		    int acc_mode, int flag)",
        "{",
        "	struct dentry *dentry = path->dentry;",
        "	struct inode *inode = dentry->d_inode;",
        "	int error;",
        "",
        "	if (!inode)",
        "		return -ENOENT;",
        "",
        "	switch (inode->i_mode & S_IFMT) {",
        "	case S_IFLNK:",
        "		return -ELOOP;",
        "	case S_IFDIR:",
        "		if (acc_mode & MAY_WRITE)",
        "			return -EISDIR;",
        "		if (acc_mode & MAY_EXEC)",
        "			return -EACCES;",
        "		break;",
        "	case S_IFBLK:",
        "	case S_IFCHR:",
        "		if (!may_open_dev(path))",
        "			return -EACCES;",
        "		fallthrough;",
        "	case S_IFIFO:",
        "	case S_IFSOCK:",
        "		if (acc_mode & MAY_EXEC)",
        "			return -EACCES;",
        "		flag &= ~O_TRUNC;",
        "		break;",
        "	case S_IFREG:",
        "		if ((acc_mode & MAY_EXEC) && path_noexec(path))",
        "			return -EACCES;",
        "		break;",
        "	}",
        "",
        "	error = inode_permission(idmap, inode, MAY_OPEN | acc_mode);",
        "	if (error)",
        "		return error;",
        "",
        "	/*",
        "	 * An append-only file must be opened in append mode for writing.",
        "	 */",
        "	if (IS_APPEND(inode)) {",
        "		if  ((flag & O_ACCMODE) != O_RDONLY && !(flag & O_APPEND))",
        "			return -EPERM;",
        "		if (flag & O_TRUNC)",
        "			return -EPERM;",
        "	}",
        "",
        "	/* O_NOATIME can only be set by the owner or superuser */",
        "	if (flag & O_NOATIME && !inode_owner_or_capable(idmap, inode))",
        "		return -EPERM;",
        "",
        "	return 0;",
        "}",
        "",
        "static int handle_truncate(struct mnt_idmap *idmap, struct file *filp)",
        "{",
        "	const struct path *path = &filp->f_path;",
        "	struct inode *inode = path->dentry->d_inode;",
        "	int error = get_write_access(inode);",
        "	if (error)",
        "		return error;",
        "",
        "	error = security_file_truncate(filp);",
        "	if (!error) {",
        "		error = do_truncate(idmap, path->dentry, 0,",
        "				    ATTR_MTIME|ATTR_CTIME|ATTR_OPEN,",
        "				    filp);",
        "	}",
        "	put_write_access(inode);",
        "	return error;",
        "}",
        "",
        "static inline int open_to_namei_flags(int flag)",
        "{",
        "	if ((flag & O_ACCMODE) == 3)",
        "		flag--;",
        "	return flag;",
        "}",
        "",
        "static int may_o_create(struct mnt_idmap *idmap,",
        "			const struct path *dir, struct dentry *dentry,",
        "			umode_t mode)",
        "{",
        "	int error = security_path_mknod(dir, dentry, mode, 0);",
        "	if (error)",
        "		return error;",
        "",
        "	if (!fsuidgid_has_mapping(dir->dentry->d_sb, idmap))",
        "		return -EOVERFLOW;",
        "",
        "	error = inode_permission(idmap, dir->dentry->d_inode,",
        "				 MAY_WRITE | MAY_EXEC);",
        "	if (error)",
        "		return error;",
        "",
        "	return security_inode_create(dir->dentry->d_inode, dentry, mode);",
        "}",
        "",
        "/*",
        " * Attempt to atomically look up, create and open a file from a negative",
        " * dentry.",
        " *",
        " * Returns 0 if successful.  The file will have been created and attached to",
        " * @file by the filesystem calling finish_open().",
        " *",
        " * If the file was looked up only or didn't need creating, FMODE_OPENED won't",
        " * be set.  The caller will need to perform the open themselves.  @path will",
        " * have been updated to point to the new dentry.  This may be negative.",
        " *",
        " * Returns an error code otherwise.",
        " */",
        "static struct dentry *atomic_open(struct nameidata *nd, struct dentry *dentry,",
        "				  struct file *file,",
        "				  int open_flag, umode_t mode)",
        "{",
        "	struct dentry *const DENTRY_NOT_SET = (void *) -1UL;",
        "	struct inode *dir =  nd->path.dentry->d_inode;",
        "	int error;",
        "",
        "	if (nd->flags & LOOKUP_DIRECTORY)",
        "		open_flag |= O_DIRECTORY;",
        "",
        "	file->f_path.dentry = DENTRY_NOT_SET;",
        "	file->f_path.mnt = nd->path.mnt;",
        "	error = dir->i_op->atomic_open(dir, dentry, file,",
        "				       open_to_namei_flags(open_flag), mode);",
        "	d_lookup_done(dentry);",
        "	if (!error) {",
        "		if (file->f_mode & FMODE_OPENED) {",
        "			if (unlikely(dentry != file->f_path.dentry)) {",
        "				dput(dentry);",
        "				dentry = dget(file->f_path.dentry);",
        "			}",
        "		} else if (WARN_ON(file->f_path.dentry == DENTRY_NOT_SET)) {",
        "			error = -EIO;",
        "		} else {",
        "			if (file->f_path.dentry) {",
        "				dput(dentry);",
        "				dentry = file->f_path.dentry;",
        "			}",
        "			if (unlikely(d_is_negative(dentry)))",
        "				error = -ENOENT;",
        "		}",
        "	}",
        "	if (error) {",
        "		dput(dentry);",
        "		dentry = ERR_PTR(error);",
        "	}",
        "	return dentry;",
        "}",
        "",
        "/*",
        " * Look up and maybe create and open the last component.",
        " *",
        " * Must be called with parent locked (exclusive in O_CREAT case).",
        " *",
        " * Returns 0 on success, that is, if",
        " *  the file was successfully atomically created (if necessary) and opened, or",
        " *  the file was not completely opened at this time, though lookups and",
        " *  creations were performed.",
        " * These case are distinguished by presence of FMODE_OPENED on file->f_mode.",
        " * In the latter case dentry returned in @path might be negative if O_CREAT",
        " * hadn't been specified.",
        " *",
        " * An error code is returned on failure.",
        " */",
        "static struct dentry *lookup_open(struct nameidata *nd, struct file *file,",
        "				  const struct open_flags *op,",
        "				  bool got_write)",
        "{",
        "	struct mnt_idmap *idmap;",
        "	struct dentry *dir = nd->path.dentry;",
        "	struct inode *dir_inode = dir->d_inode;",
        "	int open_flag = op->open_flag;",
        "	struct dentry *dentry;",
        "	int error, create_error = 0;",
        "	umode_t mode = op->mode;",
        "	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);",
        "",
        "	if (unlikely(IS_DEADDIR(dir_inode)))",
        "		return ERR_PTR(-ENOENT);",
        "",
        "	file->f_mode &= ~FMODE_CREATED;",
        "	dentry = d_lookup(dir, &nd->last);",
        "	for (;;) {",
        "		if (!dentry) {",
        "			dentry = d_alloc_parallel(dir, &nd->last, &wq);",
        "			if (IS_ERR(dentry))",
        "				return dentry;",
        "		}",
        "		if (d_in_lookup(dentry))",
        "			break;",
        "",
        "		error = d_revalidate(dentry, nd->flags);",
        "		if (likely(error > 0))",
        "			break;",
        "		if (error)",
        "			goto out_dput;",
        "		d_invalidate(dentry);",
        "		dput(dentry);",
        "		dentry = NULL;",
        "	}",
        "	if (dentry->d_inode) {",
        "		/* Cached positive dentry: will open in f_op->open */",
        "		return dentry;",
        "	}",
        "",
        "	if (open_flag & O_CREAT)",
        "		audit_inode(nd->name, dir, AUDIT_INODE_PARENT);",
        "",
        "	/*",
        "	 * Checking write permission is tricky, bacuse we don't know if we are",
        "	 * going to actually need it: O_CREAT opens should work as long as the",
        "	 * file exists.  But checking existence breaks atomicity.  The trick is",
        "	 * to check access and if not granted clear O_CREAT from the flags.",
        "	 *",
        "	 * Another problem is returing the \"right\" error value (e.g. for an",
        "	 * O_EXCL open we want to return EEXIST not EROFS).",
        "	 */",
        "	if (unlikely(!got_write))",
        "		open_flag &= ~O_TRUNC;",
        "	idmap = mnt_idmap(nd->path.mnt);",
        "	if (open_flag & O_CREAT) {",
        "		if (open_flag & O_EXCL)",
        "			open_flag &= ~O_TRUNC;",
        "		mode = vfs_prepare_mode(idmap, dir->d_inode, mode, mode, mode);",
        "		if (likely(got_write))",
        "			create_error = may_o_create(idmap, &nd->path,",
        "						    dentry, mode);",
        "		else",
        "			create_error = -EROFS;",
        "	}",
        "	if (create_error)",
        "		open_flag &= ~O_CREAT;",
        "	if (dir_inode->i_op->atomic_open) {",
        "		dentry = atomic_open(nd, dentry, file, open_flag, mode);",
        "		if (unlikely(create_error) && dentry == ERR_PTR(-ENOENT))",
        "			dentry = ERR_PTR(create_error);",
        "		return dentry;",
        "	}",
        "",
        "	if (d_in_lookup(dentry)) {",
        "		struct dentry *res = dir_inode->i_op->lookup(dir_inode, dentry,",
        "							     nd->flags);",
        "		d_lookup_done(dentry);",
        "		if (unlikely(res)) {",
        "			if (IS_ERR(res)) {",
        "				error = PTR_ERR(res);",
        "				goto out_dput;",
        "			}",
        "			dput(dentry);",
        "			dentry = res;",
        "		}",
        "	}",
        "",
        "	/* Negative dentry, just create the file */",
        "	if (!dentry->d_inode && (open_flag & O_CREAT)) {",
        "		file->f_mode |= FMODE_CREATED;",
        "		audit_inode_child(dir_inode, dentry, AUDIT_TYPE_CHILD_CREATE);",
        "		if (!dir_inode->i_op->create) {",
        "			error = -EACCES;",
        "			goto out_dput;",
        "		}",
        "",
        "		error = dir_inode->i_op->create(idmap, dir_inode, dentry,",
        "						mode, open_flag & O_EXCL);",
        "		if (error)",
        "			goto out_dput;",
        "	}",
        "	if (unlikely(create_error) && !dentry->d_inode) {",
        "		error = create_error;",
        "		goto out_dput;",
        "	}",
        "	return dentry;",
        "",
        "out_dput:",
        "	dput(dentry);",
        "	return ERR_PTR(error);",
        "}",
        "",
        "static inline bool trailing_slashes(struct nameidata *nd)",
        "{",
        "	return (bool)nd->last.name[nd->last.len];",
        "}",
        "",
        "static struct dentry *lookup_fast_for_open(struct nameidata *nd, int open_flag)",
        "{",
        "	struct dentry *dentry;",
        "",
        "	if (open_flag & O_CREAT) {",
        "		if (trailing_slashes(nd))",
        "			return ERR_PTR(-EISDIR);",
        "",
        "		/* Don't bother on an O_EXCL create */",
        "		if (open_flag & O_EXCL)",
        "			return NULL;",
        "	}",
        "",
        "	if (trailing_slashes(nd))",
        "		nd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;",
        "",
        "	dentry = lookup_fast(nd);",
        "	if (IS_ERR_OR_NULL(dentry))",
        "		return dentry;",
        "",
        "	if (open_flag & O_CREAT) {",
        "		/* Discard negative dentries. Need inode_lock to do the create */",
        "		if (!dentry->d_inode) {",
        "			if (!(nd->flags & LOOKUP_RCU))",
        "				dput(dentry);",
        "			dentry = NULL;",
        "		}",
        "	}",
        "	return dentry;",
        "}",
        "",
        "static const char *open_last_lookups(struct nameidata *nd,",
        "		   struct file *file, const struct open_flags *op)",
        "{",
        "	struct dentry *dir = nd->path.dentry;",
        "	int open_flag = op->open_flag;",
        "	bool got_write = false;",
        "	struct dentry *dentry;",
        "	const char *res;",
        "",
        "	nd->flags |= op->intent;",
        "",
        "	if (nd->last_type != LAST_NORM) {",
        "		if (nd->depth)",
        "			put_link(nd);",
        "		return handle_dots(nd, nd->last_type);",
        "	}",
        "",
        "	/* We _can_ be in RCU mode here */",
        "	dentry = lookup_fast_for_open(nd, open_flag);",
        "	if (IS_ERR(dentry))",
        "		return ERR_CAST(dentry);",
        "",
        "	if (likely(dentry))",
        "		goto finish_lookup;",
        "",
        "	if (!(open_flag & O_CREAT)) {",
        "		if (WARN_ON_ONCE(nd->flags & LOOKUP_RCU))",
        "			return ERR_PTR(-ECHILD);",
        "	} else {",
        "		if (nd->flags & LOOKUP_RCU) {",
        "			if (!try_to_unlazy(nd))",
        "				return ERR_PTR(-ECHILD);",
        "		}",
        "	}",
        "",
        "	if (open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {",
        "		got_write = !mnt_want_write(nd->path.mnt);",
        "		/*",
        "		 * do _not_ fail yet - we might not need that or fail with",
        "		 * a different error; let lookup_open() decide; we'll be",
        "		 * dropping this one anyway.",
        "		 */",
        "	}",
        "	if (open_flag & O_CREAT)",
        "		inode_lock(dir->d_inode);",
        "	else",
        "		inode_lock_shared(dir->d_inode);",
        "	dentry = lookup_open(nd, file, op, got_write);",
        "	if (!IS_ERR(dentry)) {",
        "		if (file->f_mode & FMODE_CREATED)",
        "			fsnotify_create(dir->d_inode, dentry);",
        "		if (file->f_mode & FMODE_OPENED)",
        "			fsnotify_open(file);",
        "	}",
        "	if (open_flag & O_CREAT)",
        "		inode_unlock(dir->d_inode);",
        "	else",
        "		inode_unlock_shared(dir->d_inode);",
        "",
        "	if (got_write)",
        "		mnt_drop_write(nd->path.mnt);",
        "",
        "	if (IS_ERR(dentry))",
        "		return ERR_CAST(dentry);",
        "",
        "	if (file->f_mode & (FMODE_OPENED | FMODE_CREATED)) {",
        "		dput(nd->path.dentry);",
        "		nd->path.dentry = dentry;",
        "		return NULL;",
        "	}",
        "",
        "finish_lookup:",
        "	if (nd->depth)",
        "		put_link(nd);",
        "	res = step_into(nd, WALK_TRAILING, dentry);",
        "	if (unlikely(res))",
        "		nd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);",
        "	return res;",
        "}",
        "",
        "/*",
        " * Handle the last step of open()",
        " */",
        "static int do_open(struct nameidata *nd,",
        "		   struct file *file, const struct open_flags *op)",
        "{",
        "	struct mnt_idmap *idmap;",
        "	int open_flag = op->open_flag;",
        "	bool do_truncate;",
        "	int acc_mode;",
        "	int error;",
        "",
        "	if (!(file->f_mode & (FMODE_OPENED | FMODE_CREATED))) {",
        "		error = complete_walk(nd);",
        "		if (error)",
        "			return error;",
        "	}",
        "	if (!(file->f_mode & FMODE_CREATED))",
        "		audit_inode(nd->name, nd->path.dentry, 0);",
        "	idmap = mnt_idmap(nd->path.mnt);",
        "	if (open_flag & O_CREAT) {",
        "		if ((open_flag & O_EXCL) && !(file->f_mode & FMODE_CREATED))",
        "			return -EEXIST;",
        "		if (d_is_dir(nd->path.dentry))",
        "			return -EISDIR;",
        "		error = may_create_in_sticky(idmap, nd,",
        "					     d_backing_inode(nd->path.dentry));",
        "		if (unlikely(error))",
        "			return error;",
        "	}",
        "	if ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))",
        "		return -ENOTDIR;",
        "",
        "	do_truncate = false;",
        "	acc_mode = op->acc_mode;",
        "	if (file->f_mode & FMODE_CREATED) {",
        "		/* Don't check for write permission, don't truncate */",
        "		open_flag &= ~O_TRUNC;",
        "		acc_mode = 0;",
        "	} else if (d_is_reg(nd->path.dentry) && open_flag & O_TRUNC) {",
        "		error = mnt_want_write(nd->path.mnt);",
        "		if (error)",
        "			return error;",
        "		do_truncate = true;",
        "	}",
        "	error = may_open(idmap, &nd->path, acc_mode, open_flag);",
        "	if (!error && !(file->f_mode & FMODE_OPENED))",
        "		error = vfs_open(&nd->path, file);",
        "	if (!error)",
        "		error = security_file_post_open(file, op->acc_mode);",
        "	if (!error && do_truncate)",
        "		error = handle_truncate(idmap, file);",
        "	if (unlikely(error > 0)) {",
        "		WARN_ON(1);",
        "		error = -EINVAL;",
        "	}",
        "	if (do_truncate)",
        "		mnt_drop_write(nd->path.mnt);",
        "	return error;",
        "}",
        "",
        "/**",
        " * vfs_tmpfile - create tmpfile",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @parentpath:	pointer to the path of the base directory",
        " * @file:	file descriptor of the new tmpfile",
        " * @mode:	mode of the new tmpfile",
        " *",
        " * Create a temporary file.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_tmpfile(struct mnt_idmap *idmap,",
        "		const struct path *parentpath,",
        "		struct file *file, umode_t mode)",
        "{",
        "	struct dentry *child;",
        "	struct inode *dir = d_inode(parentpath->dentry);",
        "	struct inode *inode;",
        "	int error;",
        "	int open_flag = file->f_flags;",
        "",
        "	/* we want directory to be writable */",
        "	error = inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);",
        "	if (error)",
        "		return error;",
        "	if (!dir->i_op->tmpfile)",
        "		return -EOPNOTSUPP;",
        "	child = d_alloc(parentpath->dentry, &slash_name);",
        "	if (unlikely(!child))",
        "		return -ENOMEM;",
        "	file->f_path.mnt = parentpath->mnt;",
        "	file->f_path.dentry = child;",
        "	mode = vfs_prepare_mode(idmap, dir, mode, mode, mode);",
        "	error = dir->i_op->tmpfile(idmap, dir, file, mode);",
        "	dput(child);",
        "	if (file->f_mode & FMODE_OPENED)",
        "		fsnotify_open(file);",
        "	if (error)",
        "		return error;",
        "	/* Don't check for other permissions, the inode was just created */",
        "	error = may_open(idmap, &file->f_path, 0, file->f_flags);",
        "	if (error)",
        "		return error;",
        "	inode = file_inode(file);",
        "	if (!(open_flag & O_EXCL)) {",
        "		spin_lock(&inode->i_lock);",
        "		inode->i_state |= I_LINKABLE;",
        "		spin_unlock(&inode->i_lock);",
        "	}",
        "	security_inode_post_create_tmpfile(idmap, inode);",
        "	return 0;",
        "}",
        "",
        "/**",
        " * kernel_tmpfile_open - open a tmpfile for kernel internal use",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @parentpath:	path of the base directory",
        " * @mode:	mode of the new tmpfile",
        " * @open_flag:	flags",
        " * @cred:	credentials for open",
        " *",
        " * Create and open a temporary file.  The file is not accounted in nr_files,",
        " * hence this is only for kernel internal use, and must not be installed into",
        " * file tables or such.",
        " */",
        "struct file *kernel_tmpfile_open(struct mnt_idmap *idmap,",
        "				 const struct path *parentpath,",
        "				 umode_t mode, int open_flag,",
        "				 const struct cred *cred)",
        "{",
        "	struct file *file;",
        "	int error;",
        "",
        "	file = alloc_empty_file_noaccount(open_flag, cred);",
        "	if (IS_ERR(file))",
        "		return file;",
        "",
        "	error = vfs_tmpfile(idmap, parentpath, file, mode);",
        "	if (error) {",
        "		fput(file);",
        "		file = ERR_PTR(error);",
        "	}",
        "	return file;",
        "}",
        "EXPORT_SYMBOL(kernel_tmpfile_open);",
        "",
        "static int do_tmpfile(struct nameidata *nd, unsigned flags,",
        "		const struct open_flags *op,",
        "		struct file *file)",
        "{",
        "	struct path path;",
        "	int error = path_lookupat(nd, flags | LOOKUP_DIRECTORY, &path);",
        "",
        "	if (unlikely(error))",
        "		return error;",
        "	error = mnt_want_write(path.mnt);",
        "	if (unlikely(error))",
        "		goto out;",
        "	error = vfs_tmpfile(mnt_idmap(path.mnt), &path, file, op->mode);",
        "	if (error)",
        "		goto out2;",
        "	audit_inode(nd->name, file->f_path.dentry, 0);",
        "out2:",
        "	mnt_drop_write(path.mnt);",
        "out:",
        "	path_put(&path);",
        "	return error;",
        "}",
        "",
        "static int do_o_path(struct nameidata *nd, unsigned flags, struct file *file)",
        "{",
        "	struct path path;",
        "	int error = path_lookupat(nd, flags, &path);",
        "	if (!error) {",
        "		audit_inode(nd->name, path.dentry, 0);",
        "		error = vfs_open(&path, file);",
        "		path_put(&path);",
        "	}",
        "	return error;",
        "}",
        "",
        "static struct file *path_openat(struct nameidata *nd,",
        "			const struct open_flags *op, unsigned flags)",
        "{",
        "	struct file *file;",
        "	int error;",
        "",
        "	file = alloc_empty_file(op->open_flag, current_cred());",
        "	if (IS_ERR(file))",
        "		return file;",
        "",
        "	if (unlikely(file->f_flags & __O_TMPFILE)) {",
        "		error = do_tmpfile(nd, flags, op, file);",
        "	} else if (unlikely(file->f_flags & O_PATH)) {",
        "		error = do_o_path(nd, flags, file);",
        "	} else {",
        "		const char *s = path_init(nd, flags);",
        "		while (!(error = link_path_walk(s, nd)) &&",
        "		       (s = open_last_lookups(nd, file, op)) != NULL)",
        "			;",
        "		if (!error)",
        "			error = do_open(nd, file, op);",
        "		terminate_walk(nd);",
        "	}",
        "	if (likely(!error)) {",
        "		if (likely(file->f_mode & FMODE_OPENED))",
        "			return file;",
        "		WARN_ON(1);",
        "		error = -EINVAL;",
        "	}",
        "	fput(file);",
        "	if (error == -EOPENSTALE) {",
        "		if (flags & LOOKUP_RCU)",
        "			error = -ECHILD;",
        "		else",
        "			error = -ESTALE;",
        "	}",
        "	return ERR_PTR(error);",
        "}",
        "",
        "struct file *do_filp_open(int dfd, struct filename *pathname,",
        "		const struct open_flags *op)",
        "{",
        "	struct nameidata nd;",
        "	int flags = op->lookup_flags;",
        "	struct file *filp;",
        "",
        "	set_nameidata(&nd, dfd, pathname, NULL);",
        "	filp = path_openat(&nd, op, flags | LOOKUP_RCU);",
        "	if (unlikely(filp == ERR_PTR(-ECHILD)))",
        "		filp = path_openat(&nd, op, flags);",
        "	if (unlikely(filp == ERR_PTR(-ESTALE)))",
        "		filp = path_openat(&nd, op, flags | LOOKUP_REVAL);",
        "	restore_nameidata();",
        "	return filp;",
        "}",
        "",
        "struct file *do_file_open_root(const struct path *root,",
        "		const char *name, const struct open_flags *op)",
        "{",
        "	struct nameidata nd;",
        "	struct file *file;",
        "	struct filename *filename;",
        "	int flags = op->lookup_flags;",
        "",
        "	if (d_is_symlink(root->dentry) && op->intent & LOOKUP_OPEN)",
        "		return ERR_PTR(-ELOOP);",
        "",
        "	filename = getname_kernel(name);",
        "	if (IS_ERR(filename))",
        "		return ERR_CAST(filename);",
        "",
        "	set_nameidata(&nd, -1, filename, root);",
        "	file = path_openat(&nd, op, flags | LOOKUP_RCU);",
        "	if (unlikely(file == ERR_PTR(-ECHILD)))",
        "		file = path_openat(&nd, op, flags);",
        "	if (unlikely(file == ERR_PTR(-ESTALE)))",
        "		file = path_openat(&nd, op, flags | LOOKUP_REVAL);",
        "	restore_nameidata();",
        "	putname(filename);",
        "	return file;",
        "}",
        "",
        "static struct dentry *filename_create(int dfd, struct filename *name,",
        "				      struct path *path, unsigned int lookup_flags)",
        "{",
        "	struct dentry *dentry = ERR_PTR(-EEXIST);",
        "	struct qstr last;",
        "	bool want_dir = lookup_flags & LOOKUP_DIRECTORY;",
        "	unsigned int reval_flag = lookup_flags & LOOKUP_REVAL;",
        "	unsigned int create_flags = LOOKUP_CREATE | LOOKUP_EXCL;",
        "	int type;",
        "	int err2;",
        "	int error;",
        "",
        "	error = filename_parentat(dfd, name, reval_flag, path, &last, &type);",
        "	if (error)",
        "		return ERR_PTR(error);",
        "",
        "	/*",
        "	 * Yucky last component or no last component at all?",
        "	 * (foo/., foo/.., /////)",
        "	 */",
        "	if (unlikely(type != LAST_NORM))",
        "		goto out;",
        "",
        "	/* don't fail immediately if it's r/o, at least try to report other errors */",
        "	err2 = mnt_want_write(path->mnt);",
        "	/*",
        "	 * Do the final lookup.  Suppress 'create' if there is a trailing",
        "	 * '/', and a directory wasn't requested.",
        "	 */",
        "	if (last.name[last.len] && !want_dir)",
        "		create_flags = 0;",
        "	inode_lock_nested(path->dentry->d_inode, I_MUTEX_PARENT);",
        "	dentry = lookup_one_qstr_excl(&last, path->dentry,",
        "				      reval_flag | create_flags);",
        "	if (IS_ERR(dentry))",
        "		goto unlock;",
        "",
        "	error = -EEXIST;",
        "	if (d_is_positive(dentry))",
        "		goto fail;",
        "",
        "	/*",
        "	 * Special case - lookup gave negative, but... we had foo/bar/",
        "	 * From the vfs_mknod() POV we just have a negative dentry -",
        "	 * all is fine. Let's be bastards - you had / on the end, you've",
        "	 * been asking for (non-existent) directory. -ENOENT for you.",
        "	 */",
        "	if (unlikely(!create_flags)) {",
        "		error = -ENOENT;",
        "		goto fail;",
        "	}",
        "	if (unlikely(err2)) {",
        "		error = err2;",
        "		goto fail;",
        "	}",
        "	return dentry;",
        "fail:",
        "	dput(dentry);",
        "	dentry = ERR_PTR(error);",
        "unlock:",
        "	inode_unlock(path->dentry->d_inode);",
        "	if (!err2)",
        "		mnt_drop_write(path->mnt);",
        "out:",
        "	path_put(path);",
        "	return dentry;",
        "}",
        "",
        "struct dentry *kern_path_create(int dfd, const char *pathname,",
        "				struct path *path, unsigned int lookup_flags)",
        "{",
        "	struct filename *filename = getname_kernel(pathname);",
        "	struct dentry *res = filename_create(dfd, filename, path, lookup_flags);",
        "",
        "	putname(filename);",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(kern_path_create);",
        "",
        "void done_path_create(struct path *path, struct dentry *dentry)",
        "{",
        "	dput(dentry);",
        "	inode_unlock(path->dentry->d_inode);",
        "	mnt_drop_write(path->mnt);",
        "	path_put(path);",
        "}",
        "EXPORT_SYMBOL(done_path_create);",
        "",
        "inline struct dentry *user_path_create(int dfd, const char __user *pathname,",
        "				struct path *path, unsigned int lookup_flags)",
        "{",
        "	struct filename *filename = getname(pathname);",
        "	struct dentry *res = filename_create(dfd, filename, path, lookup_flags);",
        "",
        "	putname(filename);",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(user_path_create);",
        "",
        "/**",
        " * vfs_mknod - create device node or file",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	inode of the parent directory",
        " * @dentry:	dentry of the child device node",
        " * @mode:	mode of the child device node",
        " * @dev:	device number of device to create",
        " *",
        " * Create a device node or file.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_mknod(struct mnt_idmap *idmap, struct inode *dir,",
        "	      struct dentry *dentry, umode_t mode, dev_t dev)",
        "{",
        "	bool is_whiteout = S_ISCHR(mode) && dev == WHITEOUT_DEV;",
        "	int error = may_create(idmap, dir, dentry);",
        "",
        "	if (error)",
        "		return error;",
        "",
        "	if ((S_ISCHR(mode) || S_ISBLK(mode)) && !is_whiteout &&",
        "	    !capable(CAP_MKNOD))",
        "		return -EPERM;",
        "",
        "	if (!dir->i_op->mknod)",
        "		return -EPERM;",
        "",
        "	mode = vfs_prepare_mode(idmap, dir, mode, mode, mode);",
        "	error = devcgroup_inode_mknod(mode, dev);",
        "	if (error)",
        "		return error;",
        "",
        "	error = security_inode_mknod(dir, dentry, mode, dev);",
        "	if (error)",
        "		return error;",
        "",
        "	error = dir->i_op->mknod(idmap, dir, dentry, mode, dev);",
        "	if (!error)",
        "		fsnotify_create(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_mknod);",
        "",
        "static int may_mknod(umode_t mode)",
        "{",
        "	switch (mode & S_IFMT) {",
        "	case S_IFREG:",
        "	case S_IFCHR:",
        "	case S_IFBLK:",
        "	case S_IFIFO:",
        "	case S_IFSOCK:",
        "	case 0: /* zero mode translates to S_IFREG */",
        "		return 0;",
        "	case S_IFDIR:",
        "		return -EPERM;",
        "	default:",
        "		return -EINVAL;",
        "	}",
        "}",
        "",
        "static int do_mknodat(int dfd, struct filename *name, umode_t mode,",
        "		unsigned int dev)",
        "{",
        "	struct mnt_idmap *idmap;",
        "	struct dentry *dentry;",
        "	struct path path;",
        "	int error;",
        "	unsigned int lookup_flags = 0;",
        "",
        "	error = may_mknod(mode);",
        "	if (error)",
        "		goto out1;",
        "retry:",
        "	dentry = filename_create(dfd, name, &path, lookup_flags);",
        "	error = PTR_ERR(dentry);",
        "	if (IS_ERR(dentry))",
        "		goto out1;",
        "",
        "	error = security_path_mknod(&path, dentry,",
        "			mode_strip_umask(path.dentry->d_inode, mode), dev);",
        "	if (error)",
        "		goto out2;",
        "",
        "	idmap = mnt_idmap(path.mnt);",
        "	switch (mode & S_IFMT) {",
        "		case 0: case S_IFREG:",
        "			error = vfs_create(idmap, path.dentry->d_inode,",
        "					   dentry, mode, true);",
        "			if (!error)",
        "				security_path_post_mknod(idmap, dentry);",
        "			break;",
        "		case S_IFCHR: case S_IFBLK:",
        "			error = vfs_mknod(idmap, path.dentry->d_inode,",
        "					  dentry, mode, new_decode_dev(dev));",
        "			break;",
        "		case S_IFIFO: case S_IFSOCK:",
        "			error = vfs_mknod(idmap, path.dentry->d_inode,",
        "					  dentry, mode, 0);",
        "			break;",
        "	}",
        "out2:",
        "	done_path_create(&path, dentry);",
        "	if (retry_estale(error, lookup_flags)) {",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "out1:",
        "	putname(name);",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE4(mknodat, int, dfd, const char __user *, filename, umode_t, mode,",
        "		unsigned int, dev)",
        "{",
        "	return do_mknodat(dfd, getname(filename), mode, dev);",
        "}",
        "",
        "SYSCALL_DEFINE3(mknod, const char __user *, filename, umode_t, mode, unsigned, dev)",
        "{",
        "	return do_mknodat(AT_FDCWD, getname(filename), mode, dev);",
        "}",
        "",
        "/**",
        " * vfs_mkdir - create directory",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	inode of the parent directory",
        " * @dentry:	dentry of the child directory",
        " * @mode:	mode of the child directory",
        " *",
        " * Create a directory.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_mkdir(struct mnt_idmap *idmap, struct inode *dir,",
        "	      struct dentry *dentry, umode_t mode)",
        "{",
        "	int error;",
        "	unsigned max_links = dir->i_sb->s_max_links;",
        "",
        "	error = may_create(idmap, dir, dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	if (!dir->i_op->mkdir)",
        "		return -EPERM;",
        "",
        "	mode = vfs_prepare_mode(idmap, dir, mode, S_IRWXUGO | S_ISVTX, 0);",
        "	error = security_inode_mkdir(dir, dentry, mode);",
        "	if (error)",
        "		return error;",
        "",
        "	if (max_links && dir->i_nlink >= max_links)",
        "		return -EMLINK;",
        "",
        "	error = dir->i_op->mkdir(idmap, dir, dentry, mode);",
        "	if (!error)",
        "		fsnotify_mkdir(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_mkdir);",
        "",
        "int do_mkdirat(int dfd, struct filename *name, umode_t mode)",
        "{",
        "	struct dentry *dentry;",
        "	struct path path;",
        "	int error;",
        "	unsigned int lookup_flags = LOOKUP_DIRECTORY;",
        "",
        "retry:",
        "	dentry = filename_create(dfd, name, &path, lookup_flags);",
        "	error = PTR_ERR(dentry);",
        "	if (IS_ERR(dentry))",
        "		goto out_putname;",
        "",
        "	error = security_path_mkdir(&path, dentry,",
        "			mode_strip_umask(path.dentry->d_inode, mode));",
        "	if (!error) {",
        "		error = vfs_mkdir(mnt_idmap(path.mnt), path.dentry->d_inode,",
        "				  dentry, mode);",
        "	}",
        "	done_path_create(&path, dentry);",
        "	if (retry_estale(error, lookup_flags)) {",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "out_putname:",
        "	putname(name);",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE3(mkdirat, int, dfd, const char __user *, pathname, umode_t, mode)",
        "{",
        "	return do_mkdirat(dfd, getname(pathname), mode);",
        "}",
        "",
        "SYSCALL_DEFINE2(mkdir, const char __user *, pathname, umode_t, mode)",
        "{",
        "	return do_mkdirat(AT_FDCWD, getname(pathname), mode);",
        "}",
        "",
        "/**",
        " * vfs_rmdir - remove directory",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	inode of the parent directory",
        " * @dentry:	dentry of the child directory",
        " *",
        " * Remove a directory.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_rmdir(struct mnt_idmap *idmap, struct inode *dir,",
        "		     struct dentry *dentry)",
        "{",
        "	int error = may_delete(idmap, dir, dentry, 1);",
        "",
        "	if (error)",
        "		return error;",
        "",
        "	if (!dir->i_op->rmdir)",
        "		return -EPERM;",
        "",
        "	dget(dentry);",
        "	inode_lock(dentry->d_inode);",
        "",
        "	error = -EBUSY;",
        "	if (is_local_mountpoint(dentry) ||",
        "	    (dentry->d_inode->i_flags & S_KERNEL_FILE))",
        "		goto out;",
        "",
        "	error = security_inode_rmdir(dir, dentry);",
        "	if (error)",
        "		goto out;",
        "",
        "	error = dir->i_op->rmdir(dir, dentry);",
        "	if (error)",
        "		goto out;",
        "",
        "	shrink_dcache_parent(dentry);",
        "	dentry->d_inode->i_flags |= S_DEAD;",
        "	dont_mount(dentry);",
        "	detach_mounts(dentry);",
        "",
        "out:",
        "	inode_unlock(dentry->d_inode);",
        "	dput(dentry);",
        "	if (!error)",
        "		d_delete_notify(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_rmdir);",
        "",
        "int do_rmdir(int dfd, struct filename *name)",
        "{",
        "	int error;",
        "	struct dentry *dentry;",
        "	struct path path;",
        "	struct qstr last;",
        "	int type;",
        "	unsigned int lookup_flags = 0;",
        "retry:",
        "	error = filename_parentat(dfd, name, lookup_flags, &path, &last, &type);",
        "	if (error)",
        "		goto exit1;",
        "",
        "	switch (type) {",
        "	case LAST_DOTDOT:",
        "		error = -ENOTEMPTY;",
        "		goto exit2;",
        "	case LAST_DOT:",
        "		error = -EINVAL;",
        "		goto exit2;",
        "	case LAST_ROOT:",
        "		error = -EBUSY;",
        "		goto exit2;",
        "	}",
        "",
        "	error = mnt_want_write(path.mnt);",
        "	if (error)",
        "		goto exit2;",
        "",
        "	inode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);",
        "	dentry = lookup_one_qstr_excl(&last, path.dentry, lookup_flags);",
        "	error = PTR_ERR(dentry);",
        "	if (IS_ERR(dentry))",
        "		goto exit3;",
        "	if (!dentry->d_inode) {",
        "		error = -ENOENT;",
        "		goto exit4;",
        "	}",
        "	error = security_path_rmdir(&path, dentry);",
        "	if (error)",
        "		goto exit4;",
        "	error = vfs_rmdir(mnt_idmap(path.mnt), path.dentry->d_inode, dentry);",
        "exit4:",
        "	dput(dentry);",
        "exit3:",
        "	inode_unlock(path.dentry->d_inode);",
        "	mnt_drop_write(path.mnt);",
        "exit2:",
        "	path_put(&path);",
        "	if (retry_estale(error, lookup_flags)) {",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "exit1:",
        "	putname(name);",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE1(rmdir, const char __user *, pathname)",
        "{",
        "	return do_rmdir(AT_FDCWD, getname(pathname));",
        "}",
        "",
        "/**",
        " * vfs_unlink - unlink a filesystem object",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	parent directory",
        " * @dentry:	victim",
        " * @delegated_inode: returns victim inode, if the inode is delegated.",
        " *",
        " * The caller must hold dir->i_mutex.",
        " *",
        " * If vfs_unlink discovers a delegation, it will return -EWOULDBLOCK and",
        " * return a reference to the inode in delegated_inode.  The caller",
        " * should then break the delegation on that inode and retry.  Because",
        " * breaking a delegation may take a long time, the caller should drop",
        " * dir->i_mutex before doing so.",
        " *",
        " * Alternatively, a caller may pass NULL for delegated_inode.  This may",
        " * be appropriate for callers that expect the underlying filesystem not",
        " * to be NFS exported.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_unlink(struct mnt_idmap *idmap, struct inode *dir,",
        "	       struct dentry *dentry, struct inode **delegated_inode)",
        "{",
        "	struct inode *target = dentry->d_inode;",
        "	int error = may_delete(idmap, dir, dentry, 0);",
        "",
        "	if (error)",
        "		return error;",
        "",
        "	if (!dir->i_op->unlink)",
        "		return -EPERM;",
        "",
        "	inode_lock(target);",
        "	if (IS_SWAPFILE(target))",
        "		error = -EPERM;",
        "	else if (is_local_mountpoint(dentry))",
        "		error = -EBUSY;",
        "	else {",
        "		error = security_inode_unlink(dir, dentry);",
        "		if (!error) {",
        "			error = try_break_deleg(target, delegated_inode);",
        "			if (error)",
        "				goto out;",
        "			error = dir->i_op->unlink(dir, dentry);",
        "			if (!error) {",
        "				dont_mount(dentry);",
        "				detach_mounts(dentry);",
        "			}",
        "		}",
        "	}",
        "out:",
        "	inode_unlock(target);",
        "",
        "	/* We don't d_delete() NFS sillyrenamed files--they still exist. */",
        "	if (!error && dentry->d_flags & DCACHE_NFSFS_RENAMED) {",
        "		fsnotify_unlink(dir, dentry);",
        "	} else if (!error) {",
        "		fsnotify_link_count(target);",
        "		d_delete_notify(dir, dentry);",
        "	}",
        "",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_unlink);",
        "",
        "/*",
        " * Make sure that the actual truncation of the file will occur outside its",
        " * directory's i_mutex.  Truncate can take a long time if there is a lot of",
        " * writeout happening, and we don't want to prevent access to the directory",
        " * while waiting on the I/O.",
        " */",
        "int do_unlinkat(int dfd, struct filename *name)",
        "{",
        "	int error;",
        "	struct dentry *dentry;",
        "	struct path path;",
        "	struct qstr last;",
        "	int type;",
        "	struct inode *inode = NULL;",
        "	struct inode *delegated_inode = NULL;",
        "	unsigned int lookup_flags = 0;",
        "retry:",
        "	error = filename_parentat(dfd, name, lookup_flags, &path, &last, &type);",
        "	if (error)",
        "		goto exit1;",
        "",
        "	error = -EISDIR;",
        "	if (type != LAST_NORM)",
        "		goto exit2;",
        "",
        "	error = mnt_want_write(path.mnt);",
        "	if (error)",
        "		goto exit2;",
        "retry_deleg:",
        "	inode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);",
        "	dentry = lookup_one_qstr_excl(&last, path.dentry, lookup_flags);",
        "	error = PTR_ERR(dentry);",
        "	if (!IS_ERR(dentry)) {",
        "",
        "		/* Why not before? Because we want correct error value */",
        "		if (last.name[last.len] || d_is_negative(dentry))",
        "			goto slashes;",
        "		inode = dentry->d_inode;",
        "		ihold(inode);",
        "		error = security_path_unlink(&path, dentry);",
        "		if (error)",
        "			goto exit3;",
        "		error = vfs_unlink(mnt_idmap(path.mnt), path.dentry->d_inode,",
        "				   dentry, &delegated_inode);",
        "exit3:",
        "		dput(dentry);",
        "	}",
        "	inode_unlock(path.dentry->d_inode);",
        "	if (inode)",
        "		iput(inode);	/* truncate the inode here */",
        "	inode = NULL;",
        "	if (delegated_inode) {",
        "		error = break_deleg_wait(&delegated_inode);",
        "		if (!error)",
        "			goto retry_deleg;",
        "	}",
        "	mnt_drop_write(path.mnt);",
        "exit2:",
        "	path_put(&path);",
        "	if (retry_estale(error, lookup_flags)) {",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		inode = NULL;",
        "		goto retry;",
        "	}",
        "exit1:",
        "	putname(name);",
        "	return error;",
        "",
        "slashes:",
        "	if (d_is_negative(dentry))",
        "		error = -ENOENT;",
        "	else if (d_is_dir(dentry))",
        "		error = -EISDIR;",
        "	else",
        "		error = -ENOTDIR;",
        "	goto exit3;",
        "}",
        "",
        "SYSCALL_DEFINE3(unlinkat, int, dfd, const char __user *, pathname, int, flag)",
        "{",
        "	if ((flag & ~AT_REMOVEDIR) != 0)",
        "		return -EINVAL;",
        "",
        "	if (flag & AT_REMOVEDIR)",
        "		return do_rmdir(dfd, getname(pathname));",
        "	return do_unlinkat(dfd, getname(pathname));",
        "}",
        "",
        "SYSCALL_DEFINE1(unlink, const char __user *, pathname)",
        "{",
        "	return do_unlinkat(AT_FDCWD, getname(pathname));",
        "}",
        "",
        "/**",
        " * vfs_symlink - create symlink",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dir:	inode of the parent directory",
        " * @dentry:	dentry of the child symlink file",
        " * @oldname:	name of the file to link to",
        " *",
        " * Create a symlink.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_symlink(struct mnt_idmap *idmap, struct inode *dir,",
        "		struct dentry *dentry, const char *oldname)",
        "{",
        "	int error;",
        "",
        "	error = may_create(idmap, dir, dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	if (!dir->i_op->symlink)",
        "		return -EPERM;",
        "",
        "	error = security_inode_symlink(dir, dentry, oldname);",
        "	if (error)",
        "		return error;",
        "",
        "	error = dir->i_op->symlink(idmap, dir, dentry, oldname);",
        "	if (!error)",
        "		fsnotify_create(dir, dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_symlink);",
        "",
        "int do_symlinkat(struct filename *from, int newdfd, struct filename *to)",
        "{",
        "	int error;",
        "	struct dentry *dentry;",
        "	struct path path;",
        "	unsigned int lookup_flags = 0;",
        "",
        "	if (IS_ERR(from)) {",
        "		error = PTR_ERR(from);",
        "		goto out_putnames;",
        "	}",
        "retry:",
        "	dentry = filename_create(newdfd, to, &path, lookup_flags);",
        "	error = PTR_ERR(dentry);",
        "	if (IS_ERR(dentry))",
        "		goto out_putnames;",
        "",
        "	error = security_path_symlink(&path, dentry, from->name);",
        "	if (!error)",
        "		error = vfs_symlink(mnt_idmap(path.mnt), path.dentry->d_inode,",
        "				    dentry, from->name);",
        "	done_path_create(&path, dentry);",
        "	if (retry_estale(error, lookup_flags)) {",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "out_putnames:",
        "	putname(to);",
        "	putname(from);",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE3(symlinkat, const char __user *, oldname,",
        "		int, newdfd, const char __user *, newname)",
        "{",
        "	return do_symlinkat(getname(oldname), newdfd, getname(newname));",
        "}",
        "",
        "SYSCALL_DEFINE2(symlink, const char __user *, oldname, const char __user *, newname)",
        "{",
        "	return do_symlinkat(getname(oldname), AT_FDCWD, getname(newname));",
        "}",
        "",
        "/**",
        " * vfs_link - create a new link",
        " * @old_dentry:	object to be linked",
        " * @idmap:	idmap of the mount",
        " * @dir:	new parent",
        " * @new_dentry:	where to create the new link",
        " * @delegated_inode: returns inode needing a delegation break",
        " *",
        " * The caller must hold dir->i_mutex",
        " *",
        " * If vfs_link discovers a delegation on the to-be-linked file in need",
        " * of breaking, it will return -EWOULDBLOCK and return a reference to the",
        " * inode in delegated_inode.  The caller should then break the delegation",
        " * and retry.  Because breaking a delegation may take a long time, the",
        " * caller should drop the i_mutex before doing so.",
        " *",
        " * Alternatively, a caller may pass NULL for delegated_inode.  This may",
        " * be appropriate for callers that expect the underlying filesystem not",
        " * to be NFS exported.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then take",
        " * care to map the inode according to @idmap before checking permissions.",
        " * On non-idmapped mounts or if permission checking is to be performed on the",
        " * raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int vfs_link(struct dentry *old_dentry, struct mnt_idmap *idmap,",
        "	     struct inode *dir, struct dentry *new_dentry,",
        "	     struct inode **delegated_inode)",
        "{",
        "	struct inode *inode = old_dentry->d_inode;",
        "	unsigned max_links = dir->i_sb->s_max_links;",
        "	int error;",
        "",
        "	if (!inode)",
        "		return -ENOENT;",
        "",
        "	error = may_create(idmap, dir, new_dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	if (dir->i_sb != inode->i_sb)",
        "		return -EXDEV;",
        "",
        "	/*",
        "	 * A link to an append-only or immutable file cannot be created.",
        "	 */",
        "	if (IS_APPEND(inode) || IS_IMMUTABLE(inode))",
        "		return -EPERM;",
        "	/*",
        "	 * Updating the link count will likely cause i_uid and i_gid to",
        "	 * be writen back improperly if their true value is unknown to",
        "	 * the vfs.",
        "	 */",
        "	if (HAS_UNMAPPED_ID(idmap, inode))",
        "		return -EPERM;",
        "	if (!dir->i_op->link)",
        "		return -EPERM;",
        "	if (S_ISDIR(inode->i_mode))",
        "		return -EPERM;",
        "",
        "	error = security_inode_link(old_dentry, dir, new_dentry);",
        "	if (error)",
        "		return error;",
        "",
        "	inode_lock(inode);",
        "	/* Make sure we don't allow creating hardlink to an unlinked file */",
        "	if (inode->i_nlink == 0 && !(inode->i_state & I_LINKABLE))",
        "		error =  -ENOENT;",
        "	else if (max_links && inode->i_nlink >= max_links)",
        "		error = -EMLINK;",
        "	else {",
        "		error = try_break_deleg(inode, delegated_inode);",
        "		if (!error)",
        "			error = dir->i_op->link(old_dentry, dir, new_dentry);",
        "	}",
        "",
        "	if (!error && (inode->i_state & I_LINKABLE)) {",
        "		spin_lock(&inode->i_lock);",
        "		inode->i_state &= ~I_LINKABLE;",
        "		spin_unlock(&inode->i_lock);",
        "	}",
        "	inode_unlock(inode);",
        "	if (!error)",
        "		fsnotify_link(dir, inode, new_dentry);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_link);",
        "",
        "/*",
        " * Hardlinks are often used in delicate situations.  We avoid",
        " * security-related surprises by not following symlinks on the",
        " * newname.  --KAB",
        " *",
        " * We don't follow them on the oldname either to be compatible",
        " * with linux 2.0, and to avoid hard-linking to directories",
        " * and other special files.  --ADM",
        " */",
        "int do_linkat(int olddfd, struct filename *old, int newdfd,",
        "	      struct filename *new, int flags)",
        "{",
        "	struct mnt_idmap *idmap;",
        "	struct dentry *new_dentry;",
        "	struct path old_path, new_path;",
        "	struct inode *delegated_inode = NULL;",
        "	int how = 0;",
        "	int error;",
        "",
        "	if ((flags & ~(AT_SYMLINK_FOLLOW | AT_EMPTY_PATH)) != 0) {",
        "		error = -EINVAL;",
        "		goto out_putnames;",
        "	}",
        "	/*",
        "	 * To use null names we require CAP_DAC_READ_SEARCH or",
        "	 * that the open-time creds of the dfd matches current.",
        "	 * This ensures that not everyone will be able to create",
        "	 * a hardlink using the passed file descriptor.",
        "	 */",
        "	if (flags & AT_EMPTY_PATH)",
        "		how |= LOOKUP_LINKAT_EMPTY;",
        "",
        "	if (flags & AT_SYMLINK_FOLLOW)",
        "		how |= LOOKUP_FOLLOW;",
        "retry:",
        "	error = filename_lookup(olddfd, old, how, &old_path, NULL);",
        "	if (error)",
        "		goto out_putnames;",
        "",
        "	new_dentry = filename_create(newdfd, new, &new_path,",
        "					(how & LOOKUP_REVAL));",
        "	error = PTR_ERR(new_dentry);",
        "	if (IS_ERR(new_dentry))",
        "		goto out_putpath;",
        "",
        "	error = -EXDEV;",
        "	if (old_path.mnt != new_path.mnt)",
        "		goto out_dput;",
        "	idmap = mnt_idmap(new_path.mnt);",
        "	error = may_linkat(idmap, &old_path);",
        "	if (unlikely(error))",
        "		goto out_dput;",
        "	error = security_path_link(old_path.dentry, &new_path, new_dentry);",
        "	if (error)",
        "		goto out_dput;",
        "	error = vfs_link(old_path.dentry, idmap, new_path.dentry->d_inode,",
        "			 new_dentry, &delegated_inode);",
        "out_dput:",
        "	done_path_create(&new_path, new_dentry);",
        "	if (delegated_inode) {",
        "		error = break_deleg_wait(&delegated_inode);",
        "		if (!error) {",
        "			path_put(&old_path);",
        "			goto retry;",
        "		}",
        "	}",
        "	if (retry_estale(error, how)) {",
        "		path_put(&old_path);",
        "		how |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "out_putpath:",
        "	path_put(&old_path);",
        "out_putnames:",
        "	putname(old);",
        "	putname(new);",
        "",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE5(linkat, int, olddfd, const char __user *, oldname,",
        "		int, newdfd, const char __user *, newname, int, flags)",
        "{",
        "	return do_linkat(olddfd, getname_uflags(oldname, flags),",
        "		newdfd, getname(newname), flags);",
        "}",
        "",
        "SYSCALL_DEFINE2(link, const char __user *, oldname, const char __user *, newname)",
        "{",
        "	return do_linkat(AT_FDCWD, getname(oldname), AT_FDCWD, getname(newname), 0);",
        "}",
        "",
        "/**",
        " * vfs_rename - rename a filesystem object",
        " * @rd:		pointer to &struct renamedata info",
        " *",
        " * The caller must hold multiple mutexes--see lock_rename()).",
        " *",
        " * If vfs_rename discovers a delegation in need of breaking at either",
        " * the source or destination, it will return -EWOULDBLOCK and return a",
        " * reference to the inode in delegated_inode.  The caller should then",
        " * break the delegation and retry.  Because breaking a delegation may",
        " * take a long time, the caller should drop all locks before doing",
        " * so.",
        " *",
        " * Alternatively, a caller may pass NULL for delegated_inode.  This may",
        " * be appropriate for callers that expect the underlying filesystem not",
        " * to be NFS exported.",
        " *",
        " * The worst of all namespace operations - renaming directory. \"Perverted\"",
        " * doesn't even start to describe it. Somebody in UCB had a heck of a trip...",
        " * Problems:",
        " *",
        " *	a) we can get into loop creation.",
        " *	b) race potential - two innocent renames can create a loop together.",
        " *	   That's where 4.4BSD screws up. Current fix: serialization on",
        " *	   sb->s_vfs_rename_mutex. We might be more accurate, but that's another",
        " *	   story.",
        " *	c) we may have to lock up to _four_ objects - parents and victim (if it exists),",
        " *	   and source (if it's a non-directory or a subdirectory that moves to",
        " *	   different parent).",
        " *	   And that - after we got ->i_mutex on parents (until then we don't know",
        " *	   whether the target exists).  Solution: try to be smart with locking",
        " *	   order for inodes.  We rely on the fact that tree topology may change",
        " *	   only under ->s_vfs_rename_mutex _and_ that parent of the object we",
        " *	   move will be locked.  Thus we can rank directories by the tree",
        " *	   (ancestors first) and rank all non-directories after them.",
        " *	   That works since everybody except rename does \"lock parent, lookup,",
        " *	   lock child\" and rename is under ->s_vfs_rename_mutex.",
        " *	   HOWEVER, it relies on the assumption that any object with ->lookup()",
        " *	   has no more than 1 dentry.  If \"hybrid\" objects will ever appear,",
        " *	   we'd better make sure that there's no link(2) for them.",
        " *	d) conversion from fhandle to dentry may come in the wrong moment - when",
        " *	   we are removing the target. Solution: we will have to grab ->i_mutex",
        " *	   in the fhandle_to_dentry code. [FIXME - current nfsfh.c relies on",
        " *	   ->i_mutex on parents, which works but leads to some truly excessive",
        " *	   locking].",
        " */",
        "int vfs_rename(struct renamedata *rd)",
        "{",
        "	int error;",
        "	struct inode *old_dir = rd->old_dir, *new_dir = rd->new_dir;",
        "	struct dentry *old_dentry = rd->old_dentry;",
        "	struct dentry *new_dentry = rd->new_dentry;",
        "	struct inode **delegated_inode = rd->delegated_inode;",
        "	unsigned int flags = rd->flags;",
        "	bool is_dir = d_is_dir(old_dentry);",
        "	struct inode *source = old_dentry->d_inode;",
        "	struct inode *target = new_dentry->d_inode;",
        "	bool new_is_dir = false;",
        "	unsigned max_links = new_dir->i_sb->s_max_links;",
        "	struct name_snapshot old_name;",
        "	bool lock_old_subdir, lock_new_subdir;",
        "",
        "	if (source == target)",
        "		return 0;",
        "",
        "	error = may_delete(rd->old_mnt_idmap, old_dir, old_dentry, is_dir);",
        "	if (error)",
        "		return error;",
        "",
        "	if (!target) {",
        "		error = may_create(rd->new_mnt_idmap, new_dir, new_dentry);",
        "	} else {",
        "		new_is_dir = d_is_dir(new_dentry);",
        "",
        "		if (!(flags & RENAME_EXCHANGE))",
        "			error = may_delete(rd->new_mnt_idmap, new_dir,",
        "					   new_dentry, is_dir);",
        "		else",
        "			error = may_delete(rd->new_mnt_idmap, new_dir,",
        "					   new_dentry, new_is_dir);",
        "	}",
        "	if (error)",
        "		return error;",
        "",
        "	if (!old_dir->i_op->rename)",
        "		return -EPERM;",
        "",
        "	/*",
        "	 * If we are going to change the parent - check write permissions,",
        "	 * we'll need to flip '..'.",
        "	 */",
        "	if (new_dir != old_dir) {",
        "		if (is_dir) {",
        "			error = inode_permission(rd->old_mnt_idmap, source,",
        "						 MAY_WRITE);",
        "			if (error)",
        "				return error;",
        "		}",
        "		if ((flags & RENAME_EXCHANGE) && new_is_dir) {",
        "			error = inode_permission(rd->new_mnt_idmap, target,",
        "						 MAY_WRITE);",
        "			if (error)",
        "				return error;",
        "		}",
        "	}",
        "",
        "	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,",
        "				      flags);",
        "	if (error)",
        "		return error;",
        "",
        "	take_dentry_name_snapshot(&old_name, old_dentry);",
        "	dget(new_dentry);",
        "	/*",
        "	 * Lock children.",
        "	 * The source subdirectory needs to be locked on cross-directory",
        "	 * rename or cross-directory exchange since its parent changes.",
        "	 * The target subdirectory needs to be locked on cross-directory",
        "	 * exchange due to parent change and on any rename due to becoming",
        "	 * a victim.",
        "	 * Non-directories need locking in all cases (for NFS reasons);",
        "	 * they get locked after any subdirectories (in inode address order).",
        "	 *",
        "	 * NOTE: WE ONLY LOCK UNRELATED DIRECTORIES IN CROSS-DIRECTORY CASE.",
        "	 * NEVER, EVER DO THAT WITHOUT ->s_vfs_rename_mutex.",
        "	 */",
        "	lock_old_subdir = new_dir != old_dir;",
        "	lock_new_subdir = new_dir != old_dir || !(flags & RENAME_EXCHANGE);",
        "	if (is_dir) {",
        "		if (lock_old_subdir)",
        "			inode_lock_nested(source, I_MUTEX_CHILD);",
        "		if (target && (!new_is_dir || lock_new_subdir))",
        "			inode_lock(target);",
        "	} else if (new_is_dir) {",
        "		if (lock_new_subdir)",
        "			inode_lock_nested(target, I_MUTEX_CHILD);",
        "		inode_lock(source);",
        "	} else {",
        "		lock_two_nondirectories(source, target);",
        "	}",
        "",
        "	error = -EPERM;",
        "	if (IS_SWAPFILE(source) || (target && IS_SWAPFILE(target)))",
        "		goto out;",
        "",
        "	error = -EBUSY;",
        "	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))",
        "		goto out;",
        "",
        "	if (max_links && new_dir != old_dir) {",
        "		error = -EMLINK;",
        "		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)",
        "			goto out;",
        "		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&",
        "		    old_dir->i_nlink >= max_links)",
        "			goto out;",
        "	}",
        "	if (!is_dir) {",
        "		error = try_break_deleg(source, delegated_inode);",
        "		if (error)",
        "			goto out;",
        "	}",
        "	if (target && !new_is_dir) {",
        "		error = try_break_deleg(target, delegated_inode);",
        "		if (error)",
        "			goto out;",
        "	}",
        "	error = old_dir->i_op->rename(rd->new_mnt_idmap, old_dir, old_dentry,",
        "				      new_dir, new_dentry, flags);",
        "	if (error)",
        "		goto out;",
        "",
        "	if (!(flags & RENAME_EXCHANGE) && target) {",
        "		if (is_dir) {",
        "			shrink_dcache_parent(new_dentry);",
        "			target->i_flags |= S_DEAD;",
        "		}",
        "		dont_mount(new_dentry);",
        "		detach_mounts(new_dentry);",
        "	}",
        "	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {",
        "		if (!(flags & RENAME_EXCHANGE))",
        "			d_move(old_dentry, new_dentry);",
        "		else",
        "			d_exchange(old_dentry, new_dentry);",
        "	}",
        "out:",
        "	if (!is_dir || lock_old_subdir)",
        "		inode_unlock(source);",
        "	if (target && (!new_is_dir || lock_new_subdir))",
        "		inode_unlock(target);",
        "	dput(new_dentry);",
        "	if (!error) {",
        "		fsnotify_move(old_dir, new_dir, &old_name.name, is_dir,",
        "			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);",
        "		if (flags & RENAME_EXCHANGE) {",
        "			fsnotify_move(new_dir, old_dir, &old_dentry->d_name,",
        "				      new_is_dir, NULL, new_dentry);",
        "		}",
        "	}",
        "	release_dentry_name_snapshot(&old_name);",
        "",
        "	return error;",
        "}",
        "EXPORT_SYMBOL(vfs_rename);",
        "",
        "int do_renameat2(int olddfd, struct filename *from, int newdfd,",
        "		 struct filename *to, unsigned int flags)",
        "{",
        "	struct renamedata rd;",
        "	struct dentry *old_dentry, *new_dentry;",
        "	struct dentry *trap;",
        "	struct path old_path, new_path;",
        "	struct qstr old_last, new_last;",
        "	int old_type, new_type;",
        "	struct inode *delegated_inode = NULL;",
        "	unsigned int lookup_flags = 0, target_flags = LOOKUP_RENAME_TARGET;",
        "	bool should_retry = false;",
        "	int error = -EINVAL;",
        "",
        "	if (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))",
        "		goto put_names;",
        "",
        "	if ((flags & (RENAME_NOREPLACE | RENAME_WHITEOUT)) &&",
        "	    (flags & RENAME_EXCHANGE))",
        "		goto put_names;",
        "",
        "	if (flags & RENAME_EXCHANGE)",
        "		target_flags = 0;",
        "",
        "retry:",
        "	error = filename_parentat(olddfd, from, lookup_flags, &old_path,",
        "				  &old_last, &old_type);",
        "	if (error)",
        "		goto put_names;",
        "",
        "	error = filename_parentat(newdfd, to, lookup_flags, &new_path, &new_last,",
        "				  &new_type);",
        "	if (error)",
        "		goto exit1;",
        "",
        "	error = -EXDEV;",
        "	if (old_path.mnt != new_path.mnt)",
        "		goto exit2;",
        "",
        "	error = -EBUSY;",
        "	if (old_type != LAST_NORM)",
        "		goto exit2;",
        "",
        "	if (flags & RENAME_NOREPLACE)",
        "		error = -EEXIST;",
        "	if (new_type != LAST_NORM)",
        "		goto exit2;",
        "",
        "	error = mnt_want_write(old_path.mnt);",
        "	if (error)",
        "		goto exit2;",
        "",
        "retry_deleg:",
        "	trap = lock_rename(new_path.dentry, old_path.dentry);",
        "	if (IS_ERR(trap)) {",
        "		error = PTR_ERR(trap);",
        "		goto exit_lock_rename;",
        "	}",
        "",
        "	old_dentry = lookup_one_qstr_excl(&old_last, old_path.dentry,",
        "					  lookup_flags);",
        "	error = PTR_ERR(old_dentry);",
        "	if (IS_ERR(old_dentry))",
        "		goto exit3;",
        "	/* source must exist */",
        "	error = -ENOENT;",
        "	if (d_is_negative(old_dentry))",
        "		goto exit4;",
        "	new_dentry = lookup_one_qstr_excl(&new_last, new_path.dentry,",
        "					  lookup_flags | target_flags);",
        "	error = PTR_ERR(new_dentry);",
        "	if (IS_ERR(new_dentry))",
        "		goto exit4;",
        "	error = -EEXIST;",
        "	if ((flags & RENAME_NOREPLACE) && d_is_positive(new_dentry))",
        "		goto exit5;",
        "	if (flags & RENAME_EXCHANGE) {",
        "		error = -ENOENT;",
        "		if (d_is_negative(new_dentry))",
        "			goto exit5;",
        "",
        "		if (!d_is_dir(new_dentry)) {",
        "			error = -ENOTDIR;",
        "			if (new_last.name[new_last.len])",
        "				goto exit5;",
        "		}",
        "	}",
        "	/* unless the source is a directory trailing slashes give -ENOTDIR */",
        "	if (!d_is_dir(old_dentry)) {",
        "		error = -ENOTDIR;",
        "		if (old_last.name[old_last.len])",
        "			goto exit5;",
        "		if (!(flags & RENAME_EXCHANGE) && new_last.name[new_last.len])",
        "			goto exit5;",
        "	}",
        "	/* source should not be ancestor of target */",
        "	error = -EINVAL;",
        "	if (old_dentry == trap)",
        "		goto exit5;",
        "	/* target should not be an ancestor of source */",
        "	if (!(flags & RENAME_EXCHANGE))",
        "		error = -ENOTEMPTY;",
        "	if (new_dentry == trap)",
        "		goto exit5;",
        "",
        "	error = security_path_rename(&old_path, old_dentry,",
        "				     &new_path, new_dentry, flags);",
        "	if (error)",
        "		goto exit5;",
        "",
        "	rd.old_dir	   = old_path.dentry->d_inode;",
        "	rd.old_dentry	   = old_dentry;",
        "	rd.old_mnt_idmap   = mnt_idmap(old_path.mnt);",
        "	rd.new_dir	   = new_path.dentry->d_inode;",
        "	rd.new_dentry	   = new_dentry;",
        "	rd.new_mnt_idmap   = mnt_idmap(new_path.mnt);",
        "	rd.delegated_inode = &delegated_inode;",
        "	rd.flags	   = flags;",
        "	error = vfs_rename(&rd);",
        "exit5:",
        "	dput(new_dentry);",
        "exit4:",
        "	dput(old_dentry);",
        "exit3:",
        "	unlock_rename(new_path.dentry, old_path.dentry);",
        "exit_lock_rename:",
        "	if (delegated_inode) {",
        "		error = break_deleg_wait(&delegated_inode);",
        "		if (!error)",
        "			goto retry_deleg;",
        "	}",
        "	mnt_drop_write(old_path.mnt);",
        "exit2:",
        "	if (retry_estale(error, lookup_flags))",
        "		should_retry = true;",
        "	path_put(&new_path);",
        "exit1:",
        "	path_put(&old_path);",
        "	if (should_retry) {",
        "		should_retry = false;",
        "		lookup_flags |= LOOKUP_REVAL;",
        "		goto retry;",
        "	}",
        "put_names:",
        "	putname(from);",
        "	putname(to);",
        "	return error;",
        "}",
        "",
        "SYSCALL_DEFINE5(renameat2, int, olddfd, const char __user *, oldname,",
        "		int, newdfd, const char __user *, newname, unsigned int, flags)",
        "{",
        "	return do_renameat2(olddfd, getname(oldname), newdfd, getname(newname),",
        "				flags);",
        "}",
        "",
        "SYSCALL_DEFINE4(renameat, int, olddfd, const char __user *, oldname,",
        "		int, newdfd, const char __user *, newname)",
        "{",
        "	return do_renameat2(olddfd, getname(oldname), newdfd, getname(newname),",
        "				0);",
        "}",
        "",
        "SYSCALL_DEFINE2(rename, const char __user *, oldname, const char __user *, newname)",
        "{",
        "	return do_renameat2(AT_FDCWD, getname(oldname), AT_FDCWD,",
        "				getname(newname), 0);",
        "}",
        "",
        "int readlink_copy(char __user *buffer, int buflen, const char *link)",
        "{",
        "	int len = PTR_ERR(link);",
        "	if (IS_ERR(link))",
        "		goto out;",
        "",
        "	len = strlen(link);",
        "	if (len > (unsigned) buflen)",
        "		len = buflen;",
        "	if (copy_to_user(buffer, link, len))",
        "		len = -EFAULT;",
        "out:",
        "	return len;",
        "}",
        "",
        "/**",
        " * vfs_readlink - copy symlink body into userspace buffer",
        " * @dentry: dentry on which to get symbolic link",
        " * @buffer: user memory pointer",
        " * @buflen: size of buffer",
        " *",
        " * Does not touch atime.  That's up to the caller if necessary",
        " *",
        " * Does not call security hook.",
        " */",
        "int vfs_readlink(struct dentry *dentry, char __user *buffer, int buflen)",
        "{",
        "	struct inode *inode = d_inode(dentry);",
        "	DEFINE_DELAYED_CALL(done);",
        "	const char *link;",
        "	int res;",
        "",
        "	if (unlikely(!(inode->i_opflags & IOP_DEFAULT_READLINK))) {",
        "		if (unlikely(inode->i_op->readlink))",
        "			return inode->i_op->readlink(dentry, buffer, buflen);",
        "",
        "		if (!d_is_symlink(dentry))",
        "			return -EINVAL;",
        "",
        "		spin_lock(&inode->i_lock);",
        "		inode->i_opflags |= IOP_DEFAULT_READLINK;",
        "		spin_unlock(&inode->i_lock);",
        "	}",
        "",
        "	link = READ_ONCE(inode->i_link);",
        "	if (!link) {",
        "		link = inode->i_op->get_link(dentry, inode, &done);",
        "		if (IS_ERR(link))",
        "			return PTR_ERR(link);",
        "	}",
        "	res = readlink_copy(buffer, buflen, link);",
        "	do_delayed_call(&done);",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(vfs_readlink);",
        "",
        "/**",
        " * vfs_get_link - get symlink body",
        " * @dentry: dentry on which to get symbolic link",
        " * @done: caller needs to free returned data with this",
        " *",
        " * Calls security hook and i_op->get_link() on the supplied inode.",
        " *",
        " * It does not touch atime.  That's up to the caller if necessary.",
        " *",
        " * Does not work on \"special\" symlinks like /proc/$$/fd/N",
        " */",
        "const char *vfs_get_link(struct dentry *dentry, struct delayed_call *done)",
        "{",
        "	const char *res = ERR_PTR(-EINVAL);",
        "	struct inode *inode = d_inode(dentry);",
        "",
        "	if (d_is_symlink(dentry)) {",
        "		res = ERR_PTR(security_inode_readlink(dentry));",
        "		if (!res)",
        "			res = inode->i_op->get_link(dentry, inode, done);",
        "	}",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(vfs_get_link);",
        "",
        "/* get the link contents into pagecache */",
        "static char *__page_get_link(struct dentry *dentry, struct inode *inode,",
        "			     struct delayed_call *callback)",
        "{",
        "	struct page *page;",
        "	struct address_space *mapping = inode->i_mapping;",
        "",
        "	if (!dentry) {",
        "		page = find_get_page(mapping, 0);",
        "		if (!page)",
        "			return ERR_PTR(-ECHILD);",
        "		if (!PageUptodate(page)) {",
        "			put_page(page);",
        "			return ERR_PTR(-ECHILD);",
        "		}",
        "	} else {",
        "		page = read_mapping_page(mapping, 0, NULL);",
        "		if (IS_ERR(page))",
        "			return (char*)page;",
        "	}",
        "	set_delayed_call(callback, page_put_link, page);",
        "	BUG_ON(mapping_gfp_mask(mapping) & __GFP_HIGHMEM);",
        "	return page_address(page);",
        "}",
        "",
        "const char *page_get_link_raw(struct dentry *dentry, struct inode *inode,",
        "			      struct delayed_call *callback)",
        "{",
        "	return __page_get_link(dentry, inode, callback);",
        "}",
        "EXPORT_SYMBOL_GPL(page_get_link_raw);",
        "",
        "const char *page_get_link(struct dentry *dentry, struct inode *inode,",
        "					struct delayed_call *callback)",
        "{",
        "	char *kaddr = __page_get_link(dentry, inode, callback);",
        "",
        "	if (!IS_ERR(kaddr))",
        "		nd_terminate_link(kaddr, inode->i_size, PAGE_SIZE - 1);",
        "	return kaddr;",
        "}",
        "",
        "EXPORT_SYMBOL(page_get_link);",
        "",
        "void page_put_link(void *arg)",
        "{",
        "	put_page(arg);",
        "}",
        "EXPORT_SYMBOL(page_put_link);",
        "",
        "int page_readlink(struct dentry *dentry, char __user *buffer, int buflen)",
        "{",
        "	DEFINE_DELAYED_CALL(done);",
        "	int res = readlink_copy(buffer, buflen,",
        "				page_get_link(dentry, d_inode(dentry),",
        "					      &done));",
        "	do_delayed_call(&done);",
        "	return res;",
        "}",
        "EXPORT_SYMBOL(page_readlink);",
        "",
        "int page_symlink(struct inode *inode, const char *symname, int len)",
        "{",
        "	struct address_space *mapping = inode->i_mapping;",
        "	const struct address_space_operations *aops = mapping->a_ops;",
        "	bool nofs = !mapping_gfp_constraint(mapping, __GFP_FS);",
        "	struct folio *folio;",
        "	void *fsdata = NULL;",
        "	int err;",
        "	unsigned int flags;",
        "",
        "retry:",
        "	if (nofs)",
        "		flags = memalloc_nofs_save();",
        "	err = aops->write_begin(NULL, mapping, 0, len-1, &folio, &fsdata);",
        "	if (nofs)",
        "		memalloc_nofs_restore(flags);",
        "	if (err)",
        "		goto fail;",
        "",
        "	memcpy(folio_address(folio), symname, len - 1);",
        "",
        "	err = aops->write_end(NULL, mapping, 0, len - 1, len - 1,",
        "						folio, fsdata);",
        "	if (err < 0)",
        "		goto fail;",
        "	if (err < len-1)",
        "		goto retry;",
        "",
        "	mark_inode_dirty(inode);",
        "	return 0;",
        "fail:",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(page_symlink);",
        "",
        "const struct inode_operations page_symlink_inode_operations = {",
        "	.get_link	= page_get_link,",
        "};",
        "EXPORT_SYMBOL(page_symlink_inode_operations);"
    ]
  },
  "arch_x86_include_asm_desc_h": {
    path: "arch/x86/include/asm/desc.h",
    covered: [331, 329],
    totalLines: 450,
    coveredCount: 2,
    coveragePct: 0.4,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _ASM_X86_DESC_H",
        "#define _ASM_X86_DESC_H",
        "",
        "#include <asm/desc_defs.h>",
        "#include <asm/ldt.h>",
        "#include <asm/mmu.h>",
        "#include <asm/fixmap.h>",
        "#include <asm/irq_vectors.h>",
        "#include <asm/cpu_entry_area.h>",
        "",
        "#include <linux/debug_locks.h>",
        "#include <linux/smp.h>",
        "#include <linux/percpu.h>",
        "",
        "static inline void fill_ldt(struct desc_struct *desc, const struct user_desc *info)",
        "{",
        "	desc->limit0		= info->limit & 0x0ffff;",
        "",
        "	desc->base0		= (info->base_addr & 0x0000ffff);",
        "	desc->base1		= (info->base_addr & 0x00ff0000) >> 16;",
        "",
        "	desc->type		= (info->read_exec_only ^ 1) << 1;",
        "	desc->type	       |= info->contents << 2;",
        "	/* Set the ACCESS bit so it can be mapped RO */",
        "	desc->type	       |= 1;",
        "",
        "	desc->s			= 1;",
        "	desc->dpl		= 0x3;",
        "	desc->p			= info->seg_not_present ^ 1;",
        "	desc->limit1		= (info->limit & 0xf0000) >> 16;",
        "	desc->avl		= info->useable;",
        "	desc->d			= info->seg_32bit;",
        "	desc->g			= info->limit_in_pages;",
        "",
        "	desc->base2		= (info->base_addr & 0xff000000) >> 24;",
        "	/*",
        "	 * Don't allow setting of the lm bit. It would confuse",
        "	 * user_64bit_mode and would get overridden by sysret anyway.",
        "	 */",
        "	desc->l			= 0;",
        "}",
        "",
        "struct gdt_page {",
        "	struct desc_struct gdt[GDT_ENTRIES];",
        "} __attribute__((aligned(PAGE_SIZE)));",
        "",
        "DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);",
        "DECLARE_INIT_PER_CPU(gdt_page);",
        "",
        "/* Provide the original GDT */",
        "static inline struct desc_struct *get_cpu_gdt_rw(unsigned int cpu)",
        "{",
        "	return per_cpu(gdt_page, cpu).gdt;",
        "}",
        "",
        "/* Provide the current original GDT */",
        "static inline struct desc_struct *get_current_gdt_rw(void)",
        "{",
        "	return this_cpu_ptr(&gdt_page)->gdt;",
        "}",
        "",
        "/* Provide the fixmap address of the remapped GDT */",
        "static inline struct desc_struct *get_cpu_gdt_ro(int cpu)",
        "{",
        "	return (struct desc_struct *)&get_cpu_entry_area(cpu)->gdt;",
        "}",
        "",
        "/* Provide the current read-only GDT */",
        "static inline struct desc_struct *get_current_gdt_ro(void)",
        "{",
        "	return get_cpu_gdt_ro(smp_processor_id());",
        "}",
        "",
        "/* Provide the physical address of the GDT page. */",
        "static inline phys_addr_t get_cpu_gdt_paddr(unsigned int cpu)",
        "{",
        "	return per_cpu_ptr_to_phys(get_cpu_gdt_rw(cpu));",
        "}",
        "",
        "static inline void pack_gate(gate_desc *gate, unsigned type, unsigned long func,",
        "			     unsigned dpl, unsigned ist, unsigned seg)",
        "{",
        "	gate->offset_low	= (u16) func;",
        "	gate->bits.p		= 1;",
        "	gate->bits.dpl		= dpl;",
        "	gate->bits.zero		= 0;",
        "	gate->bits.type		= type;",
        "	gate->offset_middle	= (u16) (func >> 16);",
        "#ifdef CONFIG_X86_64",
        "	gate->segment		= __KERNEL_CS;",
        "	gate->bits.ist		= ist;",
        "	gate->reserved		= 0;",
        "	gate->offset_high	= (u32) (func >> 32);",
        "#else",
        "	gate->segment		= seg;",
        "	gate->bits.ist		= 0;",
        "#endif",
        "}",
        "",
        "static inline int desc_empty(const void *ptr)",
        "{",
        "	const u32 *desc = ptr;",
        "",
        "	return !(desc[0] | desc[1]);",
        "}",
        "",
        "#ifdef CONFIG_PARAVIRT_XXL",
        "#include <asm/paravirt.h>",
        "#else",
        "#define load_TR_desc()				native_load_tr_desc()",
        "#define load_gdt(dtr)				native_load_gdt(dtr)",
        "#define load_idt(dtr)				native_load_idt(dtr)",
        "#define load_tr(tr)				asm volatile(\"ltr %0\"::\"m\" (tr))",
        "#define load_ldt(ldt)				asm volatile(\"lldt %0\"::\"m\" (ldt))",
        "",
        "#define store_gdt(dtr)				native_store_gdt(dtr)",
        "#define store_tr(tr)				(tr = native_store_tr())",
        "",
        "#define load_TLS(t, cpu)			native_load_tls(t, cpu)",
        "#define set_ldt					native_set_ldt",
        "",
        "#define write_ldt_entry(dt, entry, desc)	native_write_ldt_entry(dt, entry, desc)",
        "#define write_gdt_entry(dt, entry, desc, type)	native_write_gdt_entry(dt, entry, desc, type)",
        "#define write_idt_entry(dt, entry, g)		native_write_idt_entry(dt, entry, g)",
        "",
        "static inline void paravirt_alloc_ldt(struct desc_struct *ldt, unsigned entries)",
        "{",
        "}",
        "",
        "static inline void paravirt_free_ldt(struct desc_struct *ldt, unsigned entries)",
        "{",
        "}",
        "#endif	/* CONFIG_PARAVIRT_XXL */",
        "",
        "#define store_ldt(ldt) asm(\"sldt %0\" : \"=m\"(ldt))",
        "",
        "static inline void native_write_idt_entry(gate_desc *idt, int entry, const gate_desc *gate)",
        "{",
        "	memcpy(&idt[entry], gate, sizeof(*gate));",
        "}",
        "",
        "static inline void native_write_ldt_entry(struct desc_struct *ldt, int entry, const void *desc)",
        "{",
        "	memcpy(&ldt[entry], desc, 8);",
        "}",
        "",
        "static inline void",
        "native_write_gdt_entry(struct desc_struct *gdt, int entry, const void *desc, int type)",
        "{",
        "	unsigned int size;",
        "",
        "	switch (type) {",
        "	case DESC_TSS:	size = sizeof(tss_desc);	break;",
        "	case DESC_LDT:	size = sizeof(ldt_desc);	break;",
        "	default:	size = sizeof(*gdt);		break;",
        "	}",
        "",
        "	memcpy(&gdt[entry], desc, size);",
        "}",
        "",
        "static inline void set_tssldt_descriptor(void *d, unsigned long addr,",
        "					 unsigned type, unsigned size)",
        "{",
        "	struct ldttss_desc *desc = d;",
        "",
        "	memset(desc, 0, sizeof(*desc));",
        "",
        "	desc->limit0		= (u16) size;",
        "	desc->base0		= (u16) addr;",
        "	desc->base1		= (addr >> 16) & 0xFF;",
        "	desc->type		= type;",
        "	desc->p			= 1;",
        "	desc->limit1		= (size >> 16) & 0xF;",
        "	desc->base2		= (addr >> 24) & 0xFF;",
        "#ifdef CONFIG_X86_64",
        "	desc->base3		= (u32) (addr >> 32);",
        "#endif",
        "}",
        "",
        "static inline void __set_tss_desc(unsigned cpu, unsigned int entry, struct x86_hw_tss *addr)",
        "{",
        "	struct desc_struct *d = get_cpu_gdt_rw(cpu);",
        "	tss_desc tss;",
        "",
        "	set_tssldt_descriptor(&tss, (unsigned long)addr, DESC_TSS,",
        "			      __KERNEL_TSS_LIMIT);",
        "	write_gdt_entry(d, entry, &tss, DESC_TSS);",
        "}",
        "",
        "#define set_tss_desc(cpu, addr) __set_tss_desc(cpu, GDT_ENTRY_TSS, addr)",
        "",
        "static inline void native_set_ldt(const void *addr, unsigned int entries)",
        "{",
        "	if (likely(entries == 0))",
        "		asm volatile(\"lldt %w0\"::\"q\" (0));",
        "	else {",
        "		unsigned cpu = smp_processor_id();",
        "		ldt_desc ldt;",
        "",
        "		set_tssldt_descriptor(&ldt, (unsigned long)addr, DESC_LDT,",
        "				      entries * LDT_ENTRY_SIZE - 1);",
        "		write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_LDT,",
        "				&ldt, DESC_LDT);",
        "		asm volatile(\"lldt %w0\"::\"q\" (GDT_ENTRY_LDT*8));",
        "	}",
        "}",
        "",
        "static inline void native_load_gdt(const struct desc_ptr *dtr)",
        "{",
        "	asm volatile(\"lgdt %0\"::\"m\" (*dtr));",
        "}",
        "",
        "static __always_inline void native_load_idt(const struct desc_ptr *dtr)",
        "{",
        "	asm volatile(\"lidt %0\"::\"m\" (*dtr));",
        "}",
        "",
        "static inline void native_store_gdt(struct desc_ptr *dtr)",
        "{",
        "	asm volatile(\"sgdt %0\":\"=m\" (*dtr));",
        "}",
        "",
        "static inline void store_idt(struct desc_ptr *dtr)",
        "{",
        "	asm volatile(\"sidt %0\":\"=m\" (*dtr));",
        "}",
        "",
        "static inline void native_gdt_invalidate(void)",
        "{",
        "	const struct desc_ptr invalid_gdt = {",
        "		.address = 0,",
        "		.size = 0",
        "	};",
        "",
        "	native_load_gdt(&invalid_gdt);",
        "}",
        "",
        "static inline void native_idt_invalidate(void)",
        "{",
        "	const struct desc_ptr invalid_idt = {",
        "		.address = 0,",
        "		.size = 0",
        "	};",
        "",
        "	native_load_idt(&invalid_idt);",
        "}",
        "",
        "/*",
        " * The LTR instruction marks the TSS GDT entry as busy. On 64-bit, the GDT is",
        " * a read-only remapping. To prevent a page fault, the GDT is switched to the",
        " * original writeable version when needed.",
        " */",
        "#ifdef CONFIG_X86_64",
        "static inline void native_load_tr_desc(void)",
        "{",
        "	struct desc_ptr gdt;",
        "	int cpu = raw_smp_processor_id();",
        "	bool restore = 0;",
        "	struct desc_struct *fixmap_gdt;",
        "",
        "	native_store_gdt(&gdt);",
        "	fixmap_gdt = get_cpu_gdt_ro(cpu);",
        "",
        "	/*",
        "	 * If the current GDT is the read-only fixmap, swap to the original",
        "	 * writeable version. Swap back at the end.",
        "	 */",
        "	if (gdt.address == (unsigned long)fixmap_gdt) {",
        "		load_direct_gdt(cpu);",
        "		restore = 1;",
        "	}",
        "	asm volatile(\"ltr %w0\"::\"q\" (GDT_ENTRY_TSS*8));",
        "	if (restore)",
        "		load_fixmap_gdt(cpu);",
        "}",
        "#else",
        "static inline void native_load_tr_desc(void)",
        "{",
        "	asm volatile(\"ltr %w0\"::\"q\" (GDT_ENTRY_TSS*8));",
        "}",
        "#endif",
        "",
        "static inline unsigned long native_store_tr(void)",
        "{",
        "	unsigned long tr;",
        "",
        "	asm volatile(\"str %0\":\"=r\" (tr));",
        "",
        "	return tr;",
        "}",
        "",
        "static inline void native_load_tls(struct thread_struct *t, unsigned int cpu)",
        "{",
        "	struct desc_struct *gdt = get_cpu_gdt_rw(cpu);",
        "	unsigned int i;",
        "",
        "	for (i = 0; i < GDT_ENTRY_TLS_ENTRIES; i++)",
        "		gdt[GDT_ENTRY_TLS_MIN + i] = t->tls_array[i];",
        "}",
        "",
        "DECLARE_PER_CPU(bool, __tss_limit_invalid);",
        "",
        "static inline void force_reload_TR(void)",
        "{",
        "	struct desc_struct *d = get_current_gdt_rw();",
        "	tss_desc tss;",
        "",
        "	memcpy(&tss, &d[GDT_ENTRY_TSS], sizeof(tss_desc));",
        "",
        "	/*",
        "	 * LTR requires an available TSS, and the TSS is currently",
        "	 * busy.  Make it be available so that LTR will work.",
        "	 */",
        "	tss.type = DESC_TSS;",
        "	write_gdt_entry(d, GDT_ENTRY_TSS, &tss, DESC_TSS);",
        "",
        "	load_TR_desc();",
        "	this_cpu_write(__tss_limit_invalid, false);",
        "}",
        "",
        "/*",
        " * Call this if you need the TSS limit to be correct, which should be the case",
        " * if and only if you have TIF_IO_BITMAP set or you're switching to a task",
        " * with TIF_IO_BITMAP set.",
        " */",
        "static inline void refresh_tss_limit(void)",
        "{",
        "	DEBUG_LOCKS_WARN_ON(preemptible());",
        "",
        "	if (unlikely(this_cpu_read(__tss_limit_invalid)))",
        "		force_reload_TR();",
        "}",
        "",
        "/*",
        " * If you do something evil that corrupts the cached TSS limit (I'm looking",
        " * at you, VMX exits), call this function.",
        " *",
        " * The optimization here is that the TSS limit only matters for Linux if the",
        " * IO bitmap is in use.  If the TSS limit gets forced to its minimum value,",
        " * everything works except that IO bitmap will be ignored and all CPL 3 IO",
        " * instructions will #GP, which is exactly what we want for normal tasks.",
        " */",
        "static inline void invalidate_tss_limit(void)",
        "{",
        "	DEBUG_LOCKS_WARN_ON(preemptible());",
        "",
        "	if (unlikely(test_thread_flag(TIF_IO_BITMAP)))",
        "		force_reload_TR();",
        "	else",
        "		this_cpu_write(__tss_limit_invalid, true);",
        "}",
        "",
        "/* This intentionally ignores lm, since 32-bit apps don't have that field. */",
        "#define LDT_empty(info)					\\",
        "	((info)->base_addr		== 0	&&	\\",
        "	 (info)->limit			== 0	&&	\\",
        "	 (info)->contents		== 0	&&	\\",
        "	 (info)->read_exec_only		== 1	&&	\\",
        "	 (info)->seg_32bit		== 0	&&	\\",
        "	 (info)->limit_in_pages		== 0	&&	\\",
        "	 (info)->seg_not_present	== 1	&&	\\",
        "	 (info)->useable		== 0)",
        "",
        "/* Lots of programs expect an all-zero user_desc to mean \"no segment at all\". */",
        "static inline bool LDT_zero(const struct user_desc *info)",
        "{",
        "	return (info->base_addr		== 0 &&",
        "		info->limit		== 0 &&",
        "		info->contents		== 0 &&",
        "		info->read_exec_only	== 0 &&",
        "		info->seg_32bit		== 0 &&",
        "		info->limit_in_pages	== 0 &&",
        "		info->seg_not_present	== 0 &&",
        "		info->useable		== 0);",
        "}",
        "",
        "static inline void clear_LDT(void)",
        "{",
        "	set_ldt(NULL, 0);",
        "}",
        "",
        "static inline unsigned long get_desc_base(const struct desc_struct *desc)",
        "{",
        "	return (unsigned)(desc->base0 | ((desc->base1) << 16) | ((desc->base2) << 24));",
        "}",
        "",
        "static inline void set_desc_base(struct desc_struct *desc, unsigned long base)",
        "{",
        "	desc->base0 = base & 0xffff;",
        "	desc->base1 = (base >> 16) & 0xff;",
        "	desc->base2 = (base >> 24) & 0xff;",
        "}",
        "",
        "static inline unsigned long get_desc_limit(const struct desc_struct *desc)",
        "{",
        "	return desc->limit0 | (desc->limit1 << 16);",
        "}",
        "",
        "static inline void set_desc_limit(struct desc_struct *desc, unsigned long limit)",
        "{",
        "	desc->limit0 = limit & 0xffff;",
        "	desc->limit1 = (limit >> 16) & 0xf;",
        "}",
        "",
        "static inline void init_idt_data(struct idt_data *data, unsigned int n,",
        "				 const void *addr)",
        "{",
        "	BUG_ON(n > 0xFF);",
        "",
        "	memset(data, 0, sizeof(*data));",
        "	data->vector	= n;",
        "	data->addr	= addr;",
        "	data->segment	= __KERNEL_CS;",
        "	data->bits.type	= GATE_INTERRUPT;",
        "	data->bits.p	= 1;",
        "}",
        "",
        "static inline void idt_init_desc(gate_desc *gate, const struct idt_data *d)",
        "{",
        "	unsigned long addr = (unsigned long) d->addr;",
        "",
        "	gate->offset_low	= (u16) addr;",
        "	gate->segment		= (u16) d->segment;",
        "	gate->bits		= d->bits;",
        "	gate->offset_middle	= (u16) (addr >> 16);",
        "#ifdef CONFIG_X86_64",
        "	gate->offset_high	= (u32) (addr >> 32);",
        "	gate->reserved		= 0;",
        "#endif",
        "}",
        "",
        "extern unsigned long system_vectors[];",
        "",
        "extern void load_current_idt(void);",
        "extern void idt_setup_early_handler(void);",
        "extern void idt_setup_early_traps(void);",
        "extern void idt_setup_traps(void);",
        "extern void idt_setup_apic_and_irq_gates(void);",
        "extern bool idt_is_f00f_address(unsigned long address);",
        "",
        "#ifdef CONFIG_X86_64",
        "extern void idt_setup_early_pf(void);",
        "#else",
        "static inline void idt_setup_early_pf(void) { }",
        "#endif",
        "",
        "extern void idt_invalidate(void);",
        "",
        "#endif /* _ASM_X86_DESC_H */"
    ]
  },
  "include_linux_bsearch_h": {
    path: "include/linux/bsearch.h",
    covered: [13, 17],
    totalLines: 32,
    coveredCount: 2,
    coveragePct: 6.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_BSEARCH_H",
        "#define _LINUX_BSEARCH_H",
        "",
        "#include <linux/types.h>",
        "",
        "static __always_inline",
        "void *__inline_bsearch(const void *key, const void *base, size_t num, size_t size, cmp_func_t cmp)",
        "{",
        "	const char *pivot;",
        "	int result;",
        "",
        "	while (num > 0) {",
        "		pivot = base + (num >> 1) * size;",
        "		result = cmp(key, pivot);",
        "",
        "		if (result == 0)",
        "			return (void *)pivot;",
        "",
        "		if (result > 0) {",
        "			base = pivot + size;",
        "			num--;",
        "		}",
        "		num >>= 1;",
        "	}",
        "",
        "	return NULL;",
        "}",
        "",
        "extern void *bsearch(const void *key, const void *base, size_t num, size_t size, cmp_func_t cmp);",
        "",
        "#endif /* _LINUX_BSEARCH_H */"
    ]
  },
  "arch_x86_kernel_fpu_xstate_h": {
    path: "arch/x86/kernel/fpu/xstate.h",
    covered: [177],
    totalLines: 354,
    coveredCount: 1,
    coveragePct: 0.3,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef __X86_KERNEL_FPU_XSTATE_H",
        "#define __X86_KERNEL_FPU_XSTATE_H",
        "",
        "#include <asm/cpufeature.h>",
        "#include <asm/fpu/xstate.h>",
        "#include <asm/fpu/xcr.h>",
        "",
        "#ifdef CONFIG_X86_64",
        "DECLARE_PER_CPU(u64, xfd_state);",
        "#endif",
        "",
        "static inline void xstate_init_xcomp_bv(struct xregs_state *xsave, u64 mask)",
        "{",
        "	/*",
        "	 * XRSTORS requires these bits set in xcomp_bv, or it will",
        "	 * trigger #GP:",
        "	 */",
        "	if (cpu_feature_enabled(X86_FEATURE_XCOMPACTED))",
        "		xsave->header.xcomp_bv = mask | XCOMP_BV_COMPACTED_FORMAT;",
        "}",
        "",
        "static inline u64 xstate_get_group_perm(bool guest)",
        "{",
        "	struct fpu *fpu = &current->group_leader->thread.fpu;",
        "	struct fpu_state_perm *perm;",
        "",
        "	/* Pairs with WRITE_ONCE() in xstate_request_perm() */",
        "	perm = guest ? &fpu->guest_perm : &fpu->perm;",
        "	return READ_ONCE(perm->__state_perm);",
        "}",
        "",
        "static inline u64 xstate_get_host_group_perm(void)",
        "{",
        "	return xstate_get_group_perm(false);",
        "}",
        "",
        "enum xstate_copy_mode {",
        "	XSTATE_COPY_FP,",
        "	XSTATE_COPY_FX,",
        "	XSTATE_COPY_XSAVE,",
        "};",
        "",
        "struct membuf;",
        "extern void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,",
        "				      u64 xfeatures, u32 pkru_val,",
        "				      enum xstate_copy_mode copy_mode);",
        "extern void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,",
        "				    enum xstate_copy_mode mode);",
        "extern int copy_uabi_from_kernel_to_xstate(struct fpstate *fpstate, const void *kbuf, u32 *pkru);",
        "extern int copy_sigframe_from_user_to_xstate(struct task_struct *tsk, const void __user *ubuf);",
        "",
        "",
        "extern void fpu__init_cpu_xstate(void);",
        "extern void fpu__init_system_xstate(unsigned int legacy_size);",
        "",
        "extern void __user *get_xsave_addr_user(struct xregs_state __user *xsave, int xfeature_nr);",
        "",
        "static inline u64 xfeatures_mask_supervisor(void)",
        "{",
        "	return fpu_kernel_cfg.max_features & XFEATURE_MASK_SUPERVISOR_SUPPORTED;",
        "}",
        "",
        "static inline u64 xfeatures_mask_independent(void)",
        "{",
        "	if (!cpu_feature_enabled(X86_FEATURE_ARCH_LBR))",
        "		return fpu_kernel_cfg.independent_features & ~XFEATURE_MASK_LBR;",
        "",
        "	return fpu_kernel_cfg.independent_features;",
        "}",
        "",
        "/*",
        " * Update the value of PKRU register that was already pushed onto the signal frame.",
        " */",
        "static inline int update_pkru_in_sigframe(struct xregs_state __user *buf, u64 mask, u32 pkru)",
        "{",
        "	u64 xstate_bv;",
        "	int err;",
        "",
        "	if (unlikely(!cpu_feature_enabled(X86_FEATURE_OSPKE)))",
        "		return 0;",
        "",
        "	/* Mark PKRU as in-use so that it is restored correctly. */",
        "	xstate_bv = (mask & xfeatures_in_use()) | XFEATURE_MASK_PKRU;",
        "",
        "	err =  __put_user(xstate_bv, &buf->header.xfeatures);",
        "	if (err)",
        "		return err;",
        "",
        "	/* Update PKRU value in the userspace xsave buffer. */",
        "	return __put_user(pkru, (unsigned int __user *)get_xsave_addr_user(buf, XFEATURE_PKRU));",
        "}",
        "",
        "/* XSAVE/XRSTOR wrapper functions */",
        "",
        "#ifdef CONFIG_X86_64",
        "#define REX_PREFIX	\"0x48, \"",
        "#else",
        "#define REX_PREFIX",
        "#endif",
        "",
        "/* These macros all use (%edi)/(%rdi) as the single memory argument. */",
        "#define XSAVE		\".byte \" REX_PREFIX \"0x0f,0xae,0x27\"",
        "#define XSAVEOPT	\".byte \" REX_PREFIX \"0x0f,0xae,0x37\"",
        "#define XSAVEC		\".byte \" REX_PREFIX \"0x0f,0xc7,0x27\"",
        "#define XSAVES		\".byte \" REX_PREFIX \"0x0f,0xc7,0x2f\"",
        "#define XRSTOR		\".byte \" REX_PREFIX \"0x0f,0xae,0x2f\"",
        "#define XRSTORS		\".byte \" REX_PREFIX \"0x0f,0xc7,0x1f\"",
        "",
        "/*",
        " * After this @err contains 0 on success or the trap number when the",
        " * operation raises an exception.",
        " */",
        "#define XSTATE_OP(op, st, lmask, hmask, err)				\\",
        "	asm volatile(\"1:\" op \"\\n\\t\"					\\",
        "		     \"xor %[err], %[err]\\n\"				\\",
        "		     \"2:\\n\\t\"						\\",
        "		     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_FAULT_MCE_SAFE)	\\",
        "		     : [err] \"=a\" (err)					\\",
        "		     : \"D\" (st), \"m\" (*st), \"a\" (lmask), \"d\" (hmask)	\\",
        "		     : \"memory\")",
        "",
        "/*",
        " * If XSAVES is enabled, it replaces XSAVEC because it supports supervisor",
        " * states in addition to XSAVEC.",
        " *",
        " * Otherwise if XSAVEC is enabled, it replaces XSAVEOPT because it supports",
        " * compacted storage format in addition to XSAVEOPT.",
        " *",
        " * Otherwise, if XSAVEOPT is enabled, XSAVEOPT replaces XSAVE because XSAVEOPT",
        " * supports modified optimization which is not supported by XSAVE.",
        " *",
        " * Use XSAVE as a fallback.",
        " */",
        "#define XSTATE_XSAVE(st, lmask, hmask, err)				\\",
        "	asm volatile(\"1: \" ALTERNATIVE_3(XSAVE,				\\",
        "				   XSAVEOPT, X86_FEATURE_XSAVEOPT,	\\",
        "				   XSAVEC,   X86_FEATURE_XSAVEC,	\\",
        "				   XSAVES,   X86_FEATURE_XSAVES)	\\",
        "		     \"\\n\"						\\",
        "		     \"xor %[err], %[err]\\n\"				\\",
        "		     \"3:\\n\"						\\",
        "		     _ASM_EXTABLE_TYPE_REG(1b, 3b, EX_TYPE_EFAULT_REG, %[err]) \\",
        "		     : [err] \"=r\" (err)					\\",
        "		     : \"D\" (st), \"m\" (*st), \"a\" (lmask), \"d\" (hmask)	\\",
        "		     : \"memory\")",
        "",
        "/*",
        " * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact",
        " * XSAVE area format.",
        " */",
        "#define XSTATE_XRESTORE(st, lmask, hmask)				\\",
        "	asm volatile(\"1: \" ALTERNATIVE(XRSTOR,				\\",
        "				 XRSTORS, X86_FEATURE_XSAVES)		\\",
        "		     \"\\n\"						\\",
        "		     \"3:\\n\"						\\",
        "		     _ASM_EXTABLE_TYPE(1b, 3b, EX_TYPE_FPU_RESTORE)	\\",
        "		     :							\\",
        "		     : \"D\" (st), \"m\" (*st), \"a\" (lmask), \"d\" (hmask)	\\",
        "		     : \"memory\")",
        "",
        "#if defined(CONFIG_X86_64) && defined(CONFIG_X86_DEBUG_FPU)",
        "extern void xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor);",
        "#else",
        "static inline void xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor) { }",
        "#endif",
        "",
        "#ifdef CONFIG_X86_64",
        "static inline void xfd_set_state(u64 xfd)",
        "{",
        "	wrmsrl(MSR_IA32_XFD, xfd);",
        "	__this_cpu_write(xfd_state, xfd);",
        "}",
        "",
        "static inline void xfd_update_state(struct fpstate *fpstate)",
        "{",
        "	if (fpu_state_size_dynamic()) {",
        "		u64 xfd = fpstate->xfd;",
        "",
        "		if (__this_cpu_read(xfd_state) != xfd)",
        "			xfd_set_state(xfd);",
        "	}",
        "}",
        "",
        "extern int __xfd_enable_feature(u64 which, struct fpu_guest *guest_fpu);",
        "#else",
        "static inline void xfd_set_state(u64 xfd) { }",
        "",
        "static inline void xfd_update_state(struct fpstate *fpstate) { }",
        "",
        "static inline int __xfd_enable_feature(u64 which, struct fpu_guest *guest_fpu) {",
        "	return -EPERM;",
        "}",
        "#endif",
        "",
        "/*",
        " * Save processor xstate to xsave area.",
        " *",
        " * Uses either XSAVE or XSAVEOPT or XSAVES depending on the CPU features",
        " * and command line options. The choice is permanent until the next reboot.",
        " */",
        "static inline void os_xsave(struct fpstate *fpstate)",
        "{",
        "	u64 mask = fpstate->xfeatures;",
        "	u32 lmask = mask;",
        "	u32 hmask = mask >> 32;",
        "	int err;",
        "",
        "	WARN_ON_FPU(!alternatives_patched);",
        "	xfd_validate_state(fpstate, mask, false);",
        "",
        "	XSTATE_XSAVE(&fpstate->regs.xsave, lmask, hmask, err);",
        "",
        "	/* We should never fault when copying to a kernel buffer: */",
        "	WARN_ON_FPU(err);",
        "}",
        "",
        "/*",
        " * Restore processor xstate from xsave area.",
        " *",
        " * Uses XRSTORS when XSAVES is used, XRSTOR otherwise.",
        " */",
        "static inline void os_xrstor(struct fpstate *fpstate, u64 mask)",
        "{",
        "	u32 lmask = mask;",
        "	u32 hmask = mask >> 32;",
        "",
        "	xfd_validate_state(fpstate, mask, true);",
        "	XSTATE_XRESTORE(&fpstate->regs.xsave, lmask, hmask);",
        "}",
        "",
        "/* Restore of supervisor state. Does not require XFD */",
        "static inline void os_xrstor_supervisor(struct fpstate *fpstate)",
        "{",
        "	u64 mask = xfeatures_mask_supervisor();",
        "	u32 lmask = mask;",
        "	u32 hmask = mask >> 32;",
        "",
        "	XSTATE_XRESTORE(&fpstate->regs.xsave, lmask, hmask);",
        "}",
        "",
        "/*",
        " * XSAVE itself always writes all requested xfeatures.  Removing features",
        " * from the request bitmap reduces the features which are written.",
        " * Generate a mask of features which must be written to a sigframe.  The",
        " * unset features can be optimized away and not written.",
        " *",
        " * This optimization is user-visible.  Only use for states where",
        " * uninitialized sigframe contents are tolerable, like dynamic features.",
        " *",
        " * Users of buffers produced with this optimization must check XSTATE_BV",
        " * to determine which features have been optimized out.",
        " */",
        "static inline u64 xfeatures_need_sigframe_write(void)",
        "{",
        "	u64 xfeaures_to_write;",
        "",
        "	/* In-use features must be written: */",
        "	xfeaures_to_write = xfeatures_in_use();",
        "",
        "	/* Also write all non-optimizable sigframe features: */",
        "	xfeaures_to_write |= XFEATURE_MASK_USER_SUPPORTED &",
        "			     ~XFEATURE_MASK_SIGFRAME_INITOPT;",
        "",
        "	return xfeaures_to_write;",
        "}",
        "",
        "/*",
        " * Save xstate to user space xsave area.",
        " *",
        " * We don't use modified optimization because xrstor/xrstors might track",
        " * a different application.",
        " *",
        " * We don't use compacted format xsave area for backward compatibility for",
        " * old applications which don't understand the compacted format of the",
        " * xsave area.",
        " *",
        " * The caller has to zero buf::header before calling this because XSAVE*",
        " * does not touch the reserved fields in the header.",
        " */",
        "static inline int xsave_to_user_sigframe(struct xregs_state __user *buf, u32 pkru)",
        "{",
        "	/*",
        "	 * Include the features which are not xsaved/rstored by the kernel",
        "	 * internally, e.g. PKRU. That's user space ABI and also required",
        "	 * to allow the signal handler to modify PKRU.",
        "	 */",
        "	struct fpstate *fpstate = current->thread.fpu.fpstate;",
        "	u64 mask = fpstate->user_xfeatures;",
        "	u32 lmask;",
        "	u32 hmask;",
        "	int err;",
        "",
        "	/* Optimize away writing unnecessary xfeatures: */",
        "	if (fpu_state_size_dynamic())",
        "		mask &= xfeatures_need_sigframe_write();",
        "",
        "	lmask = mask;",
        "	hmask = mask >> 32;",
        "	xfd_validate_state(fpstate, mask, false);",
        "",
        "	stac();",
        "	XSTATE_OP(XSAVE, buf, lmask, hmask, err);",
        "	clac();",
        "",
        "	if (!err)",
        "		err = update_pkru_in_sigframe(buf, mask, pkru);",
        "",
        "	return err;",
        "}",
        "",
        "/*",
        " * Restore xstate from user space xsave area.",
        " */",
        "static inline int xrstor_from_user_sigframe(struct xregs_state __user *buf, u64 mask)",
        "{",
        "	struct xregs_state *xstate = ((__force struct xregs_state *)buf);",
        "	u32 lmask = mask;",
        "	u32 hmask = mask >> 32;",
        "	int err;",
        "",
        "	xfd_validate_state(current->thread.fpu.fpstate, mask, true);",
        "",
        "	stac();",
        "	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);",
        "	clac();",
        "",
        "	return err;",
        "}",
        "",
        "/*",
        " * Restore xstate from kernel space xsave area, return an error code instead of",
        " * an exception.",
        " */",
        "static inline int os_xrstor_safe(struct fpstate *fpstate, u64 mask)",
        "{",
        "	struct xregs_state *xstate = &fpstate->regs.xsave;",
        "	u32 lmask = mask;",
        "	u32 hmask = mask >> 32;",
        "	int err;",
        "",
        "	/* Ensure that XFD is up to date */",
        "	xfd_update_state(fpstate);",
        "",
        "	if (cpu_feature_enabled(X86_FEATURE_XSAVES))",
        "		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);",
        "	else",
        "		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);",
        "",
        "	return err;",
        "}",
        "",
        "",
        "#endif"
    ]
  },
  "security_commoncap_c": {
    path: "security/commoncap.c",
    covered: [78, 69],
    totalLines: 1478,
    coveredCount: 2,
    coveragePct: 0.1,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-or-later",
        "/* Common capabilities, needed by capability.o.",
        " */",
        "",
        "#include <linux/capability.h>",
        "#include <linux/audit.h>",
        "#include <linux/init.h>",
        "#include <linux/kernel.h>",
        "#include <linux/lsm_hooks.h>",
        "#include <linux/file.h>",
        "#include <linux/mm.h>",
        "#include <linux/mman.h>",
        "#include <linux/pagemap.h>",
        "#include <linux/swap.h>",
        "#include <linux/skbuff.h>",
        "#include <linux/netlink.h>",
        "#include <linux/ptrace.h>",
        "#include <linux/xattr.h>",
        "#include <linux/hugetlb.h>",
        "#include <linux/mount.h>",
        "#include <linux/sched.h>",
        "#include <linux/prctl.h>",
        "#include <linux/securebits.h>",
        "#include <linux/user_namespace.h>",
        "#include <linux/binfmts.h>",
        "#include <linux/personality.h>",
        "#include <linux/mnt_idmapping.h>",
        "#include <uapi/linux/lsm.h>",
        "",
        "/*",
        " * If a non-root user executes a setuid-root binary in",
        " * !secure(SECURE_NOROOT) mode, then we raise capabilities.",
        " * However if fE is also set, then the intent is for only",
        " * the file capabilities to be applied, and the setuid-root",
        " * bit is left on either to change the uid (plausible) or",
        " * to get full privilege on a kernel without file capabilities",
        " * support.  So in that case we do not raise capabilities.",
        " *",
        " * Warn if that happens, once per boot.",
        " */",
        "static void warn_setuid_and_fcaps_mixed(const char *fname)",
        "{",
        "	static int warned;",
        "	if (!warned) {",
        "		printk(KERN_INFO \"warning: `%s' has both setuid-root and\"",
        "			\" effective capabilities. Therefore not raising all\"",
        "			\" capabilities.\\n\", fname);",
        "		warned = 1;",
        "	}",
        "}",
        "",
        "/**",
        " * cap_capable - Determine whether a task has a particular effective capability",
        " * @cred: The credentials to use",
        " * @targ_ns:  The user namespace in which we need the capability",
        " * @cap: The capability to check for",
        " * @opts: Bitmask of options defined in include/linux/security.h",
        " *",
        " * Determine whether the nominated task has the specified capability amongst",
        " * its effective set, returning 0 if it does, -ve if it does not.",
        " *",
        " * NOTE WELL: cap_has_capability() cannot be used like the kernel's capable()",
        " * and has_capability() functions.  That is, it has the reverse semantics:",
        " * cap_has_capability() returns 0 when a task has a capability, but the",
        " * kernel's capable() and has_capability() returns 1 for this case.",
        " */",
        "int cap_capable(const struct cred *cred, struct user_namespace *targ_ns,",
        "		int cap, unsigned int opts)",
        "{",
        "	struct user_namespace *ns = targ_ns;",
        "",
        "	/* See if cred has the capability in the target user namespace",
        "	 * by examining the target user namespace and all of the target",
        "	 * user namespace's parents.",
        "	 */",
        "	for (;;) {",
        "		/* Do we have the necessary capabilities? */",
        "		if (ns == cred->user_ns)",
        "			return cap_raised(cred->cap_effective, cap) ? 0 : -EPERM;",
        "",
        "		/*",
        "		 * If we're already at a lower level than we're looking for,",
        "		 * we're done searching.",
        "		 */",
        "		if (ns->level <= cred->user_ns->level)",
        "			return -EPERM;",
        "",
        "		/* ",
        "		 * The owner of the user namespace in the parent of the",
        "		 * user namespace has all caps.",
        "		 */",
        "		if ((ns->parent == cred->user_ns) && uid_eq(ns->owner, cred->euid))",
        "			return 0;",
        "",
        "		/*",
        "		 * If you have a capability in a parent user ns, then you have",
        "		 * it over all children user namespaces as well.",
        "		 */",
        "		ns = ns->parent;",
        "	}",
        "",
        "	/* We never get here */",
        "}",
        "",
        "/**",
        " * cap_settime - Determine whether the current process may set the system clock",
        " * @ts: The time to set",
        " * @tz: The timezone to set",
        " *",
        " * Determine whether the current process may set the system clock and timezone",
        " * information, returning 0 if permission granted, -ve if denied.",
        " */",
        "int cap_settime(const struct timespec64 *ts, const struct timezone *tz)",
        "{",
        "	if (!capable(CAP_SYS_TIME))",
        "		return -EPERM;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * cap_ptrace_access_check - Determine whether the current process may access",
        " *			   another",
        " * @child: The process to be accessed",
        " * @mode: The mode of attachment.",
        " *",
        " * If we are in the same or an ancestor user_ns and have all the target",
        " * task's capabilities, then ptrace access is allowed.",
        " * If we have the ptrace capability to the target user_ns, then ptrace",
        " * access is allowed.",
        " * Else denied.",
        " *",
        " * Determine whether a process may access another, returning 0 if permission",
        " * granted, -ve if denied.",
        " */",
        "int cap_ptrace_access_check(struct task_struct *child, unsigned int mode)",
        "{",
        "	int ret = 0;",
        "	const struct cred *cred, *child_cred;",
        "	const kernel_cap_t *caller_caps;",
        "",
        "	rcu_read_lock();",
        "	cred = current_cred();",
        "	child_cred = __task_cred(child);",
        "	if (mode & PTRACE_MODE_FSCREDS)",
        "		caller_caps = &cred->cap_effective;",
        "	else",
        "		caller_caps = &cred->cap_permitted;",
        "	if (cred->user_ns == child_cred->user_ns &&",
        "	    cap_issubset(child_cred->cap_permitted, *caller_caps))",
        "		goto out;",
        "	if (ns_capable(child_cred->user_ns, CAP_SYS_PTRACE))",
        "		goto out;",
        "	ret = -EPERM;",
        "out:",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "",
        "/**",
        " * cap_ptrace_traceme - Determine whether another process may trace the current",
        " * @parent: The task proposed to be the tracer",
        " *",
        " * If parent is in the same or an ancestor user_ns and has all current's",
        " * capabilities, then ptrace access is allowed.",
        " * If parent has the ptrace capability to current's user_ns, then ptrace",
        " * access is allowed.",
        " * Else denied.",
        " *",
        " * Determine whether the nominated task is permitted to trace the current",
        " * process, returning 0 if permission is granted, -ve if denied.",
        " */",
        "int cap_ptrace_traceme(struct task_struct *parent)",
        "{",
        "	int ret = 0;",
        "	const struct cred *cred, *child_cred;",
        "",
        "	rcu_read_lock();",
        "	cred = __task_cred(parent);",
        "	child_cred = current_cred();",
        "	if (cred->user_ns == child_cred->user_ns &&",
        "	    cap_issubset(child_cred->cap_permitted, cred->cap_permitted))",
        "		goto out;",
        "	if (has_ns_capability(parent, child_cred->user_ns, CAP_SYS_PTRACE))",
        "		goto out;",
        "	ret = -EPERM;",
        "out:",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "",
        "/**",
        " * cap_capget - Retrieve a task's capability sets",
        " * @target: The task from which to retrieve the capability sets",
        " * @effective: The place to record the effective set",
        " * @inheritable: The place to record the inheritable set",
        " * @permitted: The place to record the permitted set",
        " *",
        " * This function retrieves the capabilities of the nominated task and returns",
        " * them to the caller.",
        " */",
        "int cap_capget(const struct task_struct *target, kernel_cap_t *effective,",
        "	       kernel_cap_t *inheritable, kernel_cap_t *permitted)",
        "{",
        "	const struct cred *cred;",
        "",
        "	/* Derived from kernel/capability.c:sys_capget. */",
        "	rcu_read_lock();",
        "	cred = __task_cred(target);",
        "	*effective   = cred->cap_effective;",
        "	*inheritable = cred->cap_inheritable;",
        "	*permitted   = cred->cap_permitted;",
        "	rcu_read_unlock();",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Determine whether the inheritable capabilities are limited to the old",
        " * permitted set.  Returns 1 if they are limited, 0 if they are not.",
        " */",
        "static inline int cap_inh_is_capped(void)",
        "{",
        "	/* they are so limited unless the current task has the CAP_SETPCAP",
        "	 * capability",
        "	 */",
        "	if (cap_capable(current_cred(), current_cred()->user_ns,",
        "			CAP_SETPCAP, CAP_OPT_NONE) == 0)",
        "		return 0;",
        "	return 1;",
        "}",
        "",
        "/**",
        " * cap_capset - Validate and apply proposed changes to current's capabilities",
        " * @new: The proposed new credentials; alterations should be made here",
        " * @old: The current task's current credentials",
        " * @effective: A pointer to the proposed new effective capabilities set",
        " * @inheritable: A pointer to the proposed new inheritable capabilities set",
        " * @permitted: A pointer to the proposed new permitted capabilities set",
        " *",
        " * This function validates and applies a proposed mass change to the current",
        " * process's capability sets.  The changes are made to the proposed new",
        " * credentials, and assuming no error, will be committed by the caller of LSM.",
        " */",
        "int cap_capset(struct cred *new,",
        "	       const struct cred *old,",
        "	       const kernel_cap_t *effective,",
        "	       const kernel_cap_t *inheritable,",
        "	       const kernel_cap_t *permitted)",
        "{",
        "	if (cap_inh_is_capped() &&",
        "	    !cap_issubset(*inheritable,",
        "			  cap_combine(old->cap_inheritable,",
        "				      old->cap_permitted)))",
        "		/* incapable of using this inheritable set */",
        "		return -EPERM;",
        "",
        "	if (!cap_issubset(*inheritable,",
        "			  cap_combine(old->cap_inheritable,",
        "				      old->cap_bset)))",
        "		/* no new pI capabilities outside bounding set */",
        "		return -EPERM;",
        "",
        "	/* verify restrictions on target's new Permitted set */",
        "	if (!cap_issubset(*permitted, old->cap_permitted))",
        "		return -EPERM;",
        "",
        "	/* verify the _new_Effective_ is a subset of the _new_Permitted_ */",
        "	if (!cap_issubset(*effective, *permitted))",
        "		return -EPERM;",
        "",
        "	new->cap_effective   = *effective;",
        "	new->cap_inheritable = *inheritable;",
        "	new->cap_permitted   = *permitted;",
        "",
        "	/*",
        "	 * Mask off ambient bits that are no longer both permitted and",
        "	 * inheritable.",
        "	 */",
        "	new->cap_ambient = cap_intersect(new->cap_ambient,",
        "					 cap_intersect(*permitted,",
        "						       *inheritable));",
        "	if (WARN_ON(!cap_ambient_invariant_ok(new)))",
        "		return -EINVAL;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * cap_inode_need_killpriv - Determine if inode change affects privileges",
        " * @dentry: The inode/dentry in being changed with change marked ATTR_KILL_PRIV",
        " *",
        " * Determine if an inode having a change applied that's marked ATTR_KILL_PRIV",
        " * affects the security markings on that inode, and if it is, should",
        " * inode_killpriv() be invoked or the change rejected.",
        " *",
        " * Return: 1 if security.capability has a value, meaning inode_killpriv()",
        " * is required, 0 otherwise, meaning inode_killpriv() is not required.",
        " */",
        "int cap_inode_need_killpriv(struct dentry *dentry)",
        "{",
        "	struct inode *inode = d_backing_inode(dentry);",
        "	int error;",
        "",
        "	error = __vfs_getxattr(dentry, inode, XATTR_NAME_CAPS, NULL, 0);",
        "	return error > 0;",
        "}",
        "",
        "/**",
        " * cap_inode_killpriv - Erase the security markings on an inode",
        " *",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dentry:	The inode/dentry to alter",
        " *",
        " * Erase the privilege-enhancing security markings on an inode.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then",
        " * take care to map the inode according to @idmap before checking",
        " * permissions. On non-idmapped mounts or if permission checking is to be",
        " * performed on the raw inode simply pass @nop_mnt_idmap.",
        " *",
        " * Return: 0 if successful, -ve on error.",
        " */",
        "int cap_inode_killpriv(struct mnt_idmap *idmap, struct dentry *dentry)",
        "{",
        "	int error;",
        "",
        "	error = __vfs_removexattr(idmap, dentry, XATTR_NAME_CAPS);",
        "	if (error == -EOPNOTSUPP)",
        "		error = 0;",
        "	return error;",
        "}",
        "",
        "static bool rootid_owns_currentns(vfsuid_t rootvfsuid)",
        "{",
        "	struct user_namespace *ns;",
        "	kuid_t kroot;",
        "",
        "	if (!vfsuid_valid(rootvfsuid))",
        "		return false;",
        "",
        "	kroot = vfsuid_into_kuid(rootvfsuid);",
        "	for (ns = current_user_ns();; ns = ns->parent) {",
        "		if (from_kuid(ns, kroot) == 0)",
        "			return true;",
        "		if (ns == &init_user_ns)",
        "			break;",
        "	}",
        "",
        "	return false;",
        "}",
        "",
        "static __u32 sansflags(__u32 m)",
        "{",
        "	return m & ~VFS_CAP_FLAGS_EFFECTIVE;",
        "}",
        "",
        "static bool is_v2header(int size, const struct vfs_cap_data *cap)",
        "{",
        "	if (size != XATTR_CAPS_SZ_2)",
        "		return false;",
        "	return sansflags(le32_to_cpu(cap->magic_etc)) == VFS_CAP_REVISION_2;",
        "}",
        "",
        "static bool is_v3header(int size, const struct vfs_cap_data *cap)",
        "{",
        "	if (size != XATTR_CAPS_SZ_3)",
        "		return false;",
        "	return sansflags(le32_to_cpu(cap->magic_etc)) == VFS_CAP_REVISION_3;",
        "}",
        "",
        "/*",
        " * getsecurity: We are called for security.* before any attempt to read the",
        " * xattr from the inode itself.",
        " *",
        " * This gives us a chance to read the on-disk value and convert it.  If we",
        " * return -EOPNOTSUPP, then vfs_getxattr() will call the i_op handler.",
        " *",
        " * Note we are not called by vfs_getxattr_alloc(), but that is only called",
        " * by the integrity subsystem, which really wants the unconverted values -",
        " * so that's good.",
        " */",
        "int cap_inode_getsecurity(struct mnt_idmap *idmap,",
        "			  struct inode *inode, const char *name, void **buffer,",
        "			  bool alloc)",
        "{",
        "	int size;",
        "	kuid_t kroot;",
        "	vfsuid_t vfsroot;",
        "	u32 nsmagic, magic;",
        "	uid_t root, mappedroot;",
        "	char *tmpbuf = NULL;",
        "	struct vfs_cap_data *cap;",
        "	struct vfs_ns_cap_data *nscap = NULL;",
        "	struct dentry *dentry;",
        "	struct user_namespace *fs_ns;",
        "",
        "	if (strcmp(name, \"capability\") != 0)",
        "		return -EOPNOTSUPP;",
        "",
        "	dentry = d_find_any_alias(inode);",
        "	if (!dentry)",
        "		return -EINVAL;",
        "	size = vfs_getxattr_alloc(idmap, dentry, XATTR_NAME_CAPS, &tmpbuf,",
        "				  sizeof(struct vfs_ns_cap_data), GFP_NOFS);",
        "	dput(dentry);",
        "	/* gcc11 complains if we don't check for !tmpbuf */",
        "	if (size < 0 || !tmpbuf)",
        "		goto out_free;",
        "",
        "	fs_ns = inode->i_sb->s_user_ns;",
        "	cap = (struct vfs_cap_data *) tmpbuf;",
        "	if (is_v2header(size, cap)) {",
        "		root = 0;",
        "	} else if (is_v3header(size, cap)) {",
        "		nscap = (struct vfs_ns_cap_data *) tmpbuf;",
        "		root = le32_to_cpu(nscap->rootid);",
        "	} else {",
        "		size = -EINVAL;",
        "		goto out_free;",
        "	}",
        "",
        "	kroot = make_kuid(fs_ns, root);",
        "",
        "	/* If this is an idmapped mount shift the kuid. */",
        "	vfsroot = make_vfsuid(idmap, fs_ns, kroot);",
        "",
        "	/* If the root kuid maps to a valid uid in current ns, then return",
        "	 * this as a nscap. */",
        "	mappedroot = from_kuid(current_user_ns(), vfsuid_into_kuid(vfsroot));",
        "	if (mappedroot != (uid_t)-1 && mappedroot != (uid_t)0) {",
        "		size = sizeof(struct vfs_ns_cap_data);",
        "		if (alloc) {",
        "			if (!nscap) {",
        "				/* v2 -> v3 conversion */",
        "				nscap = kzalloc(size, GFP_ATOMIC);",
        "				if (!nscap) {",
        "					size = -ENOMEM;",
        "					goto out_free;",
        "				}",
        "				nsmagic = VFS_CAP_REVISION_3;",
        "				magic = le32_to_cpu(cap->magic_etc);",
        "				if (magic & VFS_CAP_FLAGS_EFFECTIVE)",
        "					nsmagic |= VFS_CAP_FLAGS_EFFECTIVE;",
        "				memcpy(&nscap->data, &cap->data, sizeof(__le32) * 2 * VFS_CAP_U32);",
        "				nscap->magic_etc = cpu_to_le32(nsmagic);",
        "			} else {",
        "				/* use allocated v3 buffer */",
        "				tmpbuf = NULL;",
        "			}",
        "			nscap->rootid = cpu_to_le32(mappedroot);",
        "			*buffer = nscap;",
        "		}",
        "		goto out_free;",
        "	}",
        "",
        "	if (!rootid_owns_currentns(vfsroot)) {",
        "		size = -EOVERFLOW;",
        "		goto out_free;",
        "	}",
        "",
        "	/* This comes from a parent namespace.  Return as a v2 capability */",
        "	size = sizeof(struct vfs_cap_data);",
        "	if (alloc) {",
        "		if (nscap) {",
        "			/* v3 -> v2 conversion */",
        "			cap = kzalloc(size, GFP_ATOMIC);",
        "			if (!cap) {",
        "				size = -ENOMEM;",
        "				goto out_free;",
        "			}",
        "			magic = VFS_CAP_REVISION_2;",
        "			nsmagic = le32_to_cpu(nscap->magic_etc);",
        "			if (nsmagic & VFS_CAP_FLAGS_EFFECTIVE)",
        "				magic |= VFS_CAP_FLAGS_EFFECTIVE;",
        "			memcpy(&cap->data, &nscap->data, sizeof(__le32) * 2 * VFS_CAP_U32);",
        "			cap->magic_etc = cpu_to_le32(magic);",
        "		} else {",
        "			/* use unconverted v2 */",
        "			tmpbuf = NULL;",
        "		}",
        "		*buffer = cap;",
        "	}",
        "out_free:",
        "	kfree(tmpbuf);",
        "	return size;",
        "}",
        "",
        "/**",
        " * rootid_from_xattr - translate root uid of vfs caps",
        " *",
        " * @value:	vfs caps value which may be modified by this function",
        " * @size:	size of @ivalue",
        " * @task_ns:	user namespace of the caller",
        " */",
        "static vfsuid_t rootid_from_xattr(const void *value, size_t size,",
        "				  struct user_namespace *task_ns)",
        "{",
        "	const struct vfs_ns_cap_data *nscap = value;",
        "	uid_t rootid = 0;",
        "",
        "	if (size == XATTR_CAPS_SZ_3)",
        "		rootid = le32_to_cpu(nscap->rootid);",
        "",
        "	return VFSUIDT_INIT(make_kuid(task_ns, rootid));",
        "}",
        "",
        "static bool validheader(size_t size, const struct vfs_cap_data *cap)",
        "{",
        "	return is_v2header(size, cap) || is_v3header(size, cap);",
        "}",
        "",
        "/**",
        " * cap_convert_nscap - check vfs caps",
        " *",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dentry:	used to retrieve inode to check permissions on",
        " * @ivalue:	vfs caps value which may be modified by this function",
        " * @size:	size of @ivalue",
        " *",
        " * User requested a write of security.capability.  If needed, update the",
        " * xattr to change from v2 to v3, or to fixup the v3 rootid.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then",
        " * take care to map the inode according to @idmap before checking",
        " * permissions. On non-idmapped mounts or if permission checking is to be",
        " * performed on the raw inode simply pass @nop_mnt_idmap.",
        " *",
        " * Return: On success, return the new size; on error, return < 0.",
        " */",
        "int cap_convert_nscap(struct mnt_idmap *idmap, struct dentry *dentry,",
        "		      const void **ivalue, size_t size)",
        "{",
        "	struct vfs_ns_cap_data *nscap;",
        "	uid_t nsrootid;",
        "	const struct vfs_cap_data *cap = *ivalue;",
        "	__u32 magic, nsmagic;",
        "	struct inode *inode = d_backing_inode(dentry);",
        "	struct user_namespace *task_ns = current_user_ns(),",
        "		*fs_ns = inode->i_sb->s_user_ns;",
        "	kuid_t rootid;",
        "	vfsuid_t vfsrootid;",
        "	size_t newsize;",
        "",
        "	if (!*ivalue)",
        "		return -EINVAL;",
        "	if (!validheader(size, cap))",
        "		return -EINVAL;",
        "	if (!capable_wrt_inode_uidgid(idmap, inode, CAP_SETFCAP))",
        "		return -EPERM;",
        "	if (size == XATTR_CAPS_SZ_2 && (idmap == &nop_mnt_idmap))",
        "		if (ns_capable(inode->i_sb->s_user_ns, CAP_SETFCAP))",
        "			/* user is privileged, just write the v2 */",
        "			return size;",
        "",
        "	vfsrootid = rootid_from_xattr(*ivalue, size, task_ns);",
        "	if (!vfsuid_valid(vfsrootid))",
        "		return -EINVAL;",
        "",
        "	rootid = from_vfsuid(idmap, fs_ns, vfsrootid);",
        "	if (!uid_valid(rootid))",
        "		return -EINVAL;",
        "",
        "	nsrootid = from_kuid(fs_ns, rootid);",
        "	if (nsrootid == -1)",
        "		return -EINVAL;",
        "",
        "	newsize = sizeof(struct vfs_ns_cap_data);",
        "	nscap = kmalloc(newsize, GFP_ATOMIC);",
        "	if (!nscap)",
        "		return -ENOMEM;",
        "	nscap->rootid = cpu_to_le32(nsrootid);",
        "	nsmagic = VFS_CAP_REVISION_3;",
        "	magic = le32_to_cpu(cap->magic_etc);",
        "	if (magic & VFS_CAP_FLAGS_EFFECTIVE)",
        "		nsmagic |= VFS_CAP_FLAGS_EFFECTIVE;",
        "	nscap->magic_etc = cpu_to_le32(nsmagic);",
        "	memcpy(&nscap->data, &cap->data, sizeof(__le32) * 2 * VFS_CAP_U32);",
        "",
        "	*ivalue = nscap;",
        "	return newsize;",
        "}",
        "",
        "/*",
        " * Calculate the new process capability sets from the capability sets attached",
        " * to a file.",
        " */",
        "static inline int bprm_caps_from_vfs_caps(struct cpu_vfs_cap_data *caps,",
        "					  struct linux_binprm *bprm,",
        "					  bool *effective,",
        "					  bool *has_fcap)",
        "{",
        "	struct cred *new = bprm->cred;",
        "	int ret = 0;",
        "",
        "	if (caps->magic_etc & VFS_CAP_FLAGS_EFFECTIVE)",
        "		*effective = true;",
        "",
        "	if (caps->magic_etc & VFS_CAP_REVISION_MASK)",
        "		*has_fcap = true;",
        "",
        "	/*",
        "	 * pP' = (X & fP) | (pI & fI)",
        "	 * The addition of pA' is handled later.",
        "	 */",
        "	new->cap_permitted.val =",
        "		(new->cap_bset.val & caps->permitted.val) |",
        "		(new->cap_inheritable.val & caps->inheritable.val);",
        "",
        "	if (caps->permitted.val & ~new->cap_permitted.val)",
        "		/* insufficient to execute correctly */",
        "		ret = -EPERM;",
        "",
        "	/*",
        "	 * For legacy apps, with no internal support for recognizing they",
        "	 * do not have enough capabilities, we return an error if they are",
        "	 * missing some \"forced\" (aka file-permitted) capabilities.",
        "	 */",
        "	return *effective ? ret : 0;",
        "}",
        "",
        "/**",
        " * get_vfs_caps_from_disk - retrieve vfs caps from disk",
        " *",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dentry:	dentry from which @inode is retrieved",
        " * @cpu_caps:	vfs capabilities",
        " *",
        " * Extract the on-exec-apply capability sets for an executable file.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then",
        " * take care to map the inode according to @idmap before checking",
        " * permissions. On non-idmapped mounts or if permission checking is to be",
        " * performed on the raw inode simply pass @nop_mnt_idmap.",
        " */",
        "int get_vfs_caps_from_disk(struct mnt_idmap *idmap,",
        "			   const struct dentry *dentry,",
        "			   struct cpu_vfs_cap_data *cpu_caps)",
        "{",
        "	struct inode *inode = d_backing_inode(dentry);",
        "	__u32 magic_etc;",
        "	int size;",
        "	struct vfs_ns_cap_data data, *nscaps = &data;",
        "	struct vfs_cap_data *caps = (struct vfs_cap_data *) &data;",
        "	kuid_t rootkuid;",
        "	vfsuid_t rootvfsuid;",
        "	struct user_namespace *fs_ns;",
        "",
        "	memset(cpu_caps, 0, sizeof(struct cpu_vfs_cap_data));",
        "",
        "	if (!inode)",
        "		return -ENODATA;",
        "",
        "	fs_ns = inode->i_sb->s_user_ns;",
        "	size = __vfs_getxattr((struct dentry *)dentry, inode,",
        "			      XATTR_NAME_CAPS, &data, XATTR_CAPS_SZ);",
        "	if (size == -ENODATA || size == -EOPNOTSUPP)",
        "		/* no data, that's ok */",
        "		return -ENODATA;",
        "",
        "	if (size < 0)",
        "		return size;",
        "",
        "	if (size < sizeof(magic_etc))",
        "		return -EINVAL;",
        "",
        "	cpu_caps->magic_etc = magic_etc = le32_to_cpu(caps->magic_etc);",
        "",
        "	rootkuid = make_kuid(fs_ns, 0);",
        "	switch (magic_etc & VFS_CAP_REVISION_MASK) {",
        "	case VFS_CAP_REVISION_1:",
        "		if (size != XATTR_CAPS_SZ_1)",
        "			return -EINVAL;",
        "		break;",
        "	case VFS_CAP_REVISION_2:",
        "		if (size != XATTR_CAPS_SZ_2)",
        "			return -EINVAL;",
        "		break;",
        "	case VFS_CAP_REVISION_3:",
        "		if (size != XATTR_CAPS_SZ_3)",
        "			return -EINVAL;",
        "		rootkuid = make_kuid(fs_ns, le32_to_cpu(nscaps->rootid));",
        "		break;",
        "",
        "	default:",
        "		return -EINVAL;",
        "	}",
        "",
        "	rootvfsuid = make_vfsuid(idmap, fs_ns, rootkuid);",
        "	if (!vfsuid_valid(rootvfsuid))",
        "		return -ENODATA;",
        "",
        "	/* Limit the caps to the mounter of the filesystem",
        "	 * or the more limited uid specified in the xattr.",
        "	 */",
        "	if (!rootid_owns_currentns(rootvfsuid))",
        "		return -ENODATA;",
        "",
        "	cpu_caps->permitted.val = le32_to_cpu(caps->data[0].permitted);",
        "	cpu_caps->inheritable.val = le32_to_cpu(caps->data[0].inheritable);",
        "",
        "	/*",
        "	 * Rev1 had just a single 32-bit word, later expanded",
        "	 * to a second one for the high bits",
        "	 */",
        "	if ((magic_etc & VFS_CAP_REVISION_MASK) != VFS_CAP_REVISION_1) {",
        "		cpu_caps->permitted.val += (u64)le32_to_cpu(caps->data[1].permitted) << 32;",
        "		cpu_caps->inheritable.val += (u64)le32_to_cpu(caps->data[1].inheritable) << 32;",
        "	}",
        "",
        "	cpu_caps->permitted.val &= CAP_VALID_MASK;",
        "	cpu_caps->inheritable.val &= CAP_VALID_MASK;",
        "",
        "	cpu_caps->rootid = vfsuid_into_kuid(rootvfsuid);",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Attempt to get the on-exec apply capability sets for an executable file from",
        " * its xattrs and, if present, apply them to the proposed credentials being",
        " * constructed by execve().",
        " */",
        "static int get_file_caps(struct linux_binprm *bprm, const struct file *file,",
        "			 bool *effective, bool *has_fcap)",
        "{",
        "	int rc = 0;",
        "	struct cpu_vfs_cap_data vcaps;",
        "",
        "	cap_clear(bprm->cred->cap_permitted);",
        "",
        "	if (!file_caps_enabled)",
        "		return 0;",
        "",
        "	if (!mnt_may_suid(file->f_path.mnt))",
        "		return 0;",
        "",
        "	/*",
        "	 * This check is redundant with mnt_may_suid() but is kept to make",
        "	 * explicit that capability bits are limited to s_user_ns and its",
        "	 * descendants.",
        "	 */",
        "	if (!current_in_userns(file->f_path.mnt->mnt_sb->s_user_ns))",
        "		return 0;",
        "",
        "	rc = get_vfs_caps_from_disk(file_mnt_idmap(file),",
        "				    file->f_path.dentry, &vcaps);",
        "	if (rc < 0) {",
        "		if (rc == -EINVAL)",
        "			printk(KERN_NOTICE \"Invalid argument reading file caps for %s\\n\",",
        "					bprm->filename);",
        "		else if (rc == -ENODATA)",
        "			rc = 0;",
        "		goto out;",
        "	}",
        "",
        "	rc = bprm_caps_from_vfs_caps(&vcaps, bprm, effective, has_fcap);",
        "",
        "out:",
        "	if (rc)",
        "		cap_clear(bprm->cred->cap_permitted);",
        "",
        "	return rc;",
        "}",
        "",
        "static inline bool root_privileged(void) { return !issecure(SECURE_NOROOT); }",
        "",
        "static inline bool __is_real(kuid_t uid, struct cred *cred)",
        "{ return uid_eq(cred->uid, uid); }",
        "",
        "static inline bool __is_eff(kuid_t uid, struct cred *cred)",
        "{ return uid_eq(cred->euid, uid); }",
        "",
        "static inline bool __is_suid(kuid_t uid, struct cred *cred)",
        "{ return !__is_real(uid, cred) && __is_eff(uid, cred); }",
        "",
        "/*",
        " * handle_privileged_root - Handle case of privileged root",
        " * @bprm: The execution parameters, including the proposed creds",
        " * @has_fcap: Are any file capabilities set?",
        " * @effective: Do we have effective root privilege?",
        " * @root_uid: This namespace' root UID WRT initial USER namespace",
        " *",
        " * Handle the case where root is privileged and hasn't been neutered by",
        " * SECURE_NOROOT.  If file capabilities are set, they won't be combined with",
        " * set UID root and nothing is changed.  If we are root, cap_permitted is",
        " * updated.  If we have become set UID root, the effective bit is set.",
        " */",
        "static void handle_privileged_root(struct linux_binprm *bprm, bool has_fcap,",
        "				   bool *effective, kuid_t root_uid)",
        "{",
        "	const struct cred *old = current_cred();",
        "	struct cred *new = bprm->cred;",
        "",
        "	if (!root_privileged())",
        "		return;",
        "	/*",
        "	 * If the legacy file capability is set, then don't set privs",
        "	 * for a setuid root binary run by a non-root user.  Do set it",
        "	 * for a root user just to cause least surprise to an admin.",
        "	 */",
        "	if (has_fcap && __is_suid(root_uid, new)) {",
        "		warn_setuid_and_fcaps_mixed(bprm->filename);",
        "		return;",
        "	}",
        "	/*",
        "	 * To support inheritance of root-permissions and suid-root",
        "	 * executables under compatibility mode, we override the",
        "	 * capability sets for the file.",
        "	 */",
        "	if (__is_eff(root_uid, new) || __is_real(root_uid, new)) {",
        "		/* pP' = (cap_bset & ~0) | (pI & ~0) */",
        "		new->cap_permitted = cap_combine(old->cap_bset,",
        "						 old->cap_inheritable);",
        "	}",
        "	/*",
        "	 * If only the real uid is 0, we do not set the effective bit.",
        "	 */",
        "	if (__is_eff(root_uid, new))",
        "		*effective = true;",
        "}",
        "",
        "#define __cap_gained(field, target, source) \\",
        "	!cap_issubset(target->cap_##field, source->cap_##field)",
        "#define __cap_grew(target, source, cred) \\",
        "	!cap_issubset(cred->cap_##target, cred->cap_##source)",
        "#define __cap_full(field, cred) \\",
        "	cap_issubset(CAP_FULL_SET, cred->cap_##field)",
        "",
        "static inline bool __is_setuid(struct cred *new, const struct cred *old)",
        "{ return !uid_eq(new->euid, old->uid); }",
        "",
        "static inline bool __is_setgid(struct cred *new, const struct cred *old)",
        "{ return !gid_eq(new->egid, old->gid); }",
        "",
        "/*",
        " * 1) Audit candidate if current->cap_effective is set",
        " *",
        " * We do not bother to audit if 3 things are true:",
        " *   1) cap_effective has all caps",
        " *   2) we became root *OR* are were already root",
        " *   3) root is supposed to have all caps (SECURE_NOROOT)",
        " * Since this is just a normal root execing a process.",
        " *",
        " * Number 1 above might fail if you don't have a full bset, but I think",
        " * that is interesting information to audit.",
        " *",
        " * A number of other conditions require logging:",
        " * 2) something prevented setuid root getting all caps",
        " * 3) non-setuid root gets fcaps",
        " * 4) non-setuid root gets ambient",
        " */",
        "static inline bool nonroot_raised_pE(struct cred *new, const struct cred *old,",
        "				     kuid_t root, bool has_fcap)",
        "{",
        "	bool ret = false;",
        "",
        "	if ((__cap_grew(effective, ambient, new) &&",
        "	     !(__cap_full(effective, new) &&",
        "	       (__is_eff(root, new) || __is_real(root, new)) &&",
        "	       root_privileged())) ||",
        "	    (root_privileged() &&",
        "	     __is_suid(root, new) &&",
        "	     !__cap_full(effective, new)) ||",
        "	    (!__is_setuid(new, old) &&",
        "	     ((has_fcap &&",
        "	       __cap_gained(permitted, new, old)) ||",
        "	      __cap_gained(ambient, new, old))))",
        "",
        "		ret = true;",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " * cap_bprm_creds_from_file - Set up the proposed credentials for execve().",
        " * @bprm: The execution parameters, including the proposed creds",
        " * @file: The file to pull the credentials from",
        " *",
        " * Set up the proposed credentials for a new execution context being",
        " * constructed by execve().  The proposed creds in @bprm->cred is altered,",
        " * which won't take effect immediately.",
        " *",
        " * Return: 0 if successful, -ve on error.",
        " */",
        "int cap_bprm_creds_from_file(struct linux_binprm *bprm, const struct file *file)",
        "{",
        "	/* Process setpcap binaries and capabilities for uid 0 */",
        "	const struct cred *old = current_cred();",
        "	struct cred *new = bprm->cred;",
        "	bool effective = false, has_fcap = false, is_setid;",
        "	int ret;",
        "	kuid_t root_uid;",
        "",
        "	if (WARN_ON(!cap_ambient_invariant_ok(old)))",
        "		return -EPERM;",
        "",
        "	ret = get_file_caps(bprm, file, &effective, &has_fcap);",
        "	if (ret < 0)",
        "		return ret;",
        "",
        "	root_uid = make_kuid(new->user_ns, 0);",
        "",
        "	handle_privileged_root(bprm, has_fcap, &effective, root_uid);",
        "",
        "	/* if we have fs caps, clear dangerous personality flags */",
        "	if (__cap_gained(permitted, new, old))",
        "		bprm->per_clear |= PER_CLEAR_ON_SETID;",
        "",
        "	/* Don't let someone trace a set[ug]id/setpcap binary with the revised",
        "	 * credentials unless they have the appropriate permit.",
        "	 *",
        "	 * In addition, if NO_NEW_PRIVS, then ensure we get no new privs.",
        "	 */",
        "	is_setid = __is_setuid(new, old) || __is_setgid(new, old);",
        "",
        "	if ((is_setid || __cap_gained(permitted, new, old)) &&",
        "	    ((bprm->unsafe & ~LSM_UNSAFE_PTRACE) ||",
        "	     !ptracer_capable(current, new->user_ns))) {",
        "		/* downgrade; they get no more than they had, and maybe less */",
        "		if (!ns_capable(new->user_ns, CAP_SETUID) ||",
        "		    (bprm->unsafe & LSM_UNSAFE_NO_NEW_PRIVS)) {",
        "			new->euid = new->uid;",
        "			new->egid = new->gid;",
        "		}",
        "		new->cap_permitted = cap_intersect(new->cap_permitted,",
        "						   old->cap_permitted);",
        "	}",
        "",
        "	new->suid = new->fsuid = new->euid;",
        "	new->sgid = new->fsgid = new->egid;",
        "",
        "	/* File caps or setid cancels ambient. */",
        "	if (has_fcap || is_setid)",
        "		cap_clear(new->cap_ambient);",
        "",
        "	/*",
        "	 * Now that we've computed pA', update pP' to give:",
        "	 *   pP' = (X & fP) | (pI & fI) | pA'",
        "	 */",
        "	new->cap_permitted = cap_combine(new->cap_permitted, new->cap_ambient);",
        "",
        "	/*",
        "	 * Set pE' = (fE ? pP' : pA').  Because pA' is zero if fE is set,",
        "	 * this is the same as pE' = (fE ? pP' : 0) | pA'.",
        "	 */",
        "	if (effective)",
        "		new->cap_effective = new->cap_permitted;",
        "	else",
        "		new->cap_effective = new->cap_ambient;",
        "",
        "	if (WARN_ON(!cap_ambient_invariant_ok(new)))",
        "		return -EPERM;",
        "",
        "	if (nonroot_raised_pE(new, old, root_uid, has_fcap)) {",
        "		ret = audit_log_bprm_fcaps(bprm, new, old);",
        "		if (ret < 0)",
        "			return ret;",
        "	}",
        "",
        "	new->securebits &= ~issecure_mask(SECURE_KEEP_CAPS);",
        "",
        "	if (WARN_ON(!cap_ambient_invariant_ok(new)))",
        "		return -EPERM;",
        "",
        "	/* Check for privilege-elevated exec. */",
        "	if (is_setid ||",
        "	    (!__is_real(root_uid, new) &&",
        "	     (effective ||",
        "	      __cap_grew(permitted, ambient, new))))",
        "		bprm->secureexec = 1;",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * cap_inode_setxattr - Determine whether an xattr may be altered",
        " * @dentry: The inode/dentry being altered",
        " * @name: The name of the xattr to be changed",
        " * @value: The value that the xattr will be changed to",
        " * @size: The size of value",
        " * @flags: The replacement flag",
        " *",
        " * Determine whether an xattr may be altered or set on an inode, returning 0 if",
        " * permission is granted, -ve if denied.",
        " *",
        " * This is used to make sure security xattrs don't get updated or set by those",
        " * who aren't privileged to do so.",
        " */",
        "int cap_inode_setxattr(struct dentry *dentry, const char *name,",
        "		       const void *value, size_t size, int flags)",
        "{",
        "	struct user_namespace *user_ns = dentry->d_sb->s_user_ns;",
        "",
        "	/* Ignore non-security xattrs */",
        "	if (strncmp(name, XATTR_SECURITY_PREFIX,",
        "			XATTR_SECURITY_PREFIX_LEN) != 0)",
        "		return 0;",
        "",
        "	/*",
        "	 * For XATTR_NAME_CAPS the check will be done in",
        "	 * cap_convert_nscap(), called by setxattr()",
        "	 */",
        "	if (strcmp(name, XATTR_NAME_CAPS) == 0)",
        "		return 0;",
        "",
        "	if (!ns_capable(user_ns, CAP_SYS_ADMIN))",
        "		return -EPERM;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * cap_inode_removexattr - Determine whether an xattr may be removed",
        " *",
        " * @idmap:	idmap of the mount the inode was found from",
        " * @dentry:	The inode/dentry being altered",
        " * @name:	The name of the xattr to be changed",
        " *",
        " * Determine whether an xattr may be removed from an inode, returning 0 if",
        " * permission is granted, -ve if denied.",
        " *",
        " * If the inode has been found through an idmapped mount the idmap of",
        " * the vfsmount must be passed through @idmap. This function will then",
        " * take care to map the inode according to @idmap before checking",
        " * permissions. On non-idmapped mounts or if permission checking is to be",
        " * performed on the raw inode simply pass @nop_mnt_idmap.",
        " *",
        " * This is used to make sure security xattrs don't get removed by those who",
        " * aren't privileged to remove them.",
        " */",
        "int cap_inode_removexattr(struct mnt_idmap *idmap,",
        "			  struct dentry *dentry, const char *name)",
        "{",
        "	struct user_namespace *user_ns = dentry->d_sb->s_user_ns;",
        "",
        "	/* Ignore non-security xattrs */",
        "	if (strncmp(name, XATTR_SECURITY_PREFIX,",
        "			XATTR_SECURITY_PREFIX_LEN) != 0)",
        "		return 0;",
        "",
        "	if (strcmp(name, XATTR_NAME_CAPS) == 0) {",
        "		/* security.capability gets namespaced */",
        "		struct inode *inode = d_backing_inode(dentry);",
        "		if (!inode)",
        "			return -EINVAL;",
        "		if (!capable_wrt_inode_uidgid(idmap, inode, CAP_SETFCAP))",
        "			return -EPERM;",
        "		return 0;",
        "	}",
        "",
        "	if (!ns_capable(user_ns, CAP_SYS_ADMIN))",
        "		return -EPERM;",
        "	return 0;",
        "}",
        "",
        "/*",
        " * cap_emulate_setxuid() fixes the effective / permitted capabilities of",
        " * a process after a call to setuid, setreuid, or setresuid.",
        " *",
        " *  1) When set*uiding _from_ one of {r,e,s}uid == 0 _to_ all of",
        " *  {r,e,s}uid != 0, the permitted and effective capabilities are",
        " *  cleared.",
        " *",
        " *  2) When set*uiding _from_ euid == 0 _to_ euid != 0, the effective",
        " *  capabilities of the process are cleared.",
        " *",
        " *  3) When set*uiding _from_ euid != 0 _to_ euid == 0, the effective",
        " *  capabilities are set to the permitted capabilities.",
        " *",
        " *  fsuid is handled elsewhere. fsuid == 0 and {r,e,s}uid!= 0 should",
        " *  never happen.",
        " *",
        " *  -astor",
        " *",
        " * cevans - New behaviour, Oct '99",
        " * A process may, via prctl(), elect to keep its capabilities when it",
        " * calls setuid() and switches away from uid==0. Both permitted and",
        " * effective sets will be retained.",
        " * Without this change, it was impossible for a daemon to drop only some",
        " * of its privilege. The call to setuid(!=0) would drop all privileges!",
        " * Keeping uid 0 is not an option because uid 0 owns too many vital",
        " * files..",
        " * Thanks to Olaf Kirch and Peter Benie for spotting this.",
        " */",
        "static inline void cap_emulate_setxuid(struct cred *new, const struct cred *old)",
        "{",
        "	kuid_t root_uid = make_kuid(old->user_ns, 0);",
        "",
        "	if ((uid_eq(old->uid, root_uid) ||",
        "	     uid_eq(old->euid, root_uid) ||",
        "	     uid_eq(old->suid, root_uid)) &&",
        "	    (!uid_eq(new->uid, root_uid) &&",
        "	     !uid_eq(new->euid, root_uid) &&",
        "	     !uid_eq(new->suid, root_uid))) {",
        "		if (!issecure(SECURE_KEEP_CAPS)) {",
        "			cap_clear(new->cap_permitted);",
        "			cap_clear(new->cap_effective);",
        "		}",
        "",
        "		/*",
        "		 * Pre-ambient programs expect setresuid to nonroot followed",
        "		 * by exec to drop capabilities.  We should make sure that",
        "		 * this remains the case.",
        "		 */",
        "		cap_clear(new->cap_ambient);",
        "	}",
        "	if (uid_eq(old->euid, root_uid) && !uid_eq(new->euid, root_uid))",
        "		cap_clear(new->cap_effective);",
        "	if (!uid_eq(old->euid, root_uid) && uid_eq(new->euid, root_uid))",
        "		new->cap_effective = new->cap_permitted;",
        "}",
        "",
        "/**",
        " * cap_task_fix_setuid - Fix up the results of setuid() call",
        " * @new: The proposed credentials",
        " * @old: The current task's current credentials",
        " * @flags: Indications of what has changed",
        " *",
        " * Fix up the results of setuid() call before the credential changes are",
        " * actually applied.",
        " *",
        " * Return: 0 to grant the changes, -ve to deny them.",
        " */",
        "int cap_task_fix_setuid(struct cred *new, const struct cred *old, int flags)",
        "{",
        "	switch (flags) {",
        "	case LSM_SETID_RE:",
        "	case LSM_SETID_ID:",
        "	case LSM_SETID_RES:",
        "		/* juggle the capabilities to follow [RES]UID changes unless",
        "		 * otherwise suppressed */",
        "		if (!issecure(SECURE_NO_SETUID_FIXUP))",
        "			cap_emulate_setxuid(new, old);",
        "		break;",
        "",
        "	case LSM_SETID_FS:",
        "		/* juggle the capabilities to follow FSUID changes, unless",
        "		 * otherwise suppressed",
        "		 *",
        "		 * FIXME - is fsuser used for all CAP_FS_MASK capabilities?",
        "		 *          if not, we might be a bit too harsh here.",
        "		 */",
        "		if (!issecure(SECURE_NO_SETUID_FIXUP)) {",
        "			kuid_t root_uid = make_kuid(old->user_ns, 0);",
        "			if (uid_eq(old->fsuid, root_uid) && !uid_eq(new->fsuid, root_uid))",
        "				new->cap_effective =",
        "					cap_drop_fs_set(new->cap_effective);",
        "",
        "			if (!uid_eq(old->fsuid, root_uid) && uid_eq(new->fsuid, root_uid))",
        "				new->cap_effective =",
        "					cap_raise_fs_set(new->cap_effective,",
        "							 new->cap_permitted);",
        "		}",
        "		break;",
        "",
        "	default:",
        "		return -EINVAL;",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/*",
        " * Rationale: code calling task_setscheduler, task_setioprio, and",
        " * task_setnice, assumes that",
        " *   . if capable(cap_sys_nice), then those actions should be allowed",
        " *   . if not capable(cap_sys_nice), but acting on your own processes,",
        " *   	then those actions should be allowed",
        " * This is insufficient now since you can call code without suid, but",
        " * yet with increased caps.",
        " * So we check for increased caps on the target process.",
        " */",
        "static int cap_safe_nice(struct task_struct *p)",
        "{",
        "	int is_subset, ret = 0;",
        "",
        "	rcu_read_lock();",
        "	is_subset = cap_issubset(__task_cred(p)->cap_permitted,",
        "				 current_cred()->cap_permitted);",
        "	if (!is_subset && !ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE))",
        "		ret = -EPERM;",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " * cap_task_setscheduler - Determine if scheduler policy change is permitted",
        " * @p: The task to affect",
        " *",
        " * Determine if the requested scheduler policy change is permitted for the",
        " * specified task.",
        " *",
        " * Return: 0 if permission is granted, -ve if denied.",
        " */",
        "int cap_task_setscheduler(struct task_struct *p)",
        "{",
        "	return cap_safe_nice(p);",
        "}",
        "",
        "/**",
        " * cap_task_setioprio - Determine if I/O priority change is permitted",
        " * @p: The task to affect",
        " * @ioprio: The I/O priority to set",
        " *",
        " * Determine if the requested I/O priority change is permitted for the specified",
        " * task.",
        " *",
        " * Return: 0 if permission is granted, -ve if denied.",
        " */",
        "int cap_task_setioprio(struct task_struct *p, int ioprio)",
        "{",
        "	return cap_safe_nice(p);",
        "}",
        "",
        "/**",
        " * cap_task_setnice - Determine if task priority change is permitted",
        " * @p: The task to affect",
        " * @nice: The nice value to set",
        " *",
        " * Determine if the requested task priority change is permitted for the",
        " * specified task.",
        " *",
        " * Return: 0 if permission is granted, -ve if denied.",
        " */",
        "int cap_task_setnice(struct task_struct *p, int nice)",
        "{",
        "	return cap_safe_nice(p);",
        "}",
        "",
        "/*",
        " * Implement PR_CAPBSET_DROP.  Attempt to remove the specified capability from",
        " * the current task's bounding set.  Returns 0 on success, -ve on error.",
        " */",
        "static int cap_prctl_drop(unsigned long cap)",
        "{",
        "	struct cred *new;",
        "",
        "	if (!ns_capable(current_user_ns(), CAP_SETPCAP))",
        "		return -EPERM;",
        "	if (!cap_valid(cap))",
        "		return -EINVAL;",
        "",
        "	new = prepare_creds();",
        "	if (!new)",
        "		return -ENOMEM;",
        "	cap_lower(new->cap_bset, cap);",
        "	return commit_creds(new);",
        "}",
        "",
        "/**",
        " * cap_task_prctl - Implement process control functions for this security module",
        " * @option: The process control function requested",
        " * @arg2: The argument data for this function",
        " * @arg3: The argument data for this function",
        " * @arg4: The argument data for this function",
        " * @arg5: The argument data for this function",
        " *",
        " * Allow process control functions (sys_prctl()) to alter capabilities; may",
        " * also deny access to other functions not otherwise implemented here.",
        " *",
        " * Return: 0 or +ve on success, -ENOSYS if this function is not implemented",
        " * here, other -ve on error.  If -ENOSYS is returned, sys_prctl() and other LSM",
        " * modules will consider performing the function.",
        " */",
        "int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,",
        "		   unsigned long arg4, unsigned long arg5)",
        "{",
        "	const struct cred *old = current_cred();",
        "	struct cred *new;",
        "",
        "	switch (option) {",
        "	case PR_CAPBSET_READ:",
        "		if (!cap_valid(arg2))",
        "			return -EINVAL;",
        "		return !!cap_raised(old->cap_bset, arg2);",
        "",
        "	case PR_CAPBSET_DROP:",
        "		return cap_prctl_drop(arg2);",
        "",
        "	/*",
        "	 * The next four prctl's remain to assist with transitioning a",
        "	 * system from legacy UID=0 based privilege (when filesystem",
        "	 * capabilities are not in use) to a system using filesystem",
        "	 * capabilities only - as the POSIX.1e draft intended.",
        "	 *",
        "	 * Note:",
        "	 *",
        "	 *  PR_SET_SECUREBITS =",
        "	 *      issecure_mask(SECURE_KEEP_CAPS_LOCKED)",
        "	 *    | issecure_mask(SECURE_NOROOT)",
        "	 *    | issecure_mask(SECURE_NOROOT_LOCKED)",
        "	 *    | issecure_mask(SECURE_NO_SETUID_FIXUP)",
        "	 *    | issecure_mask(SECURE_NO_SETUID_FIXUP_LOCKED)",
        "	 *",
        "	 * will ensure that the current process and all of its",
        "	 * children will be locked into a pure",
        "	 * capability-based-privilege environment.",
        "	 */",
        "	case PR_SET_SECUREBITS:",
        "		if ((((old->securebits & SECURE_ALL_LOCKS) >> 1)",
        "		     & (old->securebits ^ arg2))			/*[1]*/",
        "		    || ((old->securebits & SECURE_ALL_LOCKS & ~arg2))	/*[2]*/",
        "		    || (arg2 & ~(SECURE_ALL_LOCKS | SECURE_ALL_BITS))	/*[3]*/",
        "		    || (cap_capable(current_cred(),",
        "				    current_cred()->user_ns,",
        "				    CAP_SETPCAP,",
        "				    CAP_OPT_NONE) != 0)			/*[4]*/",
        "			/*",
        "			 * [1] no changing of bits that are locked",
        "			 * [2] no unlocking of locks",
        "			 * [3] no setting of unsupported bits",
        "			 * [4] doing anything requires privilege (go read about",
        "			 *     the \"sendmail capabilities bug\")",
        "			 */",
        "		    )",
        "			/* cannot change a locked bit */",
        "			return -EPERM;",
        "",
        "		new = prepare_creds();",
        "		if (!new)",
        "			return -ENOMEM;",
        "		new->securebits = arg2;",
        "		return commit_creds(new);",
        "",
        "	case PR_GET_SECUREBITS:",
        "		return old->securebits;",
        "",
        "	case PR_GET_KEEPCAPS:",
        "		return !!issecure(SECURE_KEEP_CAPS);",
        "",
        "	case PR_SET_KEEPCAPS:",
        "		if (arg2 > 1) /* Note, we rely on arg2 being unsigned here */",
        "			return -EINVAL;",
        "		if (issecure(SECURE_KEEP_CAPS_LOCKED))",
        "			return -EPERM;",
        "",
        "		new = prepare_creds();",
        "		if (!new)",
        "			return -ENOMEM;",
        "		if (arg2)",
        "			new->securebits |= issecure_mask(SECURE_KEEP_CAPS);",
        "		else",
        "			new->securebits &= ~issecure_mask(SECURE_KEEP_CAPS);",
        "		return commit_creds(new);",
        "",
        "	case PR_CAP_AMBIENT:",
        "		if (arg2 == PR_CAP_AMBIENT_CLEAR_ALL) {",
        "			if (arg3 | arg4 | arg5)",
        "				return -EINVAL;",
        "",
        "			new = prepare_creds();",
        "			if (!new)",
        "				return -ENOMEM;",
        "			cap_clear(new->cap_ambient);",
        "			return commit_creds(new);",
        "		}",
        "",
        "		if (((!cap_valid(arg3)) | arg4 | arg5))",
        "			return -EINVAL;",
        "",
        "		if (arg2 == PR_CAP_AMBIENT_IS_SET) {",
        "			return !!cap_raised(current_cred()->cap_ambient, arg3);",
        "		} else if (arg2 != PR_CAP_AMBIENT_RAISE &&",
        "			   arg2 != PR_CAP_AMBIENT_LOWER) {",
        "			return -EINVAL;",
        "		} else {",
        "			if (arg2 == PR_CAP_AMBIENT_RAISE &&",
        "			    (!cap_raised(current_cred()->cap_permitted, arg3) ||",
        "			     !cap_raised(current_cred()->cap_inheritable,",
        "					 arg3) ||",
        "			     issecure(SECURE_NO_CAP_AMBIENT_RAISE)))",
        "				return -EPERM;",
        "",
        "			new = prepare_creds();",
        "			if (!new)",
        "				return -ENOMEM;",
        "			if (arg2 == PR_CAP_AMBIENT_RAISE)",
        "				cap_raise(new->cap_ambient, arg3);",
        "			else",
        "				cap_lower(new->cap_ambient, arg3);",
        "			return commit_creds(new);",
        "		}",
        "",
        "	default:",
        "		/* No functionality available - continue with default */",
        "		return -ENOSYS;",
        "	}",
        "}",
        "",
        "/**",
        " * cap_vm_enough_memory - Determine whether a new virtual mapping is permitted",
        " * @mm: The VM space in which the new mapping is to be made",
        " * @pages: The size of the mapping",
        " *",
        " * Determine whether the allocation of a new virtual mapping by the current",
        " * task is permitted.",
        " *",
        " * Return: 0 if permission granted, negative error code if not.",
        " */",
        "int cap_vm_enough_memory(struct mm_struct *mm, long pages)",
        "{",
        "	return cap_capable(current_cred(), &init_user_ns, CAP_SYS_ADMIN,",
        "			   CAP_OPT_NOAUDIT);",
        "}",
        "",
        "/**",
        " * cap_mmap_addr - check if able to map given addr",
        " * @addr: address attempting to be mapped",
        " *",
        " * If the process is attempting to map memory below dac_mmap_min_addr they need",
        " * CAP_SYS_RAWIO.  The other parameters to this function are unused by the",
        " * capability security module.",
        " *",
        " * Return: 0 if this mapping should be allowed or -EPERM if not.",
        " */",
        "int cap_mmap_addr(unsigned long addr)",
        "{",
        "	int ret = 0;",
        "",
        "	if (addr < dac_mmap_min_addr) {",
        "		ret = cap_capable(current_cred(), &init_user_ns, CAP_SYS_RAWIO,",
        "				  CAP_OPT_NONE);",
        "		/* set PF_SUPERPRIV if it turns out we allow the low mmap */",
        "		if (ret == 0)",
        "			current->flags |= PF_SUPERPRIV;",
        "	}",
        "	return ret;",
        "}",
        "",
        "int cap_mmap_file(struct file *file, unsigned long reqprot,",
        "		  unsigned long prot, unsigned long flags)",
        "{",
        "	return 0;",
        "}",
        "",
        "#ifdef CONFIG_SECURITY",
        "",
        "static const struct lsm_id capability_lsmid = {",
        "	.name = \"capability\",",
        "	.id = LSM_ID_CAPABILITY,",
        "};",
        "",
        "static struct security_hook_list capability_hooks[] __ro_after_init = {",
        "	LSM_HOOK_INIT(capable, cap_capable),",
        "	LSM_HOOK_INIT(settime, cap_settime),",
        "	LSM_HOOK_INIT(ptrace_access_check, cap_ptrace_access_check),",
        "	LSM_HOOK_INIT(ptrace_traceme, cap_ptrace_traceme),",
        "	LSM_HOOK_INIT(capget, cap_capget),",
        "	LSM_HOOK_INIT(capset, cap_capset),",
        "	LSM_HOOK_INIT(bprm_creds_from_file, cap_bprm_creds_from_file),",
        "	LSM_HOOK_INIT(inode_need_killpriv, cap_inode_need_killpriv),",
        "	LSM_HOOK_INIT(inode_killpriv, cap_inode_killpriv),",
        "	LSM_HOOK_INIT(inode_getsecurity, cap_inode_getsecurity),",
        "	LSM_HOOK_INIT(mmap_addr, cap_mmap_addr),",
        "	LSM_HOOK_INIT(mmap_file, cap_mmap_file),",
        "	LSM_HOOK_INIT(task_fix_setuid, cap_task_fix_setuid),",
        "	LSM_HOOK_INIT(task_prctl, cap_task_prctl),",
        "	LSM_HOOK_INIT(task_setscheduler, cap_task_setscheduler),",
        "	LSM_HOOK_INIT(task_setioprio, cap_task_setioprio),",
        "	LSM_HOOK_INIT(task_setnice, cap_task_setnice),",
        "	LSM_HOOK_INIT(vm_enough_memory, cap_vm_enough_memory),",
        "};",
        "",
        "static int __init capability_init(void)",
        "{",
        "	security_add_hooks(capability_hooks, ARRAY_SIZE(capability_hooks),",
        "			   &capability_lsmid);",
        "	return 0;",
        "}",
        "",
        "DEFINE_LSM(capability) = {",
        "	.name = \"capability\",",
        "	.order = LSM_ORDER_FIRST,",
        "	.init = capability_init,",
        "};",
        "",
        "#endif /* CONFIG_SECURITY */"
    ]
  },
  "include_linux_buffer_head_h": {
    path: "include/linux/buffer_head.h",
    covered: [324, 413],
    totalLines: 535,
    coveredCount: 2,
    coveragePct: 0.4,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "/*",
        " * include/linux/buffer_head.h",
        " *",
        " * Everything to do with buffer_heads.",
        " */",
        "",
        "#ifndef _LINUX_BUFFER_HEAD_H",
        "#define _LINUX_BUFFER_HEAD_H",
        "",
        "#include <linux/types.h>",
        "#include <linux/blk_types.h>",
        "#include <linux/fs.h>",
        "#include <linux/linkage.h>",
        "#include <linux/pagemap.h>",
        "#include <linux/wait.h>",
        "#include <linux/atomic.h>",
        "",
        "enum bh_state_bits {",
        "	BH_Uptodate,	/* Contains valid data */",
        "	BH_Dirty,	/* Is dirty */",
        "	BH_Lock,	/* Is locked */",
        "	BH_Req,		/* Has been submitted for I/O */",
        "",
        "	BH_Mapped,	/* Has a disk mapping */",
        "	BH_New,		/* Disk mapping was newly created by get_block */",
        "	BH_Async_Read,	/* Is under end_buffer_async_read I/O */",
        "	BH_Async_Write,	/* Is under end_buffer_async_write I/O */",
        "	BH_Delay,	/* Buffer is not yet allocated on disk */",
        "	BH_Boundary,	/* Block is followed by a discontiguity */",
        "	BH_Write_EIO,	/* I/O error on write */",
        "	BH_Unwritten,	/* Buffer is allocated on disk but not written */",
        "	BH_Quiet,	/* Buffer Error Prinks to be quiet */",
        "	BH_Meta,	/* Buffer contains metadata */",
        "	BH_Prio,	/* Buffer should be submitted with REQ_PRIO */",
        "	BH_Defer_Completion, /* Defer AIO completion to workqueue */",
        "",
        "	BH_PrivateStart,/* not a state bit, but the first bit available",
        "			 * for private allocation by other entities",
        "			 */",
        "};",
        "",
        "#define MAX_BUF_PER_PAGE (PAGE_SIZE / 512)",
        "",
        "struct page;",
        "struct buffer_head;",
        "struct address_space;",
        "typedef void (bh_end_io_t)(struct buffer_head *bh, int uptodate);",
        "",
        "/*",
        " * Historically, a buffer_head was used to map a single block",
        " * within a page, and of course as the unit of I/O through the",
        " * filesystem and block layers.  Nowadays the basic I/O unit",
        " * is the bio, and buffer_heads are used for extracting block",
        " * mappings (via a get_block_t call), for tracking state within",
        " * a folio (via a folio_mapping) and for wrapping bio submission",
        " * for backward compatibility reasons (e.g. submit_bh).",
        " */",
        "struct buffer_head {",
        "	unsigned long b_state;		/* buffer state bitmap (see above) */",
        "	struct buffer_head *b_this_page;/* circular list of page's buffers */",
        "	union {",
        "		struct page *b_page;	/* the page this bh is mapped to */",
        "		struct folio *b_folio;	/* the folio this bh is mapped to */",
        "	};",
        "",
        "	sector_t b_blocknr;		/* start block number */",
        "	size_t b_size;			/* size of mapping */",
        "	char *b_data;			/* pointer to data within the page */",
        "",
        "	struct block_device *b_bdev;",
        "	bh_end_io_t *b_end_io;		/* I/O completion */",
        " 	void *b_private;		/* reserved for b_end_io */",
        "	struct list_head b_assoc_buffers; /* associated with another mapping */",
        "	struct address_space *b_assoc_map;	/* mapping this buffer is",
        "						   associated with */",
        "	atomic_t b_count;		/* users using this buffer_head */",
        "	spinlock_t b_uptodate_lock;	/* Used by the first bh in a page, to",
        "					 * serialise IO completion of other",
        "					 * buffers in the page */",
        "};",
        "",
        "/*",
        " * macro tricks to expand the set_buffer_foo(), clear_buffer_foo()",
        " * and buffer_foo() functions.",
        " * To avoid reset buffer flags that are already set, because that causes",
        " * a costly cache line transition, check the flag first.",
        " */",
        "#define BUFFER_FNS(bit, name)						\\",
        "static __always_inline void set_buffer_##name(struct buffer_head *bh)	\\",
        "{									\\",
        "	if (!test_bit(BH_##bit, &(bh)->b_state))			\\",
        "		set_bit(BH_##bit, &(bh)->b_state);			\\",
        "}									\\",
        "static __always_inline void clear_buffer_##name(struct buffer_head *bh)	\\",
        "{									\\",
        "	clear_bit(BH_##bit, &(bh)->b_state);				\\",
        "}									\\",
        "static __always_inline int buffer_##name(const struct buffer_head *bh)	\\",
        "{									\\",
        "	return test_bit(BH_##bit, &(bh)->b_state);			\\",
        "}",
        "",
        "/*",
        " * test_set_buffer_foo() and test_clear_buffer_foo()",
        " */",
        "#define TAS_BUFFER_FNS(bit, name)					\\",
        "static __always_inline int test_set_buffer_##name(struct buffer_head *bh) \\",
        "{									\\",
        "	return test_and_set_bit(BH_##bit, &(bh)->b_state);		\\",
        "}									\\",
        "static __always_inline int test_clear_buffer_##name(struct buffer_head *bh) \\",
        "{									\\",
        "	return test_and_clear_bit(BH_##bit, &(bh)->b_state);		\\",
        "}									\\",
        "",
        "/*",
        " * Emit the buffer bitops functions.   Note that there are also functions",
        " * of the form \"mark_buffer_foo()\".  These are higher-level functions which",
        " * do something in addition to setting a b_state bit.",
        " */",
        "BUFFER_FNS(Dirty, dirty)",
        "TAS_BUFFER_FNS(Dirty, dirty)",
        "BUFFER_FNS(Lock, locked)",
        "BUFFER_FNS(Req, req)",
        "TAS_BUFFER_FNS(Req, req)",
        "BUFFER_FNS(Mapped, mapped)",
        "BUFFER_FNS(New, new)",
        "BUFFER_FNS(Async_Read, async_read)",
        "BUFFER_FNS(Async_Write, async_write)",
        "BUFFER_FNS(Delay, delay)",
        "BUFFER_FNS(Boundary, boundary)",
        "BUFFER_FNS(Write_EIO, write_io_error)",
        "BUFFER_FNS(Unwritten, unwritten)",
        "BUFFER_FNS(Meta, meta)",
        "BUFFER_FNS(Prio, prio)",
        "BUFFER_FNS(Defer_Completion, defer_completion)",
        "",
        "static __always_inline void set_buffer_uptodate(struct buffer_head *bh)",
        "{",
        "	/*",
        "	 * If somebody else already set this uptodate, they will",
        "	 * have done the memory barrier, and a reader will thus",
        "	 * see *some* valid buffer state.",
        "	 *",
        "	 * Any other serialization (with IO errors or whatever that",
        "	 * might clear the bit) has to come from other state (eg BH_Lock).",
        "	 */",
        "	if (test_bit(BH_Uptodate, &bh->b_state))",
        "		return;",
        "",
        "	/*",
        "	 * make it consistent with folio_mark_uptodate",
        "	 * pairs with smp_load_acquire in buffer_uptodate",
        "	 */",
        "	smp_mb__before_atomic();",
        "	set_bit(BH_Uptodate, &bh->b_state);",
        "}",
        "",
        "static __always_inline void clear_buffer_uptodate(struct buffer_head *bh)",
        "{",
        "	clear_bit(BH_Uptodate, &bh->b_state);",
        "}",
        "",
        "static __always_inline int buffer_uptodate(const struct buffer_head *bh)",
        "{",
        "	/*",
        "	 * make it consistent with folio_test_uptodate",
        "	 * pairs with smp_mb__before_atomic in set_buffer_uptodate",
        "	 */",
        "	return test_bit_acquire(BH_Uptodate, &bh->b_state);",
        "}",
        "",
        "static inline unsigned long bh_offset(const struct buffer_head *bh)",
        "{",
        "	return (unsigned long)(bh)->b_data & (page_size(bh->b_page) - 1);",
        "}",
        "",
        "/* If we *know* page->private refers to buffer_heads */",
        "#define page_buffers(page)					\\",
        "	({							\\",
        "		BUG_ON(!PagePrivate(page));			\\",
        "		((struct buffer_head *)page_private(page));	\\",
        "	})",
        "#define page_has_buffers(page)	PagePrivate(page)",
        "#define folio_buffers(folio)		folio_get_private(folio)",
        "",
        "void buffer_check_dirty_writeback(struct folio *folio,",
        "				     bool *dirty, bool *writeback);",
        "",
        "/*",
        " * Declarations",
        " */",
        "",
        "void mark_buffer_dirty(struct buffer_head *bh);",
        "void mark_buffer_write_io_error(struct buffer_head *bh);",
        "void touch_buffer(struct buffer_head *bh);",
        "void folio_set_bh(struct buffer_head *bh, struct folio *folio,",
        "		  unsigned long offset);",
        "struct buffer_head *folio_alloc_buffers(struct folio *folio, unsigned long size,",
        "					gfp_t gfp);",
        "struct buffer_head *alloc_page_buffers(struct page *page, unsigned long size);",
        "struct buffer_head *create_empty_buffers(struct folio *folio,",
        "		unsigned long blocksize, unsigned long b_state);",
        "void end_buffer_read_sync(struct buffer_head *bh, int uptodate);",
        "void end_buffer_write_sync(struct buffer_head *bh, int uptodate);",
        "",
        "/* Things to do with buffers at mapping->private_list */",
        "void mark_buffer_dirty_inode(struct buffer_head *bh, struct inode *inode);",
        "int generic_buffers_fsync_noflush(struct file *file, loff_t start, loff_t end,",
        "				  bool datasync);",
        "int generic_buffers_fsync(struct file *file, loff_t start, loff_t end,",
        "			  bool datasync);",
        "void clean_bdev_aliases(struct block_device *bdev, sector_t block,",
        "			sector_t len);",
        "static inline void clean_bdev_bh_alias(struct buffer_head *bh)",
        "{",
        "	clean_bdev_aliases(bh->b_bdev, bh->b_blocknr, 1);",
        "}",
        "",
        "void mark_buffer_async_write(struct buffer_head *bh);",
        "void __wait_on_buffer(struct buffer_head *);",
        "wait_queue_head_t *bh_waitq_head(struct buffer_head *bh);",
        "struct buffer_head *__find_get_block(struct block_device *bdev, sector_t block,",
        "			unsigned size);",
        "struct buffer_head *bdev_getblk(struct block_device *bdev, sector_t block,",
        "		unsigned size, gfp_t gfp);",
        "void __brelse(struct buffer_head *);",
        "void __bforget(struct buffer_head *);",
        "void __breadahead(struct block_device *, sector_t block, unsigned int size);",
        "struct buffer_head *__bread_gfp(struct block_device *,",
        "				sector_t block, unsigned size, gfp_t gfp);",
        "struct buffer_head *alloc_buffer_head(gfp_t gfp_flags);",
        "void free_buffer_head(struct buffer_head * bh);",
        "void unlock_buffer(struct buffer_head *bh);",
        "void __lock_buffer(struct buffer_head *bh);",
        "int sync_dirty_buffer(struct buffer_head *bh);",
        "int __sync_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags);",
        "void write_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags);",
        "void submit_bh(blk_opf_t, struct buffer_head *);",
        "void write_boundary_block(struct block_device *bdev,",
        "			sector_t bblock, unsigned blocksize);",
        "int bh_uptodate_or_lock(struct buffer_head *bh);",
        "int __bh_read(struct buffer_head *bh, blk_opf_t op_flags, bool wait);",
        "void __bh_read_batch(int nr, struct buffer_head *bhs[],",
        "		     blk_opf_t op_flags, bool force_lock);",
        "",
        "/*",
        " * Generic address_space_operations implementations for buffer_head-backed",
        " * address_spaces.",
        " */",
        "void block_invalidate_folio(struct folio *folio, size_t offset, size_t length);",
        "int block_write_full_folio(struct folio *folio, struct writeback_control *wbc,",
        "		void *get_block);",
        "int __block_write_full_folio(struct inode *inode, struct folio *folio,",
        "		get_block_t *get_block, struct writeback_control *wbc);",
        "int block_read_full_folio(struct folio *, get_block_t *);",
        "bool block_is_partially_uptodate(struct folio *, size_t from, size_t count);",
        "int block_write_begin(struct address_space *mapping, loff_t pos, unsigned len,",
        "		struct folio **foliop, get_block_t *get_block);",
        "int __block_write_begin(struct folio *folio, loff_t pos, unsigned len,",
        "		get_block_t *get_block);",
        "int block_write_end(struct file *, struct address_space *,",
        "				loff_t, unsigned len, unsigned copied,",
        "				struct folio *, void *);",
        "int generic_write_end(struct file *, struct address_space *,",
        "				loff_t, unsigned len, unsigned copied,",
        "				struct folio *, void *);",
        "void folio_zero_new_buffers(struct folio *folio, size_t from, size_t to);",
        "int cont_write_begin(struct file *, struct address_space *, loff_t,",
        "			unsigned, struct folio **, void **,",
        "			get_block_t *, loff_t *);",
        "int generic_cont_expand_simple(struct inode *inode, loff_t size);",
        "void block_commit_write(struct page *page, unsigned int from, unsigned int to);",
        "int block_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf,",
        "				get_block_t get_block);",
        "sector_t generic_block_bmap(struct address_space *, sector_t, get_block_t *);",
        "int block_truncate_page(struct address_space *, loff_t, get_block_t *);",
        "",
        "#ifdef CONFIG_MIGRATION",
        "extern int buffer_migrate_folio(struct address_space *,",
        "		struct folio *dst, struct folio *src, enum migrate_mode);",
        "extern int buffer_migrate_folio_norefs(struct address_space *,",
        "		struct folio *dst, struct folio *src, enum migrate_mode);",
        "#else",
        "#define buffer_migrate_folio NULL",
        "#define buffer_migrate_folio_norefs NULL",
        "#endif",
        "",
        "/*",
        " * inline definitions",
        " */",
        "",
        "static inline void get_bh(struct buffer_head *bh)",
        "{",
        "        atomic_inc(&bh->b_count);",
        "}",
        "",
        "static inline void put_bh(struct buffer_head *bh)",
        "{",
        "        smp_mb__before_atomic();",
        "        atomic_dec(&bh->b_count);",
        "}",
        "",
        "/**",
        " * brelse - Release a buffer.",
        " * @bh: The buffer to release.",
        " *",
        " * Decrement a buffer_head's reference count.  If @bh is NULL, this",
        " * function is a no-op.",
        " *",
        " * If all buffers on a folio have zero reference count, are clean",
        " * and unlocked, and if the folio is unlocked and not under writeback",
        " * then try_to_free_buffers() may strip the buffers from the folio in",
        " * preparation for freeing it (sometimes, rarely, buffers are removed",
        " * from a folio but it ends up not being freed, and buffers may later",
        " * be reattached).",
        " *",
        " * Context: Any context.",
        " */",
        "static inline void brelse(struct buffer_head *bh)",
        "{",
        "	if (bh)",
        "		__brelse(bh);",
        "}",
        "",
        "/**",
        " * bforget - Discard any dirty data in a buffer.",
        " * @bh: The buffer to forget.",
        " *",
        " * Call this function instead of brelse() if the data written to a buffer",
        " * no longer needs to be written back.  It will clear the buffer's dirty",
        " * flag so writeback of this buffer will be skipped.",
        " *",
        " * Context: Any context.",
        " */",
        "static inline void bforget(struct buffer_head *bh)",
        "{",
        "	if (bh)",
        "		__bforget(bh);",
        "}",
        "",
        "static inline struct buffer_head *",
        "sb_bread(struct super_block *sb, sector_t block)",
        "{",
        "	return __bread_gfp(sb->s_bdev, block, sb->s_blocksize, __GFP_MOVABLE);",
        "}",
        "",
        "static inline struct buffer_head *",
        "sb_bread_unmovable(struct super_block *sb, sector_t block)",
        "{",
        "	return __bread_gfp(sb->s_bdev, block, sb->s_blocksize, 0);",
        "}",
        "",
        "static inline void",
        "sb_breadahead(struct super_block *sb, sector_t block)",
        "{",
        "	__breadahead(sb->s_bdev, block, sb->s_blocksize);",
        "}",
        "",
        "static inline struct buffer_head *getblk_unmovable(struct block_device *bdev,",
        "		sector_t block, unsigned size)",
        "{",
        "	gfp_t gfp;",
        "",
        "	gfp = mapping_gfp_constraint(bdev->bd_mapping, ~__GFP_FS);",
        "	gfp |= __GFP_NOFAIL;",
        "",
        "	return bdev_getblk(bdev, block, size, gfp);",
        "}",
        "",
        "static inline struct buffer_head *__getblk(struct block_device *bdev,",
        "		sector_t block, unsigned size)",
        "{",
        "	gfp_t gfp;",
        "",
        "	gfp = mapping_gfp_constraint(bdev->bd_mapping, ~__GFP_FS);",
        "	gfp |= __GFP_MOVABLE | __GFP_NOFAIL;",
        "",
        "	return bdev_getblk(bdev, block, size, gfp);",
        "}",
        "",
        "static inline struct buffer_head *sb_getblk(struct super_block *sb,",
        "		sector_t block)",
        "{",
        "	return __getblk(sb->s_bdev, block, sb->s_blocksize);",
        "}",
        "",
        "static inline struct buffer_head *sb_getblk_gfp(struct super_block *sb,",
        "		sector_t block, gfp_t gfp)",
        "{",
        "	return bdev_getblk(sb->s_bdev, block, sb->s_blocksize, gfp);",
        "}",
        "",
        "static inline struct buffer_head *",
        "sb_find_get_block(struct super_block *sb, sector_t block)",
        "{",
        "	return __find_get_block(sb->s_bdev, block, sb->s_blocksize);",
        "}",
        "",
        "static inline void",
        "map_bh(struct buffer_head *bh, struct super_block *sb, sector_t block)",
        "{",
        "	set_buffer_mapped(bh);",
        "	bh->b_bdev = sb->s_bdev;",
        "	bh->b_blocknr = block;",
        "	bh->b_size = sb->s_blocksize;",
        "}",
        "",
        "static inline void wait_on_buffer(struct buffer_head *bh)",
        "{",
        "	might_sleep();",
        "	if (buffer_locked(bh))",
        "		__wait_on_buffer(bh);",
        "}",
        "",
        "static inline int trylock_buffer(struct buffer_head *bh)",
        "{",
        "	return likely(!test_and_set_bit_lock(BH_Lock, &bh->b_state));",
        "}",
        "",
        "static inline void lock_buffer(struct buffer_head *bh)",
        "{",
        "	might_sleep();",
        "	if (!trylock_buffer(bh))",
        "		__lock_buffer(bh);",
        "}",
        "",
        "static inline void bh_readahead(struct buffer_head *bh, blk_opf_t op_flags)",
        "{",
        "	if (!buffer_uptodate(bh) && trylock_buffer(bh)) {",
        "		if (!buffer_uptodate(bh))",
        "			__bh_read(bh, op_flags, false);",
        "		else",
        "			unlock_buffer(bh);",
        "	}",
        "}",
        "",
        "static inline void bh_read_nowait(struct buffer_head *bh, blk_opf_t op_flags)",
        "{",
        "	if (!bh_uptodate_or_lock(bh))",
        "		__bh_read(bh, op_flags, false);",
        "}",
        "",
        "/* Returns 1 if buffer uptodated, 0 on success, and -EIO on error. */",
        "static inline int bh_read(struct buffer_head *bh, blk_opf_t op_flags)",
        "{",
        "	if (bh_uptodate_or_lock(bh))",
        "		return 1;",
        "	return __bh_read(bh, op_flags, true);",
        "}",
        "",
        "static inline void bh_read_batch(int nr, struct buffer_head *bhs[])",
        "{",
        "	__bh_read_batch(nr, bhs, 0, true);",
        "}",
        "",
        "static inline void bh_readahead_batch(int nr, struct buffer_head *bhs[],",
        "				      blk_opf_t op_flags)",
        "{",
        "	__bh_read_batch(nr, bhs, op_flags, false);",
        "}",
        "",
        "/**",
        " * __bread() - Read a block.",
        " * @bdev: The block device to read from.",
        " * @block: Block number in units of block size.",
        " * @size: The block size of this device in bytes.",
        " *",
        " * Read a specified block, and return the buffer head that refers",
        " * to it.  The memory is allocated from the movable area so that it can",
        " * be migrated.  The returned buffer head has its refcount increased.",
        " * The caller should call brelse() when it has finished with the buffer.",
        " *",
        " * Context: May sleep waiting for I/O.",
        " * Return: NULL if the block was unreadable.",
        " */",
        "static inline struct buffer_head *__bread(struct block_device *bdev,",
        "		sector_t block, unsigned size)",
        "{",
        "	return __bread_gfp(bdev, block, size, __GFP_MOVABLE);",
        "}",
        "",
        "/**",
        " * get_nth_bh - Get a reference on the n'th buffer after this one.",
        " * @bh: The buffer to start counting from.",
        " * @count: How many buffers to skip.",
        " *",
        " * This is primarily useful for finding the nth buffer in a folio; in",
        " * that case you pass the head buffer and the byte offset in the folio",
        " * divided by the block size.  It can be used for other purposes, but",
        " * it will wrap at the end of the folio rather than returning NULL or",
        " * proceeding to the next folio for you.",
        " *",
        " * Return: The requested buffer with an elevated refcount.",
        " */",
        "static inline __must_check",
        "struct buffer_head *get_nth_bh(struct buffer_head *bh, unsigned int count)",
        "{",
        "	while (count--)",
        "		bh = bh->b_this_page;",
        "	get_bh(bh);",
        "	return bh;",
        "}",
        "",
        "bool block_dirty_folio(struct address_space *mapping, struct folio *folio);",
        "",
        "#ifdef CONFIG_BUFFER_HEAD",
        "",
        "void buffer_init(void);",
        "bool try_to_free_buffers(struct folio *folio);",
        "int inode_has_buffers(struct inode *inode);",
        "void invalidate_inode_buffers(struct inode *inode);",
        "int remove_inode_buffers(struct inode *inode);",
        "int sync_mapping_buffers(struct address_space *mapping);",
        "void invalidate_bh_lrus(void);",
        "void invalidate_bh_lrus_cpu(void);",
        "bool has_bh_in_lru(int cpu, void *dummy);",
        "extern int buffer_heads_over_limit;",
        "",
        "#else /* CONFIG_BUFFER_HEAD */",
        "",
        "static inline void buffer_init(void) {}",
        "static inline bool try_to_free_buffers(struct folio *folio) { return true; }",
        "static inline int inode_has_buffers(struct inode *inode) { return 0; }",
        "static inline void invalidate_inode_buffers(struct inode *inode) {}",
        "static inline int remove_inode_buffers(struct inode *inode) { return 1; }",
        "static inline int sync_mapping_buffers(struct address_space *mapping) { return 0; }",
        "static inline void invalidate_bh_lrus(void) {}",
        "static inline void invalidate_bh_lrus_cpu(void) {}",
        "static inline bool has_bh_in_lru(int cpu, void *dummy) { return false; }",
        "#define buffer_heads_over_limit 0",
        "",
        "#endif /* CONFIG_BUFFER_HEAD */",
        "#endif /* _LINUX_BUFFER_HEAD_H */"
    ]
  },
  "arch_x86_kernel_alternative_c": {
    path: "arch/x86/kernel/alternative.c",
    covered: [2386, 552, 1910, 1874, 2284, 2450, 1985, 1834, 1908, 2442, 2412, 2114, 1962, 1970, 2522, 2418, 1851, 2319, 2514, 2406, 2393, 1937, 1866, 2515, 2108, 1909, 1940, 2394, 2435, 2458, 2340, 2521, 1949, 2526, 2529, 1888, 1988, 2329, 2109],
    totalLines: 2552,
    coveredCount: 39,
    coveragePct: 1.5,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "#define pr_fmt(fmt) \"SMP alternatives: \" fmt",
        "",
        "#include <linux/module.h>",
        "#include <linux/sched.h>",
        "#include <linux/perf_event.h>",
        "#include <linux/mutex.h>",
        "#include <linux/list.h>",
        "#include <linux/stringify.h>",
        "#include <linux/highmem.h>",
        "#include <linux/mm.h>",
        "#include <linux/vmalloc.h>",
        "#include <linux/memory.h>",
        "#include <linux/stop_machine.h>",
        "#include <linux/slab.h>",
        "#include <linux/kdebug.h>",
        "#include <linux/kprobes.h>",
        "#include <linux/mmu_context.h>",
        "#include <linux/bsearch.h>",
        "#include <linux/sync_core.h>",
        "#include <asm/text-patching.h>",
        "#include <asm/alternative.h>",
        "#include <asm/sections.h>",
        "#include <asm/mce.h>",
        "#include <asm/nmi.h>",
        "#include <asm/cacheflush.h>",
        "#include <asm/tlbflush.h>",
        "#include <asm/insn.h>",
        "#include <asm/io.h>",
        "#include <asm/fixmap.h>",
        "#include <asm/paravirt.h>",
        "#include <asm/asm-prototypes.h>",
        "#include <asm/cfi.h>",
        "",
        "int __read_mostly alternatives_patched;",
        "",
        "EXPORT_SYMBOL_GPL(alternatives_patched);",
        "",
        "#define MAX_PATCH_LEN (255-1)",
        "",
        "#define DA_ALL		(~0)",
        "#define DA_ALT		0x01",
        "#define DA_RET		0x02",
        "#define DA_RETPOLINE	0x04",
        "#define DA_ENDBR	0x08",
        "#define DA_SMP		0x10",
        "",
        "static unsigned int debug_alternative;",
        "",
        "static int __init debug_alt(char *str)",
        "{",
        "	if (str && *str == '=')",
        "		str++;",
        "",
        "	if (!str || kstrtouint(str, 0, &debug_alternative))",
        "		debug_alternative = DA_ALL;",
        "",
        "	return 1;",
        "}",
        "__setup(\"debug-alternative\", debug_alt);",
        "",
        "static int noreplace_smp;",
        "",
        "static int __init setup_noreplace_smp(char *str)",
        "{",
        "	noreplace_smp = 1;",
        "	return 1;",
        "}",
        "__setup(\"noreplace-smp\", setup_noreplace_smp);",
        "",
        "#define DPRINTK(type, fmt, args...)					\\",
        "do {									\\",
        "	if (debug_alternative & DA_##type)				\\",
        "		printk(KERN_DEBUG pr_fmt(fmt) \"\\n\", ##args);		\\",
        "} while (0)",
        "",
        "#define DUMP_BYTES(type, buf, len, fmt, args...)			\\",
        "do {									\\",
        "	if (unlikely(debug_alternative & DA_##type)) {			\\",
        "		int j;							\\",
        "									\\",
        "		if (!(len))						\\",
        "			break;						\\",
        "									\\",
        "		printk(KERN_DEBUG pr_fmt(fmt), ##args);			\\",
        "		for (j = 0; j < (len) - 1; j++)				\\",
        "			printk(KERN_CONT \"%02hhx \", buf[j]);		\\",
        "		printk(KERN_CONT \"%02hhx\\n\", buf[j]);			\\",
        "	}								\\",
        "} while (0)",
        "",
        "static const unsigned char x86nops[] =",
        "{",
        "	BYTES_NOP1,",
        "	BYTES_NOP2,",
        "	BYTES_NOP3,",
        "	BYTES_NOP4,",
        "	BYTES_NOP5,",
        "	BYTES_NOP6,",
        "	BYTES_NOP7,",
        "	BYTES_NOP8,",
        "#ifdef CONFIG_64BIT",
        "	BYTES_NOP9,",
        "	BYTES_NOP10,",
        "	BYTES_NOP11,",
        "#endif",
        "};",
        "",
        "const unsigned char * const x86_nops[ASM_NOP_MAX+1] =",
        "{",
        "	NULL,",
        "	x86nops,",
        "	x86nops + 1,",
        "	x86nops + 1 + 2,",
        "	x86nops + 1 + 2 + 3,",
        "	x86nops + 1 + 2 + 3 + 4,",
        "	x86nops + 1 + 2 + 3 + 4 + 5,",
        "	x86nops + 1 + 2 + 3 + 4 + 5 + 6,",
        "	x86nops + 1 + 2 + 3 + 4 + 5 + 6 + 7,",
        "#ifdef CONFIG_64BIT",
        "	x86nops + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8,",
        "	x86nops + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9,",
        "	x86nops + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10,",
        "#endif",
        "};",
        "",
        "/*",
        " * Nomenclature for variable names to simplify and clarify this code and ease",
        " * any potential staring at it:",
        " *",
        " * @instr: source address of the original instructions in the kernel text as",
        " * generated by the compiler.",
        " *",
        " * @buf: temporary buffer on which the patching operates. This buffer is",
        " * eventually text-poked into the kernel image.",
        " *",
        " * @replacement/@repl: pointer to the opcodes which are replacing @instr, located",
        " * in the .altinstr_replacement section.",
        " */",
        "",
        "/*",
        " * Fill the buffer with a single effective instruction of size @len.",
        " *",
        " * In order not to issue an ORC stack depth tracking CFI entry (Call Frame Info)",
        " * for every single-byte NOP, try to generate the maximally available NOP of",
        " * size <= ASM_NOP_MAX such that only a single CFI entry is generated (vs one for",
        " * each single-byte NOPs). If @len to fill out is > ASM_NOP_MAX, pad with INT3 and",
        " * *jump* over instead of executing long and daft NOPs.",
        " */",
        "static void add_nop(u8 *buf, unsigned int len)",
        "{",
        "	u8 *target = buf + len;",
        "",
        "	if (!len)",
        "		return;",
        "",
        "	if (len <= ASM_NOP_MAX) {",
        "		memcpy(buf, x86_nops[len], len);",
        "		return;",
        "	}",
        "",
        "	if (len < 128) {",
        "		__text_gen_insn(buf, JMP8_INSN_OPCODE, buf, target, JMP8_INSN_SIZE);",
        "		buf += JMP8_INSN_SIZE;",
        "	} else {",
        "		__text_gen_insn(buf, JMP32_INSN_OPCODE, buf, target, JMP32_INSN_SIZE);",
        "		buf += JMP32_INSN_SIZE;",
        "	}",
        "",
        "	for (;buf < target; buf++)",
        "		*buf = INT3_INSN_OPCODE;",
        "}",
        "",
        "extern s32 __retpoline_sites[], __retpoline_sites_end[];",
        "extern s32 __return_sites[], __return_sites_end[];",
        "extern s32 __cfi_sites[], __cfi_sites_end[];",
        "extern s32 __ibt_endbr_seal[], __ibt_endbr_seal_end[];",
        "extern s32 __smp_locks[], __smp_locks_end[];",
        "void text_poke_early(void *addr, const void *opcode, size_t len);",
        "",
        "/*",
        " * Matches NOP and NOPL, not any of the other possible NOPs.",
        " */",
        "static bool insn_is_nop(struct insn *insn)",
        "{",
        "	/* Anything NOP, but no REP NOP */",
        "	if (insn->opcode.bytes[0] == 0x90 &&",
        "	    (!insn->prefixes.nbytes || insn->prefixes.bytes[0] != 0xF3))",
        "		return true;",
        "",
        "	/* NOPL */",
        "	if (insn->opcode.bytes[0] == 0x0F && insn->opcode.bytes[1] == 0x1F)",
        "		return true;",
        "",
        "	/* TODO: more nops */",
        "",
        "	return false;",
        "}",
        "",
        "/*",
        " * Find the offset of the first non-NOP instruction starting at @offset",
        " * but no further than @len.",
        " */",
        "static int skip_nops(u8 *buf, int offset, int len)",
        "{",
        "	struct insn insn;",
        "",
        "	for (; offset < len; offset += insn.length) {",
        "		if (insn_decode_kernel(&insn, &buf[offset]))",
        "			break;",
        "",
        "		if (!insn_is_nop(&insn))",
        "			break;",
        "	}",
        "",
        "	return offset;",
        "}",
        "",
        "/*",
        " * \"noinline\" to cause control flow change and thus invalidate I$ and",
        " * cause refetch after modification.",
        " */",
        "static void noinline optimize_nops(const u8 * const instr, u8 *buf, size_t len)",
        "{",
        "	for (int next, i = 0; i < len; i = next) {",
        "		struct insn insn;",
        "",
        "		if (insn_decode_kernel(&insn, &buf[i]))",
        "			return;",
        "",
        "		next = i + insn.length;",
        "",
        "		if (insn_is_nop(&insn)) {",
        "			int nop = i;",
        "",
        "			/* Has the NOP already been optimized? */",
        "			if (i + insn.length == len)",
        "				return;",
        "",
        "			next = skip_nops(buf, next, len);",
        "",
        "			add_nop(buf + nop, next - nop);",
        "			DUMP_BYTES(ALT, buf, len, \"%px: [%d:%d) optimized NOPs: \", instr, nop, next);",
        "		}",
        "	}",
        "}",
        "",
        "/*",
        " * In this context, \"source\" is where the instructions are placed in the",
        " * section .altinstr_replacement, for example during kernel build by the",
        " * toolchain.",
        " * \"Destination\" is where the instructions are being patched in by this",
        " * machinery.",
        " *",
        " * The source offset is:",
        " *",
        " *   src_imm = target - src_next_ip                  (1)",
        " *",
        " * and the target offset is:",
        " *",
        " *   dst_imm = target - dst_next_ip                  (2)",
        " *",
        " * so rework (1) as an expression for target like:",
        " *",
        " *   target = src_imm + src_next_ip                  (1a)",
        " *",
        " * and substitute in (2) to get:",
        " *",
        " *   dst_imm = (src_imm + src_next_ip) - dst_next_ip (3)",
        " *",
        " * Now, since the instruction stream is 'identical' at src and dst (it",
        " * is being copied after all) it can be stated that:",
        " *",
        " *   src_next_ip = src + ip_offset",
        " *   dst_next_ip = dst + ip_offset                   (4)",
        " *",
        " * Substitute (4) in (3) and observe ip_offset being cancelled out to",
        " * obtain:",
        " *",
        " *   dst_imm = src_imm + (src + ip_offset) - (dst + ip_offset)",
        " *           = src_imm + src - dst + ip_offset - ip_offset",
        " *           = src_imm + src - dst                   (5)",
        " *",
        " * IOW, only the relative displacement of the code block matters.",
        " */",
        "",
        "#define apply_reloc_n(n_, p_, d_)				\\",
        "	do {							\\",
        "		s32 v = *(s##n_ *)(p_);				\\",
        "		v += (d_);					\\",
        "		BUG_ON((v >> 31) != (v >> (n_-1)));		\\",
        "		*(s##n_ *)(p_) = (s##n_)v;			\\",
        "	} while (0)",
        "",
        "",
        "static __always_inline",
        "void apply_reloc(int n, void *ptr, uintptr_t diff)",
        "{",
        "	switch (n) {",
        "	case 1: apply_reloc_n(8, ptr, diff); break;",
        "	case 2: apply_reloc_n(16, ptr, diff); break;",
        "	case 4: apply_reloc_n(32, ptr, diff); break;",
        "	default: BUG();",
        "	}",
        "}",
        "",
        "static __always_inline",
        "bool need_reloc(unsigned long offset, u8 *src, size_t src_len)",
        "{",
        "	u8 *target = src + offset;",
        "	/*",
        "	 * If the target is inside the patched block, it's relative to the",
        "	 * block itself and does not need relocation.",
        "	 */",
        "	return (target < src || target > src + src_len);",
        "}",
        "",
        "static void __apply_relocation(u8 *buf, const u8 * const instr, size_t instrlen, u8 *repl, size_t repl_len)",
        "{",
        "	for (int next, i = 0; i < instrlen; i = next) {",
        "		struct insn insn;",
        "",
        "		if (WARN_ON_ONCE(insn_decode_kernel(&insn, &buf[i])))",
        "			return;",
        "",
        "		next = i + insn.length;",
        "",
        "		switch (insn.opcode.bytes[0]) {",
        "		case 0x0f:",
        "			if (insn.opcode.bytes[1] < 0x80 ||",
        "			    insn.opcode.bytes[1] > 0x8f)",
        "				break;",
        "",
        "			fallthrough;	/* Jcc.d32 */",
        "		case 0x70 ... 0x7f:	/* Jcc.d8 */",
        "		case JMP8_INSN_OPCODE:",
        "		case JMP32_INSN_OPCODE:",
        "		case CALL_INSN_OPCODE:",
        "			if (need_reloc(next + insn.immediate.value, repl, repl_len)) {",
        "				apply_reloc(insn.immediate.nbytes,",
        "					    buf + i + insn_offset_immediate(&insn),",
        "					    repl - instr);",
        "			}",
        "",
        "			/*",
        "			 * Where possible, convert JMP.d32 into JMP.d8.",
        "			 */",
        "			if (insn.opcode.bytes[0] == JMP32_INSN_OPCODE) {",
        "				s32 imm = insn.immediate.value;",
        "				imm += repl - instr;",
        "				imm += JMP32_INSN_SIZE - JMP8_INSN_SIZE;",
        "				if ((imm >> 31) == (imm >> 7)) {",
        "					buf[i+0] = JMP8_INSN_OPCODE;",
        "					buf[i+1] = (s8)imm;",
        "",
        "					memset(&buf[i+2], INT3_INSN_OPCODE, insn.length - 2);",
        "				}",
        "			}",
        "			break;",
        "		}",
        "",
        "		if (insn_rip_relative(&insn)) {",
        "			if (need_reloc(next + insn.displacement.value, repl, repl_len)) {",
        "				apply_reloc(insn.displacement.nbytes,",
        "					    buf + i + insn_offset_displacement(&insn),",
        "					    repl - instr);",
        "			}",
        "		}",
        "	}",
        "}",
        "",
        "void apply_relocation(u8 *buf, const u8 * const instr, size_t instrlen, u8 *repl, size_t repl_len)",
        "{",
        "	__apply_relocation(buf, instr, instrlen, repl, repl_len);",
        "	optimize_nops(instr, buf, instrlen);",
        "}",
        "",
        "/* Low-level backend functions usable from alternative code replacements. */",
        "DEFINE_ASM_FUNC(nop_func, \"\", .entry.text);",
        "EXPORT_SYMBOL_GPL(nop_func);",
        "",
        "noinstr void BUG_func(void)",
        "{",
        "	BUG();",
        "}",
        "EXPORT_SYMBOL(BUG_func);",
        "",
        "#define CALL_RIP_REL_OPCODE	0xff",
        "#define CALL_RIP_REL_MODRM	0x15",
        "",
        "/*",
        " * Rewrite the \"call BUG_func\" replacement to point to the target of the",
        " * indirect pv_ops call \"call *disp(%ip)\".",
        " */",
        "static int alt_replace_call(u8 *instr, u8 *insn_buff, struct alt_instr *a,",
        "			    struct module *mod)",
        "{",
        "	u8 *wr_instr = module_writable_address(mod, instr);",
        "	void *target, *bug = &BUG_func;",
        "	s32 disp;",
        "",
        "	if (a->replacementlen != 5 || insn_buff[0] != CALL_INSN_OPCODE) {",
        "		pr_err(\"ALT_FLAG_DIRECT_CALL set for a non-call replacement instruction\\n\");",
        "		BUG();",
        "	}",
        "",
        "	if (a->instrlen != 6 ||",
        "	    wr_instr[0] != CALL_RIP_REL_OPCODE ||",
        "	    wr_instr[1] != CALL_RIP_REL_MODRM) {",
        "		pr_err(\"ALT_FLAG_DIRECT_CALL set for unrecognized indirect call\\n\");",
        "		BUG();",
        "	}",
        "",
        "	/* Skip CALL_RIP_REL_OPCODE and CALL_RIP_REL_MODRM */",
        "	disp = *(s32 *)(wr_instr + 2);",
        "#ifdef CONFIG_X86_64",
        "	/* ff 15 00 00 00 00   call   *0x0(%rip) */",
        "	/* target address is stored at \"next instruction + disp\". */",
        "	target = *(void **)(instr + a->instrlen + disp);",
        "#else",
        "	/* ff 15 00 00 00 00   call   *0x0 */",
        "	/* target address is stored at disp. */",
        "	target = *(void **)disp;",
        "#endif",
        "	if (!target)",
        "		target = bug;",
        "",
        "	/* (BUG_func - .) + (target - BUG_func) := target - . */",
        "	*(s32 *)(insn_buff + 1) += target - bug;",
        "",
        "	if (target == &nop_func)",
        "		return 0;",
        "",
        "	return 5;",
        "}",
        "",
        "static inline u8 * instr_va(struct alt_instr *i)",
        "{",
        "	return (u8 *)&i->instr_offset + i->instr_offset;",
        "}",
        "",
        "/*",
        " * Replace instructions with better alternatives for this CPU type. This runs",
        " * before SMP is initialized to avoid SMP problems with self modifying code.",
        " * This implies that asymmetric systems where APs have less capabilities than",
        " * the boot processor are not handled. Tough. Make sure you disable such",
        " * features by hand.",
        " *",
        " * Marked \"noinline\" to cause control flow change and thus insn cache",
        " * to refetch changed I$ lines.",
        " */",
        "void __init_or_module noinline apply_alternatives(struct alt_instr *start,",
        "						  struct alt_instr *end,",
        "						  struct module *mod)",
        "{",
        "	u8 insn_buff[MAX_PATCH_LEN];",
        "	u8 *instr, *replacement;",
        "	struct alt_instr *a, *b;",
        "",
        "	DPRINTK(ALT, \"alt table %px, -> %px\", start, end);",
        "",
        "	/*",
        "	 * In the case CONFIG_X86_5LEVEL=y, KASAN_SHADOW_START is defined using",
        "	 * cpu_feature_enabled(X86_FEATURE_LA57) and is therefore patched here.",
        "	 * During the process, KASAN becomes confused seeing partial LA57",
        "	 * conversion and triggers a false-positive out-of-bound report.",
        "	 *",
        "	 * Disable KASAN until the patching is complete.",
        "	 */",
        "	kasan_disable_current();",
        "",
        "	/*",
        "	 * The scan order should be from start to end. A later scanned",
        "	 * alternative code can overwrite previously scanned alternative code.",
        "	 * Some kernel functions (e.g. memcpy, memset, etc) use this order to",
        "	 * patch code.",
        "	 *",
        "	 * So be careful if you want to change the scan order to any other",
        "	 * order.",
        "	 */",
        "	for (a = start; a < end; a++) {",
        "		int insn_buff_sz = 0;",
        "		u8 *wr_instr, *wr_replacement;",
        "",
        "		/*",
        "		 * In case of nested ALTERNATIVE()s the outer alternative might",
        "		 * add more padding. To ensure consistent patching find the max",
        "		 * padding for all alt_instr entries for this site (nested",
        "		 * alternatives result in consecutive entries).",
        "		 */",
        "		for (b = a+1; b < end && instr_va(b) == instr_va(a); b++) {",
        "			u8 len = max(a->instrlen, b->instrlen);",
        "			a->instrlen = b->instrlen = len;",
        "		}",
        "",
        "		instr = instr_va(a);",
        "		wr_instr = module_writable_address(mod, instr);",
        "",
        "		replacement = (u8 *)&a->repl_offset + a->repl_offset;",
        "		wr_replacement = module_writable_address(mod, replacement);",
        "",
        "		BUG_ON(a->instrlen > sizeof(insn_buff));",
        "		BUG_ON(a->cpuid >= (NCAPINTS + NBUGINTS) * 32);",
        "",
        "		/*",
        "		 * Patch if either:",
        "		 * - feature is present",
        "		 * - feature not present but ALT_FLAG_NOT is set to mean,",
        "		 *   patch if feature is *NOT* present.",
        "		 */",
        "		if (!boot_cpu_has(a->cpuid) == !(a->flags & ALT_FLAG_NOT)) {",
        "			memcpy(insn_buff, wr_instr, a->instrlen);",
        "			optimize_nops(instr, insn_buff, a->instrlen);",
        "			text_poke_early(wr_instr, insn_buff, a->instrlen);",
        "			continue;",
        "		}",
        "",
        "		DPRINTK(ALT, \"feat: %d*32+%d, old: (%pS (%px) len: %d), repl: (%px, len: %d) flags: 0x%x\",",
        "			a->cpuid >> 5,",
        "			a->cpuid & 0x1f,",
        "			instr, instr, a->instrlen,",
        "			replacement, a->replacementlen, a->flags);",
        "",
        "		memcpy(insn_buff, wr_replacement, a->replacementlen);",
        "		insn_buff_sz = a->replacementlen;",
        "",
        "		if (a->flags & ALT_FLAG_DIRECT_CALL) {",
        "			insn_buff_sz = alt_replace_call(instr, insn_buff, a,",
        "							mod);",
        "			if (insn_buff_sz < 0)",
        "				continue;",
        "		}",
        "",
        "		for (; insn_buff_sz < a->instrlen; insn_buff_sz++)",
        "			insn_buff[insn_buff_sz] = 0x90;",
        "",
        "		apply_relocation(insn_buff, instr, a->instrlen, replacement, a->replacementlen);",
        "",
        "		DUMP_BYTES(ALT, wr_instr, a->instrlen, \"%px:   old_insn: \", instr);",
        "		DUMP_BYTES(ALT, replacement, a->replacementlen, \"%px:   rpl_insn: \", replacement);",
        "		DUMP_BYTES(ALT, insn_buff, insn_buff_sz, \"%px: final_insn: \", instr);",
        "",
        "		text_poke_early(wr_instr, insn_buff, insn_buff_sz);",
        "	}",
        "",
        "	kasan_enable_current();",
        "}",
        "",
        "static inline bool is_jcc32(struct insn *insn)",
        "{",
        "	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */",
        "	return insn->opcode.bytes[0] == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80;",
        "}",
        "",
        "#if defined(CONFIG_MITIGATION_RETPOLINE) && defined(CONFIG_OBJTOOL)",
        "",
        "/*",
        " * CALL/JMP *%\\reg",
        " */",
        "static int emit_indirect(int op, int reg, u8 *bytes)",
        "{",
        "	int i = 0;",
        "	u8 modrm;",
        "",
        "	switch (op) {",
        "	case CALL_INSN_OPCODE:",
        "		modrm = 0x10; /* Reg = 2; CALL r/m */",
        "		break;",
        "",
        "	case JMP32_INSN_OPCODE:",
        "		modrm = 0x20; /* Reg = 4; JMP r/m */",
        "		break;",
        "",
        "	default:",
        "		WARN_ON_ONCE(1);",
        "		return -1;",
        "	}",
        "",
        "	if (reg >= 8) {",
        "		bytes[i++] = 0x41; /* REX.B prefix */",
        "		reg -= 8;",
        "	}",
        "",
        "	modrm |= 0xc0; /* Mod = 3 */",
        "	modrm += reg;",
        "",
        "	bytes[i++] = 0xff; /* opcode */",
        "	bytes[i++] = modrm;",
        "",
        "	return i;",
        "}",
        "",
        "static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8 *bytes)",
        "{",
        "	u8 op = insn->opcode.bytes[0];",
        "	int i = 0;",
        "",
        "	/*",
        "	 * Clang does 'weird' Jcc __x86_indirect_thunk_r11 conditional",
        "	 * tail-calls. Deal with them.",
        "	 */",
        "	if (is_jcc32(insn)) {",
        "		bytes[i++] = op;",
        "		op = insn->opcode.bytes[1];",
        "		goto clang_jcc;",
        "	}",
        "",
        "	if (insn->length == 6)",
        "		bytes[i++] = 0x2e; /* CS-prefix */",
        "",
        "	switch (op) {",
        "	case CALL_INSN_OPCODE:",
        "		__text_gen_insn(bytes+i, op, addr+i,",
        "				__x86_indirect_call_thunk_array[reg],",
        "				CALL_INSN_SIZE);",
        "		i += CALL_INSN_SIZE;",
        "		break;",
        "",
        "	case JMP32_INSN_OPCODE:",
        "clang_jcc:",
        "		__text_gen_insn(bytes+i, op, addr+i,",
        "				__x86_indirect_jump_thunk_array[reg],",
        "				JMP32_INSN_SIZE);",
        "		i += JMP32_INSN_SIZE;",
        "		break;",
        "",
        "	default:",
        "		WARN(1, \"%pS %px %*ph\\n\", addr, addr, 6, addr);",
        "		return -1;",
        "	}",
        "",
        "	WARN_ON_ONCE(i != insn->length);",
        "",
        "	return i;",
        "}",
        "",
        "/*",
        " * Rewrite the compiler generated retpoline thunk calls.",
        " *",
        " * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate",
        " * indirect instructions, avoiding the extra indirection.",
        " *",
        " * For example, convert:",
        " *",
        " *   CALL __x86_indirect_thunk_\\reg",
        " *",
        " * into:",
        " *",
        " *   CALL *%\\reg",
        " *",
        " * It also tries to inline spectre_v2=retpoline,lfence when size permits.",
        " */",
        "static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)",
        "{",
        "	retpoline_thunk_t *target;",
        "	int reg, ret, i = 0;",
        "	u8 op, cc;",
        "",
        "	target = addr + insn->length + insn->immediate.value;",
        "	reg = target - __x86_indirect_thunk_array;",
        "",
        "	if (WARN_ON_ONCE(reg & ~0xf))",
        "		return -1;",
        "",
        "	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */",
        "	BUG_ON(reg == 4);",
        "",
        "	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&",
        "	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {",
        "		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))",
        "			return emit_call_track_retpoline(addr, insn, reg, bytes);",
        "",
        "		return -1;",
        "	}",
        "",
        "	op = insn->opcode.bytes[0];",
        "",
        "	/*",
        "	 * Convert:",
        "	 *",
        "	 *   Jcc.d32 __x86_indirect_thunk_\\reg",
        "	 *",
        "	 * into:",
        "	 *",
        "	 *   Jncc.d8 1f",
        "	 *   [ LFENCE ]",
        "	 *   JMP *%\\reg",
        "	 *   [ NOP ]",
        "	 * 1:",
        "	 */",
        "	if (is_jcc32(insn)) {",
        "		cc = insn->opcode.bytes[1] & 0xf;",
        "		cc ^= 1; /* invert condition */",
        "",
        "		bytes[i++] = 0x70 + cc;        /* Jcc.d8 */",
        "		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */",
        "",
        "		/* Continue as if: JMP.d32 __x86_indirect_thunk_\\reg */",
        "		op = JMP32_INSN_OPCODE;",
        "	}",
        "",
        "	/*",
        "	 * For RETPOLINE_LFENCE: prepend the indirect CALL/JMP with an LFENCE.",
        "	 */",
        "	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {",
        "		bytes[i++] = 0x0f;",
        "		bytes[i++] = 0xae;",
        "		bytes[i++] = 0xe8; /* LFENCE */",
        "	}",
        "",
        "	ret = emit_indirect(op, reg, bytes + i);",
        "	if (ret < 0)",
        "		return ret;",
        "	i += ret;",
        "",
        "	/*",
        "	 * The compiler is supposed to EMIT an INT3 after every unconditional",
        "	 * JMP instruction due to AMD BTC. However, if the compiler is too old",
        "	 * or MITIGATION_SLS isn't enabled, we still need an INT3 after",
        "	 * indirect JMPs even on Intel.",
        "	 */",
        "	if (op == JMP32_INSN_OPCODE && i < insn->length)",
        "		bytes[i++] = INT3_INSN_OPCODE;",
        "",
        "	for (; i < insn->length;)",
        "		bytes[i++] = BYTES_NOP1;",
        "",
        "	return i;",
        "}",
        "",
        "/*",
        " * Generated by 'objtool --retpoline'.",
        " */",
        "void __init_or_module noinline apply_retpolines(s32 *start, s32 *end,",
        "						struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "		struct insn insn;",
        "		int len, ret;",
        "		u8 bytes[16];",
        "		u8 op1, op2;",
        "",
        "		ret = insn_decode_kernel(&insn, wr_addr);",
        "		if (WARN_ON_ONCE(ret < 0))",
        "			continue;",
        "",
        "		op1 = insn.opcode.bytes[0];",
        "		op2 = insn.opcode.bytes[1];",
        "",
        "		switch (op1) {",
        "		case CALL_INSN_OPCODE:",
        "		case JMP32_INSN_OPCODE:",
        "			break;",
        "",
        "		case 0x0f: /* escape */",
        "			if (op2 >= 0x80 && op2 <= 0x8f)",
        "				break;",
        "			fallthrough;",
        "		default:",
        "			WARN_ON_ONCE(1);",
        "			continue;",
        "		}",
        "",
        "		DPRINTK(RETPOLINE, \"retpoline at: %pS (%px) len: %d to: %pS\",",
        "			addr, addr, insn.length,",
        "			addr + insn.length + insn.immediate.value);",
        "",
        "		len = patch_retpoline(addr, &insn, bytes);",
        "		if (len == insn.length) {",
        "			optimize_nops(addr, bytes, len);",
        "			DUMP_BYTES(RETPOLINE, ((u8*)wr_addr),  len, \"%px: orig: \", addr);",
        "			DUMP_BYTES(RETPOLINE, ((u8*)bytes), len, \"%px: repl: \", addr);",
        "			text_poke_early(wr_addr, bytes, len);",
        "		}",
        "	}",
        "}",
        "",
        "#ifdef CONFIG_MITIGATION_RETHUNK",
        "",
        "/*",
        " * Rewrite the compiler generated return thunk tail-calls.",
        " *",
        " * For example, convert:",
        " *",
        " *   JMP __x86_return_thunk",
        " *",
        " * into:",
        " *",
        " *   RET",
        " */",
        "static int patch_return(void *addr, struct insn *insn, u8 *bytes)",
        "{",
        "	int i = 0;",
        "",
        "	/* Patch the custom return thunks... */",
        "	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {",
        "		i = JMP32_INSN_SIZE;",
        "		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);",
        "	} else {",
        "		/* ... or patch them out if not needed. */",
        "		bytes[i++] = RET_INSN_OPCODE;",
        "	}",
        "",
        "	for (; i < insn->length;)",
        "		bytes[i++] = INT3_INSN_OPCODE;",
        "	return i;",
        "}",
        "",
        "void __init_or_module noinline apply_returns(s32 *start, s32 *end,",
        "					     struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))",
        "		static_call_force_reinit();",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *dest = NULL, *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "		struct insn insn;",
        "		int len, ret;",
        "		u8 bytes[16];",
        "		u8 op;",
        "",
        "		ret = insn_decode_kernel(&insn, wr_addr);",
        "		if (WARN_ON_ONCE(ret < 0))",
        "			continue;",
        "",
        "		op = insn.opcode.bytes[0];",
        "		if (op == JMP32_INSN_OPCODE)",
        "			dest = addr + insn.length + insn.immediate.value;",
        "",
        "		if (__static_call_fixup(addr, op, dest) ||",
        "		    WARN_ONCE(dest != &__x86_return_thunk,",
        "			      \"missing return thunk: %pS-%pS: %*ph\",",
        "			      addr, dest, 5, addr))",
        "			continue;",
        "",
        "		DPRINTK(RET, \"return thunk at: %pS (%px) len: %d to: %pS\",",
        "			addr, addr, insn.length,",
        "			addr + insn.length + insn.immediate.value);",
        "",
        "		len = patch_return(addr, &insn, bytes);",
        "		if (len == insn.length) {",
        "			DUMP_BYTES(RET, ((u8*)wr_addr),  len, \"%px: orig: \", addr);",
        "			DUMP_BYTES(RET, ((u8*)bytes), len, \"%px: repl: \", addr);",
        "			text_poke_early(wr_addr, bytes, len);",
        "		}",
        "	}",
        "}",
        "#else",
        "void __init_or_module noinline apply_returns(s32 *start, s32 *end,",
        "					     struct module *mod) { }",
        "#endif /* CONFIG_MITIGATION_RETHUNK */",
        "",
        "#else /* !CONFIG_MITIGATION_RETPOLINE || !CONFIG_OBJTOOL */",
        "",
        "void __init_or_module noinline apply_retpolines(s32 *start, s32 *end,",
        "						struct module *mod) { }",
        "void __init_or_module noinline apply_returns(s32 *start, s32 *end,",
        "					     struct module *mod) { }",
        "",
        "#endif /* CONFIG_MITIGATION_RETPOLINE && CONFIG_OBJTOOL */",
        "",
        "#ifdef CONFIG_X86_KERNEL_IBT",
        "",
        "static void poison_cfi(void *addr, void *wr_addr);",
        "",
        "static void __init_or_module poison_endbr(void *addr, void *wr_addr, bool warn)",
        "{",
        "	u32 endbr, poison = gen_endbr_poison();",
        "",
        "	if (WARN_ON_ONCE(get_kernel_nofault(endbr, wr_addr)))",
        "		return;",
        "",
        "	if (!is_endbr(endbr)) {",
        "		WARN_ON_ONCE(warn);",
        "		return;",
        "	}",
        "",
        "	DPRINTK(ENDBR, \"ENDBR at: %pS (%px)\", addr, addr);",
        "",
        "	/*",
        "	 * When we have IBT, the lack of ENDBR will trigger #CP",
        "	 */",
        "	DUMP_BYTES(ENDBR, ((u8*)addr), 4, \"%px: orig: \", addr);",
        "	DUMP_BYTES(ENDBR, ((u8*)&poison), 4, \"%px: repl: \", addr);",
        "	text_poke_early(wr_addr, &poison, 4);",
        "}",
        "",
        "/*",
        " * Generated by: objtool --ibt",
        " *",
        " * Seal the functions for indirect calls by clobbering the ENDBR instructions",
        " * and the kCFI hash value.",
        " */",
        "void __init_or_module noinline apply_seal_endbr(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "",
        "		poison_endbr(addr, wr_addr, true);",
        "		if (IS_ENABLED(CONFIG_FINEIBT))",
        "			poison_cfi(addr - 16, wr_addr - 16);",
        "	}",
        "}",
        "",
        "#else",
        "",
        "void __init_or_module apply_seal_endbr(s32 *start, s32 *end, struct module *mod) { }",
        "",
        "#endif /* CONFIG_X86_KERNEL_IBT */",
        "",
        "#ifdef CONFIG_CFI_AUTO_DEFAULT",
        "#define __CFI_DEFAULT	CFI_AUTO",
        "#elif defined(CONFIG_CFI_CLANG)",
        "#define __CFI_DEFAULT	CFI_KCFI",
        "#else",
        "#define __CFI_DEFAULT	CFI_OFF",
        "#endif",
        "",
        "enum cfi_mode cfi_mode __ro_after_init = __CFI_DEFAULT;",
        "",
        "#ifdef CONFIG_CFI_CLANG",
        "struct bpf_insn;",
        "",
        "/* Must match bpf_func_t / DEFINE_BPF_PROG_RUN() */",
        "extern unsigned int __bpf_prog_runX(const void *ctx,",
        "				    const struct bpf_insn *insn);",
        "",
        "/*",
        " * Force a reference to the external symbol so the compiler generates",
        " * __kcfi_typid.",
        " */",
        "__ADDRESSABLE(__bpf_prog_runX);",
        "",
        "/* u32 __ro_after_init cfi_bpf_hash = __kcfi_typeid___bpf_prog_runX; */",
        "asm (",
        "\"	.pushsection	.data..ro_after_init,\\\"aw\\\",@progbits	\\n\"",
        "\"	.type	cfi_bpf_hash,@object				\\n\"",
        "\"	.globl	cfi_bpf_hash					\\n\"",
        "\"	.p2align	2, 0x0					\\n\"",
        "\"cfi_bpf_hash:							\\n\"",
        "\"	.long	__kcfi_typeid___bpf_prog_runX			\\n\"",
        "\"	.size	cfi_bpf_hash, 4					\\n\"",
        "\"	.popsection						\\n\"",
        ");",
        "",
        "/* Must match bpf_callback_t */",
        "extern u64 __bpf_callback_fn(u64, u64, u64, u64, u64);",
        "",
        "__ADDRESSABLE(__bpf_callback_fn);",
        "",
        "/* u32 __ro_after_init cfi_bpf_subprog_hash = __kcfi_typeid___bpf_callback_fn; */",
        "asm (",
        "\"	.pushsection	.data..ro_after_init,\\\"aw\\\",@progbits	\\n\"",
        "\"	.type	cfi_bpf_subprog_hash,@object			\\n\"",
        "\"	.globl	cfi_bpf_subprog_hash				\\n\"",
        "\"	.p2align	2, 0x0					\\n\"",
        "\"cfi_bpf_subprog_hash:						\\n\"",
        "\"	.long	__kcfi_typeid___bpf_callback_fn			\\n\"",
        "\"	.size	cfi_bpf_subprog_hash, 4				\\n\"",
        "\"	.popsection						\\n\"",
        ");",
        "",
        "u32 cfi_get_func_hash(void *func)",
        "{",
        "	u32 hash;",
        "",
        "	func -= cfi_get_offset();",
        "	switch (cfi_mode) {",
        "	case CFI_FINEIBT:",
        "		func += 7;",
        "		break;",
        "	case CFI_KCFI:",
        "		func += 1;",
        "		break;",
        "	default:",
        "		return 0;",
        "	}",
        "",
        "	if (get_kernel_nofault(hash, func))",
        "		return 0;",
        "",
        "	return hash;",
        "}",
        "#endif",
        "",
        "#ifdef CONFIG_FINEIBT",
        "",
        "static bool cfi_rand __ro_after_init = true;",
        "static u32  cfi_seed __ro_after_init;",
        "",
        "/*",
        " * Re-hash the CFI hash with a boot-time seed while making sure the result is",
        " * not a valid ENDBR instruction.",
        " */",
        "static u32 cfi_rehash(u32 hash)",
        "{",
        "	hash ^= cfi_seed;",
        "	while (unlikely(is_endbr(hash) || is_endbr(-hash))) {",
        "		bool lsb = hash & 1;",
        "		hash >>= 1;",
        "		if (lsb)",
        "			hash ^= 0x80200003;",
        "	}",
        "	return hash;",
        "}",
        "",
        "static __init int cfi_parse_cmdline(char *str)",
        "{",
        "	if (!str)",
        "		return -EINVAL;",
        "",
        "	while (str) {",
        "		char *next = strchr(str, ',');",
        "		if (next) {",
        "			*next = 0;",
        "			next++;",
        "		}",
        "",
        "		if (!strcmp(str, \"auto\")) {",
        "			cfi_mode = CFI_AUTO;",
        "		} else if (!strcmp(str, \"off\")) {",
        "			cfi_mode = CFI_OFF;",
        "			cfi_rand = false;",
        "		} else if (!strcmp(str, \"kcfi\")) {",
        "			cfi_mode = CFI_KCFI;",
        "		} else if (!strcmp(str, \"fineibt\")) {",
        "			cfi_mode = CFI_FINEIBT;",
        "		} else if (!strcmp(str, \"norand\")) {",
        "			cfi_rand = false;",
        "		} else {",
        "			pr_err(\"Ignoring unknown cfi option (%s).\", str);",
        "		}",
        "",
        "		str = next;",
        "	}",
        "",
        "	return 0;",
        "}",
        "early_param(\"cfi\", cfi_parse_cmdline);",
        "",
        "/*",
        " * kCFI						FineIBT",
        " *",
        " * __cfi_\\func:					__cfi_\\func:",
        " *	movl   $0x12345678,%eax		// 5	     endbr64			// 4",
        " *	nop					     subl   $0x12345678,%r10d   // 7",
        " *	nop					     jz     1f			// 2",
        " *	nop					     ud2			// 2",
        " *	nop					1:   nop			// 1",
        " *	nop",
        " *	nop",
        " *	nop",
        " *	nop",
        " *	nop",
        " *	nop",
        " *	nop",
        " *",
        " *",
        " * caller:					caller:",
        " *	movl	$(-0x12345678),%r10d	 // 6	     movl   $0x12345678,%r10d	// 6",
        " *	addl	$-15(%r11),%r10d	 // 4	     sub    $16,%r11		// 4",
        " *	je	1f			 // 2	     nop4			// 4",
        " *	ud2				 // 2",
        " * 1:	call	__x86_indirect_thunk_r11 // 5	     call   *%r11; nop2;	// 5",
        " *",
        " */",
        "",
        "asm(	\".pushsection .rodata			\\n\"",
        "	\"fineibt_preamble_start:		\\n\"",
        "	\"	endbr64				\\n\"",
        "	\"	subl	$0x12345678, %r10d	\\n\"",
        "	\"	je	fineibt_preamble_end	\\n\"",
        "	\"	ud2				\\n\"",
        "	\"	nop				\\n\"",
        "	\"fineibt_preamble_end:			\\n\"",
        "	\".popsection\\n\"",
        ");",
        "",
        "extern u8 fineibt_preamble_start[];",
        "extern u8 fineibt_preamble_end[];",
        "",
        "#define fineibt_preamble_size (fineibt_preamble_end - fineibt_preamble_start)",
        "#define fineibt_preamble_hash 7",
        "",
        "asm(	\".pushsection .rodata			\\n\"",
        "	\"fineibt_caller_start:			\\n\"",
        "	\"	movl	$0x12345678, %r10d	\\n\"",
        "	\"	sub	$16, %r11		\\n\"",
        "	ASM_NOP4",
        "	\"fineibt_caller_end:			\\n\"",
        "	\".popsection				\\n\"",
        ");",
        "",
        "extern u8 fineibt_caller_start[];",
        "extern u8 fineibt_caller_end[];",
        "",
        "#define fineibt_caller_size (fineibt_caller_end - fineibt_caller_start)",
        "#define fineibt_caller_hash 2",
        "",
        "#define fineibt_caller_jmp (fineibt_caller_size - 2)",
        "",
        "static u32 decode_preamble_hash(void *addr)",
        "{",
        "	u8 *p = addr;",
        "",
        "	/* b8 78 56 34 12          mov    $0x12345678,%eax */",
        "	if (p[0] == 0xb8)",
        "		return *(u32 *)(addr + 1);",
        "",
        "	return 0; /* invalid hash value */",
        "}",
        "",
        "static u32 decode_caller_hash(void *addr)",
        "{",
        "	u8 *p = addr;",
        "",
        "	/* 41 ba 78 56 34 12       mov    $0x12345678,%r10d */",
        "	if (p[0] == 0x41 && p[1] == 0xba)",
        "		return -*(u32 *)(addr + 2);",
        "",
        "	/* e8 0c 78 56 34 12	   jmp.d8  +12 */",
        "	if (p[0] == JMP8_INSN_OPCODE && p[1] == fineibt_caller_jmp)",
        "		return -*(u32 *)(addr + 2);",
        "",
        "	return 0; /* invalid hash value */",
        "}",
        "",
        "/* .retpoline_sites */",
        "static int cfi_disable_callers(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	/*",
        "	 * Disable kCFI by patching in a JMP.d8, this leaves the hash immediate",
        "	 * in tact for later usage. Also see decode_caller_hash() and",
        "	 * cfi_rewrite_callers().",
        "	 */",
        "	const u8 jmp[] = { JMP8_INSN_OPCODE, fineibt_caller_jmp };",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr;",
        "		u32 hash;",
        "",
        "		addr -= fineibt_caller_size;",
        "		wr_addr = module_writable_address(mod, addr);",
        "		hash = decode_caller_hash(wr_addr);",
        "",
        "		if (!hash) /* nocfi callers */",
        "			continue;",
        "",
        "		text_poke_early(wr_addr, jmp, 2);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static int cfi_enable_callers(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	/*",
        "	 * Re-enable kCFI, undo what cfi_disable_callers() did.",
        "	 */",
        "	const u8 mov[] = { 0x41, 0xba };",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr;",
        "		u32 hash;",
        "",
        "		addr -= fineibt_caller_size;",
        "		wr_addr = module_writable_address(mod, addr);",
        "		hash = decode_caller_hash(wr_addr);",
        "		if (!hash) /* nocfi callers */",
        "			continue;",
        "",
        "		text_poke_early(wr_addr, mov, 2);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/* .cfi_sites */",
        "static int cfi_rand_preamble(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "		u32 hash;",
        "",
        "		hash = decode_preamble_hash(wr_addr);",
        "		if (WARN(!hash, \"no CFI hash found at: %pS %px %*ph\\n\",",
        "			 addr, addr, 5, addr))",
        "			return -EINVAL;",
        "",
        "		hash = cfi_rehash(hash);",
        "		text_poke_early(wr_addr + 1, &hash, 4);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static int cfi_rewrite_preamble(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "		u32 hash;",
        "",
        "		hash = decode_preamble_hash(wr_addr);",
        "		if (WARN(!hash, \"no CFI hash found at: %pS %px %*ph\\n\",",
        "			 addr, addr, 5, addr))",
        "			return -EINVAL;",
        "",
        "		text_poke_early(wr_addr, fineibt_preamble_start, fineibt_preamble_size);",
        "		WARN_ON(*(u32 *)(wr_addr + fineibt_preamble_hash) != 0x12345678);",
        "		text_poke_early(wr_addr + fineibt_preamble_hash, &hash, 4);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static void cfi_rewrite_endbr(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr = module_writable_address(mod, addr);",
        "",
        "		poison_endbr(addr + 16, wr_addr + 16, false);",
        "	}",
        "}",
        "",
        "/* .retpoline_sites */",
        "static int cfi_rand_callers(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr;",
        "		u32 hash;",
        "",
        "		addr -= fineibt_caller_size;",
        "		wr_addr = module_writable_address(mod, addr);",
        "		hash = decode_caller_hash(wr_addr);",
        "		if (hash) {",
        "			hash = -cfi_rehash(hash);",
        "			text_poke_early(wr_addr + 2, &hash, 4);",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static int cfi_rewrite_callers(s32 *start, s32 *end, struct module *mod)",
        "{",
        "	s32 *s;",
        "",
        "	for (s = start; s < end; s++) {",
        "		void *addr = (void *)s + *s;",
        "		void *wr_addr;",
        "		u32 hash;",
        "",
        "		addr -= fineibt_caller_size;",
        "		wr_addr = module_writable_address(mod, addr);",
        "		hash = decode_caller_hash(wr_addr);",
        "		if (hash) {",
        "			text_poke_early(wr_addr, fineibt_caller_start, fineibt_caller_size);",
        "			WARN_ON(*(u32 *)(wr_addr + fineibt_caller_hash) != 0x12345678);",
        "			text_poke_early(wr_addr + fineibt_caller_hash, &hash, 4);",
        "		}",
        "		/* rely on apply_retpolines() */",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,",
        "			    s32 *start_cfi, s32 *end_cfi, struct module *mod)",
        "{",
        "	bool builtin = mod ? false : true;",
        "	int ret;",
        "",
        "	if (WARN_ONCE(fineibt_preamble_size != 16,",
        "		      \"FineIBT preamble wrong size: %ld\", fineibt_preamble_size))",
        "		return;",
        "",
        "	if (cfi_mode == CFI_AUTO) {",
        "		cfi_mode = CFI_KCFI;",
        "		if (HAS_KERNEL_IBT && cpu_feature_enabled(X86_FEATURE_IBT))",
        "			cfi_mode = CFI_FINEIBT;",
        "	}",
        "",
        "	/*",
        "	 * Rewrite the callers to not use the __cfi_ stubs, such that we might",
        "	 * rewrite them. This disables all CFI. If this succeeds but any of the",
        "	 * later stages fails, we're without CFI.",
        "	 */",
        "	ret = cfi_disable_callers(start_retpoline, end_retpoline, mod);",
        "	if (ret)",
        "		goto err;",
        "",
        "	if (cfi_rand) {",
        "		if (builtin) {",
        "			cfi_seed = get_random_u32();",
        "			cfi_bpf_hash = cfi_rehash(cfi_bpf_hash);",
        "			cfi_bpf_subprog_hash = cfi_rehash(cfi_bpf_subprog_hash);",
        "		}",
        "",
        "		ret = cfi_rand_preamble(start_cfi, end_cfi, mod);",
        "		if (ret)",
        "			goto err;",
        "",
        "		ret = cfi_rand_callers(start_retpoline, end_retpoline, mod);",
        "		if (ret)",
        "			goto err;",
        "	}",
        "",
        "	switch (cfi_mode) {",
        "	case CFI_OFF:",
        "		if (builtin)",
        "			pr_info(\"Disabling CFI\\n\");",
        "		return;",
        "",
        "	case CFI_KCFI:",
        "		ret = cfi_enable_callers(start_retpoline, end_retpoline, mod);",
        "		if (ret)",
        "			goto err;",
        "",
        "		if (builtin)",
        "			pr_info(\"Using kCFI\\n\");",
        "		return;",
        "",
        "	case CFI_FINEIBT:",
        "		/* place the FineIBT preamble at func()-16 */",
        "		ret = cfi_rewrite_preamble(start_cfi, end_cfi, mod);",
        "		if (ret)",
        "			goto err;",
        "",
        "		/* rewrite the callers to target func()-16 */",
        "		ret = cfi_rewrite_callers(start_retpoline, end_retpoline, mod);",
        "		if (ret)",
        "			goto err;",
        "",
        "		/* now that nobody targets func()+0, remove ENDBR there */",
        "		cfi_rewrite_endbr(start_cfi, end_cfi, mod);",
        "",
        "		if (builtin)",
        "			pr_info(\"Using FineIBT CFI\\n\");",
        "		return;",
        "",
        "	default:",
        "		break;",
        "	}",
        "",
        "err:",
        "	pr_err(\"Something went horribly wrong trying to rewrite the CFI implementation.\\n\");",
        "}",
        "",
        "static inline void poison_hash(void *addr)",
        "{",
        "	*(u32 *)addr = 0;",
        "}",
        "",
        "static void poison_cfi(void *addr, void *wr_addr)",
        "{",
        "	switch (cfi_mode) {",
        "	case CFI_FINEIBT:",
        "		/*",
        "		 * __cfi_\\func:",
        "		 *	osp nopl (%rax)",
        "		 *	subl	$0, %r10d",
        "		 *	jz	1f",
        "		 *	ud2",
        "		 * 1:	nop",
        "		 */",
        "		poison_endbr(addr, wr_addr, false);",
        "		poison_hash(wr_addr + fineibt_preamble_hash);",
        "		break;",
        "",
        "	case CFI_KCFI:",
        "		/*",
        "		 * __cfi_\\func:",
        "		 *	movl	$0, %eax",
        "		 *	.skip	11, 0x90",
        "		 */",
        "		poison_hash(wr_addr + 1);",
        "		break;",
        "",
        "	default:",
        "		break;",
        "	}",
        "}",
        "",
        "#else",
        "",
        "static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,",
        "			    s32 *start_cfi, s32 *end_cfi, struct module *mod)",
        "{",
        "}",
        "",
        "#ifdef CONFIG_X86_KERNEL_IBT",
        "static void poison_cfi(void *addr, void *wr_addr) { }",
        "#endif",
        "",
        "#endif",
        "",
        "void apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,",
        "		   s32 *start_cfi, s32 *end_cfi, struct module *mod)",
        "{",
        "	return __apply_fineibt(start_retpoline, end_retpoline,",
        "			       start_cfi, end_cfi, mod);",
        "}",
        "",
        "#ifdef CONFIG_SMP",
        "static void alternatives_smp_lock(const s32 *start, const s32 *end,",
        "				  u8 *text, u8 *text_end)",
        "{",
        "	const s32 *poff;",
        "",
        "	for (poff = start; poff < end; poff++) {",
        "		u8 *ptr = (u8 *)poff + *poff;",
        "",
        "		if (!*poff || ptr < text || ptr >= text_end)",
        "			continue;",
        "		/* turn DS segment override prefix into lock prefix */",
        "		if (*ptr == 0x3e)",
        "			text_poke(ptr, ((unsigned char []){0xf0}), 1);",
        "	}",
        "}",
        "",
        "static void alternatives_smp_unlock(const s32 *start, const s32 *end,",
        "				    u8 *text, u8 *text_end)",
        "{",
        "	const s32 *poff;",
        "",
        "	for (poff = start; poff < end; poff++) {",
        "		u8 *ptr = (u8 *)poff + *poff;",
        "",
        "		if (!*poff || ptr < text || ptr >= text_end)",
        "			continue;",
        "		/* turn lock prefix into DS segment override prefix */",
        "		if (*ptr == 0xf0)",
        "			text_poke(ptr, ((unsigned char []){0x3E}), 1);",
        "	}",
        "}",
        "",
        "struct smp_alt_module {",
        "	/* what is this ??? */",
        "	struct module	*mod;",
        "	char		*name;",
        "",
        "	/* ptrs to lock prefixes */",
        "	const s32	*locks;",
        "	const s32	*locks_end;",
        "",
        "	/* .text segment, needed to avoid patching init code ;) */",
        "	u8		*text;",
        "	u8		*text_end;",
        "",
        "	struct list_head next;",
        "};",
        "static LIST_HEAD(smp_alt_modules);",
        "static bool uniproc_patched = false;	/* protected by text_mutex */",
        "",
        "void __init_or_module alternatives_smp_module_add(struct module *mod,",
        "						  char *name,",
        "						  void *locks, void *locks_end,",
        "						  void *text,  void *text_end)",
        "{",
        "	struct smp_alt_module *smp;",
        "",
        "	mutex_lock(&text_mutex);",
        "	if (!uniproc_patched)",
        "		goto unlock;",
        "",
        "	if (num_possible_cpus() == 1)",
        "		/* Don't bother remembering, we'll never have to undo it. */",
        "		goto smp_unlock;",
        "",
        "	smp = kzalloc(sizeof(*smp), GFP_KERNEL);",
        "	if (NULL == smp)",
        "		/* we'll run the (safe but slow) SMP code then ... */",
        "		goto unlock;",
        "",
        "	smp->mod	= mod;",
        "	smp->name	= name;",
        "	smp->locks	= locks;",
        "	smp->locks_end	= locks_end;",
        "	smp->text	= text;",
        "	smp->text_end	= text_end;",
        "	DPRINTK(SMP, \"locks %p -> %p, text %p -> %p, name %s\\n\",",
        "		smp->locks, smp->locks_end,",
        "		smp->text, smp->text_end, smp->name);",
        "",
        "	list_add_tail(&smp->next, &smp_alt_modules);",
        "smp_unlock:",
        "	alternatives_smp_unlock(locks, locks_end, text, text_end);",
        "unlock:",
        "	mutex_unlock(&text_mutex);",
        "}",
        "",
        "void __init_or_module alternatives_smp_module_del(struct module *mod)",
        "{",
        "	struct smp_alt_module *item;",
        "",
        "	mutex_lock(&text_mutex);",
        "	list_for_each_entry(item, &smp_alt_modules, next) {",
        "		if (mod != item->mod)",
        "			continue;",
        "		list_del(&item->next);",
        "		kfree(item);",
        "		break;",
        "	}",
        "	mutex_unlock(&text_mutex);",
        "}",
        "",
        "void alternatives_enable_smp(void)",
        "{",
        "	struct smp_alt_module *mod;",
        "",
        "	/* Why bother if there are no other CPUs? */",
        "	BUG_ON(num_possible_cpus() == 1);",
        "",
        "	mutex_lock(&text_mutex);",
        "",
        "	if (uniproc_patched) {",
        "		pr_info(\"switching to SMP code\\n\");",
        "		BUG_ON(num_online_cpus() != 1);",
        "		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);",
        "		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);",
        "		list_for_each_entry(mod, &smp_alt_modules, next)",
        "			alternatives_smp_lock(mod->locks, mod->locks_end,",
        "					      mod->text, mod->text_end);",
        "		uniproc_patched = false;",
        "	}",
        "	mutex_unlock(&text_mutex);",
        "}",
        "",
        "/*",
        " * Return 1 if the address range is reserved for SMP-alternatives.",
        " * Must hold text_mutex.",
        " */",
        "int alternatives_text_reserved(void *start, void *end)",
        "{",
        "	struct smp_alt_module *mod;",
        "	const s32 *poff;",
        "	u8 *text_start = start;",
        "	u8 *text_end = end;",
        "",
        "	lockdep_assert_held(&text_mutex);",
        "",
        "	list_for_each_entry(mod, &smp_alt_modules, next) {",
        "		if (mod->text > text_end || mod->text_end < text_start)",
        "			continue;",
        "		for (poff = mod->locks; poff < mod->locks_end; poff++) {",
        "			const u8 *ptr = (const u8 *)poff + *poff;",
        "",
        "			if (text_start <= ptr && text_end > ptr)",
        "				return 1;",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "#endif /* CONFIG_SMP */",
        "",
        "/*",
        " * Self-test for the INT3 based CALL emulation code.",
        " *",
        " * This exercises int3_emulate_call() to make sure INT3 pt_regs are set up",
        " * properly and that there is a stack gap between the INT3 frame and the",
        " * previous context. Without this gap doing a virtual PUSH on the interrupted",
        " * stack would corrupt the INT3 IRET frame.",
        " *",
        " * See entry_{32,64}.S for more details.",
        " */",
        "",
        "/*",
        " * We define the int3_magic() function in assembly to control the calling",
        " * convention such that we can 'call' it from assembly.",
        " */",
        "",
        "extern void int3_magic(unsigned int *ptr); /* defined in asm */",
        "",
        "asm (",
        "\"	.pushsection	.init.text, \\\"ax\\\", @progbits\\n\"",
        "\"	.type		int3_magic, @function\\n\"",
        "\"int3_magic:\\n\"",
        "	ANNOTATE_NOENDBR",
        "\"	movl	$1, (%\" _ASM_ARG1 \")\\n\"",
        "	ASM_RET",
        "\"	.size		int3_magic, .-int3_magic\\n\"",
        "\"	.popsection\\n\"",
        ");",
        "",
        "extern void int3_selftest_ip(void); /* defined in asm below */",
        "",
        "static int __init",
        "int3_exception_notify(struct notifier_block *self, unsigned long val, void *data)",
        "{",
        "	unsigned long selftest = (unsigned long)&int3_selftest_ip;",
        "	struct die_args *args = data;",
        "	struct pt_regs *regs = args->regs;",
        "",
        "	OPTIMIZER_HIDE_VAR(selftest);",
        "",
        "	if (!regs || user_mode(regs))",
        "		return NOTIFY_DONE;",
        "",
        "	if (val != DIE_INT3)",
        "		return NOTIFY_DONE;",
        "",
        "	if (regs->ip - INT3_INSN_SIZE != selftest)",
        "		return NOTIFY_DONE;",
        "",
        "	int3_emulate_call(regs, (unsigned long)&int3_magic);",
        "	return NOTIFY_STOP;",
        "}",
        "",
        "/* Must be noinline to ensure uniqueness of int3_selftest_ip. */",
        "static noinline void __init int3_selftest(void)",
        "{",
        "	static __initdata struct notifier_block int3_exception_nb = {",
        "		.notifier_call	= int3_exception_notify,",
        "		.priority	= INT_MAX-1, /* last */",
        "	};",
        "	unsigned int val = 0;",
        "",
        "	BUG_ON(register_die_notifier(&int3_exception_nb));",
        "",
        "	/*",
        "	 * Basically: int3_magic(&val); but really complicated :-)",
        "	 *",
        "	 * INT3 padded with NOP to CALL_INSN_SIZE. The int3_exception_nb",
        "	 * notifier above will emulate CALL for us.",
        "	 */",
        "	asm volatile (\"int3_selftest_ip:\\n\\t\"",
        "		      ANNOTATE_NOENDBR",
        "		      \"    int3; nop; nop; nop; nop\\n\\t\"",
        "		      : ASM_CALL_CONSTRAINT",
        "		      : __ASM_SEL_RAW(a, D) (&val)",
        "		      : \"memory\");",
        "",
        "	BUG_ON(val != 1);",
        "",
        "	unregister_die_notifier(&int3_exception_nb);",
        "}",
        "",
        "static __initdata int __alt_reloc_selftest_addr;",
        "",
        "extern void __init __alt_reloc_selftest(void *arg);",
        "__visible noinline void __init __alt_reloc_selftest(void *arg)",
        "{",
        "	WARN_ON(arg != &__alt_reloc_selftest_addr);",
        "}",
        "",
        "static noinline void __init alt_reloc_selftest(void)",
        "{",
        "	/*",
        "	 * Tests apply_relocation().",
        "	 *",
        "	 * This has a relative immediate (CALL) in a place other than the first",
        "	 * instruction and additionally on x86_64 we get a RIP-relative LEA:",
        "	 *",
        "	 *   lea    0x0(%rip),%rdi  # 5d0: R_X86_64_PC32    .init.data+0x5566c",
        "	 *   call   +0              # 5d5: R_X86_64_PLT32   __alt_reloc_selftest-0x4",
        "	 *",
        "	 * Getting this wrong will either crash and burn or tickle the WARN",
        "	 * above.",
        "	 */",
        "	asm_inline volatile (",
        "		ALTERNATIVE(\"\", \"lea %[mem], %%\" _ASM_ARG1 \"; call __alt_reloc_selftest;\", X86_FEATURE_ALWAYS)",
        "		: ASM_CALL_CONSTRAINT",
        "		: [mem] \"m\" (__alt_reloc_selftest_addr)",
        "		: _ASM_ARG1",
        "	);",
        "}",
        "",
        "void __init alternative_instructions(void)",
        "{",
        "	int3_selftest();",
        "",
        "	/*",
        "	 * The patching is not fully atomic, so try to avoid local",
        "	 * interruptions that might execute the to be patched code.",
        "	 * Other CPUs are not running.",
        "	 */",
        "	stop_nmi();",
        "",
        "	/*",
        "	 * Don't stop machine check exceptions while patching.",
        "	 * MCEs only happen when something got corrupted and in this",
        "	 * case we must do something about the corruption.",
        "	 * Ignoring it is worse than an unlikely patching race.",
        "	 * Also machine checks tend to be broadcast and if one CPU",
        "	 * goes into machine check the others follow quickly, so we don't",
        "	 * expect a machine check to cause undue problems during to code",
        "	 * patching.",
        "	 */",
        "",
        "	/*",
        "	 * Make sure to set (artificial) features depending on used paravirt",
        "	 * functions which can later influence alternative patching.",
        "	 */",
        "	paravirt_set_cap();",
        "",
        "	__apply_fineibt(__retpoline_sites, __retpoline_sites_end,",
        "			__cfi_sites, __cfi_sites_end, NULL);",
        "",
        "	/*",
        "	 * Rewrite the retpolines, must be done before alternatives since",
        "	 * those can rewrite the retpoline thunks.",
        "	 */",
        "	apply_retpolines(__retpoline_sites, __retpoline_sites_end, NULL);",
        "	apply_returns(__return_sites, __return_sites_end, NULL);",
        "",
        "	apply_alternatives(__alt_instructions, __alt_instructions_end, NULL);",
        "",
        "	/*",
        "	 * Now all calls are established. Apply the call thunks if",
        "	 * required.",
        "	 */",
        "	callthunks_patch_builtin_calls();",
        "",
        "	/*",
        "	 * Seal all functions that do not have their address taken.",
        "	 */",
        "	apply_seal_endbr(__ibt_endbr_seal, __ibt_endbr_seal_end, NULL);",
        "",
        "#ifdef CONFIG_SMP",
        "	/* Patch to UP if other cpus not imminent. */",
        "	if (!noreplace_smp && (num_present_cpus() == 1 || setup_max_cpus <= 1)) {",
        "		uniproc_patched = true;",
        "		alternatives_smp_module_add(NULL, \"core kernel\",",
        "					    __smp_locks, __smp_locks_end,",
        "					    _text, _etext);",
        "	}",
        "",
        "	if (!uniproc_patched || num_possible_cpus() == 1) {",
        "		free_init_pages(\"SMP alternatives\",",
        "				(unsigned long)__smp_locks,",
        "				(unsigned long)__smp_locks_end);",
        "	}",
        "#endif",
        "",
        "	restart_nmi();",
        "	alternatives_patched = 1;",
        "",
        "	alt_reloc_selftest();",
        "}",
        "",
        "/**",
        " * text_poke_early - Update instructions on a live kernel at boot time",
        " * @addr: address to modify",
        " * @opcode: source of the copy",
        " * @len: length to copy",
        " *",
        " * When you use this code to patch more than one byte of an instruction",
        " * you need to make sure that other CPUs cannot execute this code in parallel.",
        " * Also no thread must be currently preempted in the middle of these",
        " * instructions. And on the local CPU you need to be protected against NMI or",
        " * MCE handlers seeing an inconsistent instruction while you patch.",
        " */",
        "void __init_or_module text_poke_early(void *addr, const void *opcode,",
        "				      size_t len)",
        "{",
        "	unsigned long flags;",
        "",
        "	if (boot_cpu_has(X86_FEATURE_NX) &&",
        "	    is_module_text_address((unsigned long)addr)) {",
        "		/*",
        "		 * Modules text is marked initially as non-executable, so the",
        "		 * code cannot be running and speculative code-fetches are",
        "		 * prevented. Just change the code.",
        "		 */",
        "		memcpy(addr, opcode, len);",
        "	} else {",
        "		local_irq_save(flags);",
        "		memcpy(addr, opcode, len);",
        "		sync_core();",
        "		local_irq_restore(flags);",
        "",
        "		/*",
        "		 * Could also do a CLFLUSH here to speed up CPU recovery; but",
        "		 * that causes hangs on some VIA CPUs.",
        "		 */",
        "	}",
        "}",
        "",
        "typedef struct {",
        "	struct mm_struct *mm;",
        "} temp_mm_state_t;",
        "",
        "/*",
        " * Using a temporary mm allows to set temporary mappings that are not accessible",
        " * by other CPUs. Such mappings are needed to perform sensitive memory writes",
        " * that override the kernel memory protections (e.g., W^X), without exposing the",
        " * temporary page-table mappings that are required for these write operations to",
        " * other CPUs. Using a temporary mm also allows to avoid TLB shootdowns when the",
        " * mapping is torn down.",
        " *",
        " * Context: The temporary mm needs to be used exclusively by a single core. To",
        " *          harden security IRQs must be disabled while the temporary mm is",
        " *          loaded, thereby preventing interrupt handler bugs from overriding",
        " *          the kernel memory protection.",
        " */",
        "static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)",
        "{",
        "	temp_mm_state_t temp_state;",
        "",
        "	lockdep_assert_irqs_disabled();",
        "",
        "	/*",
        "	 * Make sure not to be in TLB lazy mode, as otherwise we'll end up",
        "	 * with a stale address space WITHOUT being in lazy mode after",
        "	 * restoring the previous mm.",
        "	 */",
        "	if (this_cpu_read(cpu_tlbstate_shared.is_lazy))",
        "		leave_mm();",
        "",
        "	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);",
        "	switch_mm_irqs_off(NULL, mm, current);",
        "",
        "	/*",
        "	 * If breakpoints are enabled, disable them while the temporary mm is",
        "	 * used. Userspace might set up watchpoints on addresses that are used",
        "	 * in the temporary mm, which would lead to wrong signals being sent or",
        "	 * crashes.",
        "	 *",
        "	 * Note that breakpoints are not disabled selectively, which also causes",
        "	 * kernel breakpoints (e.g., perf's) to be disabled. This might be",
        "	 * undesirable, but still seems reasonable as the code that runs in the",
        "	 * temporary mm should be short.",
        "	 */",
        "	if (hw_breakpoint_active())",
        "		hw_breakpoint_disable();",
        "",
        "	return temp_state;",
        "}",
        "",
        "static inline void unuse_temporary_mm(temp_mm_state_t prev_state)",
        "{",
        "	lockdep_assert_irqs_disabled();",
        "	switch_mm_irqs_off(NULL, prev_state.mm, current);",
        "",
        "	/*",
        "	 * Restore the breakpoints if they were disabled before the temporary mm",
        "	 * was loaded.",
        "	 */",
        "	if (hw_breakpoint_active())",
        "		hw_breakpoint_restore();",
        "}",
        "",
        "__ro_after_init struct mm_struct *poking_mm;",
        "__ro_after_init unsigned long poking_addr;",
        "",
        "static void text_poke_memcpy(void *dst, const void *src, size_t len)",
        "{",
        "	memcpy(dst, src, len);",
        "}",
        "",
        "static void text_poke_memset(void *dst, const void *src, size_t len)",
        "{",
        "	int c = *(const int *)src;",
        "",
        "	memset(dst, c, len);",
        "}",
        "",
        "typedef void text_poke_f(void *dst, const void *src, size_t len);",
        "",
        "static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t len)",
        "{",
        "	bool cross_page_boundary = offset_in_page(addr) + len > PAGE_SIZE;",
        "	struct page *pages[2] = {NULL};",
        "	temp_mm_state_t prev;",
        "	unsigned long flags;",
        "	pte_t pte, *ptep;",
        "	spinlock_t *ptl;",
        "	pgprot_t pgprot;",
        "",
        "	/*",
        "	 * While boot memory allocator is running we cannot use struct pages as",
        "	 * they are not yet initialized. There is no way to recover.",
        "	 */",
        "	BUG_ON(!after_bootmem);",
        "",
        "	if (!core_kernel_text((unsigned long)addr)) {",
        "		pages[0] = vmalloc_to_page(addr);",
        "		if (cross_page_boundary)",
        "			pages[1] = vmalloc_to_page(addr + PAGE_SIZE);",
        "	} else {",
        "		pages[0] = virt_to_page(addr);",
        "		WARN_ON(!PageReserved(pages[0]));",
        "		if (cross_page_boundary)",
        "			pages[1] = virt_to_page(addr + PAGE_SIZE);",
        "	}",
        "	/*",
        "	 * If something went wrong, crash and burn since recovery paths are not",
        "	 * implemented.",
        "	 */",
        "	BUG_ON(!pages[0] || (cross_page_boundary && !pages[1]));",
        "",
        "	/*",
        "	 * Map the page without the global bit, as TLB flushing is done with",
        "	 * flush_tlb_mm_range(), which is intended for non-global PTEs.",
        "	 */",
        "	pgprot = __pgprot(pgprot_val(PAGE_KERNEL) & ~_PAGE_GLOBAL);",
        "",
        "	/*",
        "	 * The lock is not really needed, but this allows to avoid open-coding.",
        "	 */",
        "	ptep = get_locked_pte(poking_mm, poking_addr, &ptl);",
        "",
        "	/*",
        "	 * This must not fail; preallocated in poking_init().",
        "	 */",
        "	VM_BUG_ON(!ptep);",
        "",
        "	local_irq_save(flags);",
        "",
        "	pte = mk_pte(pages[0], pgprot);",
        "	set_pte_at(poking_mm, poking_addr, ptep, pte);",
        "",
        "	if (cross_page_boundary) {",
        "		pte = mk_pte(pages[1], pgprot);",
        "		set_pte_at(poking_mm, poking_addr + PAGE_SIZE, ptep + 1, pte);",
        "	}",
        "",
        "	/*",
        "	 * Loading the temporary mm behaves as a compiler barrier, which",
        "	 * guarantees that the PTE will be set at the time memcpy() is done.",
        "	 */",
        "	prev = use_temporary_mm(poking_mm);",
        "",
        "	kasan_disable_current();",
        "	func((u8 *)poking_addr + offset_in_page(addr), src, len);",
        "	kasan_enable_current();",
        "",
        "	/*",
        "	 * Ensure that the PTE is only cleared after the instructions of memcpy",
        "	 * were issued by using a compiler barrier.",
        "	 */",
        "	barrier();",
        "",
        "	pte_clear(poking_mm, poking_addr, ptep);",
        "	if (cross_page_boundary)",
        "		pte_clear(poking_mm, poking_addr + PAGE_SIZE, ptep + 1);",
        "",
        "	/*",
        "	 * Loading the previous page-table hierarchy requires a serializing",
        "	 * instruction that already allows the core to see the updated version.",
        "	 * Xen-PV is assumed to serialize execution in a similar manner.",
        "	 */",
        "	unuse_temporary_mm(prev);",
        "",
        "	/*",
        "	 * Flushing the TLB might involve IPIs, which would require enabled",
        "	 * IRQs, but not if the mm is not used, as it is in this point.",
        "	 */",
        "	flush_tlb_mm_range(poking_mm, poking_addr, poking_addr +",
        "			   (cross_page_boundary ? 2 : 1) * PAGE_SIZE,",
        "			   PAGE_SHIFT, false);",
        "",
        "	if (func == text_poke_memcpy) {",
        "		/*",
        "		 * If the text does not match what we just wrote then something is",
        "		 * fundamentally screwy; there's nothing we can really do about that.",
        "		 */",
        "		BUG_ON(memcmp(addr, src, len));",
        "	}",
        "",
        "	local_irq_restore(flags);",
        "	pte_unmap_unlock(ptep, ptl);",
        "	return addr;",
        "}",
        "",
        "/**",
        " * text_poke - Update instructions on a live kernel",
        " * @addr: address to modify",
        " * @opcode: source of the copy",
        " * @len: length to copy",
        " *",
        " * Only atomic text poke/set should be allowed when not doing early patching.",
        " * It means the size must be writable atomically and the address must be aligned",
        " * in a way that permits an atomic write. It also makes sure we fit on a single",
        " * page.",
        " *",
        " * Note that the caller must ensure that if the modified code is part of a",
        " * module, the module would not be removed during poking. This can be achieved",
        " * by registering a module notifier, and ordering module removal and patching",
        " * through a mutex.",
        " */",
        "void *text_poke(void *addr, const void *opcode, size_t len)",
        "{",
        "	lockdep_assert_held(&text_mutex);",
        "",
        "	return __text_poke(text_poke_memcpy, addr, opcode, len);",
        "}",
        "",
        "/**",
        " * text_poke_kgdb - Update instructions on a live kernel by kgdb",
        " * @addr: address to modify",
        " * @opcode: source of the copy",
        " * @len: length to copy",
        " *",
        " * Only atomic text poke/set should be allowed when not doing early patching.",
        " * It means the size must be writable atomically and the address must be aligned",
        " * in a way that permits an atomic write. It also makes sure we fit on a single",
        " * page.",
        " *",
        " * Context: should only be used by kgdb, which ensures no other core is running,",
        " *	    despite the fact it does not hold the text_mutex.",
        " */",
        "void *text_poke_kgdb(void *addr, const void *opcode, size_t len)",
        "{",
        "	return __text_poke(text_poke_memcpy, addr, opcode, len);",
        "}",
        "",
        "void *text_poke_copy_locked(void *addr, const void *opcode, size_t len,",
        "			    bool core_ok)",
        "{",
        "	unsigned long start = (unsigned long)addr;",
        "	size_t patched = 0;",
        "",
        "	if (WARN_ON_ONCE(!core_ok && core_kernel_text(start)))",
        "		return NULL;",
        "",
        "	while (patched < len) {",
        "		unsigned long ptr = start + patched;",
        "		size_t s;",
        "",
        "		s = min_t(size_t, PAGE_SIZE * 2 - offset_in_page(ptr), len - patched);",
        "",
        "		__text_poke(text_poke_memcpy, (void *)ptr, opcode + patched, s);",
        "		patched += s;",
        "	}",
        "	return addr;",
        "}",
        "",
        "/**",
        " * text_poke_copy - Copy instructions into (an unused part of) RX memory",
        " * @addr: address to modify",
        " * @opcode: source of the copy",
        " * @len: length to copy, could be more than 2x PAGE_SIZE",
        " *",
        " * Not safe against concurrent execution; useful for JITs to dump",
        " * new code blocks into unused regions of RX memory. Can be used in",
        " * conjunction with synchronize_rcu_tasks() to wait for existing",
        " * execution to quiesce after having made sure no existing functions",
        " * pointers are live.",
        " */",
        "void *text_poke_copy(void *addr, const void *opcode, size_t len)",
        "{",
        "	mutex_lock(&text_mutex);",
        "	addr = text_poke_copy_locked(addr, opcode, len, false);",
        "	mutex_unlock(&text_mutex);",
        "	return addr;",
        "}",
        "",
        "/**",
        " * text_poke_set - memset into (an unused part of) RX memory",
        " * @addr: address to modify",
        " * @c: the byte to fill the area with",
        " * @len: length to copy, could be more than 2x PAGE_SIZE",
        " *",
        " * This is useful to overwrite unused regions of RX memory with illegal",
        " * instructions.",
        " */",
        "void *text_poke_set(void *addr, int c, size_t len)",
        "{",
        "	unsigned long start = (unsigned long)addr;",
        "	size_t patched = 0;",
        "",
        "	if (WARN_ON_ONCE(core_kernel_text(start)))",
        "		return NULL;",
        "",
        "	mutex_lock(&text_mutex);",
        "	while (patched < len) {",
        "		unsigned long ptr = start + patched;",
        "		size_t s;",
        "",
        "		s = min_t(size_t, PAGE_SIZE * 2 - offset_in_page(ptr), len - patched);",
        "",
        "		__text_poke(text_poke_memset, (void *)ptr, (void *)&c, s);",
        "		patched += s;",
        "	}",
        "	mutex_unlock(&text_mutex);",
        "	return addr;",
        "}",
        "",
        "static void do_sync_core(void *info)",
        "{",
        "	sync_core();",
        "}",
        "",
        "void text_poke_sync(void)",
        "{",
        "	on_each_cpu(do_sync_core, NULL, 1);",
        "}",
        "",
        "/*",
        " * NOTE: crazy scheme to allow patching Jcc.d32 but not increase the size of",
        " * this thing. When len == 6 everything is prefixed with 0x0f and we map",
        " * opcode to Jcc.d8, using len to distinguish.",
        " */",
        "struct text_poke_loc {",
        "	/* addr := _stext + rel_addr */",
        "	s32 rel_addr;",
        "	s32 disp;",
        "	u8 len;",
        "	u8 opcode;",
        "	const u8 text[POKE_MAX_OPCODE_SIZE];",
        "	/* see text_poke_bp_batch() */",
        "	u8 old;",
        "};",
        "",
        "struct bp_patching_desc {",
        "	struct text_poke_loc *vec;",
        "	int nr_entries;",
        "	atomic_t refs;",
        "};",
        "",
        "static struct bp_patching_desc bp_desc;",
        "",
        "static __always_inline",
        "struct bp_patching_desc *try_get_desc(void)",
        "{",
        "	struct bp_patching_desc *desc = &bp_desc;",
        "",
        "	if (!raw_atomic_inc_not_zero(&desc->refs))",
        "		return NULL;",
        "",
        "	return desc;",
        "}",
        "",
        "static __always_inline void put_desc(void)",
        "{",
        "	struct bp_patching_desc *desc = &bp_desc;",
        "",
        "	smp_mb__before_atomic();",
        "	raw_atomic_dec(&desc->refs);",
        "}",
        "",
        "static __always_inline void *text_poke_addr(struct text_poke_loc *tp)",
        "{",
        "	return _stext + tp->rel_addr;",
        "}",
        "",
        "static __always_inline int patch_cmp(const void *key, const void *elt)",
        "{",
        "	struct text_poke_loc *tp = (struct text_poke_loc *) elt;",
        "",
        "	if (key < text_poke_addr(tp))",
        "		return -1;",
        "	if (key > text_poke_addr(tp))",
        "		return 1;",
        "	return 0;",
        "}",
        "",
        "noinstr int poke_int3_handler(struct pt_regs *regs)",
        "{",
        "	struct bp_patching_desc *desc;",
        "	struct text_poke_loc *tp;",
        "	int ret = 0;",
        "	void *ip;",
        "",
        "	if (user_mode(regs))",
        "		return 0;",
        "",
        "	/*",
        "	 * Having observed our INT3 instruction, we now must observe",
        "	 * bp_desc with non-zero refcount:",
        "	 *",
        "	 *	bp_desc.refs = 1		INT3",
        "	 *	WMB				RMB",
        "	 *	write INT3			if (bp_desc.refs != 0)",
        "	 */",
        "	smp_rmb();",
        "",
        "	desc = try_get_desc();",
        "	if (!desc)",
        "		return 0;",
        "",
        "	/*",
        "	 * Discount the INT3. See text_poke_bp_batch().",
        "	 */",
        "	ip = (void *) regs->ip - INT3_INSN_SIZE;",
        "",
        "	/*",
        "	 * Skip the binary search if there is a single member in the vector.",
        "	 */",
        "	if (unlikely(desc->nr_entries > 1)) {",
        "		tp = __inline_bsearch(ip, desc->vec, desc->nr_entries,",
        "				      sizeof(struct text_poke_loc),",
        "				      patch_cmp);",
        "		if (!tp)",
        "			goto out_put;",
        "	} else {",
        "		tp = desc->vec;",
        "		if (text_poke_addr(tp) != ip)",
        "			goto out_put;",
        "	}",
        "",
        "	ip += tp->len;",
        "",
        "	switch (tp->opcode) {",
        "	case INT3_INSN_OPCODE:",
        "		/*",
        "		 * Someone poked an explicit INT3, they'll want to handle it,",
        "		 * do not consume.",
        "		 */",
        "		goto out_put;",
        "",
        "	case RET_INSN_OPCODE:",
        "		int3_emulate_ret(regs);",
        "		break;",
        "",
        "	case CALL_INSN_OPCODE:",
        "		int3_emulate_call(regs, (long)ip + tp->disp);",
        "		break;",
        "",
        "	case JMP32_INSN_OPCODE:",
        "	case JMP8_INSN_OPCODE:",
        "		int3_emulate_jmp(regs, (long)ip + tp->disp);",
        "		break;",
        "",
        "	case 0x70 ... 0x7f: /* Jcc */",
        "		int3_emulate_jcc(regs, tp->opcode & 0xf, (long)ip, tp->disp);",
        "		break;",
        "",
        "	default:",
        "		BUG();",
        "	}",
        "",
        "	ret = 1;",
        "",
        "out_put:",
        "	put_desc();",
        "	return ret;",
        "}",
        "",
        "#define TP_VEC_MAX (PAGE_SIZE / sizeof(struct text_poke_loc))",
        "static struct text_poke_loc tp_vec[TP_VEC_MAX];",
        "static int tp_vec_nr;",
        "",
        "/**",
        " * text_poke_bp_batch() -- update instructions on live kernel on SMP",
        " * @tp:			vector of instructions to patch",
        " * @nr_entries:		number of entries in the vector",
        " *",
        " * Modify multi-byte instruction by using int3 breakpoint on SMP.",
        " * We completely avoid stop_machine() here, and achieve the",
        " * synchronization using int3 breakpoint.",
        " *",
        " * The way it is done:",
        " *	- For each entry in the vector:",
        " *		- add a int3 trap to the address that will be patched",
        " *	- sync cores",
        " *	- For each entry in the vector:",
        " *		- update all but the first byte of the patched range",
        " *	- sync cores",
        " *	- For each entry in the vector:",
        " *		- replace the first byte (int3) by the first byte of",
        " *		  replacing opcode",
        " *	- sync cores",
        " */",
        "static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries)",
        "{",
        "	unsigned char int3 = INT3_INSN_OPCODE;",
        "	unsigned int i;",
        "	int do_sync;",
        "",
        "	lockdep_assert_held(&text_mutex);",
        "",
        "	bp_desc.vec = tp;",
        "	bp_desc.nr_entries = nr_entries;",
        "",
        "	/*",
        "	 * Corresponds to the implicit memory barrier in try_get_desc() to",
        "	 * ensure reading a non-zero refcount provides up to date bp_desc data.",
        "	 */",
        "	atomic_set_release(&bp_desc.refs, 1);",
        "",
        "	/*",
        "	 * Function tracing can enable thousands of places that need to be",
        "	 * updated. This can take quite some time, and with full kernel debugging",
        "	 * enabled, this could cause the softlockup watchdog to trigger.",
        "	 * This function gets called every 256 entries added to be patched.",
        "	 * Call cond_resched() here to make sure that other tasks can get scheduled",
        "	 * while processing all the functions being patched.",
        "	 */",
        "	cond_resched();",
        "",
        "	/*",
        "	 * Corresponding read barrier in int3 notifier for making sure the",
        "	 * nr_entries and handler are correctly ordered wrt. patching.",
        "	 */",
        "	smp_wmb();",
        "",
        "	/*",
        "	 * First step: add a int3 trap to the address that will be patched.",
        "	 */",
        "	for (i = 0; i < nr_entries; i++) {",
        "		tp[i].old = *(u8 *)text_poke_addr(&tp[i]);",
        "		text_poke(text_poke_addr(&tp[i]), &int3, INT3_INSN_SIZE);",
        "	}",
        "",
        "	text_poke_sync();",
        "",
        "	/*",
        "	 * Second step: update all but the first byte of the patched range.",
        "	 */",
        "	for (do_sync = 0, i = 0; i < nr_entries; i++) {",
        "		u8 old[POKE_MAX_OPCODE_SIZE+1] = { tp[i].old, };",
        "		u8 _new[POKE_MAX_OPCODE_SIZE+1];",
        "		const u8 *new = tp[i].text;",
        "		int len = tp[i].len;",
        "",
        "		if (len - INT3_INSN_SIZE > 0) {",
        "			memcpy(old + INT3_INSN_SIZE,",
        "			       text_poke_addr(&tp[i]) + INT3_INSN_SIZE,",
        "			       len - INT3_INSN_SIZE);",
        "",
        "			if (len == 6) {",
        "				_new[0] = 0x0f;",
        "				memcpy(_new + 1, new, 5);",
        "				new = _new;",
        "			}",
        "",
        "			text_poke(text_poke_addr(&tp[i]) + INT3_INSN_SIZE,",
        "				  new + INT3_INSN_SIZE,",
        "				  len - INT3_INSN_SIZE);",
        "",
        "			do_sync++;",
        "		}",
        "",
        "		/*",
        "		 * Emit a perf event to record the text poke, primarily to",
        "		 * support Intel PT decoding which must walk the executable code",
        "		 * to reconstruct the trace. The flow up to here is:",
        "		 *   - write INT3 byte",
        "		 *   - IPI-SYNC",
        "		 *   - write instruction tail",
        "		 * At this point the actual control flow will be through the",
        "		 * INT3 and handler and not hit the old or new instruction.",
        "		 * Intel PT outputs FUP/TIP packets for the INT3, so the flow",
        "		 * can still be decoded. Subsequently:",
        "		 *   - emit RECORD_TEXT_POKE with the new instruction",
        "		 *   - IPI-SYNC",
        "		 *   - write first byte",
        "		 *   - IPI-SYNC",
        "		 * So before the text poke event timestamp, the decoder will see",
        "		 * either the old instruction flow or FUP/TIP of INT3. After the",
        "		 * text poke event timestamp, the decoder will see either the",
        "		 * new instruction flow or FUP/TIP of INT3. Thus decoders can",
        "		 * use the timestamp as the point at which to modify the",
        "		 * executable code.",
        "		 * The old instruction is recorded so that the event can be",
        "		 * processed forwards or backwards.",
        "		 */",
        "		perf_event_text_poke(text_poke_addr(&tp[i]), old, len, new, len);",
        "	}",
        "",
        "	if (do_sync) {",
        "		/*",
        "		 * According to Intel, this core syncing is very likely",
        "		 * not necessary and we'd be safe even without it. But",
        "		 * better safe than sorry (plus there's not only Intel).",
        "		 */",
        "		text_poke_sync();",
        "	}",
        "",
        "	/*",
        "	 * Third step: replace the first byte (int3) by the first byte of",
        "	 * replacing opcode.",
        "	 */",
        "	for (do_sync = 0, i = 0; i < nr_entries; i++) {",
        "		u8 byte = tp[i].text[0];",
        "",
        "		if (tp[i].len == 6)",
        "			byte = 0x0f;",
        "",
        "		if (byte == INT3_INSN_OPCODE)",
        "			continue;",
        "",
        "		text_poke(text_poke_addr(&tp[i]), &byte, INT3_INSN_SIZE);",
        "		do_sync++;",
        "	}",
        "",
        "	if (do_sync)",
        "		text_poke_sync();",
        "",
        "	/*",
        "	 * Remove and wait for refs to be zero.",
        "	 */",
        "	if (!atomic_dec_and_test(&bp_desc.refs))",
        "		atomic_cond_read_acquire(&bp_desc.refs, !VAL);",
        "}",
        "",
        "static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,",
        "			       const void *opcode, size_t len, const void *emulate)",
        "{",
        "	struct insn insn;",
        "	int ret, i = 0;",
        "",
        "	if (len == 6)",
        "		i = 1;",
        "	memcpy((void *)tp->text, opcode+i, len-i);",
        "	if (!emulate)",
        "		emulate = opcode;",
        "",
        "	ret = insn_decode_kernel(&insn, emulate);",
        "	BUG_ON(ret < 0);",
        "",
        "	tp->rel_addr = addr - (void *)_stext;",
        "	tp->len = len;",
        "	tp->opcode = insn.opcode.bytes[0];",
        "",
        "	if (is_jcc32(&insn)) {",
        "		/*",
        "		 * Map Jcc.d32 onto Jcc.d8 and use len to distinguish.",
        "		 */",
        "		tp->opcode = insn.opcode.bytes[1] - 0x10;",
        "	}",
        "",
        "	switch (tp->opcode) {",
        "	case RET_INSN_OPCODE:",
        "	case JMP32_INSN_OPCODE:",
        "	case JMP8_INSN_OPCODE:",
        "		/*",
        "		 * Control flow instructions without implied execution of the",
        "		 * next instruction can be padded with INT3.",
        "		 */",
        "		for (i = insn.length; i < len; i++)",
        "			BUG_ON(tp->text[i] != INT3_INSN_OPCODE);",
        "		break;",
        "",
        "	default:",
        "		BUG_ON(len != insn.length);",
        "	}",
        "",
        "	switch (tp->opcode) {",
        "	case INT3_INSN_OPCODE:",
        "	case RET_INSN_OPCODE:",
        "		break;",
        "",
        "	case CALL_INSN_OPCODE:",
        "	case JMP32_INSN_OPCODE:",
        "	case JMP8_INSN_OPCODE:",
        "	case 0x70 ... 0x7f: /* Jcc */",
        "		tp->disp = insn.immediate.value;",
        "		break;",
        "",
        "	default: /* assume NOP */",
        "		switch (len) {",
        "		case 2: /* NOP2 -- emulate as JMP8+0 */",
        "			BUG_ON(memcmp(emulate, x86_nops[len], len));",
        "			tp->opcode = JMP8_INSN_OPCODE;",
        "			tp->disp = 0;",
        "			break;",
        "",
        "		case 5: /* NOP5 -- emulate as JMP32+0 */",
        "			BUG_ON(memcmp(emulate, x86_nops[len], len));",
        "			tp->opcode = JMP32_INSN_OPCODE;",
        "			tp->disp = 0;",
        "			break;",
        "",
        "		default: /* unknown instruction */",
        "			BUG();",
        "		}",
        "		break;",
        "	}",
        "}",
        "",
        "/*",
        " * We hard rely on the tp_vec being ordered; ensure this is so by flushing",
        " * early if needed.",
        " */",
        "static bool tp_order_fail(void *addr)",
        "{",
        "	struct text_poke_loc *tp;",
        "",
        "	if (!tp_vec_nr)",
        "		return false;",
        "",
        "	if (!addr) /* force */",
        "		return true;",
        "",
        "	tp = &tp_vec[tp_vec_nr - 1];",
        "	if ((unsigned long)text_poke_addr(tp) > (unsigned long)addr)",
        "		return true;",
        "",
        "	return false;",
        "}",
        "",
        "static void text_poke_flush(void *addr)",
        "{",
        "	if (tp_vec_nr == TP_VEC_MAX || tp_order_fail(addr)) {",
        "		text_poke_bp_batch(tp_vec, tp_vec_nr);",
        "		tp_vec_nr = 0;",
        "	}",
        "}",
        "",
        "void text_poke_finish(void)",
        "{",
        "	text_poke_flush(NULL);",
        "}",
        "",
        "void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)",
        "{",
        "	struct text_poke_loc *tp;",
        "",
        "	text_poke_flush(addr);",
        "",
        "	tp = &tp_vec[tp_vec_nr++];",
        "	text_poke_loc_init(tp, addr, opcode, len, emulate);",
        "}",
        "",
        "/**",
        " * text_poke_bp() -- update instructions on live kernel on SMP",
        " * @addr:	address to patch",
        " * @opcode:	opcode of new instruction",
        " * @len:	length to copy",
        " * @emulate:	instruction to be emulated",
        " *",
        " * Update a single instruction with the vector in the stack, avoiding",
        " * dynamically allocated memory. This function should be used when it is",
        " * not possible to allocate memory.",
        " */",
        "void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)",
        "{",
        "	struct text_poke_loc tp;",
        "",
        "	text_poke_loc_init(&tp, addr, opcode, len, emulate);",
        "	text_poke_bp_batch(&tp, 1);",
        "}"
    ]
  },
  "security_security_c": {
    path: "security/security.c",
    covered: [4067, 5681, 2908, 728, 1141, 2857, 5636, 2316, 5770, 5682, 2852, 3040, 5699, 4068, 3041, 1142, 5635, 5651, 5771, 5700, 5650, 2853, 2907, 2319],
    totalLines: 6009,
    coveredCount: 24,
    coveragePct: 0.4,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-or-later",
        "/*",
        " * Security plug functions",
        " *",
        " * Copyright (C) 2001 WireX Communications, Inc <chris@wirex.com>",
        " * Copyright (C) 2001-2002 Greg Kroah-Hartman <greg@kroah.com>",
        " * Copyright (C) 2001 Networks Associates Technology, Inc <ssmalley@nai.com>",
        " * Copyright (C) 2016 Mellanox Technologies",
        " * Copyright (C) 2023 Microsoft Corporation <paul@paul-moore.com>",
        " */",
        "",
        "#define pr_fmt(fmt) \"LSM: \" fmt",
        "",
        "#include <linux/bpf.h>",
        "#include <linux/capability.h>",
        "#include <linux/dcache.h>",
        "#include <linux/export.h>",
        "#include <linux/init.h>",
        "#include <linux/kernel.h>",
        "#include <linux/kernel_read_file.h>",
        "#include <linux/lsm_hooks.h>",
        "#include <linux/mman.h>",
        "#include <linux/mount.h>",
        "#include <linux/personality.h>",
        "#include <linux/backing-dev.h>",
        "#include <linux/string.h>",
        "#include <linux/xattr.h>",
        "#include <linux/msg.h>",
        "#include <linux/overflow.h>",
        "#include <linux/perf_event.h>",
        "#include <linux/fs.h>",
        "#include <net/flow.h>",
        "#include <net/sock.h>",
        "",
        "#define SECURITY_HOOK_ACTIVE_KEY(HOOK, IDX) security_hook_active_##HOOK##_##IDX",
        "",
        "/*",
        " * Identifier for the LSM static calls.",
        " * HOOK is an LSM hook as defined in linux/lsm_hookdefs.h",
        " * IDX is the index of the static call. 0 <= NUM < MAX_LSM_COUNT",
        " */",
        "#define LSM_STATIC_CALL(HOOK, IDX) lsm_static_call_##HOOK##_##IDX",
        "",
        "/*",
        " * Call the macro M for each LSM hook MAX_LSM_COUNT times.",
        " */",
        "#define LSM_LOOP_UNROLL(M, ...) 		\\",
        "do {						\\",
        "	UNROLL(MAX_LSM_COUNT, M, __VA_ARGS__)	\\",
        "} while (0)",
        "",
        "#define LSM_DEFINE_UNROLL(M, ...) UNROLL(MAX_LSM_COUNT, M, __VA_ARGS__)",
        "",
        "/*",
        " * These are descriptions of the reasons that can be passed to the",
        " * security_locked_down() LSM hook. Placing this array here allows",
        " * all security modules to use the same descriptions for auditing",
        " * purposes.",
        " */",
        "const char *const lockdown_reasons[LOCKDOWN_CONFIDENTIALITY_MAX + 1] = {",
        "	[LOCKDOWN_NONE] = \"none\",",
        "	[LOCKDOWN_MODULE_SIGNATURE] = \"unsigned module loading\",",
        "	[LOCKDOWN_DEV_MEM] = \"/dev/mem,kmem,port\",",
        "	[LOCKDOWN_EFI_TEST] = \"/dev/efi_test access\",",
        "	[LOCKDOWN_KEXEC] = \"kexec of unsigned images\",",
        "	[LOCKDOWN_HIBERNATION] = \"hibernation\",",
        "	[LOCKDOWN_PCI_ACCESS] = \"direct PCI access\",",
        "	[LOCKDOWN_IOPORT] = \"raw io port access\",",
        "	[LOCKDOWN_MSR] = \"raw MSR access\",",
        "	[LOCKDOWN_ACPI_TABLES] = \"modifying ACPI tables\",",
        "	[LOCKDOWN_DEVICE_TREE] = \"modifying device tree contents\",",
        "	[LOCKDOWN_PCMCIA_CIS] = \"direct PCMCIA CIS storage\",",
        "	[LOCKDOWN_TIOCSSERIAL] = \"reconfiguration of serial port IO\",",
        "	[LOCKDOWN_MODULE_PARAMETERS] = \"unsafe module parameters\",",
        "	[LOCKDOWN_MMIOTRACE] = \"unsafe mmio\",",
        "	[LOCKDOWN_DEBUGFS] = \"debugfs access\",",
        "	[LOCKDOWN_XMON_WR] = \"xmon write access\",",
        "	[LOCKDOWN_BPF_WRITE_USER] = \"use of bpf to write user RAM\",",
        "	[LOCKDOWN_DBG_WRITE_KERNEL] = \"use of kgdb/kdb to write kernel RAM\",",
        "	[LOCKDOWN_RTAS_ERROR_INJECTION] = \"RTAS error injection\",",
        "	[LOCKDOWN_INTEGRITY_MAX] = \"integrity\",",
        "	[LOCKDOWN_KCORE] = \"/proc/kcore access\",",
        "	[LOCKDOWN_KPROBES] = \"use of kprobes\",",
        "	[LOCKDOWN_BPF_READ_KERNEL] = \"use of bpf to read kernel RAM\",",
        "	[LOCKDOWN_DBG_READ_KERNEL] = \"use of kgdb/kdb to read kernel RAM\",",
        "	[LOCKDOWN_PERF] = \"unsafe use of perf\",",
        "	[LOCKDOWN_TRACEFS] = \"use of tracefs\",",
        "	[LOCKDOWN_XMON_RW] = \"xmon read and write access\",",
        "	[LOCKDOWN_XFRM_SECRET] = \"xfrm SA secret\",",
        "	[LOCKDOWN_CONFIDENTIALITY_MAX] = \"confidentiality\",",
        "};",
        "",
        "static BLOCKING_NOTIFIER_HEAD(blocking_lsm_notifier_chain);",
        "",
        "static struct kmem_cache *lsm_file_cache;",
        "static struct kmem_cache *lsm_inode_cache;",
        "",
        "char *lsm_names;",
        "static struct lsm_blob_sizes blob_sizes __ro_after_init;",
        "",
        "/* Boot-time LSM user choice */",
        "static __initdata const char *chosen_lsm_order;",
        "static __initdata const char *chosen_major_lsm;",
        "",
        "static __initconst const char *const builtin_lsm_order = CONFIG_LSM;",
        "",
        "/* Ordered list of LSMs to initialize. */",
        "static __initdata struct lsm_info *ordered_lsms[MAX_LSM_COUNT + 1];",
        "static __initdata struct lsm_info *exclusive;",
        "",
        "#ifdef CONFIG_HAVE_STATIC_CALL",
        "#define LSM_HOOK_TRAMP(NAME, NUM) \\",
        "	&STATIC_CALL_TRAMP(LSM_STATIC_CALL(NAME, NUM))",
        "#else",
        "#define LSM_HOOK_TRAMP(NAME, NUM) NULL",
        "#endif",
        "",
        "/*",
        " * Define static calls and static keys for each LSM hook.",
        " */",
        "#define DEFINE_LSM_STATIC_CALL(NUM, NAME, RET, ...)			\\",
        "	DEFINE_STATIC_CALL_NULL(LSM_STATIC_CALL(NAME, NUM),		\\",
        "				*((RET(*)(__VA_ARGS__))NULL));		\\",
        "	DEFINE_STATIC_KEY_FALSE(SECURITY_HOOK_ACTIVE_KEY(NAME, NUM));",
        "",
        "#define LSM_HOOK(RET, DEFAULT, NAME, ...)				\\",
        "	LSM_DEFINE_UNROLL(DEFINE_LSM_STATIC_CALL, NAME, RET, __VA_ARGS__)",
        "#include <linux/lsm_hook_defs.h>",
        "#undef LSM_HOOK",
        "#undef DEFINE_LSM_STATIC_CALL",
        "",
        "/*",
        " * Initialise a table of static calls for each LSM hook.",
        " * DEFINE_STATIC_CALL_NULL invocation above generates a key (STATIC_CALL_KEY)",
        " * and a trampoline (STATIC_CALL_TRAMP) which are used to call",
        " * __static_call_update when updating the static call.",
        " *",
        " * The static calls table is used by early LSMs, some architectures can fault on",
        " * unaligned accesses and the fault handling code may not be ready by then.",
        " * Thus, the static calls table should be aligned to avoid any unhandled faults",
        " * in early init.",
        " */",
        "struct lsm_static_calls_table",
        "	static_calls_table __ro_after_init __aligned(sizeof(u64)) = {",
        "#define INIT_LSM_STATIC_CALL(NUM, NAME)					\\",
        "	(struct lsm_static_call) {					\\",
        "		.key = &STATIC_CALL_KEY(LSM_STATIC_CALL(NAME, NUM)),	\\",
        "		.trampoline = LSM_HOOK_TRAMP(NAME, NUM),		\\",
        "		.active = &SECURITY_HOOK_ACTIVE_KEY(NAME, NUM),		\\",
        "	},",
        "#define LSM_HOOK(RET, DEFAULT, NAME, ...)				\\",
        "	.NAME = {							\\",
        "		LSM_DEFINE_UNROLL(INIT_LSM_STATIC_CALL, NAME)		\\",
        "	},",
        "#include <linux/lsm_hook_defs.h>",
        "#undef LSM_HOOK",
        "#undef INIT_LSM_STATIC_CALL",
        "	};",
        "",
        "static __initdata bool debug;",
        "#define init_debug(...)						\\",
        "	do {							\\",
        "		if (debug)					\\",
        "			pr_info(__VA_ARGS__);			\\",
        "	} while (0)",
        "",
        "static bool __init is_enabled(struct lsm_info *lsm)",
        "{",
        "	if (!lsm->enabled)",
        "		return false;",
        "",
        "	return *lsm->enabled;",
        "}",
        "",
        "/* Mark an LSM's enabled flag. */",
        "static int lsm_enabled_true __initdata = 1;",
        "static int lsm_enabled_false __initdata = 0;",
        "static void __init set_enabled(struct lsm_info *lsm, bool enabled)",
        "{",
        "	/*",
        "	 * When an LSM hasn't configured an enable variable, we can use",
        "	 * a hard-coded location for storing the default enabled state.",
        "	 */",
        "	if (!lsm->enabled) {",
        "		if (enabled)",
        "			lsm->enabled = &lsm_enabled_true;",
        "		else",
        "			lsm->enabled = &lsm_enabled_false;",
        "	} else if (lsm->enabled == &lsm_enabled_true) {",
        "		if (!enabled)",
        "			lsm->enabled = &lsm_enabled_false;",
        "	} else if (lsm->enabled == &lsm_enabled_false) {",
        "		if (enabled)",
        "			lsm->enabled = &lsm_enabled_true;",
        "	} else {",
        "		*lsm->enabled = enabled;",
        "	}",
        "}",
        "",
        "/* Is an LSM already listed in the ordered LSMs list? */",
        "static bool __init exists_ordered_lsm(struct lsm_info *lsm)",
        "{",
        "	struct lsm_info **check;",
        "",
        "	for (check = ordered_lsms; *check; check++)",
        "		if (*check == lsm)",
        "			return true;",
        "",
        "	return false;",
        "}",
        "",
        "/* Append an LSM to the list of ordered LSMs to initialize. */",
        "static int last_lsm __initdata;",
        "static void __init append_ordered_lsm(struct lsm_info *lsm, const char *from)",
        "{",
        "	/* Ignore duplicate selections. */",
        "	if (exists_ordered_lsm(lsm))",
        "		return;",
        "",
        "	if (WARN(last_lsm == MAX_LSM_COUNT, \"%s: out of LSM static calls!?\\n\", from))",
        "		return;",
        "",
        "	/* Enable this LSM, if it is not already set. */",
        "	if (!lsm->enabled)",
        "		lsm->enabled = &lsm_enabled_true;",
        "	ordered_lsms[last_lsm++] = lsm;",
        "",
        "	init_debug(\"%s ordered: %s (%s)\\n\", from, lsm->name,",
        "		   is_enabled(lsm) ? \"enabled\" : \"disabled\");",
        "}",
        "",
        "/* Is an LSM allowed to be initialized? */",
        "static bool __init lsm_allowed(struct lsm_info *lsm)",
        "{",
        "	/* Skip if the LSM is disabled. */",
        "	if (!is_enabled(lsm))",
        "		return false;",
        "",
        "	/* Not allowed if another exclusive LSM already initialized. */",
        "	if ((lsm->flags & LSM_FLAG_EXCLUSIVE) && exclusive) {",
        "		init_debug(\"exclusive disabled: %s\\n\", lsm->name);",
        "		return false;",
        "	}",
        "",
        "	return true;",
        "}",
        "",
        "static void __init lsm_set_blob_size(int *need, int *lbs)",
        "{",
        "	int offset;",
        "",
        "	if (*need <= 0)",
        "		return;",
        "",
        "	offset = ALIGN(*lbs, sizeof(void *));",
        "	*lbs = offset + *need;",
        "	*need = offset;",
        "}",
        "",
        "static void __init lsm_set_blob_sizes(struct lsm_blob_sizes *needed)",
        "{",
        "	if (!needed)",
        "		return;",
        "",
        "	lsm_set_blob_size(&needed->lbs_cred, &blob_sizes.lbs_cred);",
        "	lsm_set_blob_size(&needed->lbs_file, &blob_sizes.lbs_file);",
        "	lsm_set_blob_size(&needed->lbs_ib, &blob_sizes.lbs_ib);",
        "	/*",
        "	 * The inode blob gets an rcu_head in addition to",
        "	 * what the modules might need.",
        "	 */",
        "	if (needed->lbs_inode && blob_sizes.lbs_inode == 0)",
        "		blob_sizes.lbs_inode = sizeof(struct rcu_head);",
        "	lsm_set_blob_size(&needed->lbs_inode, &blob_sizes.lbs_inode);",
        "	lsm_set_blob_size(&needed->lbs_ipc, &blob_sizes.lbs_ipc);",
        "	lsm_set_blob_size(&needed->lbs_key, &blob_sizes.lbs_key);",
        "	lsm_set_blob_size(&needed->lbs_msg_msg, &blob_sizes.lbs_msg_msg);",
        "	lsm_set_blob_size(&needed->lbs_perf_event, &blob_sizes.lbs_perf_event);",
        "	lsm_set_blob_size(&needed->lbs_sock, &blob_sizes.lbs_sock);",
        "	lsm_set_blob_size(&needed->lbs_superblock, &blob_sizes.lbs_superblock);",
        "	lsm_set_blob_size(&needed->lbs_task, &blob_sizes.lbs_task);",
        "	lsm_set_blob_size(&needed->lbs_tun_dev, &blob_sizes.lbs_tun_dev);",
        "	lsm_set_blob_size(&needed->lbs_xattr_count,",
        "			  &blob_sizes.lbs_xattr_count);",
        "	lsm_set_blob_size(&needed->lbs_bdev, &blob_sizes.lbs_bdev);",
        "}",
        "",
        "/* Prepare LSM for initialization. */",
        "static void __init prepare_lsm(struct lsm_info *lsm)",
        "{",
        "	int enabled = lsm_allowed(lsm);",
        "",
        "	/* Record enablement (to handle any following exclusive LSMs). */",
        "	set_enabled(lsm, enabled);",
        "",
        "	/* If enabled, do pre-initialization work. */",
        "	if (enabled) {",
        "		if ((lsm->flags & LSM_FLAG_EXCLUSIVE) && !exclusive) {",
        "			exclusive = lsm;",
        "			init_debug(\"exclusive chosen:   %s\\n\", lsm->name);",
        "		}",
        "",
        "		lsm_set_blob_sizes(lsm->blobs);",
        "	}",
        "}",
        "",
        "/* Initialize a given LSM, if it is enabled. */",
        "static void __init initialize_lsm(struct lsm_info *lsm)",
        "{",
        "	if (is_enabled(lsm)) {",
        "		int ret;",
        "",
        "		init_debug(\"initializing %s\\n\", lsm->name);",
        "		ret = lsm->init();",
        "		WARN(ret, \"%s failed to initialize: %d\\n\", lsm->name, ret);",
        "	}",
        "}",
        "",
        "/*",
        " * Current index to use while initializing the lsm id list.",
        " */",
        "u32 lsm_active_cnt __ro_after_init;",
        "const struct lsm_id *lsm_idlist[MAX_LSM_COUNT];",
        "",
        "/* Populate ordered LSMs list from comma-separated LSM name list. */",
        "static void __init ordered_lsm_parse(const char *order, const char *origin)",
        "{",
        "	struct lsm_info *lsm;",
        "	char *sep, *name, *next;",
        "",
        "	/* LSM_ORDER_FIRST is always first. */",
        "	for (lsm = __start_lsm_info; lsm < __end_lsm_info; lsm++) {",
        "		if (lsm->order == LSM_ORDER_FIRST)",
        "			append_ordered_lsm(lsm, \"  first\");",
        "	}",
        "",
        "	/* Process \"security=\", if given. */",
        "	if (chosen_major_lsm) {",
        "		struct lsm_info *major;",
        "",
        "		/*",
        "		 * To match the original \"security=\" behavior, this",
        "		 * explicitly does NOT fallback to another Legacy Major",
        "		 * if the selected one was separately disabled: disable",
        "		 * all non-matching Legacy Major LSMs.",
        "		 */",
        "		for (major = __start_lsm_info; major < __end_lsm_info;",
        "		     major++) {",
        "			if ((major->flags & LSM_FLAG_LEGACY_MAJOR) &&",
        "			    strcmp(major->name, chosen_major_lsm) != 0) {",
        "				set_enabled(major, false);",
        "				init_debug(\"security=%s disabled: %s (only one legacy major LSM)\\n\",",
        "					   chosen_major_lsm, major->name);",
        "			}",
        "		}",
        "	}",
        "",
        "	sep = kstrdup(order, GFP_KERNEL);",
        "	next = sep;",
        "	/* Walk the list, looking for matching LSMs. */",
        "	while ((name = strsep(&next, \",\")) != NULL) {",
        "		bool found = false;",
        "",
        "		for (lsm = __start_lsm_info; lsm < __end_lsm_info; lsm++) {",
        "			if (strcmp(lsm->name, name) == 0) {",
        "				if (lsm->order == LSM_ORDER_MUTABLE)",
        "					append_ordered_lsm(lsm, origin);",
        "				found = true;",
        "			}",
        "		}",
        "",
        "		if (!found)",
        "			init_debug(\"%s ignored: %s (not built into kernel)\\n\",",
        "				   origin, name);",
        "	}",
        "",
        "	/* Process \"security=\", if given. */",
        "	if (chosen_major_lsm) {",
        "		for (lsm = __start_lsm_info; lsm < __end_lsm_info; lsm++) {",
        "			if (exists_ordered_lsm(lsm))",
        "				continue;",
        "			if (strcmp(lsm->name, chosen_major_lsm) == 0)",
        "				append_ordered_lsm(lsm, \"security=\");",
        "		}",
        "	}",
        "",
        "	/* LSM_ORDER_LAST is always last. */",
        "	for (lsm = __start_lsm_info; lsm < __end_lsm_info; lsm++) {",
        "		if (lsm->order == LSM_ORDER_LAST)",
        "			append_ordered_lsm(lsm, \"   last\");",
        "	}",
        "",
        "	/* Disable all LSMs not in the ordered list. */",
        "	for (lsm = __start_lsm_info; lsm < __end_lsm_info; lsm++) {",
        "		if (exists_ordered_lsm(lsm))",
        "			continue;",
        "		set_enabled(lsm, false);",
        "		init_debug(\"%s skipped: %s (not in requested order)\\n\",",
        "			   origin, lsm->name);",
        "	}",
        "",
        "	kfree(sep);",
        "}",
        "",
        "static void __init lsm_static_call_init(struct security_hook_list *hl)",
        "{",
        "	struct lsm_static_call *scall = hl->scalls;",
        "	int i;",
        "",
        "	for (i = 0; i < MAX_LSM_COUNT; i++) {",
        "		/* Update the first static call that is not used yet */",
        "		if (!scall->hl) {",
        "			__static_call_update(scall->key, scall->trampoline,",
        "					     hl->hook.lsm_func_addr);",
        "			scall->hl = hl;",
        "			static_branch_enable(scall->active);",
        "			return;",
        "		}",
        "		scall++;",
        "	}",
        "	panic(\"%s - Ran out of static slots.\\n\", __func__);",
        "}",
        "",
        "static void __init lsm_early_cred(struct cred *cred);",
        "static void __init lsm_early_task(struct task_struct *task);",
        "",
        "static int lsm_append(const char *new, char **result);",
        "",
        "static void __init report_lsm_order(void)",
        "{",
        "	struct lsm_info **lsm, *early;",
        "	int first = 0;",
        "",
        "	pr_info(\"initializing lsm=\");",
        "",
        "	/* Report each enabled LSM name, comma separated. */",
        "	for (early = __start_early_lsm_info;",
        "	     early < __end_early_lsm_info; early++)",
        "		if (is_enabled(early))",
        "			pr_cont(\"%s%s\", first++ == 0 ? \"\" : \",\", early->name);",
        "	for (lsm = ordered_lsms; *lsm; lsm++)",
        "		if (is_enabled(*lsm))",
        "			pr_cont(\"%s%s\", first++ == 0 ? \"\" : \",\", (*lsm)->name);",
        "",
        "	pr_cont(\"\\n\");",
        "}",
        "",
        "static void __init ordered_lsm_init(void)",
        "{",
        "	struct lsm_info **lsm;",
        "",
        "	if (chosen_lsm_order) {",
        "		if (chosen_major_lsm) {",
        "			pr_warn(\"security=%s is ignored because it is superseded by lsm=%s\\n\",",
        "				chosen_major_lsm, chosen_lsm_order);",
        "			chosen_major_lsm = NULL;",
        "		}",
        "		ordered_lsm_parse(chosen_lsm_order, \"cmdline\");",
        "	} else",
        "		ordered_lsm_parse(builtin_lsm_order, \"builtin\");",
        "",
        "	for (lsm = ordered_lsms; *lsm; lsm++)",
        "		prepare_lsm(*lsm);",
        "",
        "	report_lsm_order();",
        "",
        "	init_debug(\"cred blob size       = %d\\n\", blob_sizes.lbs_cred);",
        "	init_debug(\"file blob size       = %d\\n\", blob_sizes.lbs_file);",
        "	init_debug(\"ib blob size         = %d\\n\", blob_sizes.lbs_ib);",
        "	init_debug(\"inode blob size      = %d\\n\", blob_sizes.lbs_inode);",
        "	init_debug(\"ipc blob size        = %d\\n\", blob_sizes.lbs_ipc);",
        "#ifdef CONFIG_KEYS",
        "	init_debug(\"key blob size        = %d\\n\", blob_sizes.lbs_key);",
        "#endif /* CONFIG_KEYS */",
        "	init_debug(\"msg_msg blob size    = %d\\n\", blob_sizes.lbs_msg_msg);",
        "	init_debug(\"sock blob size       = %d\\n\", blob_sizes.lbs_sock);",
        "	init_debug(\"superblock blob size = %d\\n\", blob_sizes.lbs_superblock);",
        "	init_debug(\"perf event blob size = %d\\n\", blob_sizes.lbs_perf_event);",
        "	init_debug(\"task blob size       = %d\\n\", blob_sizes.lbs_task);",
        "	init_debug(\"tun device blob size = %d\\n\", blob_sizes.lbs_tun_dev);",
        "	init_debug(\"xattr slots          = %d\\n\", blob_sizes.lbs_xattr_count);",
        "	init_debug(\"bdev blob size       = %d\\n\", blob_sizes.lbs_bdev);",
        "",
        "	/*",
        "	 * Create any kmem_caches needed for blobs",
        "	 */",
        "	if (blob_sizes.lbs_file)",
        "		lsm_file_cache = kmem_cache_create(\"lsm_file_cache\",",
        "						   blob_sizes.lbs_file, 0,",
        "						   SLAB_PANIC, NULL);",
        "	if (blob_sizes.lbs_inode)",
        "		lsm_inode_cache = kmem_cache_create(\"lsm_inode_cache\",",
        "						    blob_sizes.lbs_inode, 0,",
        "						    SLAB_PANIC, NULL);",
        "",
        "	lsm_early_cred((struct cred *) current->cred);",
        "	lsm_early_task(current);",
        "	for (lsm = ordered_lsms; *lsm; lsm++)",
        "		initialize_lsm(*lsm);",
        "}",
        "",
        "int __init early_security_init(void)",
        "{",
        "	struct lsm_info *lsm;",
        "",
        "	for (lsm = __start_early_lsm_info; lsm < __end_early_lsm_info; lsm++) {",
        "		if (!lsm->enabled)",
        "			lsm->enabled = &lsm_enabled_true;",
        "		prepare_lsm(lsm);",
        "		initialize_lsm(lsm);",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * security_init - initializes the security framework",
        " *",
        " * This should be called early in the kernel initialization sequence.",
        " */",
        "int __init security_init(void)",
        "{",
        "	struct lsm_info *lsm;",
        "",
        "	init_debug(\"legacy security=%s\\n\", chosen_major_lsm ? : \" *unspecified*\");",
        "	init_debug(\"  CONFIG_LSM=%s\\n\", builtin_lsm_order);",
        "	init_debug(\"boot arg lsm=%s\\n\", chosen_lsm_order ? : \" *unspecified*\");",
        "",
        "	/*",
        "	 * Append the names of the early LSM modules now that kmalloc() is",
        "	 * available",
        "	 */",
        "	for (lsm = __start_early_lsm_info; lsm < __end_early_lsm_info; lsm++) {",
        "		init_debug(\"  early started: %s (%s)\\n\", lsm->name,",
        "			   is_enabled(lsm) ? \"enabled\" : \"disabled\");",
        "		if (lsm->enabled)",
        "			lsm_append(lsm->name, &lsm_names);",
        "	}",
        "",
        "	/* Load LSMs in specified order. */",
        "	ordered_lsm_init();",
        "",
        "	return 0;",
        "}",
        "",
        "/* Save user chosen LSM */",
        "static int __init choose_major_lsm(char *str)",
        "{",
        "	chosen_major_lsm = str;",
        "	return 1;",
        "}",
        "__setup(\"security=\", choose_major_lsm);",
        "",
        "/* Explicitly choose LSM initialization order. */",
        "static int __init choose_lsm_order(char *str)",
        "{",
        "	chosen_lsm_order = str;",
        "	return 1;",
        "}",
        "__setup(\"lsm=\", choose_lsm_order);",
        "",
        "/* Enable LSM order debugging. */",
        "static int __init enable_debug(char *str)",
        "{",
        "	debug = true;",
        "	return 1;",
        "}",
        "__setup(\"lsm.debug\", enable_debug);",
        "",
        "static bool match_last_lsm(const char *list, const char *lsm)",
        "{",
        "	const char *last;",
        "",
        "	if (WARN_ON(!list || !lsm))",
        "		return false;",
        "	last = strrchr(list, ',');",
        "	if (last)",
        "		/* Pass the comma, strcmp() will check for '\\0' */",
        "		last++;",
        "	else",
        "		last = list;",
        "	return !strcmp(last, lsm);",
        "}",
        "",
        "static int lsm_append(const char *new, char **result)",
        "{",
        "	char *cp;",
        "",
        "	if (*result == NULL) {",
        "		*result = kstrdup(new, GFP_KERNEL);",
        "		if (*result == NULL)",
        "			return -ENOMEM;",
        "	} else {",
        "		/* Check if it is the last registered name */",
        "		if (match_last_lsm(*result, new))",
        "			return 0;",
        "		cp = kasprintf(GFP_KERNEL, \"%s,%s\", *result, new);",
        "		if (cp == NULL)",
        "			return -ENOMEM;",
        "		kfree(*result);",
        "		*result = cp;",
        "	}",
        "	return 0;",
        "}",
        "",
        "/**",
        " * security_add_hooks - Add a modules hooks to the hook lists.",
        " * @hooks: the hooks to add",
        " * @count: the number of hooks to add",
        " * @lsmid: the identification information for the security module",
        " *",
        " * Each LSM has to register its hooks with the infrastructure.",
        " */",
        "void __init security_add_hooks(struct security_hook_list *hooks, int count,",
        "			       const struct lsm_id *lsmid)",
        "{",
        "	int i;",
        "",
        "	/*",
        "	 * A security module may call security_add_hooks() more",
        "	 * than once during initialization, and LSM initialization",
        "	 * is serialized. Landlock is one such case.",
        "	 * Look at the previous entry, if there is one, for duplication.",
        "	 */",
        "	if (lsm_active_cnt == 0 || lsm_idlist[lsm_active_cnt - 1] != lsmid) {",
        "		if (lsm_active_cnt >= MAX_LSM_COUNT)",
        "			panic(\"%s Too many LSMs registered.\\n\", __func__);",
        "		lsm_idlist[lsm_active_cnt++] = lsmid;",
        "	}",
        "",
        "	for (i = 0; i < count; i++) {",
        "		hooks[i].lsmid = lsmid;",
        "		lsm_static_call_init(&hooks[i]);",
        "	}",
        "",
        "	/*",
        "	 * Don't try to append during early_security_init(), we'll come back",
        "	 * and fix this up afterwards.",
        "	 */",
        "	if (slab_is_available()) {",
        "		if (lsm_append(lsmid->name, &lsm_names) < 0)",
        "			panic(\"%s - Cannot get early memory.\\n\", __func__);",
        "	}",
        "}",
        "",
        "int call_blocking_lsm_notifier(enum lsm_event event, void *data)",
        "{",
        "	return blocking_notifier_call_chain(&blocking_lsm_notifier_chain,",
        "					    event, data);",
        "}",
        "EXPORT_SYMBOL(call_blocking_lsm_notifier);",
        "",
        "int register_blocking_lsm_notifier(struct notifier_block *nb)",
        "{",
        "	return blocking_notifier_chain_register(&blocking_lsm_notifier_chain,",
        "						nb);",
        "}",
        "EXPORT_SYMBOL(register_blocking_lsm_notifier);",
        "",
        "int unregister_blocking_lsm_notifier(struct notifier_block *nb)",
        "{",
        "	return blocking_notifier_chain_unregister(&blocking_lsm_notifier_chain,",
        "						  nb);",
        "}",
        "EXPORT_SYMBOL(unregister_blocking_lsm_notifier);",
        "",
        "/**",
        " * lsm_blob_alloc - allocate a composite blob",
        " * @dest: the destination for the blob",
        " * @size: the size of the blob",
        " * @gfp: allocation type",
        " *",
        " * Allocate a blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_blob_alloc(void **dest, size_t size, gfp_t gfp)",
        "{",
        "	if (size == 0) {",
        "		*dest = NULL;",
        "		return 0;",
        "	}",
        "",
        "	*dest = kzalloc(size, gfp);",
        "	if (*dest == NULL)",
        "		return -ENOMEM;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * lsm_cred_alloc - allocate a composite cred blob",
        " * @cred: the cred that needs a blob",
        " * @gfp: allocation type",
        " *",
        " * Allocate the cred blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_cred_alloc(struct cred *cred, gfp_t gfp)",
        "{",
        "	return lsm_blob_alloc(&cred->security, blob_sizes.lbs_cred, gfp);",
        "}",
        "",
        "/**",
        " * lsm_early_cred - during initialization allocate a composite cred blob",
        " * @cred: the cred that needs a blob",
        " *",
        " * Allocate the cred blob for all the modules",
        " */",
        "static void __init lsm_early_cred(struct cred *cred)",
        "{",
        "	int rc = lsm_cred_alloc(cred, GFP_KERNEL);",
        "",
        "	if (rc)",
        "		panic(\"%s: Early cred alloc failed.\\n\", __func__);",
        "}",
        "",
        "/**",
        " * lsm_file_alloc - allocate a composite file blob",
        " * @file: the file that needs a blob",
        " *",
        " * Allocate the file blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_file_alloc(struct file *file)",
        "{",
        "	if (!lsm_file_cache) {",
        "		file->f_security = NULL;",
        "		return 0;",
        "	}",
        "",
        "	file->f_security = kmem_cache_zalloc(lsm_file_cache, GFP_KERNEL);",
        "	if (file->f_security == NULL)",
        "		return -ENOMEM;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * lsm_inode_alloc - allocate a composite inode blob",
        " * @inode: the inode that needs a blob",
        " * @gfp: allocation flags",
        " *",
        " * Allocate the inode blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_inode_alloc(struct inode *inode, gfp_t gfp)",
        "{",
        "	if (!lsm_inode_cache) {",
        "		inode->i_security = NULL;",
        "		return 0;",
        "	}",
        "",
        "	inode->i_security = kmem_cache_zalloc(lsm_inode_cache, gfp);",
        "	if (inode->i_security == NULL)",
        "		return -ENOMEM;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * lsm_task_alloc - allocate a composite task blob",
        " * @task: the task that needs a blob",
        " *",
        " * Allocate the task blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_task_alloc(struct task_struct *task)",
        "{",
        "	return lsm_blob_alloc(&task->security, blob_sizes.lbs_task, GFP_KERNEL);",
        "}",
        "",
        "/**",
        " * lsm_ipc_alloc - allocate a composite ipc blob",
        " * @kip: the ipc that needs a blob",
        " *",
        " * Allocate the ipc blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_ipc_alloc(struct kern_ipc_perm *kip)",
        "{",
        "	return lsm_blob_alloc(&kip->security, blob_sizes.lbs_ipc, GFP_KERNEL);",
        "}",
        "",
        "#ifdef CONFIG_KEYS",
        "/**",
        " * lsm_key_alloc - allocate a composite key blob",
        " * @key: the key that needs a blob",
        " *",
        " * Allocate the key blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_key_alloc(struct key *key)",
        "{",
        "	return lsm_blob_alloc(&key->security, blob_sizes.lbs_key, GFP_KERNEL);",
        "}",
        "#endif /* CONFIG_KEYS */",
        "",
        "/**",
        " * lsm_msg_msg_alloc - allocate a composite msg_msg blob",
        " * @mp: the msg_msg that needs a blob",
        " *",
        " * Allocate the ipc blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_msg_msg_alloc(struct msg_msg *mp)",
        "{",
        "	return lsm_blob_alloc(&mp->security, blob_sizes.lbs_msg_msg,",
        "			      GFP_KERNEL);",
        "}",
        "",
        "/**",
        " * lsm_bdev_alloc - allocate a composite block_device blob",
        " * @bdev: the block_device that needs a blob",
        " *",
        " * Allocate the block_device blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_bdev_alloc(struct block_device *bdev)",
        "{",
        "	if (blob_sizes.lbs_bdev == 0) {",
        "		bdev->bd_security = NULL;",
        "		return 0;",
        "	}",
        "",
        "	bdev->bd_security = kzalloc(blob_sizes.lbs_bdev, GFP_KERNEL);",
        "	if (!bdev->bd_security)",
        "		return -ENOMEM;",
        "",
        "	return 0;",
        "}",
        "",
        "/**",
        " * lsm_early_task - during initialization allocate a composite task blob",
        " * @task: the task that needs a blob",
        " *",
        " * Allocate the task blob for all the modules",
        " */",
        "static void __init lsm_early_task(struct task_struct *task)",
        "{",
        "	int rc = lsm_task_alloc(task);",
        "",
        "	if (rc)",
        "		panic(\"%s: Early task alloc failed.\\n\", __func__);",
        "}",
        "",
        "/**",
        " * lsm_superblock_alloc - allocate a composite superblock blob",
        " * @sb: the superblock that needs a blob",
        " *",
        " * Allocate the superblock blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_superblock_alloc(struct super_block *sb)",
        "{",
        "	return lsm_blob_alloc(&sb->s_security, blob_sizes.lbs_superblock,",
        "			      GFP_KERNEL);",
        "}",
        "",
        "/**",
        " * lsm_fill_user_ctx - Fill a user space lsm_ctx structure",
        " * @uctx: a userspace LSM context to be filled",
        " * @uctx_len: available uctx size (input), used uctx size (output)",
        " * @val: the new LSM context value",
        " * @val_len: the size of the new LSM context value",
        " * @id: LSM id",
        " * @flags: LSM defined flags",
        " *",
        " * Fill all of the fields in a userspace lsm_ctx structure.  If @uctx is NULL",
        " * simply calculate the required size to output via @utc_len and return",
        " * success.",
        " *",
        " * Returns 0 on success, -E2BIG if userspace buffer is not large enough,",
        " * -EFAULT on a copyout error, -ENOMEM if memory can't be allocated.",
        " */",
        "int lsm_fill_user_ctx(struct lsm_ctx __user *uctx, u32 *uctx_len,",
        "		      void *val, size_t val_len,",
        "		      u64 id, u64 flags)",
        "{",
        "	struct lsm_ctx *nctx = NULL;",
        "	size_t nctx_len;",
        "	int rc = 0;",
        "",
        "	nctx_len = ALIGN(struct_size(nctx, ctx, val_len), sizeof(void *));",
        "	if (nctx_len > *uctx_len) {",
        "		rc = -E2BIG;",
        "		goto out;",
        "	}",
        "",
        "	/* no buffer - return success/0 and set @uctx_len to the req size */",
        "	if (!uctx)",
        "		goto out;",
        "",
        "	nctx = kzalloc(nctx_len, GFP_KERNEL);",
        "	if (nctx == NULL) {",
        "		rc = -ENOMEM;",
        "		goto out;",
        "	}",
        "	nctx->id = id;",
        "	nctx->flags = flags;",
        "	nctx->len = nctx_len;",
        "	nctx->ctx_len = val_len;",
        "	memcpy(nctx->ctx, val, val_len);",
        "",
        "	if (copy_to_user(uctx, nctx, nctx_len))",
        "		rc = -EFAULT;",
        "",
        "out:",
        "	kfree(nctx);",
        "	*uctx_len = nctx_len;",
        "	return rc;",
        "}",
        "",
        "/*",
        " * The default value of the LSM hook is defined in linux/lsm_hook_defs.h and",
        " * can be accessed with:",
        " *",
        " *	LSM_RET_DEFAULT(<hook_name>)",
        " *",
        " * The macros below define static constants for the default value of each",
        " * LSM hook.",
        " */",
        "#define LSM_RET_DEFAULT(NAME) (NAME##_default)",
        "#define DECLARE_LSM_RET_DEFAULT_void(DEFAULT, NAME)",
        "#define DECLARE_LSM_RET_DEFAULT_int(DEFAULT, NAME) \\",
        "	static const int __maybe_unused LSM_RET_DEFAULT(NAME) = (DEFAULT);",
        "#define LSM_HOOK(RET, DEFAULT, NAME, ...) \\",
        "	DECLARE_LSM_RET_DEFAULT_##RET(DEFAULT, NAME)",
        "",
        "#include <linux/lsm_hook_defs.h>",
        "#undef LSM_HOOK",
        "",
        "/*",
        " * Hook list operation macros.",
        " *",
        " * call_void_hook:",
        " *	This is a hook that does not return a value.",
        " *",
        " * call_int_hook:",
        " *	This is a hook that returns a value.",
        " */",
        "#define __CALL_STATIC_VOID(NUM, HOOK, ...)				     \\",
        "do {									     \\",
        "	if (static_branch_unlikely(&SECURITY_HOOK_ACTIVE_KEY(HOOK, NUM))) {    \\",
        "		static_call(LSM_STATIC_CALL(HOOK, NUM))(__VA_ARGS__);	     \\",
        "	}								     \\",
        "} while (0);",
        "",
        "#define call_void_hook(HOOK, ...)                                 \\",
        "	do {                                                      \\",
        "		LSM_LOOP_UNROLL(__CALL_STATIC_VOID, HOOK, __VA_ARGS__); \\",
        "	} while (0)",
        "",
        "",
        "#define __CALL_STATIC_INT(NUM, R, HOOK, LABEL, ...)			     \\",
        "do {									     \\",
        "	if (static_branch_unlikely(&SECURITY_HOOK_ACTIVE_KEY(HOOK, NUM))) {  \\",
        "		R = static_call(LSM_STATIC_CALL(HOOK, NUM))(__VA_ARGS__);    \\",
        "		if (R != LSM_RET_DEFAULT(HOOK))				     \\",
        "			goto LABEL;					     \\",
        "	}								     \\",
        "} while (0);",
        "",
        "#define call_int_hook(HOOK, ...)					\\",
        "({									\\",
        "	__label__ OUT;							\\",
        "	int RC = LSM_RET_DEFAULT(HOOK);					\\",
        "									\\",
        "	LSM_LOOP_UNROLL(__CALL_STATIC_INT, RC, HOOK, OUT, __VA_ARGS__);	\\",
        "OUT:									\\",
        "	RC;								\\",
        "})",
        "",
        "#define lsm_for_each_hook(scall, NAME)					\\",
        "	for (scall = static_calls_table.NAME;				\\",
        "	     scall - static_calls_table.NAME < MAX_LSM_COUNT; scall++)  \\",
        "		if (static_key_enabled(&scall->active->key))",
        "",
        "/* Security operations */",
        "",
        "/**",
        " * security_binder_set_context_mgr() - Check if becoming binder ctx mgr is ok",
        " * @mgr: task credentials of current binder process",
        " *",
        " * Check whether @mgr is allowed to be the binder context manager.",
        " *",
        " * Return: Return 0 if permission is granted.",
        " */",
        "int security_binder_set_context_mgr(const struct cred *mgr)",
        "{",
        "	return call_int_hook(binder_set_context_mgr, mgr);",
        "}",
        "",
        "/**",
        " * security_binder_transaction() - Check if a binder transaction is allowed",
        " * @from: sending process",
        " * @to: receiving process",
        " *",
        " * Check whether @from is allowed to invoke a binder transaction call to @to.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_binder_transaction(const struct cred *from,",
        "				const struct cred *to)",
        "{",
        "	return call_int_hook(binder_transaction, from, to);",
        "}",
        "",
        "/**",
        " * security_binder_transfer_binder() - Check if a binder transfer is allowed",
        " * @from: sending process",
        " * @to: receiving process",
        " *",
        " * Check whether @from is allowed to transfer a binder reference to @to.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_binder_transfer_binder(const struct cred *from,",
        "				    const struct cred *to)",
        "{",
        "	return call_int_hook(binder_transfer_binder, from, to);",
        "}",
        "",
        "/**",
        " * security_binder_transfer_file() - Check if a binder file xfer is allowed",
        " * @from: sending process",
        " * @to: receiving process",
        " * @file: file being transferred",
        " *",
        " * Check whether @from is allowed to transfer @file to @to.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_binder_transfer_file(const struct cred *from,",
        "				  const struct cred *to, const struct file *file)",
        "{",
        "	return call_int_hook(binder_transfer_file, from, to, file);",
        "}",
        "",
        "/**",
        " * security_ptrace_access_check() - Check if tracing is allowed",
        " * @child: target process",
        " * @mode: PTRACE_MODE flags",
        " *",
        " * Check permission before allowing the current process to trace the @child",
        " * process.  Security modules may also want to perform a process tracing check",
        " * during an execve in the set_security or apply_creds hooks of tracing check",
        " * during an execve in the bprm_set_creds hook of binprm_security_ops if the",
        " * process is being traced and its security attributes would be changed by the",
        " * execve.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_ptrace_access_check(struct task_struct *child, unsigned int mode)",
        "{",
        "	return call_int_hook(ptrace_access_check, child, mode);",
        "}",
        "",
        "/**",
        " * security_ptrace_traceme() - Check if tracing is allowed",
        " * @parent: tracing process",
        " *",
        " * Check that the @parent process has sufficient permission to trace the",
        " * current process before allowing the current process to present itself to the",
        " * @parent process for tracing.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_ptrace_traceme(struct task_struct *parent)",
        "{",
        "	return call_int_hook(ptrace_traceme, parent);",
        "}",
        "",
        "/**",
        " * security_capget() - Get the capability sets for a process",
        " * @target: target process",
        " * @effective: effective capability set",
        " * @inheritable: inheritable capability set",
        " * @permitted: permitted capability set",
        " *",
        " * Get the @effective, @inheritable, and @permitted capability sets for the",
        " * @target process.  The hook may also perform permission checking to determine",
        " * if the current process is allowed to see the capability sets of the @target",
        " * process.",
        " *",
        " * Return: Returns 0 if the capability sets were successfully obtained.",
        " */",
        "int security_capget(const struct task_struct *target,",
        "		    kernel_cap_t *effective,",
        "		    kernel_cap_t *inheritable,",
        "		    kernel_cap_t *permitted)",
        "{",
        "	return call_int_hook(capget, target, effective, inheritable, permitted);",
        "}",
        "",
        "/**",
        " * security_capset() - Set the capability sets for a process",
        " * @new: new credentials for the target process",
        " * @old: current credentials of the target process",
        " * @effective: effective capability set",
        " * @inheritable: inheritable capability set",
        " * @permitted: permitted capability set",
        " *",
        " * Set the @effective, @inheritable, and @permitted capability sets for the",
        " * current process.",
        " *",
        " * Return: Returns 0 and update @new if permission is granted.",
        " */",
        "int security_capset(struct cred *new, const struct cred *old,",
        "		    const kernel_cap_t *effective,",
        "		    const kernel_cap_t *inheritable,",
        "		    const kernel_cap_t *permitted)",
        "{",
        "	return call_int_hook(capset, new, old, effective, inheritable,",
        "			     permitted);",
        "}",
        "",
        "/**",
        " * security_capable() - Check if a process has the necessary capability",
        " * @cred: credentials to examine",
        " * @ns: user namespace",
        " * @cap: capability requested",
        " * @opts: capability check options",
        " *",
        " * Check whether the @tsk process has the @cap capability in the indicated",
        " * credentials.  @cap contains the capability <include/linux/capability.h>.",
        " * @opts contains options for the capable check <include/linux/security.h>.",
        " *",
        " * Return: Returns 0 if the capability is granted.",
        " */",
        "int security_capable(const struct cred *cred,",
        "		     struct user_namespace *ns,",
        "		     int cap,",
        "		     unsigned int opts)",
        "{",
        "	return call_int_hook(capable, cred, ns, cap, opts);",
        "}",
        "",
        "/**",
        " * security_quotactl() - Check if a quotactl() syscall is allowed for this fs",
        " * @cmds: commands",
        " * @type: type",
        " * @id: id",
        " * @sb: filesystem",
        " *",
        " * Check whether the quotactl syscall is allowed for this @sb.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_quotactl(int cmds, int type, int id, const struct super_block *sb)",
        "{",
        "	return call_int_hook(quotactl, cmds, type, id, sb);",
        "}",
        "",
        "/**",
        " * security_quota_on() - Check if QUOTAON is allowed for a dentry",
        " * @dentry: dentry",
        " *",
        " * Check whether QUOTAON is allowed for @dentry.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_quota_on(struct dentry *dentry)",
        "{",
        "	return call_int_hook(quota_on, dentry);",
        "}",
        "",
        "/**",
        " * security_syslog() - Check if accessing the kernel message ring is allowed",
        " * @type: SYSLOG_ACTION_* type",
        " *",
        " * Check permission before accessing the kernel message ring or changing",
        " * logging to the console.  See the syslog(2) manual page for an explanation of",
        " * the @type values.",
        " *",
        " * Return: Return 0 if permission is granted.",
        " */",
        "int security_syslog(int type)",
        "{",
        "	return call_int_hook(syslog, type);",
        "}",
        "",
        "/**",
        " * security_settime64() - Check if changing the system time is allowed",
        " * @ts: new time",
        " * @tz: timezone",
        " *",
        " * Check permission to change the system time, struct timespec64 is defined in",
        " * <include/linux/time64.h> and timezone is defined in <include/linux/time.h>.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_settime64(const struct timespec64 *ts, const struct timezone *tz)",
        "{",
        "	return call_int_hook(settime, ts, tz);",
        "}",
        "",
        "/**",
        " * security_vm_enough_memory_mm() - Check if allocating a new mem map is allowed",
        " * @mm: mm struct",
        " * @pages: number of pages",
        " *",
        " * Check permissions for allocating a new virtual mapping.  If all LSMs return",
        " * a positive value, __vm_enough_memory() will be called with cap_sys_admin",
        " * set. If at least one LSM returns 0 or negative, __vm_enough_memory() will be",
        " * called with cap_sys_admin cleared.",
        " *",
        " * Return: Returns 0 if permission is granted by the LSM infrastructure to the",
        " *         caller.",
        " */",
        "int security_vm_enough_memory_mm(struct mm_struct *mm, long pages)",
        "{",
        "	struct lsm_static_call *scall;",
        "	int cap_sys_admin = 1;",
        "	int rc;",
        "",
        "	/*",
        "	 * The module will respond with 0 if it thinks the __vm_enough_memory()",
        "	 * call should be made with the cap_sys_admin set. If all of the modules",
        "	 * agree that it should be set it will. If any module thinks it should",
        "	 * not be set it won't.",
        "	 */",
        "	lsm_for_each_hook(scall, vm_enough_memory) {",
        "		rc = scall->hl->hook.vm_enough_memory(mm, pages);",
        "		if (rc < 0) {",
        "			cap_sys_admin = 0;",
        "			break;",
        "		}",
        "	}",
        "	return __vm_enough_memory(mm, pages, cap_sys_admin);",
        "}",
        "",
        "/**",
        " * security_bprm_creds_for_exec() - Prepare the credentials for exec()",
        " * @bprm: binary program information",
        " *",
        " * If the setup in prepare_exec_creds did not setup @bprm->cred->security",
        " * properly for executing @bprm->file, update the LSM's portion of",
        " * @bprm->cred->security to be what commit_creds needs to install for the new",
        " * program.  This hook may also optionally check permissions (e.g. for",
        " * transitions between security domains).  The hook must set @bprm->secureexec",
        " * to 1 if AT_SECURE should be set to request libc enable secure mode.  @bprm",
        " * contains the linux_binprm structure.",
        " *",
        " * Return: Returns 0 if the hook is successful and permission is granted.",
        " */",
        "int security_bprm_creds_for_exec(struct linux_binprm *bprm)",
        "{",
        "	return call_int_hook(bprm_creds_for_exec, bprm);",
        "}",
        "",
        "/**",
        " * security_bprm_creds_from_file() - Update linux_binprm creds based on file",
        " * @bprm: binary program information",
        " * @file: associated file",
        " *",
        " * If @file is setpcap, suid, sgid or otherwise marked to change privilege upon",
        " * exec, update @bprm->cred to reflect that change. This is called after",
        " * finding the binary that will be executed without an interpreter.  This",
        " * ensures that the credentials will not be derived from a script that the",
        " * binary will need to reopen, which when reopend may end up being a completely",
        " * different file.  This hook may also optionally check permissions (e.g. for",
        " * transitions between security domains).  The hook must set @bprm->secureexec",
        " * to 1 if AT_SECURE should be set to request libc enable secure mode.  The",
        " * hook must add to @bprm->per_clear any personality flags that should be",
        " * cleared from current->personality.  @bprm contains the linux_binprm",
        " * structure.",
        " *",
        " * Return: Returns 0 if the hook is successful and permission is granted.",
        " */",
        "int security_bprm_creds_from_file(struct linux_binprm *bprm, const struct file *file)",
        "{",
        "	return call_int_hook(bprm_creds_from_file, bprm, file);",
        "}",
        "",
        "/**",
        " * security_bprm_check() - Mediate binary handler search",
        " * @bprm: binary program information",
        " *",
        " * This hook mediates the point when a search for a binary handler will begin.",
        " * It allows a check against the @bprm->cred->security value which was set in",
        " * the preceding creds_for_exec call.  The argv list and envp list are reliably",
        " * available in @bprm.  This hook may be called multiple times during a single",
        " * execve.  @bprm contains the linux_binprm structure.",
        " *",
        " * Return: Returns 0 if the hook is successful and permission is granted.",
        " */",
        "int security_bprm_check(struct linux_binprm *bprm)",
        "{",
        "	return call_int_hook(bprm_check_security, bprm);",
        "}",
        "",
        "/**",
        " * security_bprm_committing_creds() - Install creds for a process during exec()",
        " * @bprm: binary program information",
        " *",
        " * Prepare to install the new security attributes of a process being",
        " * transformed by an execve operation, based on the old credentials pointed to",
        " * by @current->cred and the information set in @bprm->cred by the",
        " * bprm_creds_for_exec hook.  @bprm points to the linux_binprm structure.  This",
        " * hook is a good place to perform state changes on the process such as closing",
        " * open file descriptors to which access will no longer be granted when the",
        " * attributes are changed.  This is called immediately before commit_creds().",
        " */",
        "void security_bprm_committing_creds(const struct linux_binprm *bprm)",
        "{",
        "	call_void_hook(bprm_committing_creds, bprm);",
        "}",
        "",
        "/**",
        " * security_bprm_committed_creds() - Tidy up after cred install during exec()",
        " * @bprm: binary program information",
        " *",
        " * Tidy up after the installation of the new security attributes of a process",
        " * being transformed by an execve operation.  The new credentials have, by this",
        " * point, been set to @current->cred.  @bprm points to the linux_binprm",
        " * structure.  This hook is a good place to perform state changes on the",
        " * process such as clearing out non-inheritable signal state.  This is called",
        " * immediately after commit_creds().",
        " */",
        "void security_bprm_committed_creds(const struct linux_binprm *bprm)",
        "{",
        "	call_void_hook(bprm_committed_creds, bprm);",
        "}",
        "",
        "/**",
        " * security_fs_context_submount() - Initialise fc->security",
        " * @fc: new filesystem context",
        " * @reference: dentry reference for submount/remount",
        " *",
        " * Fill out the ->security field for a new fs_context.",
        " *",
        " * Return: Returns 0 on success or negative error code on failure.",
        " */",
        "int security_fs_context_submount(struct fs_context *fc, struct super_block *reference)",
        "{",
        "	return call_int_hook(fs_context_submount, fc, reference);",
        "}",
        "",
        "/**",
        " * security_fs_context_dup() - Duplicate a fs_context LSM blob",
        " * @fc: destination filesystem context",
        " * @src_fc: source filesystem context",
        " *",
        " * Allocate and attach a security structure to sc->security.  This pointer is",
        " * initialised to NULL by the caller.  @fc indicates the new filesystem context.",
        " * @src_fc indicates the original filesystem context.",
        " *",
        " * Return: Returns 0 on success or a negative error code on failure.",
        " */",
        "int security_fs_context_dup(struct fs_context *fc, struct fs_context *src_fc)",
        "{",
        "	return call_int_hook(fs_context_dup, fc, src_fc);",
        "}",
        "",
        "/**",
        " * security_fs_context_parse_param() - Configure a filesystem context",
        " * @fc: filesystem context",
        " * @param: filesystem parameter",
        " *",
        " * Userspace provided a parameter to configure a superblock.  The LSM can",
        " * consume the parameter or return it to the caller for use elsewhere.",
        " *",
        " * Return: If the parameter is used by the LSM it should return 0, if it is",
        " *         returned to the caller -ENOPARAM is returned, otherwise a negative",
        " *         error code is returned.",
        " */",
        "int security_fs_context_parse_param(struct fs_context *fc,",
        "				    struct fs_parameter *param)",
        "{",
        "	struct lsm_static_call *scall;",
        "	int trc;",
        "	int rc = -ENOPARAM;",
        "",
        "	lsm_for_each_hook(scall, fs_context_parse_param) {",
        "		trc = scall->hl->hook.fs_context_parse_param(fc, param);",
        "		if (trc == 0)",
        "			rc = 0;",
        "		else if (trc != -ENOPARAM)",
        "			return trc;",
        "	}",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_sb_alloc() - Allocate a super_block LSM blob",
        " * @sb: filesystem superblock",
        " *",
        " * Allocate and attach a security structure to the sb->s_security field.  The",
        " * s_security field is initialized to NULL when the structure is allocated.",
        " * @sb contains the super_block structure to be modified.",
        " *",
        " * Return: Returns 0 if operation was successful.",
        " */",
        "int security_sb_alloc(struct super_block *sb)",
        "{",
        "	int rc = lsm_superblock_alloc(sb);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(sb_alloc_security, sb);",
        "	if (unlikely(rc))",
        "		security_sb_free(sb);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_sb_delete() - Release super_block LSM associated objects",
        " * @sb: filesystem superblock",
        " *",
        " * Release objects tied to a superblock (e.g. inodes).  @sb contains the",
        " * super_block structure being released.",
        " */",
        "void security_sb_delete(struct super_block *sb)",
        "{",
        "	call_void_hook(sb_delete, sb);",
        "}",
        "",
        "/**",
        " * security_sb_free() - Free a super_block LSM blob",
        " * @sb: filesystem superblock",
        " *",
        " * Deallocate and clear the sb->s_security field.  @sb contains the super_block",
        " * structure to be modified.",
        " */",
        "void security_sb_free(struct super_block *sb)",
        "{",
        "	call_void_hook(sb_free_security, sb);",
        "	kfree(sb->s_security);",
        "	sb->s_security = NULL;",
        "}",
        "",
        "/**",
        " * security_free_mnt_opts() - Free memory associated with mount options",
        " * @mnt_opts: LSM processed mount options",
        " *",
        " * Free memory associated with @mnt_ops.",
        " */",
        "void security_free_mnt_opts(void **mnt_opts)",
        "{",
        "	if (!*mnt_opts)",
        "		return;",
        "	call_void_hook(sb_free_mnt_opts, *mnt_opts);",
        "	*mnt_opts = NULL;",
        "}",
        "EXPORT_SYMBOL(security_free_mnt_opts);",
        "",
        "/**",
        " * security_sb_eat_lsm_opts() - Consume LSM mount options",
        " * @options: mount options",
        " * @mnt_opts: LSM processed mount options",
        " *",
        " * Eat (scan @options) and save them in @mnt_opts.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_sb_eat_lsm_opts(char *options, void **mnt_opts)",
        "{",
        "	return call_int_hook(sb_eat_lsm_opts, options, mnt_opts);",
        "}",
        "EXPORT_SYMBOL(security_sb_eat_lsm_opts);",
        "",
        "/**",
        " * security_sb_mnt_opts_compat() - Check if new mount options are allowed",
        " * @sb: filesystem superblock",
        " * @mnt_opts: new mount options",
        " *",
        " * Determine if the new mount options in @mnt_opts are allowed given the",
        " * existing mounted filesystem at @sb.  @sb superblock being compared.",
        " *",
        " * Return: Returns 0 if options are compatible.",
        " */",
        "int security_sb_mnt_opts_compat(struct super_block *sb,",
        "				void *mnt_opts)",
        "{",
        "	return call_int_hook(sb_mnt_opts_compat, sb, mnt_opts);",
        "}",
        "EXPORT_SYMBOL(security_sb_mnt_opts_compat);",
        "",
        "/**",
        " * security_sb_remount() - Verify no incompatible mount changes during remount",
        " * @sb: filesystem superblock",
        " * @mnt_opts: (re)mount options",
        " *",
        " * Extracts security system specific mount options and verifies no changes are",
        " * being made to those options.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_remount(struct super_block *sb,",
        "			void *mnt_opts)",
        "{",
        "	return call_int_hook(sb_remount, sb, mnt_opts);",
        "}",
        "EXPORT_SYMBOL(security_sb_remount);",
        "",
        "/**",
        " * security_sb_kern_mount() - Check if a kernel mount is allowed",
        " * @sb: filesystem superblock",
        " *",
        " * Mount this @sb if allowed by permissions.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_kern_mount(const struct super_block *sb)",
        "{",
        "	return call_int_hook(sb_kern_mount, sb);",
        "}",
        "",
        "/**",
        " * security_sb_show_options() - Output the mount options for a superblock",
        " * @m: output file",
        " * @sb: filesystem superblock",
        " *",
        " * Show (print on @m) mount options for this @sb.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_sb_show_options(struct seq_file *m, struct super_block *sb)",
        "{",
        "	return call_int_hook(sb_show_options, m, sb);",
        "}",
        "",
        "/**",
        " * security_sb_statfs() - Check if accessing fs stats is allowed",
        " * @dentry: superblock handle",
        " *",
        " * Check permission before obtaining filesystem statistics for the @mnt",
        " * mountpoint.  @dentry is a handle on the superblock for the filesystem.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_statfs(struct dentry *dentry)",
        "{",
        "	return call_int_hook(sb_statfs, dentry);",
        "}",
        "",
        "/**",
        " * security_sb_mount() - Check permission for mounting a filesystem",
        " * @dev_name: filesystem backing device",
        " * @path: mount point",
        " * @type: filesystem type",
        " * @flags: mount flags",
        " * @data: filesystem specific data",
        " *",
        " * Check permission before an object specified by @dev_name is mounted on the",
        " * mount point named by @nd.  For an ordinary mount, @dev_name identifies a",
        " * device if the file system type requires a device.  For a remount",
        " * (@flags & MS_REMOUNT), @dev_name is irrelevant.  For a loopback/bind mount",
        " * (@flags & MS_BIND), @dev_name identifies the	pathname of the object being",
        " * mounted.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_mount(const char *dev_name, const struct path *path,",
        "		      const char *type, unsigned long flags, void *data)",
        "{",
        "	return call_int_hook(sb_mount, dev_name, path, type, flags, data);",
        "}",
        "",
        "/**",
        " * security_sb_umount() - Check permission for unmounting a filesystem",
        " * @mnt: mounted filesystem",
        " * @flags: unmount flags",
        " *",
        " * Check permission before the @mnt file system is unmounted.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_umount(struct vfsmount *mnt, int flags)",
        "{",
        "	return call_int_hook(sb_umount, mnt, flags);",
        "}",
        "",
        "/**",
        " * security_sb_pivotroot() - Check permissions for pivoting the rootfs",
        " * @old_path: new location for current rootfs",
        " * @new_path: location of the new rootfs",
        " *",
        " * Check permission before pivoting the root filesystem.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sb_pivotroot(const struct path *old_path,",
        "			  const struct path *new_path)",
        "{",
        "	return call_int_hook(sb_pivotroot, old_path, new_path);",
        "}",
        "",
        "/**",
        " * security_sb_set_mnt_opts() - Set the mount options for a filesystem",
        " * @sb: filesystem superblock",
        " * @mnt_opts: binary mount options",
        " * @kern_flags: kernel flags (in)",
        " * @set_kern_flags: kernel flags (out)",
        " *",
        " * Set the security relevant mount options used for a superblock.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_sb_set_mnt_opts(struct super_block *sb,",
        "			     void *mnt_opts,",
        "			     unsigned long kern_flags,",
        "			     unsigned long *set_kern_flags)",
        "{",
        "	struct lsm_static_call *scall;",
        "	int rc = mnt_opts ? -EOPNOTSUPP : LSM_RET_DEFAULT(sb_set_mnt_opts);",
        "",
        "	lsm_for_each_hook(scall, sb_set_mnt_opts) {",
        "		rc = scall->hl->hook.sb_set_mnt_opts(sb, mnt_opts, kern_flags,",
        "					      set_kern_flags);",
        "		if (rc != LSM_RET_DEFAULT(sb_set_mnt_opts))",
        "			break;",
        "	}",
        "	return rc;",
        "}",
        "EXPORT_SYMBOL(security_sb_set_mnt_opts);",
        "",
        "/**",
        " * security_sb_clone_mnt_opts() - Duplicate superblock mount options",
        " * @oldsb: source superblock",
        " * @newsb: destination superblock",
        " * @kern_flags: kernel flags (in)",
        " * @set_kern_flags: kernel flags (out)",
        " *",
        " * Copy all security options from a given superblock to another.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_sb_clone_mnt_opts(const struct super_block *oldsb,",
        "			       struct super_block *newsb,",
        "			       unsigned long kern_flags,",
        "			       unsigned long *set_kern_flags)",
        "{",
        "	return call_int_hook(sb_clone_mnt_opts, oldsb, newsb,",
        "			     kern_flags, set_kern_flags);",
        "}",
        "EXPORT_SYMBOL(security_sb_clone_mnt_opts);",
        "",
        "/**",
        " * security_move_mount() - Check permissions for moving a mount",
        " * @from_path: source mount point",
        " * @to_path: destination mount point",
        " *",
        " * Check permission before a mount is moved.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_move_mount(const struct path *from_path,",
        "			const struct path *to_path)",
        "{",
        "	return call_int_hook(move_mount, from_path, to_path);",
        "}",
        "",
        "/**",
        " * security_path_notify() - Check if setting a watch is allowed",
        " * @path: file path",
        " * @mask: event mask",
        " * @obj_type: file path type",
        " *",
        " * Check permissions before setting a watch on events as defined by @mask, on",
        " * an object at @path, whose type is defined by @obj_type.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_notify(const struct path *path, u64 mask,",
        "			 unsigned int obj_type)",
        "{",
        "	return call_int_hook(path_notify, path, mask, obj_type);",
        "}",
        "",
        "/**",
        " * security_inode_alloc() - Allocate an inode LSM blob",
        " * @inode: the inode",
        " * @gfp: allocation flags",
        " *",
        " * Allocate and attach a security structure to @inode->i_security.  The",
        " * i_security field is initialized to NULL when the inode structure is",
        " * allocated.",
        " *",
        " * Return: Return 0 if operation was successful.",
        " */",
        "int security_inode_alloc(struct inode *inode, gfp_t gfp)",
        "{",
        "	int rc = lsm_inode_alloc(inode, gfp);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(inode_alloc_security, inode);",
        "	if (unlikely(rc))",
        "		security_inode_free(inode);",
        "	return rc;",
        "}",
        "",
        "static void inode_free_by_rcu(struct rcu_head *head)",
        "{",
        "	/* The rcu head is at the start of the inode blob */",
        "	call_void_hook(inode_free_security_rcu, head);",
        "	kmem_cache_free(lsm_inode_cache, head);",
        "}",
        "",
        "/**",
        " * security_inode_free() - Free an inode's LSM blob",
        " * @inode: the inode",
        " *",
        " * Release any LSM resources associated with @inode, although due to the",
        " * inode's RCU protections it is possible that the resources will not be",
        " * fully released until after the current RCU grace period has elapsed.",
        " *",
        " * It is important for LSMs to note that despite being present in a call to",
        " * security_inode_free(), @inode may still be referenced in a VFS path walk",
        " * and calls to security_inode_permission() may be made during, or after,",
        " * a call to security_inode_free().  For this reason the inode->i_security",
        " * field is released via a call_rcu() callback and any LSMs which need to",
        " * retain inode state for use in security_inode_permission() should only",
        " * release that state in the inode_free_security_rcu() LSM hook callback.",
        " */",
        "void security_inode_free(struct inode *inode)",
        "{",
        "	call_void_hook(inode_free_security, inode);",
        "	if (!inode->i_security)",
        "		return;",
        "	call_rcu((struct rcu_head *)inode->i_security, inode_free_by_rcu);",
        "}",
        "",
        "/**",
        " * security_dentry_init_security() - Perform dentry initialization",
        " * @dentry: the dentry to initialize",
        " * @mode: mode used to determine resource type",
        " * @name: name of the last path component",
        " * @xattr_name: name of the security/LSM xattr",
        " * @ctx: pointer to the resulting LSM context",
        " * @ctxlen: length of @ctx",
        " *",
        " * Compute a context for a dentry as the inode is not yet available since NFSv4",
        " * has no label backed by an EA anyway.  It is important to note that",
        " * @xattr_name does not need to be free'd by the caller, it is a static string.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_dentry_init_security(struct dentry *dentry, int mode,",
        "				  const struct qstr *name,",
        "				  const char **xattr_name, void **ctx,",
        "				  u32 *ctxlen)",
        "{",
        "	return call_int_hook(dentry_init_security, dentry, mode, name,",
        "			     xattr_name, ctx, ctxlen);",
        "}",
        "EXPORT_SYMBOL(security_dentry_init_security);",
        "",
        "/**",
        " * security_dentry_create_files_as() - Perform dentry initialization",
        " * @dentry: the dentry to initialize",
        " * @mode: mode used to determine resource type",
        " * @name: name of the last path component",
        " * @old: creds to use for LSM context calculations",
        " * @new: creds to modify",
        " *",
        " * Compute a context for a dentry as the inode is not yet available and set",
        " * that context in passed in creds so that new files are created using that",
        " * context. Context is calculated using the passed in creds and not the creds",
        " * of the caller.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_dentry_create_files_as(struct dentry *dentry, int mode,",
        "				    struct qstr *name,",
        "				    const struct cred *old, struct cred *new)",
        "{",
        "	return call_int_hook(dentry_create_files_as, dentry, mode,",
        "			     name, old, new);",
        "}",
        "EXPORT_SYMBOL(security_dentry_create_files_as);",
        "",
        "/**",
        " * security_inode_init_security() - Initialize an inode's LSM context",
        " * @inode: the inode",
        " * @dir: parent directory",
        " * @qstr: last component of the pathname",
        " * @initxattrs: callback function to write xattrs",
        " * @fs_data: filesystem specific data",
        " *",
        " * Obtain the security attribute name suffix and value to set on a newly",
        " * created inode and set up the incore security field for the new inode.  This",
        " * hook is called by the fs code as part of the inode creation transaction and",
        " * provides for atomic labeling of the inode, unlike the post_create/mkdir/...",
        " * hooks called by the VFS.",
        " *",
        " * The hook function is expected to populate the xattrs array, by calling",
        " * lsm_get_xattr_slot() to retrieve the slots reserved by the security module",
        " * with the lbs_xattr_count field of the lsm_blob_sizes structure.  For each",
        " * slot, the hook function should set ->name to the attribute name suffix",
        " * (e.g. selinux), to allocate ->value (will be freed by the caller) and set it",
        " * to the attribute value, to set ->value_len to the length of the value.  If",
        " * the security module does not use security attributes or does not wish to put",
        " * a security attribute on this particular inode, then it should return",
        " * -EOPNOTSUPP to skip this processing.",
        " *",
        " * Return: Returns 0 if the LSM successfully initialized all of the inode",
        " *         security attributes that are required, negative values otherwise.",
        " */",
        "int security_inode_init_security(struct inode *inode, struct inode *dir,",
        "				 const struct qstr *qstr,",
        "				 const initxattrs initxattrs, void *fs_data)",
        "{",
        "	struct lsm_static_call *scall;",
        "	struct xattr *new_xattrs = NULL;",
        "	int ret = -EOPNOTSUPP, xattr_count = 0;",
        "",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return 0;",
        "",
        "	if (!blob_sizes.lbs_xattr_count)",
        "		return 0;",
        "",
        "	if (initxattrs) {",
        "		/* Allocate +1 as terminator. */",
        "		new_xattrs = kcalloc(blob_sizes.lbs_xattr_count + 1,",
        "				     sizeof(*new_xattrs), GFP_NOFS);",
        "		if (!new_xattrs)",
        "			return -ENOMEM;",
        "	}",
        "",
        "	lsm_for_each_hook(scall, inode_init_security) {",
        "		ret = scall->hl->hook.inode_init_security(inode, dir, qstr, new_xattrs,",
        "						  &xattr_count);",
        "		if (ret && ret != -EOPNOTSUPP)",
        "			goto out;",
        "		/*",
        "		 * As documented in lsm_hooks.h, -EOPNOTSUPP in this context",
        "		 * means that the LSM is not willing to provide an xattr, not",
        "		 * that it wants to signal an error. Thus, continue to invoke",
        "		 * the remaining LSMs.",
        "		 */",
        "	}",
        "",
        "	/* If initxattrs() is NULL, xattr_count is zero, skip the call. */",
        "	if (!xattr_count)",
        "		goto out;",
        "",
        "	ret = initxattrs(inode, new_xattrs, fs_data);",
        "out:",
        "	for (; xattr_count > 0; xattr_count--)",
        "		kfree(new_xattrs[xattr_count - 1].value);",
        "	kfree(new_xattrs);",
        "	return (ret == -EOPNOTSUPP) ? 0 : ret;",
        "}",
        "EXPORT_SYMBOL(security_inode_init_security);",
        "",
        "/**",
        " * security_inode_init_security_anon() - Initialize an anonymous inode",
        " * @inode: the inode",
        " * @name: the anonymous inode class",
        " * @context_inode: an optional related inode",
        " *",
        " * Set up the incore security field for the new anonymous inode and return",
        " * whether the inode creation is permitted by the security module or not.",
        " *",
        " * Return: Returns 0 on success, -EACCES if the security module denies the",
        " * creation of this inode, or another -errno upon other errors.",
        " */",
        "int security_inode_init_security_anon(struct inode *inode,",
        "				      const struct qstr *name,",
        "				      const struct inode *context_inode)",
        "{",
        "	return call_int_hook(inode_init_security_anon, inode, name,",
        "			     context_inode);",
        "}",
        "",
        "#ifdef CONFIG_SECURITY_PATH",
        "/**",
        " * security_path_mknod() - Check if creating a special file is allowed",
        " * @dir: parent directory",
        " * @dentry: new file",
        " * @mode: new file mode",
        " * @dev: device number",
        " *",
        " * Check permissions when creating a file. Note that this hook is called even",
        " * if mknod operation is being done for a regular file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_mknod(const struct path *dir, struct dentry *dentry,",
        "			umode_t mode, unsigned int dev)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dir->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_mknod, dir, dentry, mode, dev);",
        "}",
        "EXPORT_SYMBOL(security_path_mknod);",
        "",
        "/**",
        " * security_path_post_mknod() - Update inode security after reg file creation",
        " * @idmap: idmap of the mount",
        " * @dentry: new file",
        " *",
        " * Update inode security field after a regular file has been created.",
        " */",
        "void security_path_post_mknod(struct mnt_idmap *idmap, struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(path_post_mknod, idmap, dentry);",
        "}",
        "",
        "/**",
        " * security_path_mkdir() - Check if creating a new directory is allowed",
        " * @dir: parent directory",
        " * @dentry: new directory",
        " * @mode: new directory mode",
        " *",
        " * Check permissions to create a new directory in the existing directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_mkdir(const struct path *dir, struct dentry *dentry,",
        "			umode_t mode)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dir->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_mkdir, dir, dentry, mode);",
        "}",
        "EXPORT_SYMBOL(security_path_mkdir);",
        "",
        "/**",
        " * security_path_rmdir() - Check if removing a directory is allowed",
        " * @dir: parent directory",
        " * @dentry: directory to remove",
        " *",
        " * Check the permission to remove a directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_rmdir(const struct path *dir, struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dir->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_rmdir, dir, dentry);",
        "}",
        "",
        "/**",
        " * security_path_unlink() - Check if removing a hard link is allowed",
        " * @dir: parent directory",
        " * @dentry: file",
        " *",
        " * Check the permission to remove a hard link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_unlink(const struct path *dir, struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dir->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_unlink, dir, dentry);",
        "}",
        "EXPORT_SYMBOL(security_path_unlink);",
        "",
        "/**",
        " * security_path_symlink() - Check if creating a symbolic link is allowed",
        " * @dir: parent directory",
        " * @dentry: symbolic link",
        " * @old_name: file pathname",
        " *",
        " * Check the permission to create a symbolic link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_symlink(const struct path *dir, struct dentry *dentry,",
        "			  const char *old_name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dir->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_symlink, dir, dentry, old_name);",
        "}",
        "",
        "/**",
        " * security_path_link - Check if creating a hard link is allowed",
        " * @old_dentry: existing file",
        " * @new_dir: new parent directory",
        " * @new_dentry: new link",
        " *",
        " * Check permission before creating a new hard link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_link(struct dentry *old_dentry, const struct path *new_dir,",
        "		       struct dentry *new_dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(old_dentry))))",
        "		return 0;",
        "	return call_int_hook(path_link, old_dentry, new_dir, new_dentry);",
        "}",
        "",
        "/**",
        " * security_path_rename() - Check if renaming a file is allowed",
        " * @old_dir: parent directory of the old file",
        " * @old_dentry: the old file",
        " * @new_dir: parent directory of the new file",
        " * @new_dentry: the new file",
        " * @flags: flags",
        " *",
        " * Check for permission to rename a file or directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_rename(const struct path *old_dir, struct dentry *old_dentry,",
        "			 const struct path *new_dir, struct dentry *new_dentry,",
        "			 unsigned int flags)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(old_dentry)) ||",
        "		     (d_is_positive(new_dentry) &&",
        "		      IS_PRIVATE(d_backing_inode(new_dentry)))))",
        "		return 0;",
        "",
        "	return call_int_hook(path_rename, old_dir, old_dentry, new_dir,",
        "			     new_dentry, flags);",
        "}",
        "EXPORT_SYMBOL(security_path_rename);",
        "",
        "/**",
        " * security_path_truncate() - Check if truncating a file is allowed",
        " * @path: file",
        " *",
        " * Check permission before truncating the file indicated by path.  Note that",
        " * truncation permissions may also be checked based on already opened files,",
        " * using the security_file_truncate() hook.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_truncate(const struct path *path)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(path->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_truncate, path);",
        "}",
        "",
        "/**",
        " * security_path_chmod() - Check if changing the file's mode is allowed",
        " * @path: file",
        " * @mode: new mode",
        " *",
        " * Check for permission to change a mode of the file @path. The new mode is",
        " * specified in @mode which is a bitmask of constants from",
        " * <include/uapi/linux/stat.h>.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_chmod(const struct path *path, umode_t mode)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(path->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_chmod, path, mode);",
        "}",
        "",
        "/**",
        " * security_path_chown() - Check if changing the file's owner/group is allowed",
        " * @path: file",
        " * @uid: file owner",
        " * @gid: file group",
        " *",
        " * Check for permission to change owner/group of a file or directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_chown(const struct path *path, kuid_t uid, kgid_t gid)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(path->dentry))))",
        "		return 0;",
        "	return call_int_hook(path_chown, path, uid, gid);",
        "}",
        "",
        "/**",
        " * security_path_chroot() - Check if changing the root directory is allowed",
        " * @path: directory",
        " *",
        " * Check for permission to change root directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_path_chroot(const struct path *path)",
        "{",
        "	return call_int_hook(path_chroot, path);",
        "}",
        "#endif /* CONFIG_SECURITY_PATH */",
        "",
        "/**",
        " * security_inode_create() - Check if creating a file is allowed",
        " * @dir: the parent directory",
        " * @dentry: the file being created",
        " * @mode: requested file mode",
        " *",
        " * Check permission to create a regular file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_create(struct inode *dir, struct dentry *dentry,",
        "			  umode_t mode)",
        "{",
        "	if (unlikely(IS_PRIVATE(dir)))",
        "		return 0;",
        "	return call_int_hook(inode_create, dir, dentry, mode);",
        "}",
        "EXPORT_SYMBOL_GPL(security_inode_create);",
        "",
        "/**",
        " * security_inode_post_create_tmpfile() - Update inode security of new tmpfile",
        " * @idmap: idmap of the mount",
        " * @inode: inode of the new tmpfile",
        " *",
        " * Update inode security data after a tmpfile has been created.",
        " */",
        "void security_inode_post_create_tmpfile(struct mnt_idmap *idmap,",
        "					struct inode *inode)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return;",
        "	call_void_hook(inode_post_create_tmpfile, idmap, inode);",
        "}",
        "",
        "/**",
        " * security_inode_link() - Check if creating a hard link is allowed",
        " * @old_dentry: existing file",
        " * @dir: new parent directory",
        " * @new_dentry: new link",
        " *",
        " * Check permission before creating a new hard link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_link(struct dentry *old_dentry, struct inode *dir,",
        "			struct dentry *new_dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(old_dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_link, old_dentry, dir, new_dentry);",
        "}",
        "",
        "/**",
        " * security_inode_unlink() - Check if removing a hard link is allowed",
        " * @dir: parent directory",
        " * @dentry: file",
        " *",
        " * Check the permission to remove a hard link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_unlink(struct inode *dir, struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_unlink, dir, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_symlink() - Check if creating a symbolic link is allowed",
        " * @dir: parent directory",
        " * @dentry: symbolic link",
        " * @old_name: existing filename",
        " *",
        " * Check the permission to create a symbolic link to a file.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_symlink(struct inode *dir, struct dentry *dentry,",
        "			   const char *old_name)",
        "{",
        "	if (unlikely(IS_PRIVATE(dir)))",
        "		return 0;",
        "	return call_int_hook(inode_symlink, dir, dentry, old_name);",
        "}",
        "",
        "/**",
        " * security_inode_mkdir() - Check if creation a new director is allowed",
        " * @dir: parent directory",
        " * @dentry: new directory",
        " * @mode: new directory mode",
        " *",
        " * Check permissions to create a new directory in the existing directory",
        " * associated with inode structure @dir.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)",
        "{",
        "	if (unlikely(IS_PRIVATE(dir)))",
        "		return 0;",
        "	return call_int_hook(inode_mkdir, dir, dentry, mode);",
        "}",
        "EXPORT_SYMBOL_GPL(security_inode_mkdir);",
        "",
        "/**",
        " * security_inode_rmdir() - Check if removing a directory is allowed",
        " * @dir: parent directory",
        " * @dentry: directory to be removed",
        " *",
        " * Check the permission to remove a directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_rmdir(struct inode *dir, struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_rmdir, dir, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_mknod() - Check if creating a special file is allowed",
        " * @dir: parent directory",
        " * @dentry: new file",
        " * @mode: new file mode",
        " * @dev: device number",
        " *",
        " * Check permissions when creating a special file (or a socket or a fifo file",
        " * created via the mknod system call).  Note that if mknod operation is being",
        " * done for a regular file, then the create hook will be called and not this",
        " * hook.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_mknod(struct inode *dir, struct dentry *dentry,",
        "			 umode_t mode, dev_t dev)",
        "{",
        "	if (unlikely(IS_PRIVATE(dir)))",
        "		return 0;",
        "	return call_int_hook(inode_mknod, dir, dentry, mode, dev);",
        "}",
        "",
        "/**",
        " * security_inode_rename() - Check if renaming a file is allowed",
        " * @old_dir: parent directory of the old file",
        " * @old_dentry: the old file",
        " * @new_dir: parent directory of the new file",
        " * @new_dentry: the new file",
        " * @flags: flags",
        " *",
        " * Check for permission to rename a file or directory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_rename(struct inode *old_dir, struct dentry *old_dentry,",
        "			  struct inode *new_dir, struct dentry *new_dentry,",
        "			  unsigned int flags)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(old_dentry)) ||",
        "		     (d_is_positive(new_dentry) &&",
        "		      IS_PRIVATE(d_backing_inode(new_dentry)))))",
        "		return 0;",
        "",
        "	if (flags & RENAME_EXCHANGE) {",
        "		int err = call_int_hook(inode_rename, new_dir, new_dentry,",
        "					old_dir, old_dentry);",
        "		if (err)",
        "			return err;",
        "	}",
        "",
        "	return call_int_hook(inode_rename, old_dir, old_dentry,",
        "			     new_dir, new_dentry);",
        "}",
        "",
        "/**",
        " * security_inode_readlink() - Check if reading a symbolic link is allowed",
        " * @dentry: link",
        " *",
        " * Check the permission to read the symbolic link.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_readlink(struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_readlink, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_follow_link() - Check if following a symbolic link is allowed",
        " * @dentry: link dentry",
        " * @inode: link inode",
        " * @rcu: true if in RCU-walk mode",
        " *",
        " * Check permission to follow a symbolic link when looking up a pathname.  If",
        " * @rcu is true, @inode is not stable.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_follow_link(struct dentry *dentry, struct inode *inode,",
        "			       bool rcu)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return 0;",
        "	return call_int_hook(inode_follow_link, dentry, inode, rcu);",
        "}",
        "",
        "/**",
        " * security_inode_permission() - Check if accessing an inode is allowed",
        " * @inode: inode",
        " * @mask: access mask",
        " *",
        " * Check permission before accessing an inode.  This hook is called by the",
        " * existing Linux permission function, so a security module can use it to",
        " * provide additional checking for existing Linux permission checks.  Notice",
        " * that this hook is called when a file is opened (as well as many other",
        " * operations), whereas the file_security_ops permission hook is called when",
        " * the actual read/write operations are performed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_permission(struct inode *inode, int mask)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return 0;",
        "	return call_int_hook(inode_permission, inode, mask);",
        "}",
        "",
        "/**",
        " * security_inode_setattr() - Check if setting file attributes is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @attr: new attributes",
        " *",
        " * Check permission before setting file attributes.  Note that the kernel call",
        " * to notify_change is performed from several locations, whenever file",
        " * attributes change (such as when a file is truncated, chown/chmod operations,",
        " * transferring disk quotas, etc).",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_setattr(struct mnt_idmap *idmap,",
        "			   struct dentry *dentry, struct iattr *attr)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_setattr, idmap, dentry, attr);",
        "}",
        "EXPORT_SYMBOL_GPL(security_inode_setattr);",
        "",
        "/**",
        " * security_inode_post_setattr() - Update the inode after a setattr operation",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @ia_valid: file attributes set",
        " *",
        " * Update inode security field after successful setting file attributes.",
        " */",
        "void security_inode_post_setattr(struct mnt_idmap *idmap, struct dentry *dentry,",
        "				 int ia_valid)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(inode_post_setattr, idmap, dentry, ia_valid);",
        "}",
        "",
        "/**",
        " * security_inode_getattr() - Check if getting file attributes is allowed",
        " * @path: file",
        " *",
        " * Check permission before obtaining file attributes.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_getattr(const struct path *path)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(path->dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_getattr, path);",
        "}",
        "",
        "/**",
        " * security_inode_setxattr() - Check if setting file xattrs is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @name: xattr name",
        " * @value: xattr value",
        " * @size: size of xattr value",
        " * @flags: flags",
        " *",
        " * This hook performs the desired permission checks before setting the extended",
        " * attributes (xattrs) on @dentry.  It is important to note that we have some",
        " * additional logic before the main LSM implementation calls to detect if we",
        " * need to perform an additional capability check at the LSM layer.",
        " *",
        " * Normally we enforce a capability check prior to executing the various LSM",
        " * hook implementations, but if a LSM wants to avoid this capability check,",
        " * it can register a 'inode_xattr_skipcap' hook and return a value of 1 for",
        " * xattrs that it wants to avoid the capability check, leaving the LSM fully",
        " * responsible for enforcing the access control for the specific xattr.  If all",
        " * of the enabled LSMs refrain from registering a 'inode_xattr_skipcap' hook,",
        " * or return a 0 (the default return value), the capability check is still",
        " * performed.  If no 'inode_xattr_skipcap' hooks are registered the capability",
        " * check is performed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_setxattr(struct mnt_idmap *idmap,",
        "			    struct dentry *dentry, const char *name,",
        "			    const void *value, size_t size, int flags)",
        "{",
        "	int rc;",
        "",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "",
        "	/* enforce the capability checks at the lsm layer, if needed */",
        "	if (!call_int_hook(inode_xattr_skipcap, name)) {",
        "		rc = cap_inode_setxattr(dentry, name, value, size, flags);",
        "		if (rc)",
        "			return rc;",
        "	}",
        "",
        "	return call_int_hook(inode_setxattr, idmap, dentry, name, value, size,",
        "			     flags);",
        "}",
        "",
        "/**",
        " * security_inode_set_acl() - Check if setting posix acls is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @acl_name: acl name",
        " * @kacl: acl struct",
        " *",
        " * Check permission before setting posix acls, the posix acls in @kacl are",
        " * identified by @acl_name.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_set_acl(struct mnt_idmap *idmap,",
        "			   struct dentry *dentry, const char *acl_name,",
        "			   struct posix_acl *kacl)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_set_acl, idmap, dentry, acl_name, kacl);",
        "}",
        "",
        "/**",
        " * security_inode_post_set_acl() - Update inode security from posix acls set",
        " * @dentry: file",
        " * @acl_name: acl name",
        " * @kacl: acl struct",
        " *",
        " * Update inode security data after successfully setting posix acls on @dentry.",
        " * The posix acls in @kacl are identified by @acl_name.",
        " */",
        "void security_inode_post_set_acl(struct dentry *dentry, const char *acl_name,",
        "				 struct posix_acl *kacl)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(inode_post_set_acl, dentry, acl_name, kacl);",
        "}",
        "",
        "/**",
        " * security_inode_get_acl() - Check if reading posix acls is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @acl_name: acl name",
        " *",
        " * Check permission before getting osix acls, the posix acls are identified by",
        " * @acl_name.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_get_acl(struct mnt_idmap *idmap,",
        "			   struct dentry *dentry, const char *acl_name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_get_acl, idmap, dentry, acl_name);",
        "}",
        "",
        "/**",
        " * security_inode_remove_acl() - Check if removing a posix acl is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @acl_name: acl name",
        " *",
        " * Check permission before removing posix acls, the posix acls are identified",
        " * by @acl_name.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_remove_acl(struct mnt_idmap *idmap,",
        "			      struct dentry *dentry, const char *acl_name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_remove_acl, idmap, dentry, acl_name);",
        "}",
        "",
        "/**",
        " * security_inode_post_remove_acl() - Update inode security after rm posix acls",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @acl_name: acl name",
        " *",
        " * Update inode security data after successfully removing posix acls on",
        " * @dentry in @idmap. The posix acls are identified by @acl_name.",
        " */",
        "void security_inode_post_remove_acl(struct mnt_idmap *idmap,",
        "				    struct dentry *dentry, const char *acl_name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(inode_post_remove_acl, idmap, dentry, acl_name);",
        "}",
        "",
        "/**",
        " * security_inode_post_setxattr() - Update the inode after a setxattr operation",
        " * @dentry: file",
        " * @name: xattr name",
        " * @value: xattr value",
        " * @size: xattr value size",
        " * @flags: flags",
        " *",
        " * Update inode security field after successful setxattr operation.",
        " */",
        "void security_inode_post_setxattr(struct dentry *dentry, const char *name,",
        "				  const void *value, size_t size, int flags)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(inode_post_setxattr, dentry, name, value, size, flags);",
        "}",
        "",
        "/**",
        " * security_inode_getxattr() - Check if xattr access is allowed",
        " * @dentry: file",
        " * @name: xattr name",
        " *",
        " * Check permission before obtaining the extended attributes identified by",
        " * @name for @dentry.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_getxattr(struct dentry *dentry, const char *name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_getxattr, dentry, name);",
        "}",
        "",
        "/**",
        " * security_inode_listxattr() - Check if listing xattrs is allowed",
        " * @dentry: file",
        " *",
        " * Check permission before obtaining the list of extended attribute names for",
        " * @dentry.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_listxattr(struct dentry *dentry)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "	return call_int_hook(inode_listxattr, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_removexattr() - Check if removing an xattr is allowed",
        " * @idmap: idmap of the mount",
        " * @dentry: file",
        " * @name: xattr name",
        " *",
        " * This hook performs the desired permission checks before setting the extended",
        " * attributes (xattrs) on @dentry.  It is important to note that we have some",
        " * additional logic before the main LSM implementation calls to detect if we",
        " * need to perform an additional capability check at the LSM layer.",
        " *",
        " * Normally we enforce a capability check prior to executing the various LSM",
        " * hook implementations, but if a LSM wants to avoid this capability check,",
        " * it can register a 'inode_xattr_skipcap' hook and return a value of 1 for",
        " * xattrs that it wants to avoid the capability check, leaving the LSM fully",
        " * responsible for enforcing the access control for the specific xattr.  If all",
        " * of the enabled LSMs refrain from registering a 'inode_xattr_skipcap' hook,",
        " * or return a 0 (the default return value), the capability check is still",
        " * performed.  If no 'inode_xattr_skipcap' hooks are registered the capability",
        " * check is performed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inode_removexattr(struct mnt_idmap *idmap,",
        "			       struct dentry *dentry, const char *name)",
        "{",
        "	int rc;",
        "",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return 0;",
        "",
        "	/* enforce the capability checks at the lsm layer, if needed */",
        "	if (!call_int_hook(inode_xattr_skipcap, name)) {",
        "		rc = cap_inode_removexattr(idmap, dentry, name);",
        "		if (rc)",
        "			return rc;",
        "	}",
        "",
        "	return call_int_hook(inode_removexattr, idmap, dentry, name);",
        "}",
        "",
        "/**",
        " * security_inode_post_removexattr() - Update the inode after a removexattr op",
        " * @dentry: file",
        " * @name: xattr name",
        " *",
        " * Update the inode after a successful removexattr operation.",
        " */",
        "void security_inode_post_removexattr(struct dentry *dentry, const char *name)",
        "{",
        "	if (unlikely(IS_PRIVATE(d_backing_inode(dentry))))",
        "		return;",
        "	call_void_hook(inode_post_removexattr, dentry, name);",
        "}",
        "",
        "/**",
        " * security_inode_need_killpriv() - Check if security_inode_killpriv() required",
        " * @dentry: associated dentry",
        " *",
        " * Called when an inode has been changed to determine if",
        " * security_inode_killpriv() should be called.",
        " *",
        " * Return: Return <0 on error to abort the inode change operation, return 0 if",
        " *         security_inode_killpriv() does not need to be called, return >0 if",
        " *         security_inode_killpriv() does need to be called.",
        " */",
        "int security_inode_need_killpriv(struct dentry *dentry)",
        "{",
        "	return call_int_hook(inode_need_killpriv, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_killpriv() - The setuid bit is removed, update LSM state",
        " * @idmap: idmap of the mount",
        " * @dentry: associated dentry",
        " *",
        " * The @dentry's setuid bit is being removed.  Remove similar security labels.",
        " * Called with the dentry->d_inode->i_mutex held.",
        " *",
        " * Return: Return 0 on success.  If error is returned, then the operation",
        " *         causing setuid bit removal is failed.",
        " */",
        "int security_inode_killpriv(struct mnt_idmap *idmap,",
        "			    struct dentry *dentry)",
        "{",
        "	return call_int_hook(inode_killpriv, idmap, dentry);",
        "}",
        "",
        "/**",
        " * security_inode_getsecurity() - Get the xattr security label of an inode",
        " * @idmap: idmap of the mount",
        " * @inode: inode",
        " * @name: xattr name",
        " * @buffer: security label buffer",
        " * @alloc: allocation flag",
        " *",
        " * Retrieve a copy of the extended attribute representation of the security",
        " * label associated with @name for @inode via @buffer.  Note that @name is the",
        " * remainder of the attribute name after the security prefix has been removed.",
        " * @alloc is used to specify if the call should return a value via the buffer",
        " * or just the value length.",
        " *",
        " * Return: Returns size of buffer on success.",
        " */",
        "int security_inode_getsecurity(struct mnt_idmap *idmap,",
        "			       struct inode *inode, const char *name,",
        "			       void **buffer, bool alloc)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return LSM_RET_DEFAULT(inode_getsecurity);",
        "",
        "	return call_int_hook(inode_getsecurity, idmap, inode, name, buffer,",
        "			     alloc);",
        "}",
        "",
        "/**",
        " * security_inode_setsecurity() - Set the xattr security label of an inode",
        " * @inode: inode",
        " * @name: xattr name",
        " * @value: security label",
        " * @size: length of security label",
        " * @flags: flags",
        " *",
        " * Set the security label associated with @name for @inode from the extended",
        " * attribute value @value.  @size indicates the size of the @value in bytes.",
        " * @flags may be XATTR_CREATE, XATTR_REPLACE, or 0. Note that @name is the",
        " * remainder of the attribute name after the security. prefix has been removed.",
        " *",
        " * Return: Returns 0 on success.",
        " */",
        "int security_inode_setsecurity(struct inode *inode, const char *name,",
        "			       const void *value, size_t size, int flags)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return LSM_RET_DEFAULT(inode_setsecurity);",
        "",
        "	return call_int_hook(inode_setsecurity, inode, name, value, size,",
        "			     flags);",
        "}",
        "",
        "/**",
        " * security_inode_listsecurity() - List the xattr security label names",
        " * @inode: inode",
        " * @buffer: buffer",
        " * @buffer_size: size of buffer",
        " *",
        " * Copy the extended attribute names for the security labels associated with",
        " * @inode into @buffer.  The maximum size of @buffer is specified by",
        " * @buffer_size.  @buffer may be NULL to request the size of the buffer",
        " * required.",
        " *",
        " * Return: Returns number of bytes used/required on success.",
        " */",
        "int security_inode_listsecurity(struct inode *inode,",
        "				char *buffer, size_t buffer_size)",
        "{",
        "	if (unlikely(IS_PRIVATE(inode)))",
        "		return 0;",
        "	return call_int_hook(inode_listsecurity, inode, buffer, buffer_size);",
        "}",
        "EXPORT_SYMBOL(security_inode_listsecurity);",
        "",
        "/**",
        " * security_inode_getlsmprop() - Get an inode's LSM data",
        " * @inode: inode",
        " * @prop: lsm specific information to return",
        " *",
        " * Get the lsm specific information associated with the node.",
        " */",
        "void security_inode_getlsmprop(struct inode *inode, struct lsm_prop *prop)",
        "{",
        "	call_void_hook(inode_getlsmprop, inode, prop);",
        "}",
        "",
        "/**",
        " * security_inode_copy_up() - Create new creds for an overlayfs copy-up op",
        " * @src: union dentry of copy-up file",
        " * @new: newly created creds",
        " *",
        " * A file is about to be copied up from lower layer to upper layer of overlay",
        " * filesystem. Security module can prepare a set of new creds and modify as",
        " * need be and return new creds. Caller will switch to new creds temporarily to",
        " * create new file and release newly allocated creds.",
        " *",
        " * Return: Returns 0 on success or a negative error code on error.",
        " */",
        "int security_inode_copy_up(struct dentry *src, struct cred **new)",
        "{",
        "	return call_int_hook(inode_copy_up, src, new);",
        "}",
        "EXPORT_SYMBOL(security_inode_copy_up);",
        "",
        "/**",
        " * security_inode_copy_up_xattr() - Filter xattrs in an overlayfs copy-up op",
        " * @src: union dentry of copy-up file",
        " * @name: xattr name",
        " *",
        " * Filter the xattrs being copied up when a unioned file is copied up from a",
        " * lower layer to the union/overlay layer.   The caller is responsible for",
        " * reading and writing the xattrs, this hook is merely a filter.",
        " *",
        " * Return: Returns 0 to accept the xattr, -ECANCELED to discard the xattr,",
        " *         -EOPNOTSUPP if the security module does not know about attribute,",
        " *         or a negative error code to abort the copy up.",
        " */",
        "int security_inode_copy_up_xattr(struct dentry *src, const char *name)",
        "{",
        "	int rc;",
        "",
        "	rc = call_int_hook(inode_copy_up_xattr, src, name);",
        "	if (rc != LSM_RET_DEFAULT(inode_copy_up_xattr))",
        "		return rc;",
        "",
        "	return LSM_RET_DEFAULT(inode_copy_up_xattr);",
        "}",
        "EXPORT_SYMBOL(security_inode_copy_up_xattr);",
        "",
        "/**",
        " * security_inode_setintegrity() - Set the inode's integrity data",
        " * @inode: inode",
        " * @type: type of integrity, e.g. hash digest, signature, etc",
        " * @value: the integrity value",
        " * @size: size of the integrity value",
        " *",
        " * Register a verified integrity measurement of a inode with LSMs.",
        " * LSMs should free the previously saved data if @value is NULL.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_inode_setintegrity(const struct inode *inode,",
        "				enum lsm_integrity_type type, const void *value,",
        "				size_t size)",
        "{",
        "	return call_int_hook(inode_setintegrity, inode, type, value, size);",
        "}",
        "EXPORT_SYMBOL(security_inode_setintegrity);",
        "",
        "/**",
        " * security_kernfs_init_security() - Init LSM context for a kernfs node",
        " * @kn_dir: parent kernfs node",
        " * @kn: the kernfs node to initialize",
        " *",
        " * Initialize the security context of a newly created kernfs node based on its",
        " * own and its parent's attributes.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_kernfs_init_security(struct kernfs_node *kn_dir,",
        "				  struct kernfs_node *kn)",
        "{",
        "	return call_int_hook(kernfs_init_security, kn_dir, kn);",
        "}",
        "",
        "/**",
        " * security_file_permission() - Check file permissions",
        " * @file: file",
        " * @mask: requested permissions",
        " *",
        " * Check file permissions before accessing an open file.  This hook is called",
        " * by various operations that read or write files.  A security module can use",
        " * this hook to perform additional checking on these operations, e.g. to",
        " * revalidate permissions on use to support privilege bracketing or policy",
        " * changes.  Notice that this hook is used when the actual read/write",
        " * operations are performed, whereas the inode_security_ops hook is called when",
        " * a file is opened (as well as many other operations).  Although this hook can",
        " * be used to revalidate permissions for various system call operations that",
        " * read or write files, it does not address the revalidation of permissions for",
        " * memory-mapped files.  Security modules must handle this separately if they",
        " * need such revalidation.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_permission(struct file *file, int mask)",
        "{",
        "	return call_int_hook(file_permission, file, mask);",
        "}",
        "",
        "/**",
        " * security_file_alloc() - Allocate and init a file's LSM blob",
        " * @file: the file",
        " *",
        " * Allocate and attach a security structure to the file->f_security field.  The",
        " * security field is initialized to NULL when the structure is first created.",
        " *",
        " * Return: Return 0 if the hook is successful and permission is granted.",
        " */",
        "int security_file_alloc(struct file *file)",
        "{",
        "	int rc = lsm_file_alloc(file);",
        "",
        "	if (rc)",
        "		return rc;",
        "	rc = call_int_hook(file_alloc_security, file);",
        "	if (unlikely(rc))",
        "		security_file_free(file);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_file_release() - Perform actions before releasing the file ref",
        " * @file: the file",
        " *",
        " * Perform actions before releasing the last reference to a file.",
        " */",
        "void security_file_release(struct file *file)",
        "{",
        "	call_void_hook(file_release, file);",
        "}",
        "",
        "/**",
        " * security_file_free() - Free a file's LSM blob",
        " * @file: the file",
        " *",
        " * Deallocate and free any security structures stored in file->f_security.",
        " */",
        "void security_file_free(struct file *file)",
        "{",
        "	void *blob;",
        "",
        "	call_void_hook(file_free_security, file);",
        "",
        "	blob = file->f_security;",
        "	if (blob) {",
        "		file->f_security = NULL;",
        "		kmem_cache_free(lsm_file_cache, blob);",
        "	}",
        "}",
        "",
        "/**",
        " * security_file_ioctl() - Check if an ioctl is allowed",
        " * @file: associated file",
        " * @cmd: ioctl cmd",
        " * @arg: ioctl arguments",
        " *",
        " * Check permission for an ioctl operation on @file.  Note that @arg sometimes",
        " * represents a user space pointer; in other cases, it may be a simple integer",
        " * value.  When @arg represents a user space pointer, it should never be used",
        " * by the security module.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_ioctl(struct file *file, unsigned int cmd, unsigned long arg)",
        "{",
        "	return call_int_hook(file_ioctl, file, cmd, arg);",
        "}",
        "EXPORT_SYMBOL_GPL(security_file_ioctl);",
        "",
        "/**",
        " * security_file_ioctl_compat() - Check if an ioctl is allowed in compat mode",
        " * @file: associated file",
        " * @cmd: ioctl cmd",
        " * @arg: ioctl arguments",
        " *",
        " * Compat version of security_file_ioctl() that correctly handles 32-bit",
        " * processes running on 64-bit kernels.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_ioctl_compat(struct file *file, unsigned int cmd,",
        "			       unsigned long arg)",
        "{",
        "	return call_int_hook(file_ioctl_compat, file, cmd, arg);",
        "}",
        "EXPORT_SYMBOL_GPL(security_file_ioctl_compat);",
        "",
        "static inline unsigned long mmap_prot(struct file *file, unsigned long prot)",
        "{",
        "	/*",
        "	 * Does we have PROT_READ and does the application expect",
        "	 * it to imply PROT_EXEC?  If not, nothing to talk about...",
        "	 */",
        "	if ((prot & (PROT_READ | PROT_EXEC)) != PROT_READ)",
        "		return prot;",
        "	if (!(current->personality & READ_IMPLIES_EXEC))",
        "		return prot;",
        "	/*",
        "	 * if that's an anonymous mapping, let it.",
        "	 */",
        "	if (!file)",
        "		return prot | PROT_EXEC;",
        "	/*",
        "	 * ditto if it's not on noexec mount, except that on !MMU we need",
        "	 * NOMMU_MAP_EXEC (== VM_MAYEXEC) in this case",
        "	 */",
        "	if (!path_noexec(&file->f_path)) {",
        "#ifndef CONFIG_MMU",
        "		if (file->f_op->mmap_capabilities) {",
        "			unsigned caps = file->f_op->mmap_capabilities(file);",
        "			if (!(caps & NOMMU_MAP_EXEC))",
        "				return prot;",
        "		}",
        "#endif",
        "		return prot | PROT_EXEC;",
        "	}",
        "	/* anything on noexec mount won't get PROT_EXEC */",
        "	return prot;",
        "}",
        "",
        "/**",
        " * security_mmap_file() - Check if mmap'ing a file is allowed",
        " * @file: file",
        " * @prot: protection applied by the kernel",
        " * @flags: flags",
        " *",
        " * Check permissions for a mmap operation.  The @file may be NULL, e.g. if",
        " * mapping anonymous memory.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_mmap_file(struct file *file, unsigned long prot,",
        "		       unsigned long flags)",
        "{",
        "	return call_int_hook(mmap_file, file, prot, mmap_prot(file, prot),",
        "			     flags);",
        "}",
        "",
        "/**",
        " * security_mmap_addr() - Check if mmap'ing an address is allowed",
        " * @addr: address",
        " *",
        " * Check permissions for a mmap operation at @addr.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_mmap_addr(unsigned long addr)",
        "{",
        "	return call_int_hook(mmap_addr, addr);",
        "}",
        "",
        "/**",
        " * security_file_mprotect() - Check if changing memory protections is allowed",
        " * @vma: memory region",
        " * @reqprot: application requested protection",
        " * @prot: protection applied by the kernel",
        " *",
        " * Check permissions before changing memory access permissions.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_mprotect(struct vm_area_struct *vma, unsigned long reqprot,",
        "			   unsigned long prot)",
        "{",
        "	return call_int_hook(file_mprotect, vma, reqprot, prot);",
        "}",
        "",
        "/**",
        " * security_file_lock() - Check if a file lock is allowed",
        " * @file: file",
        " * @cmd: lock operation (e.g. F_RDLCK, F_WRLCK)",
        " *",
        " * Check permission before performing file locking operations.  Note the hook",
        " * mediates both flock and fcntl style locks.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_lock(struct file *file, unsigned int cmd)",
        "{",
        "	return call_int_hook(file_lock, file, cmd);",
        "}",
        "",
        "/**",
        " * security_file_fcntl() - Check if fcntl() op is allowed",
        " * @file: file",
        " * @cmd: fcntl command",
        " * @arg: command argument",
        " *",
        " * Check permission before allowing the file operation specified by @cmd from",
        " * being performed on the file @file.  Note that @arg sometimes represents a",
        " * user space pointer; in other cases, it may be a simple integer value.  When",
        " * @arg represents a user space pointer, it should never be used by the",
        " * security module.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_fcntl(struct file *file, unsigned int cmd, unsigned long arg)",
        "{",
        "	return call_int_hook(file_fcntl, file, cmd, arg);",
        "}",
        "",
        "/**",
        " * security_file_set_fowner() - Set the file owner info in the LSM blob",
        " * @file: the file",
        " *",
        " * Save owner security information (typically from current->security) in",
        " * file->f_security for later use by the send_sigiotask hook.",
        " *",
        " * This hook is called with file->f_owner.lock held.",
        " *",
        " * Return: Returns 0 on success.",
        " */",
        "void security_file_set_fowner(struct file *file)",
        "{",
        "	call_void_hook(file_set_fowner, file);",
        "}",
        "",
        "/**",
        " * security_file_send_sigiotask() - Check if sending SIGIO/SIGURG is allowed",
        " * @tsk: target task",
        " * @fown: signal sender",
        " * @sig: signal to be sent, SIGIO is sent if 0",
        " *",
        " * Check permission for the file owner @fown to send SIGIO or SIGURG to the",
        " * process @tsk.  Note that this hook is sometimes called from interrupt.  Note",
        " * that the fown_struct, @fown, is never outside the context of a struct file,",
        " * so the file structure (and associated security information) can always be",
        " * obtained: container_of(fown, struct file, f_owner).",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_send_sigiotask(struct task_struct *tsk,",
        "				 struct fown_struct *fown, int sig)",
        "{",
        "	return call_int_hook(file_send_sigiotask, tsk, fown, sig);",
        "}",
        "",
        "/**",
        " * security_file_receive() - Check if receiving a file via IPC is allowed",
        " * @file: file being received",
        " *",
        " * This hook allows security modules to control the ability of a process to",
        " * receive an open file descriptor via socket IPC.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_receive(struct file *file)",
        "{",
        "	return call_int_hook(file_receive, file);",
        "}",
        "",
        "/**",
        " * security_file_open() - Save open() time state for late use by the LSM",
        " * @file:",
        " *",
        " * Save open-time permission checking state for later use upon file_permission,",
        " * and recheck access if anything has changed since inode_permission.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_open(struct file *file)",
        "{",
        "	return call_int_hook(file_open, file);",
        "}",
        "",
        "/**",
        " * security_file_post_open() - Evaluate a file after it has been opened",
        " * @file: the file",
        " * @mask: access mask",
        " *",
        " * Evaluate an opened file and the access mask requested with open(). The hook",
        " * is useful for LSMs that require the file content to be available in order to",
        " * make decisions.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_post_open(struct file *file, int mask)",
        "{",
        "	return call_int_hook(file_post_open, file, mask);",
        "}",
        "EXPORT_SYMBOL_GPL(security_file_post_open);",
        "",
        "/**",
        " * security_file_truncate() - Check if truncating a file is allowed",
        " * @file: file",
        " *",
        " * Check permission before truncating a file, i.e. using ftruncate.  Note that",
        " * truncation permission may also be checked based on the path, using the",
        " * @path_truncate hook.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_file_truncate(struct file *file)",
        "{",
        "	return call_int_hook(file_truncate, file);",
        "}",
        "",
        "/**",
        " * security_task_alloc() - Allocate a task's LSM blob",
        " * @task: the task",
        " * @clone_flags: flags indicating what is being shared",
        " *",
        " * Handle allocation of task-related resources.",
        " *",
        " * Return: Returns a zero on success, negative values on failure.",
        " */",
        "int security_task_alloc(struct task_struct *task, unsigned long clone_flags)",
        "{",
        "	int rc = lsm_task_alloc(task);",
        "",
        "	if (rc)",
        "		return rc;",
        "	rc = call_int_hook(task_alloc, task, clone_flags);",
        "	if (unlikely(rc))",
        "		security_task_free(task);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_task_free() - Free a task's LSM blob and related resources",
        " * @task: task",
        " *",
        " * Handle release of task-related resources.  Note that this can be called from",
        " * interrupt context.",
        " */",
        "void security_task_free(struct task_struct *task)",
        "{",
        "	call_void_hook(task_free, task);",
        "",
        "	kfree(task->security);",
        "	task->security = NULL;",
        "}",
        "",
        "/**",
        " * security_cred_alloc_blank() - Allocate the min memory to allow cred_transfer",
        " * @cred: credentials",
        " * @gfp: gfp flags",
        " *",
        " * Only allocate sufficient memory and attach to @cred such that",
        " * cred_transfer() will not get ENOMEM.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_cred_alloc_blank(struct cred *cred, gfp_t gfp)",
        "{",
        "	int rc = lsm_cred_alloc(cred, gfp);",
        "",
        "	if (rc)",
        "		return rc;",
        "",
        "	rc = call_int_hook(cred_alloc_blank, cred, gfp);",
        "	if (unlikely(rc))",
        "		security_cred_free(cred);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_cred_free() - Free the cred's LSM blob and associated resources",
        " * @cred: credentials",
        " *",
        " * Deallocate and clear the cred->security field in a set of credentials.",
        " */",
        "void security_cred_free(struct cred *cred)",
        "{",
        "	/*",
        "	 * There is a failure case in prepare_creds() that",
        "	 * may result in a call here with ->security being NULL.",
        "	 */",
        "	if (unlikely(cred->security == NULL))",
        "		return;",
        "",
        "	call_void_hook(cred_free, cred);",
        "",
        "	kfree(cred->security);",
        "	cred->security = NULL;",
        "}",
        "",
        "/**",
        " * security_prepare_creds() - Prepare a new set of credentials",
        " * @new: new credentials",
        " * @old: original credentials",
        " * @gfp: gfp flags",
        " *",
        " * Prepare a new set of credentials by copying the data from the old set.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_prepare_creds(struct cred *new, const struct cred *old, gfp_t gfp)",
        "{",
        "	int rc = lsm_cred_alloc(new, gfp);",
        "",
        "	if (rc)",
        "		return rc;",
        "",
        "	rc = call_int_hook(cred_prepare, new, old, gfp);",
        "	if (unlikely(rc))",
        "		security_cred_free(new);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_transfer_creds() - Transfer creds",
        " * @new: target credentials",
        " * @old: original credentials",
        " *",
        " * Transfer data from original creds to new creds.",
        " */",
        "void security_transfer_creds(struct cred *new, const struct cred *old)",
        "{",
        "	call_void_hook(cred_transfer, new, old);",
        "}",
        "",
        "/**",
        " * security_cred_getsecid() - Get the secid from a set of credentials",
        " * @c: credentials",
        " * @secid: secid value",
        " *",
        " * Retrieve the security identifier of the cred structure @c.  In case of",
        " * failure, @secid will be set to zero.",
        " */",
        "void security_cred_getsecid(const struct cred *c, u32 *secid)",
        "{",
        "	*secid = 0;",
        "	call_void_hook(cred_getsecid, c, secid);",
        "}",
        "EXPORT_SYMBOL(security_cred_getsecid);",
        "",
        "/**",
        " * security_cred_getlsmprop() - Get the LSM data from a set of credentials",
        " * @c: credentials",
        " * @prop: destination for the LSM data",
        " *",
        " * Retrieve the security data of the cred structure @c.  In case of",
        " * failure, @prop will be cleared.",
        " */",
        "void security_cred_getlsmprop(const struct cred *c, struct lsm_prop *prop)",
        "{",
        "	lsmprop_init(prop);",
        "	call_void_hook(cred_getlsmprop, c, prop);",
        "}",
        "EXPORT_SYMBOL(security_cred_getlsmprop);",
        "",
        "/**",
        " * security_kernel_act_as() - Set the kernel credentials to act as secid",
        " * @new: credentials",
        " * @secid: secid",
        " *",
        " * Set the credentials for a kernel service to act as (subjective context).",
        " * The current task must be the one that nominated @secid.",
        " *",
        " * Return: Returns 0 if successful.",
        " */",
        "int security_kernel_act_as(struct cred *new, u32 secid)",
        "{",
        "	return call_int_hook(kernel_act_as, new, secid);",
        "}",
        "",
        "/**",
        " * security_kernel_create_files_as() - Set file creation context using an inode",
        " * @new: target credentials",
        " * @inode: reference inode",
        " *",
        " * Set the file creation context in a set of credentials to be the same as the",
        " * objective context of the specified inode.  The current task must be the one",
        " * that nominated @inode.",
        " *",
        " * Return: Returns 0 if successful.",
        " */",
        "int security_kernel_create_files_as(struct cred *new, struct inode *inode)",
        "{",
        "	return call_int_hook(kernel_create_files_as, new, inode);",
        "}",
        "",
        "/**",
        " * security_kernel_module_request() - Check if loading a module is allowed",
        " * @kmod_name: module name",
        " *",
        " * Ability to trigger the kernel to automatically upcall to userspace for",
        " * userspace to load a kernel module with the given name.",
        " *",
        " * Return: Returns 0 if successful.",
        " */",
        "int security_kernel_module_request(char *kmod_name)",
        "{",
        "	return call_int_hook(kernel_module_request, kmod_name);",
        "}",
        "",
        "/**",
        " * security_kernel_read_file() - Read a file specified by userspace",
        " * @file: file",
        " * @id: file identifier",
        " * @contents: trust if security_kernel_post_read_file() will be called",
        " *",
        " * Read a file specified by userspace.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_kernel_read_file(struct file *file, enum kernel_read_file_id id,",
        "			      bool contents)",
        "{",
        "	return call_int_hook(kernel_read_file, file, id, contents);",
        "}",
        "EXPORT_SYMBOL_GPL(security_kernel_read_file);",
        "",
        "/**",
        " * security_kernel_post_read_file() - Read a file specified by userspace",
        " * @file: file",
        " * @buf: file contents",
        " * @size: size of file contents",
        " * @id: file identifier",
        " *",
        " * Read a file specified by userspace.  This must be paired with a prior call",
        " * to security_kernel_read_file() call that indicated this hook would also be",
        " * called, see security_kernel_read_file() for more information.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_kernel_post_read_file(struct file *file, char *buf, loff_t size,",
        "				   enum kernel_read_file_id id)",
        "{",
        "	return call_int_hook(kernel_post_read_file, file, buf, size, id);",
        "}",
        "EXPORT_SYMBOL_GPL(security_kernel_post_read_file);",
        "",
        "/**",
        " * security_kernel_load_data() - Load data provided by userspace",
        " * @id: data identifier",
        " * @contents: true if security_kernel_post_load_data() will be called",
        " *",
        " * Load data provided by userspace.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_kernel_load_data(enum kernel_load_data_id id, bool contents)",
        "{",
        "	return call_int_hook(kernel_load_data, id, contents);",
        "}",
        "EXPORT_SYMBOL_GPL(security_kernel_load_data);",
        "",
        "/**",
        " * security_kernel_post_load_data() - Load userspace data from a non-file source",
        " * @buf: data",
        " * @size: size of data",
        " * @id: data identifier",
        " * @description: text description of data, specific to the id value",
        " *",
        " * Load data provided by a non-file source (usually userspace buffer).  This",
        " * must be paired with a prior security_kernel_load_data() call that indicated",
        " * this hook would also be called, see security_kernel_load_data() for more",
        " * information.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_kernel_post_load_data(char *buf, loff_t size,",
        "				   enum kernel_load_data_id id,",
        "				   char *description)",
        "{",
        "	return call_int_hook(kernel_post_load_data, buf, size, id, description);",
        "}",
        "EXPORT_SYMBOL_GPL(security_kernel_post_load_data);",
        "",
        "/**",
        " * security_task_fix_setuid() - Update LSM with new user id attributes",
        " * @new: updated credentials",
        " * @old: credentials being replaced",
        " * @flags: LSM_SETID_* flag values",
        " *",
        " * Update the module's state after setting one or more of the user identity",
        " * attributes of the current process.  The @flags parameter indicates which of",
        " * the set*uid system calls invoked this hook.  If @new is the set of",
        " * credentials that will be installed.  Modifications should be made to this",
        " * rather than to @current->cred.",
        " *",
        " * Return: Returns 0 on success.",
        " */",
        "int security_task_fix_setuid(struct cred *new, const struct cred *old,",
        "			     int flags)",
        "{",
        "	return call_int_hook(task_fix_setuid, new, old, flags);",
        "}",
        "",
        "/**",
        " * security_task_fix_setgid() - Update LSM with new group id attributes",
        " * @new: updated credentials",
        " * @old: credentials being replaced",
        " * @flags: LSM_SETID_* flag value",
        " *",
        " * Update the module's state after setting one or more of the group identity",
        " * attributes of the current process.  The @flags parameter indicates which of",
        " * the set*gid system calls invoked this hook.  @new is the set of credentials",
        " * that will be installed.  Modifications should be made to this rather than to",
        " * @current->cred.",
        " *",
        " * Return: Returns 0 on success.",
        " */",
        "int security_task_fix_setgid(struct cred *new, const struct cred *old,",
        "			     int flags)",
        "{",
        "	return call_int_hook(task_fix_setgid, new, old, flags);",
        "}",
        "",
        "/**",
        " * security_task_fix_setgroups() - Update LSM with new supplementary groups",
        " * @new: updated credentials",
        " * @old: credentials being replaced",
        " *",
        " * Update the module's state after setting the supplementary group identity",
        " * attributes of the current process.  @new is the set of credentials that will",
        " * be installed.  Modifications should be made to this rather than to",
        " * @current->cred.",
        " *",
        " * Return: Returns 0 on success.",
        " */",
        "int security_task_fix_setgroups(struct cred *new, const struct cred *old)",
        "{",
        "	return call_int_hook(task_fix_setgroups, new, old);",
        "}",
        "",
        "/**",
        " * security_task_setpgid() - Check if setting the pgid is allowed",
        " * @p: task being modified",
        " * @pgid: new pgid",
        " *",
        " * Check permission before setting the process group identifier of the process",
        " * @p to @pgid.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_setpgid(struct task_struct *p, pid_t pgid)",
        "{",
        "	return call_int_hook(task_setpgid, p, pgid);",
        "}",
        "",
        "/**",
        " * security_task_getpgid() - Check if getting the pgid is allowed",
        " * @p: task",
        " *",
        " * Check permission before getting the process group identifier of the process",
        " * @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_getpgid(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_getpgid, p);",
        "}",
        "",
        "/**",
        " * security_task_getsid() - Check if getting the session id is allowed",
        " * @p: task",
        " *",
        " * Check permission before getting the session identifier of the process @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_getsid(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_getsid, p);",
        "}",
        "",
        "/**",
        " * security_current_getlsmprop_subj() - Current task's subjective LSM data",
        " * @prop: lsm specific information",
        " *",
        " * Retrieve the subjective security identifier of the current task and return",
        " * it in @prop.",
        " */",
        "void security_current_getlsmprop_subj(struct lsm_prop *prop)",
        "{",
        "	lsmprop_init(prop);",
        "	call_void_hook(current_getlsmprop_subj, prop);",
        "}",
        "EXPORT_SYMBOL(security_current_getlsmprop_subj);",
        "",
        "/**",
        " * security_task_getlsmprop_obj() - Get a task's objective LSM data",
        " * @p: target task",
        " * @prop: lsm specific information",
        " *",
        " * Retrieve the objective security identifier of the task_struct in @p and",
        " * return it in @prop.",
        " */",
        "void security_task_getlsmprop_obj(struct task_struct *p, struct lsm_prop *prop)",
        "{",
        "	lsmprop_init(prop);",
        "	call_void_hook(task_getlsmprop_obj, p, prop);",
        "}",
        "EXPORT_SYMBOL(security_task_getlsmprop_obj);",
        "",
        "/**",
        " * security_task_setnice() - Check if setting a task's nice value is allowed",
        " * @p: target task",
        " * @nice: nice value",
        " *",
        " * Check permission before setting the nice value of @p to @nice.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_setnice(struct task_struct *p, int nice)",
        "{",
        "	return call_int_hook(task_setnice, p, nice);",
        "}",
        "",
        "/**",
        " * security_task_setioprio() - Check if setting a task's ioprio is allowed",
        " * @p: target task",
        " * @ioprio: ioprio value",
        " *",
        " * Check permission before setting the ioprio value of @p to @ioprio.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_setioprio(struct task_struct *p, int ioprio)",
        "{",
        "	return call_int_hook(task_setioprio, p, ioprio);",
        "}",
        "",
        "/**",
        " * security_task_getioprio() - Check if getting a task's ioprio is allowed",
        " * @p: task",
        " *",
        " * Check permission before getting the ioprio value of @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_getioprio(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_getioprio, p);",
        "}",
        "",
        "/**",
        " * security_task_prlimit() - Check if get/setting resources limits is allowed",
        " * @cred: current task credentials",
        " * @tcred: target task credentials",
        " * @flags: LSM_PRLIMIT_* flag bits indicating a get/set/both",
        " *",
        " * Check permission before getting and/or setting the resource limits of",
        " * another task.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_prlimit(const struct cred *cred, const struct cred *tcred,",
        "			  unsigned int flags)",
        "{",
        "	return call_int_hook(task_prlimit, cred, tcred, flags);",
        "}",
        "",
        "/**",
        " * security_task_setrlimit() - Check if setting a new rlimit value is allowed",
        " * @p: target task's group leader",
        " * @resource: resource whose limit is being set",
        " * @new_rlim: new resource limit",
        " *",
        " * Check permission before setting the resource limits of process @p for",
        " * @resource to @new_rlim.  The old resource limit values can be examined by",
        " * dereferencing (p->signal->rlim + resource).",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_setrlimit(struct task_struct *p, unsigned int resource,",
        "			    struct rlimit *new_rlim)",
        "{",
        "	return call_int_hook(task_setrlimit, p, resource, new_rlim);",
        "}",
        "",
        "/**",
        " * security_task_setscheduler() - Check if setting sched policy/param is allowed",
        " * @p: target task",
        " *",
        " * Check permission before setting scheduling policy and/or parameters of",
        " * process @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_setscheduler(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_setscheduler, p);",
        "}",
        "",
        "/**",
        " * security_task_getscheduler() - Check if getting scheduling info is allowed",
        " * @p: target task",
        " *",
        " * Check permission before obtaining scheduling information for process @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_getscheduler(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_getscheduler, p);",
        "}",
        "",
        "/**",
        " * security_task_movememory() - Check if moving memory is allowed",
        " * @p: task",
        " *",
        " * Check permission before moving memory owned by process @p.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_movememory(struct task_struct *p)",
        "{",
        "	return call_int_hook(task_movememory, p);",
        "}",
        "",
        "/**",
        " * security_task_kill() - Check if sending a signal is allowed",
        " * @p: target process",
        " * @info: signal information",
        " * @sig: signal value",
        " * @cred: credentials of the signal sender, NULL if @current",
        " *",
        " * Check permission before sending signal @sig to @p.  @info can be NULL, the",
        " * constant 1, or a pointer to a kernel_siginfo structure.  If @info is 1 or",
        " * SI_FROMKERNEL(info) is true, then the signal should be viewed as coming from",
        " * the kernel and should typically be permitted.  SIGIO signals are handled",
        " * separately by the send_sigiotask hook in file_security_ops.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_task_kill(struct task_struct *p, struct kernel_siginfo *info,",
        "		       int sig, const struct cred *cred)",
        "{",
        "	return call_int_hook(task_kill, p, info, sig, cred);",
        "}",
        "",
        "/**",
        " * security_task_prctl() - Check if a prctl op is allowed",
        " * @option: operation",
        " * @arg2: argument",
        " * @arg3: argument",
        " * @arg4: argument",
        " * @arg5: argument",
        " *",
        " * Check permission before performing a process control operation on the",
        " * current process.",
        " *",
        " * Return: Return -ENOSYS if no-one wanted to handle this op, any other value",
        " *         to cause prctl() to return immediately with that value.",
        " */",
        "int security_task_prctl(int option, unsigned long arg2, unsigned long arg3,",
        "			unsigned long arg4, unsigned long arg5)",
        "{",
        "	int thisrc;",
        "	int rc = LSM_RET_DEFAULT(task_prctl);",
        "	struct lsm_static_call *scall;",
        "",
        "	lsm_for_each_hook(scall, task_prctl) {",
        "		thisrc = scall->hl->hook.task_prctl(option, arg2, arg3, arg4, arg5);",
        "		if (thisrc != LSM_RET_DEFAULT(task_prctl)) {",
        "			rc = thisrc;",
        "			if (thisrc != 0)",
        "				break;",
        "		}",
        "	}",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_task_to_inode() - Set the security attributes of a task's inode",
        " * @p: task",
        " * @inode: inode",
        " *",
        " * Set the security attributes for an inode based on an associated task's",
        " * security attributes, e.g. for /proc/pid inodes.",
        " */",
        "void security_task_to_inode(struct task_struct *p, struct inode *inode)",
        "{",
        "	call_void_hook(task_to_inode, p, inode);",
        "}",
        "",
        "/**",
        " * security_create_user_ns() - Check if creating a new userns is allowed",
        " * @cred: prepared creds",
        " *",
        " * Check permission prior to creating a new user namespace.",
        " *",
        " * Return: Returns 0 if successful, otherwise < 0 error code.",
        " */",
        "int security_create_user_ns(const struct cred *cred)",
        "{",
        "	return call_int_hook(userns_create, cred);",
        "}",
        "",
        "/**",
        " * security_ipc_permission() - Check if sysv ipc access is allowed",
        " * @ipcp: ipc permission structure",
        " * @flag: requested permissions",
        " *",
        " * Check permissions for access to IPC.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_ipc_permission(struct kern_ipc_perm *ipcp, short flag)",
        "{",
        "	return call_int_hook(ipc_permission, ipcp, flag);",
        "}",
        "",
        "/**",
        " * security_ipc_getlsmprop() - Get the sysv ipc object LSM data",
        " * @ipcp: ipc permission structure",
        " * @prop: pointer to lsm information",
        " *",
        " * Get the lsm information associated with the ipc object.",
        " */",
        "",
        "void security_ipc_getlsmprop(struct kern_ipc_perm *ipcp, struct lsm_prop *prop)",
        "{",
        "	lsmprop_init(prop);",
        "	call_void_hook(ipc_getlsmprop, ipcp, prop);",
        "}",
        "",
        "/**",
        " * security_msg_msg_alloc() - Allocate a sysv ipc message LSM blob",
        " * @msg: message structure",
        " *",
        " * Allocate and attach a security structure to the msg->security field.  The",
        " * security field is initialized to NULL when the structure is first created.",
        " *",
        " * Return: Return 0 if operation was successful and permission is granted.",
        " */",
        "int security_msg_msg_alloc(struct msg_msg *msg)",
        "{",
        "	int rc = lsm_msg_msg_alloc(msg);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(msg_msg_alloc_security, msg);",
        "	if (unlikely(rc))",
        "		security_msg_msg_free(msg);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_msg_msg_free() - Free a sysv ipc message LSM blob",
        " * @msg: message structure",
        " *",
        " * Deallocate the security structure for this message.",
        " */",
        "void security_msg_msg_free(struct msg_msg *msg)",
        "{",
        "	call_void_hook(msg_msg_free_security, msg);",
        "	kfree(msg->security);",
        "	msg->security = NULL;",
        "}",
        "",
        "/**",
        " * security_msg_queue_alloc() - Allocate a sysv ipc msg queue LSM blob",
        " * @msq: sysv ipc permission structure",
        " *",
        " * Allocate and attach a security structure to @msg. The security field is",
        " * initialized to NULL when the structure is first created.",
        " *",
        " * Return: Returns 0 if operation was successful and permission is granted.",
        " */",
        "int security_msg_queue_alloc(struct kern_ipc_perm *msq)",
        "{",
        "	int rc = lsm_ipc_alloc(msq);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(msg_queue_alloc_security, msq);",
        "	if (unlikely(rc))",
        "		security_msg_queue_free(msq);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_msg_queue_free() - Free a sysv ipc msg queue LSM blob",
        " * @msq: sysv ipc permission structure",
        " *",
        " * Deallocate security field @perm->security for the message queue.",
        " */",
        "void security_msg_queue_free(struct kern_ipc_perm *msq)",
        "{",
        "	call_void_hook(msg_queue_free_security, msq);",
        "	kfree(msq->security);",
        "	msq->security = NULL;",
        "}",
        "",
        "/**",
        " * security_msg_queue_associate() - Check if a msg queue operation is allowed",
        " * @msq: sysv ipc permission structure",
        " * @msqflg: operation flags",
        " *",
        " * Check permission when a message queue is requested through the msgget system",
        " * call. This hook is only called when returning the message queue identifier",
        " * for an existing message queue, not when a new message queue is created.",
        " *",
        " * Return: Return 0 if permission is granted.",
        " */",
        "int security_msg_queue_associate(struct kern_ipc_perm *msq, int msqflg)",
        "{",
        "	return call_int_hook(msg_queue_associate, msq, msqflg);",
        "}",
        "",
        "/**",
        " * security_msg_queue_msgctl() - Check if a msg queue operation is allowed",
        " * @msq: sysv ipc permission structure",
        " * @cmd: operation",
        " *",
        " * Check permission when a message control operation specified by @cmd is to be",
        " * performed on the message queue with permissions.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_msg_queue_msgctl(struct kern_ipc_perm *msq, int cmd)",
        "{",
        "	return call_int_hook(msg_queue_msgctl, msq, cmd);",
        "}",
        "",
        "/**",
        " * security_msg_queue_msgsnd() - Check if sending a sysv ipc message is allowed",
        " * @msq: sysv ipc permission structure",
        " * @msg: message",
        " * @msqflg: operation flags",
        " *",
        " * Check permission before a message, @msg, is enqueued on the message queue",
        " * with permissions specified in @msq.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_msg_queue_msgsnd(struct kern_ipc_perm *msq,",
        "			      struct msg_msg *msg, int msqflg)",
        "{",
        "	return call_int_hook(msg_queue_msgsnd, msq, msg, msqflg);",
        "}",
        "",
        "/**",
        " * security_msg_queue_msgrcv() - Check if receiving a sysv ipc msg is allowed",
        " * @msq: sysv ipc permission structure",
        " * @msg: message",
        " * @target: target task",
        " * @type: type of message requested",
        " * @mode: operation flags",
        " *",
        " * Check permission before a message, @msg, is removed from the message	queue.",
        " * The @target task structure contains a pointer to the process that will be",
        " * receiving the message (not equal to the current process when inline receives",
        " * are being performed).",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_msg_queue_msgrcv(struct kern_ipc_perm *msq, struct msg_msg *msg,",
        "			      struct task_struct *target, long type, int mode)",
        "{",
        "	return call_int_hook(msg_queue_msgrcv, msq, msg, target, type, mode);",
        "}",
        "",
        "/**",
        " * security_shm_alloc() - Allocate a sysv shm LSM blob",
        " * @shp: sysv ipc permission structure",
        " *",
        " * Allocate and attach a security structure to the @shp security field.  The",
        " * security field is initialized to NULL when the structure is first created.",
        " *",
        " * Return: Returns 0 if operation was successful and permission is granted.",
        " */",
        "int security_shm_alloc(struct kern_ipc_perm *shp)",
        "{",
        "	int rc = lsm_ipc_alloc(shp);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(shm_alloc_security, shp);",
        "	if (unlikely(rc))",
        "		security_shm_free(shp);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_shm_free() - Free a sysv shm LSM blob",
        " * @shp: sysv ipc permission structure",
        " *",
        " * Deallocate the security structure @perm->security for the memory segment.",
        " */",
        "void security_shm_free(struct kern_ipc_perm *shp)",
        "{",
        "	call_void_hook(shm_free_security, shp);",
        "	kfree(shp->security);",
        "	shp->security = NULL;",
        "}",
        "",
        "/**",
        " * security_shm_associate() - Check if a sysv shm operation is allowed",
        " * @shp: sysv ipc permission structure",
        " * @shmflg: operation flags",
        " *",
        " * Check permission when a shared memory region is requested through the shmget",
        " * system call. This hook is only called when returning the shared memory",
        " * region identifier for an existing region, not when a new shared memory",
        " * region is created.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_shm_associate(struct kern_ipc_perm *shp, int shmflg)",
        "{",
        "	return call_int_hook(shm_associate, shp, shmflg);",
        "}",
        "",
        "/**",
        " * security_shm_shmctl() - Check if a sysv shm operation is allowed",
        " * @shp: sysv ipc permission structure",
        " * @cmd: operation",
        " *",
        " * Check permission when a shared memory control operation specified by @cmd is",
        " * to be performed on the shared memory region with permissions in @shp.",
        " *",
        " * Return: Return 0 if permission is granted.",
        " */",
        "int security_shm_shmctl(struct kern_ipc_perm *shp, int cmd)",
        "{",
        "	return call_int_hook(shm_shmctl, shp, cmd);",
        "}",
        "",
        "/**",
        " * security_shm_shmat() - Check if a sysv shm attach operation is allowed",
        " * @shp: sysv ipc permission structure",
        " * @shmaddr: address of memory region to attach",
        " * @shmflg: operation flags",
        " *",
        " * Check permissions prior to allowing the shmat system call to attach the",
        " * shared memory segment with permissions @shp to the data segment of the",
        " * calling process. The attaching address is specified by @shmaddr.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_shm_shmat(struct kern_ipc_perm *shp,",
        "		       char __user *shmaddr, int shmflg)",
        "{",
        "	return call_int_hook(shm_shmat, shp, shmaddr, shmflg);",
        "}",
        "",
        "/**",
        " * security_sem_alloc() - Allocate a sysv semaphore LSM blob",
        " * @sma: sysv ipc permission structure",
        " *",
        " * Allocate and attach a security structure to the @sma security field. The",
        " * security field is initialized to NULL when the structure is first created.",
        " *",
        " * Return: Returns 0 if operation was successful and permission is granted.",
        " */",
        "int security_sem_alloc(struct kern_ipc_perm *sma)",
        "{",
        "	int rc = lsm_ipc_alloc(sma);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(sem_alloc_security, sma);",
        "	if (unlikely(rc))",
        "		security_sem_free(sma);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_sem_free() - Free a sysv semaphore LSM blob",
        " * @sma: sysv ipc permission structure",
        " *",
        " * Deallocate security structure @sma->security for the semaphore.",
        " */",
        "void security_sem_free(struct kern_ipc_perm *sma)",
        "{",
        "	call_void_hook(sem_free_security, sma);",
        "	kfree(sma->security);",
        "	sma->security = NULL;",
        "}",
        "",
        "/**",
        " * security_sem_associate() - Check if a sysv semaphore operation is allowed",
        " * @sma: sysv ipc permission structure",
        " * @semflg: operation flags",
        " *",
        " * Check permission when a semaphore is requested through the semget system",
        " * call. This hook is only called when returning the semaphore identifier for",
        " * an existing semaphore, not when a new one must be created.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sem_associate(struct kern_ipc_perm *sma, int semflg)",
        "{",
        "	return call_int_hook(sem_associate, sma, semflg);",
        "}",
        "",
        "/**",
        " * security_sem_semctl() - Check if a sysv semaphore operation is allowed",
        " * @sma: sysv ipc permission structure",
        " * @cmd: operation",
        " *",
        " * Check permission when a semaphore operation specified by @cmd is to be",
        " * performed on the semaphore.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sem_semctl(struct kern_ipc_perm *sma, int cmd)",
        "{",
        "	return call_int_hook(sem_semctl, sma, cmd);",
        "}",
        "",
        "/**",
        " * security_sem_semop() - Check if a sysv semaphore operation is allowed",
        " * @sma: sysv ipc permission structure",
        " * @sops: operations to perform",
        " * @nsops: number of operations",
        " * @alter: flag indicating changes will be made",
        " *",
        " * Check permissions before performing operations on members of the semaphore",
        " * set. If the @alter flag is nonzero, the semaphore set may be modified.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sem_semop(struct kern_ipc_perm *sma, struct sembuf *sops,",
        "		       unsigned nsops, int alter)",
        "{",
        "	return call_int_hook(sem_semop, sma, sops, nsops, alter);",
        "}",
        "",
        "/**",
        " * security_d_instantiate() - Populate an inode's LSM state based on a dentry",
        " * @dentry: dentry",
        " * @inode: inode",
        " *",
        " * Fill in @inode security information for a @dentry if allowed.",
        " */",
        "void security_d_instantiate(struct dentry *dentry, struct inode *inode)",
        "{",
        "	if (unlikely(inode && IS_PRIVATE(inode)))",
        "		return;",
        "	call_void_hook(d_instantiate, dentry, inode);",
        "}",
        "EXPORT_SYMBOL(security_d_instantiate);",
        "",
        "/*",
        " * Please keep this in sync with it's counterpart in security/lsm_syscalls.c",
        " */",
        "",
        "/**",
        " * security_getselfattr - Read an LSM attribute of the current process.",
        " * @attr: which attribute to return",
        " * @uctx: the user-space destination for the information, or NULL",
        " * @size: pointer to the size of space available to receive the data",
        " * @flags: special handling options. LSM_FLAG_SINGLE indicates that only",
        " * attributes associated with the LSM identified in the passed @ctx be",
        " * reported.",
        " *",
        " * A NULL value for @uctx can be used to get both the number of attributes",
        " * and the size of the data.",
        " *",
        " * Returns the number of attributes found on success, negative value",
        " * on error. @size is reset to the total size of the data.",
        " * If @size is insufficient to contain the data -E2BIG is returned.",
        " */",
        "int security_getselfattr(unsigned int attr, struct lsm_ctx __user *uctx,",
        "			 u32 __user *size, u32 flags)",
        "{",
        "	struct lsm_static_call *scall;",
        "	struct lsm_ctx lctx = { .id = LSM_ID_UNDEF, };",
        "	u8 __user *base = (u8 __user *)uctx;",
        "	u32 entrysize;",
        "	u32 total = 0;",
        "	u32 left;",
        "	bool toobig = false;",
        "	bool single = false;",
        "	int count = 0;",
        "	int rc;",
        "",
        "	if (attr == LSM_ATTR_UNDEF)",
        "		return -EINVAL;",
        "	if (size == NULL)",
        "		return -EINVAL;",
        "	if (get_user(left, size))",
        "		return -EFAULT;",
        "",
        "	if (flags) {",
        "		/*",
        "		 * Only flag supported is LSM_FLAG_SINGLE",
        "		 */",
        "		if (flags != LSM_FLAG_SINGLE || !uctx)",
        "			return -EINVAL;",
        "		if (copy_from_user(&lctx, uctx, sizeof(lctx)))",
        "			return -EFAULT;",
        "		/*",
        "		 * If the LSM ID isn't specified it is an error.",
        "		 */",
        "		if (lctx.id == LSM_ID_UNDEF)",
        "			return -EINVAL;",
        "		single = true;",
        "	}",
        "",
        "	/*",
        "	 * In the usual case gather all the data from the LSMs.",
        "	 * In the single case only get the data from the LSM specified.",
        "	 */",
        "	lsm_for_each_hook(scall, getselfattr) {",
        "		if (single && lctx.id != scall->hl->lsmid->id)",
        "			continue;",
        "		entrysize = left;",
        "		if (base)",
        "			uctx = (struct lsm_ctx __user *)(base + total);",
        "		rc = scall->hl->hook.getselfattr(attr, uctx, &entrysize, flags);",
        "		if (rc == -EOPNOTSUPP) {",
        "			rc = 0;",
        "			continue;",
        "		}",
        "		if (rc == -E2BIG) {",
        "			rc = 0;",
        "			left = 0;",
        "			toobig = true;",
        "		} else if (rc < 0)",
        "			return rc;",
        "		else",
        "			left -= entrysize;",
        "",
        "		total += entrysize;",
        "		count += rc;",
        "		if (single)",
        "			break;",
        "	}",
        "	if (put_user(total, size))",
        "		return -EFAULT;",
        "	if (toobig)",
        "		return -E2BIG;",
        "	if (count == 0)",
        "		return LSM_RET_DEFAULT(getselfattr);",
        "	return count;",
        "}",
        "",
        "/*",
        " * Please keep this in sync with it's counterpart in security/lsm_syscalls.c",
        " */",
        "",
        "/**",
        " * security_setselfattr - Set an LSM attribute on the current process.",
        " * @attr: which attribute to set",
        " * @uctx: the user-space source for the information",
        " * @size: the size of the data",
        " * @flags: reserved for future use, must be 0",
        " *",
        " * Set an LSM attribute for the current process. The LSM, attribute",
        " * and new value are included in @uctx.",
        " *",
        " * Returns 0 on success, -EINVAL if the input is inconsistent, -EFAULT",
        " * if the user buffer is inaccessible, E2BIG if size is too big, or an",
        " * LSM specific failure.",
        " */",
        "int security_setselfattr(unsigned int attr, struct lsm_ctx __user *uctx,",
        "			 u32 size, u32 flags)",
        "{",
        "	struct lsm_static_call *scall;",
        "	struct lsm_ctx *lctx;",
        "	int rc = LSM_RET_DEFAULT(setselfattr);",
        "	u64 required_len;",
        "",
        "	if (flags)",
        "		return -EINVAL;",
        "	if (size < sizeof(*lctx))",
        "		return -EINVAL;",
        "	if (size > PAGE_SIZE)",
        "		return -E2BIG;",
        "",
        "	lctx = memdup_user(uctx, size);",
        "	if (IS_ERR(lctx))",
        "		return PTR_ERR(lctx);",
        "",
        "	if (size < lctx->len ||",
        "	    check_add_overflow(sizeof(*lctx), lctx->ctx_len, &required_len) ||",
        "	    lctx->len < required_len) {",
        "		rc = -EINVAL;",
        "		goto free_out;",
        "	}",
        "",
        "	lsm_for_each_hook(scall, setselfattr)",
        "		if ((scall->hl->lsmid->id) == lctx->id) {",
        "			rc = scall->hl->hook.setselfattr(attr, lctx, size, flags);",
        "			break;",
        "		}",
        "",
        "free_out:",
        "	kfree(lctx);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_getprocattr() - Read an attribute for a task",
        " * @p: the task",
        " * @lsmid: LSM identification",
        " * @name: attribute name",
        " * @value: attribute value",
        " *",
        " * Read attribute @name for task @p and store it into @value if allowed.",
        " *",
        " * Return: Returns the length of @value on success, a negative value otherwise.",
        " */",
        "int security_getprocattr(struct task_struct *p, int lsmid, const char *name,",
        "			 char **value)",
        "{",
        "	struct lsm_static_call *scall;",
        "",
        "	lsm_for_each_hook(scall, getprocattr) {",
        "		if (lsmid != 0 && lsmid != scall->hl->lsmid->id)",
        "			continue;",
        "		return scall->hl->hook.getprocattr(p, name, value);",
        "	}",
        "	return LSM_RET_DEFAULT(getprocattr);",
        "}",
        "",
        "/**",
        " * security_setprocattr() - Set an attribute for a task",
        " * @lsmid: LSM identification",
        " * @name: attribute name",
        " * @value: attribute value",
        " * @size: attribute value size",
        " *",
        " * Write (set) the current task's attribute @name to @value, size @size if",
        " * allowed.",
        " *",
        " * Return: Returns bytes written on success, a negative value otherwise.",
        " */",
        "int security_setprocattr(int lsmid, const char *name, void *value, size_t size)",
        "{",
        "	struct lsm_static_call *scall;",
        "",
        "	lsm_for_each_hook(scall, setprocattr) {",
        "		if (lsmid != 0 && lsmid != scall->hl->lsmid->id)",
        "			continue;",
        "		return scall->hl->hook.setprocattr(name, value, size);",
        "	}",
        "	return LSM_RET_DEFAULT(setprocattr);",
        "}",
        "",
        "/**",
        " * security_netlink_send() - Save info and check if netlink sending is allowed",
        " * @sk: sending socket",
        " * @skb: netlink message",
        " *",
        " * Save security information for a netlink message so that permission checking",
        " * can be performed when the message is processed.  The security information",
        " * can be saved using the eff_cap field of the netlink_skb_parms structure.",
        " * Also may be used to provide fine grained control over message transmission.",
        " *",
        " * Return: Returns 0 if the information was successfully saved and message is",
        " *         allowed to be transmitted.",
        " */",
        "int security_netlink_send(struct sock *sk, struct sk_buff *skb)",
        "{",
        "	return call_int_hook(netlink_send, sk, skb);",
        "}",
        "",
        "/**",
        " * security_ismaclabel() - Check if the named attribute is a MAC label",
        " * @name: full extended attribute name",
        " *",
        " * Check if the extended attribute specified by @name represents a MAC label.",
        " *",
        " * Return: Returns 1 if name is a MAC attribute otherwise returns 0.",
        " */",
        "int security_ismaclabel(const char *name)",
        "{",
        "	return call_int_hook(ismaclabel, name);",
        "}",
        "EXPORT_SYMBOL(security_ismaclabel);",
        "",
        "/**",
        " * security_secid_to_secctx() - Convert a secid to a secctx",
        " * @secid: secid",
        " * @secdata: secctx",
        " * @seclen: secctx length",
        " *",
        " * Convert secid to security context.  If @secdata is NULL the length of the",
        " * result will be returned in @seclen, but no @secdata will be returned.  This",
        " * does mean that the length could change between calls to check the length and",
        " * the next call which actually allocates and returns the @secdata.",
        " *",
        " * Return: Return 0 on success, error on failure.",
        " */",
        "int security_secid_to_secctx(u32 secid, char **secdata, u32 *seclen)",
        "{",
        "	return call_int_hook(secid_to_secctx, secid, secdata, seclen);",
        "}",
        "EXPORT_SYMBOL(security_secid_to_secctx);",
        "",
        "/**",
        " * security_lsmprop_to_secctx() - Convert a lsm_prop to a secctx",
        " * @prop: lsm specific information",
        " * @secdata: secctx",
        " * @seclen: secctx length",
        " *",
        " * Convert a @prop entry to security context.  If @secdata is NULL the",
        " * length of the result will be returned in @seclen, but no @secdata",
        " * will be returned.  This does mean that the length could change between",
        " * calls to check the length and the next call which actually allocates",
        " * and returns the @secdata.",
        " *",
        " * Return: Return 0 on success, error on failure.",
        " */",
        "int security_lsmprop_to_secctx(struct lsm_prop *prop, char **secdata,",
        "			       u32 *seclen)",
        "{",
        "	return call_int_hook(lsmprop_to_secctx, prop, secdata, seclen);",
        "}",
        "EXPORT_SYMBOL(security_lsmprop_to_secctx);",
        "",
        "/**",
        " * security_secctx_to_secid() - Convert a secctx to a secid",
        " * @secdata: secctx",
        " * @seclen: length of secctx",
        " * @secid: secid",
        " *",
        " * Convert security context to secid.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_secctx_to_secid(const char *secdata, u32 seclen, u32 *secid)",
        "{",
        "	*secid = 0;",
        "	return call_int_hook(secctx_to_secid, secdata, seclen, secid);",
        "}",
        "EXPORT_SYMBOL(security_secctx_to_secid);",
        "",
        "/**",
        " * security_release_secctx() - Free a secctx buffer",
        " * @secdata: secctx",
        " * @seclen: length of secctx",
        " *",
        " * Release the security context.",
        " */",
        "void security_release_secctx(char *secdata, u32 seclen)",
        "{",
        "	call_void_hook(release_secctx, secdata, seclen);",
        "}",
        "EXPORT_SYMBOL(security_release_secctx);",
        "",
        "/**",
        " * security_inode_invalidate_secctx() - Invalidate an inode's security label",
        " * @inode: inode",
        " *",
        " * Notify the security module that it must revalidate the security context of",
        " * an inode.",
        " */",
        "void security_inode_invalidate_secctx(struct inode *inode)",
        "{",
        "	call_void_hook(inode_invalidate_secctx, inode);",
        "}",
        "EXPORT_SYMBOL(security_inode_invalidate_secctx);",
        "",
        "/**",
        " * security_inode_notifysecctx() - Notify the LSM of an inode's security label",
        " * @inode: inode",
        " * @ctx: secctx",
        " * @ctxlen: length of secctx",
        " *",
        " * Notify the security module of what the security context of an inode should",
        " * be.  Initializes the incore security context managed by the security module",
        " * for this inode.  Example usage: NFS client invokes this hook to initialize",
        " * the security context in its incore inode to the value provided by the server",
        " * for the file when the server returned the file's attributes to the client.",
        " * Must be called with inode->i_mutex locked.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_inode_notifysecctx(struct inode *inode, void *ctx, u32 ctxlen)",
        "{",
        "	return call_int_hook(inode_notifysecctx, inode, ctx, ctxlen);",
        "}",
        "EXPORT_SYMBOL(security_inode_notifysecctx);",
        "",
        "/**",
        " * security_inode_setsecctx() - Change the security label of an inode",
        " * @dentry: inode",
        " * @ctx: secctx",
        " * @ctxlen: length of secctx",
        " *",
        " * Change the security context of an inode.  Updates the incore security",
        " * context managed by the security module and invokes the fs code as needed",
        " * (via __vfs_setxattr_noperm) to update any backing xattrs that represent the",
        " * context.  Example usage: NFS server invokes this hook to change the security",
        " * context in its incore inode and on the backing filesystem to a value",
        " * provided by the client on a SETATTR operation.  Must be called with",
        " * inode->i_mutex locked.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_inode_setsecctx(struct dentry *dentry, void *ctx, u32 ctxlen)",
        "{",
        "	return call_int_hook(inode_setsecctx, dentry, ctx, ctxlen);",
        "}",
        "EXPORT_SYMBOL(security_inode_setsecctx);",
        "",
        "/**",
        " * security_inode_getsecctx() - Get the security label of an inode",
        " * @inode: inode",
        " * @ctx: secctx",
        " * @ctxlen: length of secctx",
        " *",
        " * On success, returns 0 and fills out @ctx and @ctxlen with the security",
        " * context for the given @inode.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_inode_getsecctx(struct inode *inode, void **ctx, u32 *ctxlen)",
        "{",
        "	return call_int_hook(inode_getsecctx, inode, ctx, ctxlen);",
        "}",
        "EXPORT_SYMBOL(security_inode_getsecctx);",
        "",
        "#ifdef CONFIG_WATCH_QUEUE",
        "/**",
        " * security_post_notification() - Check if a watch notification can be posted",
        " * @w_cred: credentials of the task that set the watch",
        " * @cred: credentials of the task which triggered the watch",
        " * @n: the notification",
        " *",
        " * Check to see if a watch notification can be posted to a particular queue.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_post_notification(const struct cred *w_cred,",
        "			       const struct cred *cred,",
        "			       struct watch_notification *n)",
        "{",
        "	return call_int_hook(post_notification, w_cred, cred, n);",
        "}",
        "#endif /* CONFIG_WATCH_QUEUE */",
        "",
        "#ifdef CONFIG_KEY_NOTIFICATIONS",
        "/**",
        " * security_watch_key() - Check if a task is allowed to watch for key events",
        " * @key: the key to watch",
        " *",
        " * Check to see if a process is allowed to watch for event notifications from",
        " * a key or keyring.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_watch_key(struct key *key)",
        "{",
        "	return call_int_hook(watch_key, key);",
        "}",
        "#endif /* CONFIG_KEY_NOTIFICATIONS */",
        "",
        "#ifdef CONFIG_SECURITY_NETWORK",
        "/**",
        " * security_unix_stream_connect() - Check if a AF_UNIX stream is allowed",
        " * @sock: originating sock",
        " * @other: peer sock",
        " * @newsk: new sock",
        " *",
        " * Check permissions before establishing a Unix domain stream connection",
        " * between @sock and @other.",
        " *",
        " * The @unix_stream_connect and @unix_may_send hooks were necessary because",
        " * Linux provides an alternative to the conventional file name space for Unix",
        " * domain sockets.  Whereas binding and connecting to sockets in the file name",
        " * space is mediated by the typical file permissions (and caught by the mknod",
        " * and permission hooks in inode_security_ops), binding and connecting to",
        " * sockets in the abstract name space is completely unmediated.  Sufficient",
        " * control of Unix domain sockets in the abstract name space isn't possible",
        " * using only the socket layer hooks, since we need to know the actual target",
        " * socket, which is not looked up until we are inside the af_unix code.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_unix_stream_connect(struct sock *sock, struct sock *other,",
        "				 struct sock *newsk)",
        "{",
        "	return call_int_hook(unix_stream_connect, sock, other, newsk);",
        "}",
        "EXPORT_SYMBOL(security_unix_stream_connect);",
        "",
        "/**",
        " * security_unix_may_send() - Check if AF_UNIX socket can send datagrams",
        " * @sock: originating sock",
        " * @other: peer sock",
        " *",
        " * Check permissions before connecting or sending datagrams from @sock to",
        " * @other.",
        " *",
        " * The @unix_stream_connect and @unix_may_send hooks were necessary because",
        " * Linux provides an alternative to the conventional file name space for Unix",
        " * domain sockets.  Whereas binding and connecting to sockets in the file name",
        " * space is mediated by the typical file permissions (and caught by the mknod",
        " * and permission hooks in inode_security_ops), binding and connecting to",
        " * sockets in the abstract name space is completely unmediated.  Sufficient",
        " * control of Unix domain sockets in the abstract name space isn't possible",
        " * using only the socket layer hooks, since we need to know the actual target",
        " * socket, which is not looked up until we are inside the af_unix code.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_unix_may_send(struct socket *sock,  struct socket *other)",
        "{",
        "	return call_int_hook(unix_may_send, sock, other);",
        "}",
        "EXPORT_SYMBOL(security_unix_may_send);",
        "",
        "/**",
        " * security_socket_create() - Check if creating a new socket is allowed",
        " * @family: protocol family",
        " * @type: communications type",
        " * @protocol: requested protocol",
        " * @kern: set to 1 if a kernel socket is requested",
        " *",
        " * Check permissions prior to creating a new socket.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_create(int family, int type, int protocol, int kern)",
        "{",
        "	return call_int_hook(socket_create, family, type, protocol, kern);",
        "}",
        "",
        "/**",
        " * security_socket_post_create() - Initialize a newly created socket",
        " * @sock: socket",
        " * @family: protocol family",
        " * @type: communications type",
        " * @protocol: requested protocol",
        " * @kern: set to 1 if a kernel socket is requested",
        " *",
        " * This hook allows a module to update or allocate a per-socket security",
        " * structure. Note that the security field was not added directly to the socket",
        " * structure, but rather, the socket security information is stored in the",
        " * associated inode.  Typically, the inode alloc_security hook will allocate",
        " * and attach security information to SOCK_INODE(sock)->i_security.  This hook",
        " * may be used to update the SOCK_INODE(sock)->i_security field with additional",
        " * information that wasn't available when the inode was allocated.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_post_create(struct socket *sock, int family,",
        "				int type, int protocol, int kern)",
        "{",
        "	return call_int_hook(socket_post_create, sock, family, type,",
        "			     protocol, kern);",
        "}",
        "",
        "/**",
        " * security_socket_socketpair() - Check if creating a socketpair is allowed",
        " * @socka: first socket",
        " * @sockb: second socket",
        " *",
        " * Check permissions before creating a fresh pair of sockets.",
        " *",
        " * Return: Returns 0 if permission is granted and the connection was",
        " *         established.",
        " */",
        "int security_socket_socketpair(struct socket *socka, struct socket *sockb)",
        "{",
        "	return call_int_hook(socket_socketpair, socka, sockb);",
        "}",
        "EXPORT_SYMBOL(security_socket_socketpair);",
        "",
        "/**",
        " * security_socket_bind() - Check if a socket bind operation is allowed",
        " * @sock: socket",
        " * @address: requested bind address",
        " * @addrlen: length of address",
        " *",
        " * Check permission before socket protocol layer bind operation is performed",
        " * and the socket @sock is bound to the address specified in the @address",
        " * parameter.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_bind(struct socket *sock,",
        "			 struct sockaddr *address, int addrlen)",
        "{",
        "	return call_int_hook(socket_bind, sock, address, addrlen);",
        "}",
        "",
        "/**",
        " * security_socket_connect() - Check if a socket connect operation is allowed",
        " * @sock: socket",
        " * @address: address of remote connection point",
        " * @addrlen: length of address",
        " *",
        " * Check permission before socket protocol layer connect operation attempts to",
        " * connect socket @sock to a remote address, @address.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_connect(struct socket *sock,",
        "			    struct sockaddr *address, int addrlen)",
        "{",
        "	return call_int_hook(socket_connect, sock, address, addrlen);",
        "}",
        "",
        "/**",
        " * security_socket_listen() - Check if a socket is allowed to listen",
        " * @sock: socket",
        " * @backlog: connection queue size",
        " *",
        " * Check permission before socket protocol layer listen operation.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_listen(struct socket *sock, int backlog)",
        "{",
        "	return call_int_hook(socket_listen, sock, backlog);",
        "}",
        "",
        "/**",
        " * security_socket_accept() - Check if a socket is allowed to accept connections",
        " * @sock: listening socket",
        " * @newsock: newly creation connection socket",
        " *",
        " * Check permission before accepting a new connection.  Note that the new",
        " * socket, @newsock, has been created and some information copied to it, but",
        " * the accept operation has not actually been performed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_accept(struct socket *sock, struct socket *newsock)",
        "{",
        "	return call_int_hook(socket_accept, sock, newsock);",
        "}",
        "",
        "/**",
        " * security_socket_sendmsg() - Check if sending a message is allowed",
        " * @sock: sending socket",
        " * @msg: message to send",
        " * @size: size of message",
        " *",
        " * Check permission before transmitting a message to another socket.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_sendmsg(struct socket *sock, struct msghdr *msg, int size)",
        "{",
        "	return call_int_hook(socket_sendmsg, sock, msg, size);",
        "}",
        "",
        "/**",
        " * security_socket_recvmsg() - Check if receiving a message is allowed",
        " * @sock: receiving socket",
        " * @msg: message to receive",
        " * @size: size of message",
        " * @flags: operational flags",
        " *",
        " * Check permission before receiving a message from a socket.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_recvmsg(struct socket *sock, struct msghdr *msg,",
        "			    int size, int flags)",
        "{",
        "	return call_int_hook(socket_recvmsg, sock, msg, size, flags);",
        "}",
        "",
        "/**",
        " * security_socket_getsockname() - Check if reading the socket addr is allowed",
        " * @sock: socket",
        " *",
        " * Check permission before reading the local address (name) of the socket",
        " * object.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_getsockname(struct socket *sock)",
        "{",
        "	return call_int_hook(socket_getsockname, sock);",
        "}",
        "",
        "/**",
        " * security_socket_getpeername() - Check if reading the peer's addr is allowed",
        " * @sock: socket",
        " *",
        " * Check permission before the remote address (name) of a socket object.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_getpeername(struct socket *sock)",
        "{",
        "	return call_int_hook(socket_getpeername, sock);",
        "}",
        "",
        "/**",
        " * security_socket_getsockopt() - Check if reading a socket option is allowed",
        " * @sock: socket",
        " * @level: option's protocol level",
        " * @optname: option name",
        " *",
        " * Check permissions before retrieving the options associated with socket",
        " * @sock.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_getsockopt(struct socket *sock, int level, int optname)",
        "{",
        "	return call_int_hook(socket_getsockopt, sock, level, optname);",
        "}",
        "",
        "/**",
        " * security_socket_setsockopt() - Check if setting a socket option is allowed",
        " * @sock: socket",
        " * @level: option's protocol level",
        " * @optname: option name",
        " *",
        " * Check permissions before setting the options associated with socket @sock.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_setsockopt(struct socket *sock, int level, int optname)",
        "{",
        "	return call_int_hook(socket_setsockopt, sock, level, optname);",
        "}",
        "",
        "/**",
        " * security_socket_shutdown() - Checks if shutting down the socket is allowed",
        " * @sock: socket",
        " * @how: flag indicating how sends and receives are handled",
        " *",
        " * Checks permission before all or part of a connection on the socket @sock is",
        " * shut down.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_socket_shutdown(struct socket *sock, int how)",
        "{",
        "	return call_int_hook(socket_shutdown, sock, how);",
        "}",
        "",
        "/**",
        " * security_sock_rcv_skb() - Check if an incoming network packet is allowed",
        " * @sk: destination sock",
        " * @skb: incoming packet",
        " *",
        " * Check permissions on incoming network packets.  This hook is distinct from",
        " * Netfilter's IP input hooks since it is the first time that the incoming",
        " * sk_buff @skb has been associated with a particular socket, @sk.  Must not",
        " * sleep inside this hook because some callers hold spinlocks.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sock_rcv_skb(struct sock *sk, struct sk_buff *skb)",
        "{",
        "	return call_int_hook(socket_sock_rcv_skb, sk, skb);",
        "}",
        "EXPORT_SYMBOL(security_sock_rcv_skb);",
        "",
        "/**",
        " * security_socket_getpeersec_stream() - Get the remote peer label",
        " * @sock: socket",
        " * @optval: destination buffer",
        " * @optlen: size of peer label copied into the buffer",
        " * @len: maximum size of the destination buffer",
        " *",
        " * This hook allows the security module to provide peer socket security state",
        " * for unix or connected tcp sockets to userspace via getsockopt SO_GETPEERSEC.",
        " * For tcp sockets this can be meaningful if the socket is associated with an",
        " * ipsec SA.",
        " *",
        " * Return: Returns 0 if all is well, otherwise, typical getsockopt return",
        " *         values.",
        " */",
        "int security_socket_getpeersec_stream(struct socket *sock, sockptr_t optval,",
        "				      sockptr_t optlen, unsigned int len)",
        "{",
        "	return call_int_hook(socket_getpeersec_stream, sock, optval, optlen,",
        "			     len);",
        "}",
        "",
        "/**",
        " * security_socket_getpeersec_dgram() - Get the remote peer label",
        " * @sock: socket",
        " * @skb: datagram packet",
        " * @secid: remote peer label secid",
        " *",
        " * This hook allows the security module to provide peer socket security state",
        " * for udp sockets on a per-packet basis to userspace via getsockopt",
        " * SO_GETPEERSEC. The application must first have indicated the IP_PASSSEC",
        " * option via getsockopt. It can then retrieve the security state returned by",
        " * this hook for a packet via the SCM_SECURITY ancillary message type.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_socket_getpeersec_dgram(struct socket *sock,",
        "				     struct sk_buff *skb, u32 *secid)",
        "{",
        "	return call_int_hook(socket_getpeersec_dgram, sock, skb, secid);",
        "}",
        "EXPORT_SYMBOL(security_socket_getpeersec_dgram);",
        "",
        "/**",
        " * lsm_sock_alloc - allocate a composite sock blob",
        " * @sock: the sock that needs a blob",
        " * @gfp: allocation mode",
        " *",
        " * Allocate the sock blob for all the modules",
        " *",
        " * Returns 0, or -ENOMEM if memory can't be allocated.",
        " */",
        "static int lsm_sock_alloc(struct sock *sock, gfp_t gfp)",
        "{",
        "	return lsm_blob_alloc(&sock->sk_security, blob_sizes.lbs_sock, gfp);",
        "}",
        "",
        "/**",
        " * security_sk_alloc() - Allocate and initialize a sock's LSM blob",
        " * @sk: sock",
        " * @family: protocol family",
        " * @priority: gfp flags",
        " *",
        " * Allocate and attach a security structure to the sk->sk_security field, which",
        " * is used to copy security attributes between local stream sockets.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_sk_alloc(struct sock *sk, int family, gfp_t priority)",
        "{",
        "	int rc = lsm_sock_alloc(sk, priority);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(sk_alloc_security, sk, family, priority);",
        "	if (unlikely(rc))",
        "		security_sk_free(sk);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_sk_free() - Free the sock's LSM blob",
        " * @sk: sock",
        " *",
        " * Deallocate security structure.",
        " */",
        "void security_sk_free(struct sock *sk)",
        "{",
        "	call_void_hook(sk_free_security, sk);",
        "	kfree(sk->sk_security);",
        "	sk->sk_security = NULL;",
        "}",
        "",
        "/**",
        " * security_sk_clone() - Clone a sock's LSM state",
        " * @sk: original sock",
        " * @newsk: target sock",
        " *",
        " * Clone/copy security structure.",
        " */",
        "void security_sk_clone(const struct sock *sk, struct sock *newsk)",
        "{",
        "	call_void_hook(sk_clone_security, sk, newsk);",
        "}",
        "EXPORT_SYMBOL(security_sk_clone);",
        "",
        "/**",
        " * security_sk_classify_flow() - Set a flow's secid based on socket",
        " * @sk: original socket",
        " * @flic: target flow",
        " *",
        " * Set the target flow's secid to socket's secid.",
        " */",
        "void security_sk_classify_flow(const struct sock *sk, struct flowi_common *flic)",
        "{",
        "	call_void_hook(sk_getsecid, sk, &flic->flowic_secid);",
        "}",
        "EXPORT_SYMBOL(security_sk_classify_flow);",
        "",
        "/**",
        " * security_req_classify_flow() - Set a flow's secid based on request_sock",
        " * @req: request_sock",
        " * @flic: target flow",
        " *",
        " * Sets @flic's secid to @req's secid.",
        " */",
        "void security_req_classify_flow(const struct request_sock *req,",
        "				struct flowi_common *flic)",
        "{",
        "	call_void_hook(req_classify_flow, req, flic);",
        "}",
        "EXPORT_SYMBOL(security_req_classify_flow);",
        "",
        "/**",
        " * security_sock_graft() - Reconcile LSM state when grafting a sock on a socket",
        " * @sk: sock being grafted",
        " * @parent: target parent socket",
        " *",
        " * Sets @parent's inode secid to @sk's secid and update @sk with any necessary",
        " * LSM state from @parent.",
        " */",
        "void security_sock_graft(struct sock *sk, struct socket *parent)",
        "{",
        "	call_void_hook(sock_graft, sk, parent);",
        "}",
        "EXPORT_SYMBOL(security_sock_graft);",
        "",
        "/**",
        " * security_inet_conn_request() - Set request_sock state using incoming connect",
        " * @sk: parent listening sock",
        " * @skb: incoming connection",
        " * @req: new request_sock",
        " *",
        " * Initialize the @req LSM state based on @sk and the incoming connect in @skb.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_inet_conn_request(const struct sock *sk,",
        "			       struct sk_buff *skb, struct request_sock *req)",
        "{",
        "	return call_int_hook(inet_conn_request, sk, skb, req);",
        "}",
        "EXPORT_SYMBOL(security_inet_conn_request);",
        "",
        "/**",
        " * security_inet_csk_clone() - Set new sock LSM state based on request_sock",
        " * @newsk: new sock",
        " * @req: connection request_sock",
        " *",
        " * Set that LSM state of @sock using the LSM state from @req.",
        " */",
        "void security_inet_csk_clone(struct sock *newsk,",
        "			     const struct request_sock *req)",
        "{",
        "	call_void_hook(inet_csk_clone, newsk, req);",
        "}",
        "",
        "/**",
        " * security_inet_conn_established() - Update sock's LSM state with connection",
        " * @sk: sock",
        " * @skb: connection packet",
        " *",
        " * Update @sock's LSM state to represent a new connection from @skb.",
        " */",
        "void security_inet_conn_established(struct sock *sk,",
        "				    struct sk_buff *skb)",
        "{",
        "	call_void_hook(inet_conn_established, sk, skb);",
        "}",
        "EXPORT_SYMBOL(security_inet_conn_established);",
        "",
        "/**",
        " * security_secmark_relabel_packet() - Check if setting a secmark is allowed",
        " * @secid: new secmark value",
        " *",
        " * Check if the process should be allowed to relabel packets to @secid.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_secmark_relabel_packet(u32 secid)",
        "{",
        "	return call_int_hook(secmark_relabel_packet, secid);",
        "}",
        "EXPORT_SYMBOL(security_secmark_relabel_packet);",
        "",
        "/**",
        " * security_secmark_refcount_inc() - Increment the secmark labeling rule count",
        " *",
        " * Tells the LSM to increment the number of secmark labeling rules loaded.",
        " */",
        "void security_secmark_refcount_inc(void)",
        "{",
        "	call_void_hook(secmark_refcount_inc);",
        "}",
        "EXPORT_SYMBOL(security_secmark_refcount_inc);",
        "",
        "/**",
        " * security_secmark_refcount_dec() - Decrement the secmark labeling rule count",
        " *",
        " * Tells the LSM to decrement the number of secmark labeling rules loaded.",
        " */",
        "void security_secmark_refcount_dec(void)",
        "{",
        "	call_void_hook(secmark_refcount_dec);",
        "}",
        "EXPORT_SYMBOL(security_secmark_refcount_dec);",
        "",
        "/**",
        " * security_tun_dev_alloc_security() - Allocate a LSM blob for a TUN device",
        " * @security: pointer to the LSM blob",
        " *",
        " * This hook allows a module to allocate a security structure for a TUN	device,",
        " * returning the pointer in @security.",
        " *",
        " * Return: Returns a zero on success, negative values on failure.",
        " */",
        "int security_tun_dev_alloc_security(void **security)",
        "{",
        "	int rc;",
        "",
        "	rc = lsm_blob_alloc(security, blob_sizes.lbs_tun_dev, GFP_KERNEL);",
        "	if (rc)",
        "		return rc;",
        "",
        "	rc = call_int_hook(tun_dev_alloc_security, *security);",
        "	if (rc) {",
        "		kfree(*security);",
        "		*security = NULL;",
        "	}",
        "	return rc;",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_alloc_security);",
        "",
        "/**",
        " * security_tun_dev_free_security() - Free a TUN device LSM blob",
        " * @security: LSM blob",
        " *",
        " * This hook allows a module to free the security structure for a TUN device.",
        " */",
        "void security_tun_dev_free_security(void *security)",
        "{",
        "	kfree(security);",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_free_security);",
        "",
        "/**",
        " * security_tun_dev_create() - Check if creating a TUN device is allowed",
        " *",
        " * Check permissions prior to creating a new TUN device.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_tun_dev_create(void)",
        "{",
        "	return call_int_hook(tun_dev_create);",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_create);",
        "",
        "/**",
        " * security_tun_dev_attach_queue() - Check if attaching a TUN queue is allowed",
        " * @security: TUN device LSM blob",
        " *",
        " * Check permissions prior to attaching to a TUN device queue.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_tun_dev_attach_queue(void *security)",
        "{",
        "	return call_int_hook(tun_dev_attach_queue, security);",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_attach_queue);",
        "",
        "/**",
        " * security_tun_dev_attach() - Update TUN device LSM state on attach",
        " * @sk: associated sock",
        " * @security: TUN device LSM blob",
        " *",
        " * This hook can be used by the module to update any security state associated",
        " * with the TUN device's sock structure.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_tun_dev_attach(struct sock *sk, void *security)",
        "{",
        "	return call_int_hook(tun_dev_attach, sk, security);",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_attach);",
        "",
        "/**",
        " * security_tun_dev_open() - Update TUN device LSM state on open",
        " * @security: TUN device LSM blob",
        " *",
        " * This hook can be used by the module to update any security state associated",
        " * with the TUN device's security structure.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_tun_dev_open(void *security)",
        "{",
        "	return call_int_hook(tun_dev_open, security);",
        "}",
        "EXPORT_SYMBOL(security_tun_dev_open);",
        "",
        "/**",
        " * security_sctp_assoc_request() - Update the LSM on a SCTP association req",
        " * @asoc: SCTP association",
        " * @skb: packet requesting the association",
        " *",
        " * Passes the @asoc and @chunk->skb of the association INIT packet to the LSM.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_sctp_assoc_request(struct sctp_association *asoc,",
        "				struct sk_buff *skb)",
        "{",
        "	return call_int_hook(sctp_assoc_request, asoc, skb);",
        "}",
        "EXPORT_SYMBOL(security_sctp_assoc_request);",
        "",
        "/**",
        " * security_sctp_bind_connect() - Validate a list of addrs for a SCTP option",
        " * @sk: socket",
        " * @optname: SCTP option to validate",
        " * @address: list of IP addresses to validate",
        " * @addrlen: length of the address list",
        " *",
        " * Validiate permissions required for each address associated with sock	@sk.",
        " * Depending on @optname, the addresses will be treated as either a connect or",
        " * bind service. The @addrlen is calculated on each IPv4 and IPv6 address using",
        " * sizeof(struct sockaddr_in) or sizeof(struct sockaddr_in6).",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_sctp_bind_connect(struct sock *sk, int optname,",
        "			       struct sockaddr *address, int addrlen)",
        "{",
        "	return call_int_hook(sctp_bind_connect, sk, optname, address, addrlen);",
        "}",
        "EXPORT_SYMBOL(security_sctp_bind_connect);",
        "",
        "/**",
        " * security_sctp_sk_clone() - Clone a SCTP sock's LSM state",
        " * @asoc: SCTP association",
        " * @sk: original sock",
        " * @newsk: target sock",
        " *",
        " * Called whenever a new socket is created by accept(2) (i.e. a TCP style",
        " * socket) or when a socket is 'peeled off' e.g userspace calls",
        " * sctp_peeloff(3).",
        " */",
        "void security_sctp_sk_clone(struct sctp_association *asoc, struct sock *sk,",
        "			    struct sock *newsk)",
        "{",
        "	call_void_hook(sctp_sk_clone, asoc, sk, newsk);",
        "}",
        "EXPORT_SYMBOL(security_sctp_sk_clone);",
        "",
        "/**",
        " * security_sctp_assoc_established() - Update LSM state when assoc established",
        " * @asoc: SCTP association",
        " * @skb: packet establishing the association",
        " *",
        " * Passes the @asoc and @chunk->skb of the association COOKIE_ACK packet to the",
        " * security module.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_sctp_assoc_established(struct sctp_association *asoc,",
        "				    struct sk_buff *skb)",
        "{",
        "	return call_int_hook(sctp_assoc_established, asoc, skb);",
        "}",
        "EXPORT_SYMBOL(security_sctp_assoc_established);",
        "",
        "/**",
        " * security_mptcp_add_subflow() - Inherit the LSM label from the MPTCP socket",
        " * @sk: the owning MPTCP socket",
        " * @ssk: the new subflow",
        " *",
        " * Update the labeling for the given MPTCP subflow, to match the one of the",
        " * owning MPTCP socket. This hook has to be called after the socket creation and",
        " * initialization via the security_socket_create() and",
        " * security_socket_post_create() LSM hooks.",
        " *",
        " * Return: Returns 0 on success or a negative error code on failure.",
        " */",
        "int security_mptcp_add_subflow(struct sock *sk, struct sock *ssk)",
        "{",
        "	return call_int_hook(mptcp_add_subflow, sk, ssk);",
        "}",
        "",
        "#endif	/* CONFIG_SECURITY_NETWORK */",
        "",
        "#ifdef CONFIG_SECURITY_INFINIBAND",
        "/**",
        " * security_ib_pkey_access() - Check if access to an IB pkey is allowed",
        " * @sec: LSM blob",
        " * @subnet_prefix: subnet prefix of the port",
        " * @pkey: IB pkey",
        " *",
        " * Check permission to access a pkey when modifying a QP.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_ib_pkey_access(void *sec, u64 subnet_prefix, u16 pkey)",
        "{",
        "	return call_int_hook(ib_pkey_access, sec, subnet_prefix, pkey);",
        "}",
        "EXPORT_SYMBOL(security_ib_pkey_access);",
        "",
        "/**",
        " * security_ib_endport_manage_subnet() - Check if SMPs traffic is allowed",
        " * @sec: LSM blob",
        " * @dev_name: IB device name",
        " * @port_num: port number",
        " *",
        " * Check permissions to send and receive SMPs on a end port.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_ib_endport_manage_subnet(void *sec,",
        "				      const char *dev_name, u8 port_num)",
        "{",
        "	return call_int_hook(ib_endport_manage_subnet, sec, dev_name, port_num);",
        "}",
        "EXPORT_SYMBOL(security_ib_endport_manage_subnet);",
        "",
        "/**",
        " * security_ib_alloc_security() - Allocate an Infiniband LSM blob",
        " * @sec: LSM blob",
        " *",
        " * Allocate a security structure for Infiniband objects.",
        " *",
        " * Return: Returns 0 on success, non-zero on failure.",
        " */",
        "int security_ib_alloc_security(void **sec)",
        "{",
        "	int rc;",
        "",
        "	rc = lsm_blob_alloc(sec, blob_sizes.lbs_ib, GFP_KERNEL);",
        "	if (rc)",
        "		return rc;",
        "",
        "	rc = call_int_hook(ib_alloc_security, *sec);",
        "	if (rc) {",
        "		kfree(*sec);",
        "		*sec = NULL;",
        "	}",
        "	return rc;",
        "}",
        "EXPORT_SYMBOL(security_ib_alloc_security);",
        "",
        "/**",
        " * security_ib_free_security() - Free an Infiniband LSM blob",
        " * @sec: LSM blob",
        " *",
        " * Deallocate an Infiniband security structure.",
        " */",
        "void security_ib_free_security(void *sec)",
        "{",
        "	kfree(sec);",
        "}",
        "EXPORT_SYMBOL(security_ib_free_security);",
        "#endif	/* CONFIG_SECURITY_INFINIBAND */",
        "",
        "#ifdef CONFIG_SECURITY_NETWORK_XFRM",
        "/**",
        " * security_xfrm_policy_alloc() - Allocate a xfrm policy LSM blob",
        " * @ctxp: xfrm security context being added to the SPD",
        " * @sec_ctx: security label provided by userspace",
        " * @gfp: gfp flags",
        " *",
        " * Allocate a security structure to the xp->security field; the security field",
        " * is initialized to NULL when the xfrm_policy is allocated.",
        " *",
        " * Return:  Return 0 if operation was successful.",
        " */",
        "int security_xfrm_policy_alloc(struct xfrm_sec_ctx **ctxp,",
        "			       struct xfrm_user_sec_ctx *sec_ctx,",
        "			       gfp_t gfp)",
        "{",
        "	return call_int_hook(xfrm_policy_alloc_security, ctxp, sec_ctx, gfp);",
        "}",
        "EXPORT_SYMBOL(security_xfrm_policy_alloc);",
        "",
        "/**",
        " * security_xfrm_policy_clone() - Clone xfrm policy LSM state",
        " * @old_ctx: xfrm security context",
        " * @new_ctxp: target xfrm security context",
        " *",
        " * Allocate a security structure in new_ctxp that contains the information from",
        " * the old_ctx structure.",
        " *",
        " * Return: Return 0 if operation was successful.",
        " */",
        "int security_xfrm_policy_clone(struct xfrm_sec_ctx *old_ctx,",
        "			       struct xfrm_sec_ctx **new_ctxp)",
        "{",
        "	return call_int_hook(xfrm_policy_clone_security, old_ctx, new_ctxp);",
        "}",
        "",
        "/**",
        " * security_xfrm_policy_free() - Free a xfrm security context",
        " * @ctx: xfrm security context",
        " *",
        " * Free LSM resources associated with @ctx.",
        " */",
        "void security_xfrm_policy_free(struct xfrm_sec_ctx *ctx)",
        "{",
        "	call_void_hook(xfrm_policy_free_security, ctx);",
        "}",
        "EXPORT_SYMBOL(security_xfrm_policy_free);",
        "",
        "/**",
        " * security_xfrm_policy_delete() - Check if deleting a xfrm policy is allowed",
        " * @ctx: xfrm security context",
        " *",
        " * Authorize deletion of a SPD entry.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_xfrm_policy_delete(struct xfrm_sec_ctx *ctx)",
        "{",
        "	return call_int_hook(xfrm_policy_delete_security, ctx);",
        "}",
        "",
        "/**",
        " * security_xfrm_state_alloc() - Allocate a xfrm state LSM blob",
        " * @x: xfrm state being added to the SAD",
        " * @sec_ctx: security label provided by userspace",
        " *",
        " * Allocate a security structure to the @x->security field; the security field",
        " * is initialized to NULL when the xfrm_state is allocated. Set the context to",
        " * correspond to @sec_ctx.",
        " *",
        " * Return: Return 0 if operation was successful.",
        " */",
        "int security_xfrm_state_alloc(struct xfrm_state *x,",
        "			      struct xfrm_user_sec_ctx *sec_ctx)",
        "{",
        "	return call_int_hook(xfrm_state_alloc, x, sec_ctx);",
        "}",
        "EXPORT_SYMBOL(security_xfrm_state_alloc);",
        "",
        "/**",
        " * security_xfrm_state_alloc_acquire() - Allocate a xfrm state LSM blob",
        " * @x: xfrm state being added to the SAD",
        " * @polsec: associated policy's security context",
        " * @secid: secid from the flow",
        " *",
        " * Allocate a security structure to the x->security field; the security field",
        " * is initialized to NULL when the xfrm_state is allocated.  Set the context to",
        " * correspond to secid.",
        " *",
        " * Return: Returns 0 if operation was successful.",
        " */",
        "int security_xfrm_state_alloc_acquire(struct xfrm_state *x,",
        "				      struct xfrm_sec_ctx *polsec, u32 secid)",
        "{",
        "	return call_int_hook(xfrm_state_alloc_acquire, x, polsec, secid);",
        "}",
        "",
        "/**",
        " * security_xfrm_state_delete() - Check if deleting a xfrm state is allowed",
        " * @x: xfrm state",
        " *",
        " * Authorize deletion of x->security.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_xfrm_state_delete(struct xfrm_state *x)",
        "{",
        "	return call_int_hook(xfrm_state_delete_security, x);",
        "}",
        "EXPORT_SYMBOL(security_xfrm_state_delete);",
        "",
        "/**",
        " * security_xfrm_state_free() - Free a xfrm state",
        " * @x: xfrm state",
        " *",
        " * Deallocate x->security.",
        " */",
        "void security_xfrm_state_free(struct xfrm_state *x)",
        "{",
        "	call_void_hook(xfrm_state_free_security, x);",
        "}",
        "",
        "/**",
        " * security_xfrm_policy_lookup() - Check if using a xfrm policy is allowed",
        " * @ctx: target xfrm security context",
        " * @fl_secid: flow secid used to authorize access",
        " *",
        " * Check permission when a flow selects a xfrm_policy for processing XFRMs on a",
        " * packet.  The hook is called when selecting either a per-socket policy or a",
        " * generic xfrm policy.",
        " *",
        " * Return: Return 0 if permission is granted, -ESRCH otherwise, or -errno on",
        " *         other errors.",
        " */",
        "int security_xfrm_policy_lookup(struct xfrm_sec_ctx *ctx, u32 fl_secid)",
        "{",
        "	return call_int_hook(xfrm_policy_lookup, ctx, fl_secid);",
        "}",
        "",
        "/**",
        " * security_xfrm_state_pol_flow_match() - Check for a xfrm match",
        " * @x: xfrm state to match",
        " * @xp: xfrm policy to check for a match",
        " * @flic: flow to check for a match.",
        " *",
        " * Check @xp and @flic for a match with @x.",
        " *",
        " * Return: Returns 1 if there is a match.",
        " */",
        "int security_xfrm_state_pol_flow_match(struct xfrm_state *x,",
        "				       struct xfrm_policy *xp,",
        "				       const struct flowi_common *flic)",
        "{",
        "	struct lsm_static_call *scall;",
        "	int rc = LSM_RET_DEFAULT(xfrm_state_pol_flow_match);",
        "",
        "	/*",
        "	 * Since this function is expected to return 0 or 1, the judgment",
        "	 * becomes difficult if multiple LSMs supply this call. Fortunately,",
        "	 * we can use the first LSM's judgment because currently only SELinux",
        "	 * supplies this call.",
        "	 *",
        "	 * For speed optimization, we explicitly break the loop rather than",
        "	 * using the macro",
        "	 */",
        "	lsm_for_each_hook(scall, xfrm_state_pol_flow_match) {",
        "		rc = scall->hl->hook.xfrm_state_pol_flow_match(x, xp, flic);",
        "		break;",
        "	}",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_xfrm_decode_session() - Determine the xfrm secid for a packet",
        " * @skb: xfrm packet",
        " * @secid: secid",
        " *",
        " * Decode the packet in @skb and return the security label in @secid.",
        " *",
        " * Return: Return 0 if all xfrms used have the same secid.",
        " */",
        "int security_xfrm_decode_session(struct sk_buff *skb, u32 *secid)",
        "{",
        "	return call_int_hook(xfrm_decode_session, skb, secid, 1);",
        "}",
        "",
        "void security_skb_classify_flow(struct sk_buff *skb, struct flowi_common *flic)",
        "{",
        "	int rc = call_int_hook(xfrm_decode_session, skb, &flic->flowic_secid,",
        "			       0);",
        "",
        "	BUG_ON(rc);",
        "}",
        "EXPORT_SYMBOL(security_skb_classify_flow);",
        "#endif	/* CONFIG_SECURITY_NETWORK_XFRM */",
        "",
        "#ifdef CONFIG_KEYS",
        "/**",
        " * security_key_alloc() - Allocate and initialize a kernel key LSM blob",
        " * @key: key",
        " * @cred: credentials",
        " * @flags: allocation flags",
        " *",
        " * Permit allocation of a key and assign security data. Note that key does not",
        " * have a serial number assigned at this point.",
        " *",
        " * Return: Return 0 if permission is granted, -ve error otherwise.",
        " */",
        "int security_key_alloc(struct key *key, const struct cred *cred,",
        "		       unsigned long flags)",
        "{",
        "	int rc = lsm_key_alloc(key);",
        "",
        "	if (unlikely(rc))",
        "		return rc;",
        "	rc = call_int_hook(key_alloc, key, cred, flags);",
        "	if (unlikely(rc))",
        "		security_key_free(key);",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_key_free() - Free a kernel key LSM blob",
        " * @key: key",
        " *",
        " * Notification of destruction; free security data.",
        " */",
        "void security_key_free(struct key *key)",
        "{",
        "	kfree(key->security);",
        "	key->security = NULL;",
        "}",
        "",
        "/**",
        " * security_key_permission() - Check if a kernel key operation is allowed",
        " * @key_ref: key reference",
        " * @cred: credentials of actor requesting access",
        " * @need_perm: requested permissions",
        " *",
        " * See whether a specific operational right is granted to a process on a key.",
        " *",
        " * Return: Return 0 if permission is granted, -ve error otherwise.",
        " */",
        "int security_key_permission(key_ref_t key_ref, const struct cred *cred,",
        "			    enum key_need_perm need_perm)",
        "{",
        "	return call_int_hook(key_permission, key_ref, cred, need_perm);",
        "}",
        "",
        "/**",
        " * security_key_getsecurity() - Get the key's security label",
        " * @key: key",
        " * @buffer: security label buffer",
        " *",
        " * Get a textual representation of the security context attached to a key for",
        " * the purposes of honouring KEYCTL_GETSECURITY.  This function allocates the",
        " * storage for the NUL-terminated string and the caller should free it.",
        " *",
        " * Return: Returns the length of @buffer (including terminating NUL) or -ve if",
        " *         an error occurs.  May also return 0 (and a NULL buffer pointer) if",
        " *         there is no security label assigned to the key.",
        " */",
        "int security_key_getsecurity(struct key *key, char **buffer)",
        "{",
        "	*buffer = NULL;",
        "	return call_int_hook(key_getsecurity, key, buffer);",
        "}",
        "",
        "/**",
        " * security_key_post_create_or_update() - Notification of key create or update",
        " * @keyring: keyring to which the key is linked to",
        " * @key: created or updated key",
        " * @payload: data used to instantiate or update the key",
        " * @payload_len: length of payload",
        " * @flags: key flags",
        " * @create: flag indicating whether the key was created or updated",
        " *",
        " * Notify the caller of a key creation or update.",
        " */",
        "void security_key_post_create_or_update(struct key *keyring, struct key *key,",
        "					const void *payload, size_t payload_len,",
        "					unsigned long flags, bool create)",
        "{",
        "	call_void_hook(key_post_create_or_update, keyring, key, payload,",
        "		       payload_len, flags, create);",
        "}",
        "#endif	/* CONFIG_KEYS */",
        "",
        "#ifdef CONFIG_AUDIT",
        "/**",
        " * security_audit_rule_init() - Allocate and init an LSM audit rule struct",
        " * @field: audit action",
        " * @op: rule operator",
        " * @rulestr: rule context",
        " * @lsmrule: receive buffer for audit rule struct",
        " * @gfp: GFP flag used for kmalloc",
        " *",
        " * Allocate and initialize an LSM audit rule structure.",
        " *",
        " * Return: Return 0 if @lsmrule has been successfully set, -EINVAL in case of",
        " *         an invalid rule.",
        " */",
        "int security_audit_rule_init(u32 field, u32 op, char *rulestr, void **lsmrule,",
        "			     gfp_t gfp)",
        "{",
        "	return call_int_hook(audit_rule_init, field, op, rulestr, lsmrule, gfp);",
        "}",
        "",
        "/**",
        " * security_audit_rule_known() - Check if an audit rule contains LSM fields",
        " * @krule: audit rule",
        " *",
        " * Specifies whether given @krule contains any fields related to the current",
        " * LSM.",
        " *",
        " * Return: Returns 1 in case of relation found, 0 otherwise.",
        " */",
        "int security_audit_rule_known(struct audit_krule *krule)",
        "{",
        "	return call_int_hook(audit_rule_known, krule);",
        "}",
        "",
        "/**",
        " * security_audit_rule_free() - Free an LSM audit rule struct",
        " * @lsmrule: audit rule struct",
        " *",
        " * Deallocate the LSM audit rule structure previously allocated by",
        " * audit_rule_init().",
        " */",
        "void security_audit_rule_free(void *lsmrule)",
        "{",
        "	call_void_hook(audit_rule_free, lsmrule);",
        "}",
        "",
        "/**",
        " * security_audit_rule_match() - Check if a label matches an audit rule",
        " * @prop: security label",
        " * @field: LSM audit field",
        " * @op: matching operator",
        " * @lsmrule: audit rule",
        " *",
        " * Determine if given @secid matches a rule previously approved by",
        " * security_audit_rule_known().",
        " *",
        " * Return: Returns 1 if secid matches the rule, 0 if it does not, -ERRNO on",
        " *         failure.",
        " */",
        "int security_audit_rule_match(struct lsm_prop *prop, u32 field, u32 op,",
        "			      void *lsmrule)",
        "{",
        "	return call_int_hook(audit_rule_match, prop, field, op, lsmrule);",
        "}",
        "#endif /* CONFIG_AUDIT */",
        "",
        "#ifdef CONFIG_BPF_SYSCALL",
        "/**",
        " * security_bpf() - Check if the bpf syscall operation is allowed",
        " * @cmd: command",
        " * @attr: bpf attribute",
        " * @size: size",
        " *",
        " * Do a initial check for all bpf syscalls after the attribute is copied into",
        " * the kernel. The actual security module can implement their own rules to",
        " * check the specific cmd they need.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_bpf(int cmd, union bpf_attr *attr, unsigned int size)",
        "{",
        "	return call_int_hook(bpf, cmd, attr, size);",
        "}",
        "",
        "/**",
        " * security_bpf_map() - Check if access to a bpf map is allowed",
        " * @map: bpf map",
        " * @fmode: mode",
        " *",
        " * Do a check when the kernel generates and returns a file descriptor for eBPF",
        " * maps.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_bpf_map(struct bpf_map *map, fmode_t fmode)",
        "{",
        "	return call_int_hook(bpf_map, map, fmode);",
        "}",
        "",
        "/**",
        " * security_bpf_prog() - Check if access to a bpf program is allowed",
        " * @prog: bpf program",
        " *",
        " * Do a check when the kernel generates and returns a file descriptor for eBPF",
        " * programs.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_bpf_prog(struct bpf_prog *prog)",
        "{",
        "	return call_int_hook(bpf_prog, prog);",
        "}",
        "",
        "/**",
        " * security_bpf_map_create() - Check if BPF map creation is allowed",
        " * @map: BPF map object",
        " * @attr: BPF syscall attributes used to create BPF map",
        " * @token: BPF token used to grant user access",
        " *",
        " * Do a check when the kernel creates a new BPF map. This is also the",
        " * point where LSM blob is allocated for LSMs that need them.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_bpf_map_create(struct bpf_map *map, union bpf_attr *attr,",
        "			    struct bpf_token *token)",
        "{",
        "	return call_int_hook(bpf_map_create, map, attr, token);",
        "}",
        "",
        "/**",
        " * security_bpf_prog_load() - Check if loading of BPF program is allowed",
        " * @prog: BPF program object",
        " * @attr: BPF syscall attributes used to create BPF program",
        " * @token: BPF token used to grant user access to BPF subsystem",
        " *",
        " * Perform an access control check when the kernel loads a BPF program and",
        " * allocates associated BPF program object. This hook is also responsible for",
        " * allocating any required LSM state for the BPF program.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_bpf_prog_load(struct bpf_prog *prog, union bpf_attr *attr,",
        "			   struct bpf_token *token)",
        "{",
        "	return call_int_hook(bpf_prog_load, prog, attr, token);",
        "}",
        "",
        "/**",
        " * security_bpf_token_create() - Check if creating of BPF token is allowed",
        " * @token: BPF token object",
        " * @attr: BPF syscall attributes used to create BPF token",
        " * @path: path pointing to BPF FS mount point from which BPF token is created",
        " *",
        " * Do a check when the kernel instantiates a new BPF token object from BPF FS",
        " * instance. This is also the point where LSM blob can be allocated for LSMs.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_bpf_token_create(struct bpf_token *token, union bpf_attr *attr,",
        "			      const struct path *path)",
        "{",
        "	return call_int_hook(bpf_token_create, token, attr, path);",
        "}",
        "",
        "/**",
        " * security_bpf_token_cmd() - Check if BPF token is allowed to delegate",
        " * requested BPF syscall command",
        " * @token: BPF token object",
        " * @cmd: BPF syscall command requested to be delegated by BPF token",
        " *",
        " * Do a check when the kernel decides whether provided BPF token should allow",
        " * delegation of requested BPF syscall command.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_bpf_token_cmd(const struct bpf_token *token, enum bpf_cmd cmd)",
        "{",
        "	return call_int_hook(bpf_token_cmd, token, cmd);",
        "}",
        "",
        "/**",
        " * security_bpf_token_capable() - Check if BPF token is allowed to delegate",
        " * requested BPF-related capability",
        " * @token: BPF token object",
        " * @cap: capabilities requested to be delegated by BPF token",
        " *",
        " * Do a check when the kernel decides whether provided BPF token should allow",
        " * delegation of requested BPF-related capabilities.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_bpf_token_capable(const struct bpf_token *token, int cap)",
        "{",
        "	return call_int_hook(bpf_token_capable, token, cap);",
        "}",
        "",
        "/**",
        " * security_bpf_map_free() - Free a bpf map's LSM blob",
        " * @map: bpf map",
        " *",
        " * Clean up the security information stored inside bpf map.",
        " */",
        "void security_bpf_map_free(struct bpf_map *map)",
        "{",
        "	call_void_hook(bpf_map_free, map);",
        "}",
        "",
        "/**",
        " * security_bpf_prog_free() - Free a BPF program's LSM blob",
        " * @prog: BPF program struct",
        " *",
        " * Clean up the security information stored inside BPF program.",
        " */",
        "void security_bpf_prog_free(struct bpf_prog *prog)",
        "{",
        "	call_void_hook(bpf_prog_free, prog);",
        "}",
        "",
        "/**",
        " * security_bpf_token_free() - Free a BPF token's LSM blob",
        " * @token: BPF token struct",
        " *",
        " * Clean up the security information stored inside BPF token.",
        " */",
        "void security_bpf_token_free(struct bpf_token *token)",
        "{",
        "	call_void_hook(bpf_token_free, token);",
        "}",
        "#endif /* CONFIG_BPF_SYSCALL */",
        "",
        "/**",
        " * security_locked_down() - Check if a kernel feature is allowed",
        " * @what: requested kernel feature",
        " *",
        " * Determine whether a kernel feature that potentially enables arbitrary code",
        " * execution in kernel space should be permitted.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_locked_down(enum lockdown_reason what)",
        "{",
        "	return call_int_hook(locked_down, what);",
        "}",
        "EXPORT_SYMBOL(security_locked_down);",
        "",
        "/**",
        " * security_bdev_alloc() - Allocate a block device LSM blob",
        " * @bdev: block device",
        " *",
        " * Allocate and attach a security structure to @bdev->bd_security.  The",
        " * security field is initialized to NULL when the bdev structure is",
        " * allocated.",
        " *",
        " * Return: Return 0 if operation was successful.",
        " */",
        "int security_bdev_alloc(struct block_device *bdev)",
        "{",
        "	int rc = 0;",
        "",
        "	rc = lsm_bdev_alloc(bdev);",
        "	if (unlikely(rc))",
        "		return rc;",
        "",
        "	rc = call_int_hook(bdev_alloc_security, bdev);",
        "	if (unlikely(rc))",
        "		security_bdev_free(bdev);",
        "",
        "	return rc;",
        "}",
        "EXPORT_SYMBOL(security_bdev_alloc);",
        "",
        "/**",
        " * security_bdev_free() - Free a block device's LSM blob",
        " * @bdev: block device",
        " *",
        " * Deallocate the bdev security structure and set @bdev->bd_security to NULL.",
        " */",
        "void security_bdev_free(struct block_device *bdev)",
        "{",
        "	if (!bdev->bd_security)",
        "		return;",
        "",
        "	call_void_hook(bdev_free_security, bdev);",
        "",
        "	kfree(bdev->bd_security);",
        "	bdev->bd_security = NULL;",
        "}",
        "EXPORT_SYMBOL(security_bdev_free);",
        "",
        "/**",
        " * security_bdev_setintegrity() - Set the device's integrity data",
        " * @bdev: block device",
        " * @type: type of integrity, e.g. hash digest, signature, etc",
        " * @value: the integrity value",
        " * @size: size of the integrity value",
        " *",
        " * Register a verified integrity measurement of a bdev with LSMs.",
        " * LSMs should free the previously saved data if @value is NULL.",
        " * Please note that the new hook should be invoked every time the security",
        " * information is updated to keep these data current. For example, in dm-verity,",
        " * if the mapping table is reloaded and configured to use a different dm-verity",
        " * target with a new roothash and signing information, the previously stored",
        " * data in the LSM blob will become obsolete. It is crucial to re-invoke the",
        " * hook to refresh these data and ensure they are up to date. This necessity",
        " * arises from the design of device-mapper, where a device-mapper device is",
        " * first created, and then targets are subsequently loaded into it. These",
        " * targets can be modified multiple times during the device's lifetime.",
        " * Therefore, while the LSM blob is allocated during the creation of the block",
        " * device, its actual contents are not initialized at this stage and can change",
        " * substantially over time. This includes alterations from data that the LSMs",
        " * 'trusts' to those they do not, making it essential to handle these changes",
        " * correctly. Failure to address this dynamic aspect could potentially allow",
        " * for bypassing LSM checks.",
        " *",
        " * Return: Returns 0 on success, negative values on failure.",
        " */",
        "int security_bdev_setintegrity(struct block_device *bdev,",
        "			       enum lsm_integrity_type type, const void *value,",
        "			       size_t size)",
        "{",
        "	return call_int_hook(bdev_setintegrity, bdev, type, value, size);",
        "}",
        "EXPORT_SYMBOL(security_bdev_setintegrity);",
        "",
        "#ifdef CONFIG_PERF_EVENTS",
        "/**",
        " * security_perf_event_open() - Check if a perf event open is allowed",
        " * @attr: perf event attribute",
        " * @type: type of event",
        " *",
        " * Check whether the @type of perf_event_open syscall is allowed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_perf_event_open(struct perf_event_attr *attr, int type)",
        "{",
        "	return call_int_hook(perf_event_open, attr, type);",
        "}",
        "",
        "/**",
        " * security_perf_event_alloc() - Allocate a perf event LSM blob",
        " * @event: perf event",
        " *",
        " * Allocate and save perf_event security info.",
        " *",
        " * Return: Returns 0 on success, error on failure.",
        " */",
        "int security_perf_event_alloc(struct perf_event *event)",
        "{",
        "	int rc;",
        "",
        "	rc = lsm_blob_alloc(&event->security, blob_sizes.lbs_perf_event,",
        "			    GFP_KERNEL);",
        "	if (rc)",
        "		return rc;",
        "",
        "	rc = call_int_hook(perf_event_alloc, event);",
        "	if (rc) {",
        "		kfree(event->security);",
        "		event->security = NULL;",
        "	}",
        "	return rc;",
        "}",
        "",
        "/**",
        " * security_perf_event_free() - Free a perf event LSM blob",
        " * @event: perf event",
        " *",
        " * Release (free) perf_event security info.",
        " */",
        "void security_perf_event_free(struct perf_event *event)",
        "{",
        "	kfree(event->security);",
        "	event->security = NULL;",
        "}",
        "",
        "/**",
        " * security_perf_event_read() - Check if reading a perf event label is allowed",
        " * @event: perf event",
        " *",
        " * Read perf_event security info if allowed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_perf_event_read(struct perf_event *event)",
        "{",
        "	return call_int_hook(perf_event_read, event);",
        "}",
        "",
        "/**",
        " * security_perf_event_write() - Check if writing a perf event label is allowed",
        " * @event: perf event",
        " *",
        " * Write perf_event security info if allowed.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_perf_event_write(struct perf_event *event)",
        "{",
        "	return call_int_hook(perf_event_write, event);",
        "}",
        "#endif /* CONFIG_PERF_EVENTS */",
        "",
        "#ifdef CONFIG_IO_URING",
        "/**",
        " * security_uring_override_creds() - Check if overriding creds is allowed",
        " * @new: new credentials",
        " *",
        " * Check if the current task, executing an io_uring operation, is allowed to",
        " * override it's credentials with @new.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_uring_override_creds(const struct cred *new)",
        "{",
        "	return call_int_hook(uring_override_creds, new);",
        "}",
        "",
        "/**",
        " * security_uring_sqpoll() - Check if IORING_SETUP_SQPOLL is allowed",
        " *",
        " * Check whether the current task is allowed to spawn a io_uring polling thread",
        " * (IORING_SETUP_SQPOLL).",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_uring_sqpoll(void)",
        "{",
        "	return call_int_hook(uring_sqpoll);",
        "}",
        "",
        "/**",
        " * security_uring_cmd() - Check if a io_uring passthrough command is allowed",
        " * @ioucmd: command",
        " *",
        " * Check whether the file_operations uring_cmd is allowed to run.",
        " *",
        " * Return: Returns 0 if permission is granted.",
        " */",
        "int security_uring_cmd(struct io_uring_cmd *ioucmd)",
        "{",
        "	return call_int_hook(uring_cmd, ioucmd);",
        "}",
        "#endif /* CONFIG_IO_URING */",
        "",
        "/**",
        " * security_initramfs_populated() - Notify LSMs that initramfs has been loaded",
        " *",
        " * Tells the LSMs the initramfs has been unpacked into the rootfs.",
        " */",
        "void security_initramfs_populated(void)",
        "{",
        "	call_void_hook(initramfs_populated);",
        "}"
    ]
  },
  "include_linux_pid_h": {
    path: "include/linux/pid.h",
    covered: [149],
    totalLines: 334,
    coveredCount: 1,
    coveragePct: 0.3,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_PID_H",
        "#define _LINUX_PID_H",
        "",
        "#include <linux/pid_types.h>",
        "#include <linux/rculist.h>",
        "#include <linux/rcupdate.h>",
        "#include <linux/refcount.h>",
        "#include <linux/sched.h>",
        "#include <linux/wait.h>",
        "",
        "/*",
        " * What is struct pid?",
        " *",
        " * A struct pid is the kernel's internal notion of a process identifier.",
        " * It refers to individual tasks, process groups, and sessions.  While",
        " * there are processes attached to it the struct pid lives in a hash",
        " * table, so it and then the processes that it refers to can be found",
        " * quickly from the numeric pid value.  The attached processes may be",
        " * quickly accessed by following pointers from struct pid.",
        " *",
        " * Storing pid_t values in the kernel and referring to them later has a",
        " * problem.  The process originally with that pid may have exited and the",
        " * pid allocator wrapped, and another process could have come along",
        " * and been assigned that pid.",
        " *",
        " * Referring to user space processes by holding a reference to struct",
        " * task_struct has a problem.  When the user space process exits",
        " * the now useless task_struct is still kept.  A task_struct plus a",
        " * stack consumes around 10K of low kernel memory.  More precisely",
        " * this is THREAD_SIZE + sizeof(struct task_struct).  By comparison",
        " * a struct pid is about 64 bytes.",
        " *",
        " * Holding a reference to struct pid solves both of these problems.",
        " * It is small so holding a reference does not consume a lot of",
        " * resources, and since a new struct pid is allocated when the numeric pid",
        " * value is reused (when pids wrap around) we don't mistakenly refer to new",
        " * processes.",
        " */",
        "",
        "",
        "/*",
        " * struct upid is used to get the id of the struct pid, as it is",
        " * seen in particular namespace. Later the struct pid is found with",
        " * find_pid_ns() using the int nr and struct pid_namespace *ns.",
        " */",
        "",
        "#define RESERVED_PIDS 300",
        "",
        "struct upid {",
        "	int nr;",
        "	struct pid_namespace *ns;",
        "};",
        "",
        "struct pid",
        "{",
        "	refcount_t count;",
        "	unsigned int level;",
        "	spinlock_t lock;",
        "	struct dentry *stashed;",
        "	u64 ino;",
        "	/* lists of tasks that use this pid */",
        "	struct hlist_head tasks[PIDTYPE_MAX];",
        "	struct hlist_head inodes;",
        "	/* wait queue for pidfd notifications */",
        "	wait_queue_head_t wait_pidfd;",
        "	struct rcu_head rcu;",
        "	struct upid numbers[];",
        "};",
        "",
        "extern struct pid init_struct_pid;",
        "",
        "struct file;",
        "",
        "struct pid *pidfd_pid(const struct file *file);",
        "struct pid *pidfd_get_pid(unsigned int fd, unsigned int *flags);",
        "struct task_struct *pidfd_get_task(int pidfd, unsigned int *flags);",
        "int pidfd_prepare(struct pid *pid, unsigned int flags, struct file **ret);",
        "void do_notify_pidfd(struct task_struct *task);",
        "",
        "static inline struct pid *get_pid(struct pid *pid)",
        "{",
        "	if (pid)",
        "		refcount_inc(&pid->count);",
        "	return pid;",
        "}",
        "",
        "extern void put_pid(struct pid *pid);",
        "extern struct task_struct *pid_task(struct pid *pid, enum pid_type);",
        "static inline bool pid_has_task(struct pid *pid, enum pid_type type)",
        "{",
        "	return !hlist_empty(&pid->tasks[type]);",
        "}",
        "extern struct task_struct *get_pid_task(struct pid *pid, enum pid_type);",
        "",
        "extern struct pid *get_task_pid(struct task_struct *task, enum pid_type type);",
        "",
        "/*",
        " * these helpers must be called with the tasklist_lock write-held.",
        " */",
        "extern void attach_pid(struct task_struct *task, enum pid_type);",
        "extern void detach_pid(struct task_struct *task, enum pid_type);",
        "extern void change_pid(struct task_struct *task, enum pid_type,",
        "			struct pid *pid);",
        "extern void exchange_tids(struct task_struct *task, struct task_struct *old);",
        "extern void transfer_pid(struct task_struct *old, struct task_struct *new,",
        "			 enum pid_type);",
        "",
        "extern int pid_max;",
        "extern int pid_max_min, pid_max_max;",
        "",
        "/*",
        " * look up a PID in the hash table. Must be called with the tasklist_lock",
        " * or rcu_read_lock() held.",
        " *",
        " * find_pid_ns() finds the pid in the namespace specified",
        " * find_vpid() finds the pid by its virtual id, i.e. in the current namespace",
        " *",
        " * see also find_task_by_vpid() set in include/linux/sched.h",
        " */",
        "extern struct pid *find_pid_ns(int nr, struct pid_namespace *ns);",
        "extern struct pid *find_vpid(int nr);",
        "",
        "/*",
        " * Lookup a PID in the hash table, and return with it's count elevated.",
        " */",
        "extern struct pid *find_get_pid(int nr);",
        "extern struct pid *find_ge_pid(int nr, struct pid_namespace *);",
        "",
        "extern struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid,",
        "			     size_t set_tid_size);",
        "extern void free_pid(struct pid *pid);",
        "extern void disable_pid_allocation(struct pid_namespace *ns);",
        "",
        "/*",
        " * ns_of_pid() returns the pid namespace in which the specified pid was",
        " * allocated.",
        " *",
        " * NOTE:",
        " * 	ns_of_pid() is expected to be called for a process (task) that has",
        " * 	an attached 'struct pid' (see attach_pid(), detach_pid()) i.e @pid",
        " * 	is expected to be non-NULL. If @pid is NULL, caller should handle",
        " * 	the resulting NULL pid-ns.",
        " */",
        "static inline struct pid_namespace *ns_of_pid(struct pid *pid)",
        "{",
        "	struct pid_namespace *ns = NULL;",
        "	if (pid)",
        "		ns = pid->numbers[pid->level].ns;",
        "	return ns;",
        "}",
        "",
        "/*",
        " * is_child_reaper returns true if the pid is the init process",
        " * of the current namespace. As this one could be checked before",
        " * pid_ns->child_reaper is assigned in copy_process, we check",
        " * with the pid number.",
        " */",
        "static inline bool is_child_reaper(struct pid *pid)",
        "{",
        "	return pid->numbers[pid->level].nr == 1;",
        "}",
        "",
        "/*",
        " * the helpers to get the pid's id seen from different namespaces",
        " *",
        " * pid_nr()    : global id, i.e. the id seen from the init namespace;",
        " * pid_vnr()   : virtual id, i.e. the id seen from the pid namespace of",
        " *               current.",
        " * pid_nr_ns() : id seen from the ns specified.",
        " *",
        " * see also task_xid_nr() etc in include/linux/sched.h",
        " */",
        "",
        "static inline pid_t pid_nr(struct pid *pid)",
        "{",
        "	pid_t nr = 0;",
        "	if (pid)",
        "		nr = pid->numbers[0].nr;",
        "	return nr;",
        "}",
        "",
        "pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns);",
        "pid_t pid_vnr(struct pid *pid);",
        "",
        "#define do_each_pid_task(pid, type, task)				\\",
        "	do {								\\",
        "		if ((pid) != NULL)					\\",
        "			hlist_for_each_entry_rcu((task),		\\",
        "				&(pid)->tasks[type], pid_links[type]) {",
        "",
        "			/*",
        "			 * Both old and new leaders may be attached to",
        "			 * the same pid in the middle of de_thread().",
        "			 */",
        "#define while_each_pid_task(pid, type, task)				\\",
        "				if (type == PIDTYPE_PID)		\\",
        "					break;				\\",
        "			}						\\",
        "	} while (0)",
        "",
        "#define do_each_pid_thread(pid, type, task)				\\",
        "	do_each_pid_task(pid, type, task) {				\\",
        "		struct task_struct *tg___ = task;			\\",
        "		for_each_thread(tg___, task) {",
        "",
        "#define while_each_pid_thread(pid, type, task)				\\",
        "		}							\\",
        "		task = tg___;						\\",
        "	} while_each_pid_task(pid, type, task)",
        "",
        "static inline struct pid *task_pid(struct task_struct *task)",
        "{",
        "	return task->thread_pid;",
        "}",
        "",
        "/*",
        " * the helpers to get the task's different pids as they are seen",
        " * from various namespaces",
        " *",
        " * task_xid_nr()     : global id, i.e. the id seen from the init namespace;",
        " * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of",
        " *                     current.",
        " * task_xid_nr_ns()  : id seen from the ns specified;",
        " *",
        " * see also pid_nr() etc in include/linux/pid.h",
        " */",
        "pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);",
        "",
        "static inline pid_t task_pid_nr(struct task_struct *tsk)",
        "{",
        "	return tsk->pid;",
        "}",
        "",
        "static inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);",
        "}",
        "",
        "static inline pid_t task_pid_vnr(struct task_struct *tsk)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);",
        "}",
        "",
        "",
        "static inline pid_t task_tgid_nr(struct task_struct *tsk)",
        "{",
        "	return tsk->tgid;",
        "}",
        "",
        "/**",
        " * pid_alive - check that a task structure is not stale",
        " * @p: Task structure to be checked.",
        " *",
        " * Test if a process is not yet dead (at most zombie state)",
        " * If pid_alive fails, then pointers within the task structure",
        " * can be stale and must not be dereferenced.",
        " *",
        " * Return: 1 if the process is alive. 0 otherwise.",
        " */",
        "static inline int pid_alive(const struct task_struct *p)",
        "{",
        "	return p->thread_pid != NULL;",
        "}",
        "",
        "static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);",
        "}",
        "",
        "static inline pid_t task_pgrp_vnr(struct task_struct *tsk)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);",
        "}",
        "",
        "",
        "static inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);",
        "}",
        "",
        "static inline pid_t task_session_vnr(struct task_struct *tsk)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);",
        "}",
        "",
        "static inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);",
        "}",
        "",
        "static inline pid_t task_tgid_vnr(struct task_struct *tsk)",
        "{",
        "	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, NULL);",
        "}",
        "",
        "static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)",
        "{",
        "	pid_t pid = 0;",
        "",
        "	rcu_read_lock();",
        "	if (pid_alive(tsk))",
        "		pid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);",
        "	rcu_read_unlock();",
        "",
        "	return pid;",
        "}",
        "",
        "static inline pid_t task_ppid_nr(const struct task_struct *tsk)",
        "{",
        "	return task_ppid_nr_ns(tsk, &init_pid_ns);",
        "}",
        "",
        "/* Obsolete, do not use: */",
        "static inline pid_t task_pgrp_nr(struct task_struct *tsk)",
        "{",
        "	return task_pgrp_nr_ns(tsk, &init_pid_ns);",
        "}",
        "",
        "/**",
        " * is_global_init - check if a task structure is init. Since init",
        " * is free to have sub-threads we need to check tgid.",
        " * @tsk: Task structure to be checked.",
        " *",
        " * Check if a task structure is the first user space task the kernel created.",
        " *",
        " * Return: 1 if the task structure is init. 0 otherwise.",
        " */",
        "static inline int is_global_init(struct task_struct *tsk)",
        "{",
        "	return task_tgid_nr(tsk) == 1;",
        "}",
        "",
        "#endif /* _LINUX_PID_H */"
    ]
  },
  "kernel_bpf_tcx_c": {
    path: "kernel/bpf/tcx.c",
    covered: [135, 123, 73, 60, 74, 131, 69],
    totalLines: 346,
    coveredCount: 7,
    coveragePct: 2.0,
    source: [
        "// SPDX-License-Identifier: GPL-2.0",
        "/* Copyright (c) 2023 Isovalent */",
        "",
        "#include <linux/bpf.h>",
        "#include <linux/bpf_mprog.h>",
        "#include <linux/netdevice.h>",
        "",
        "#include <net/tcx.h>",
        "",
        "int tcx_prog_attach(const union bpf_attr *attr, struct bpf_prog *prog)",
        "{",
        "	bool created, ingress = attr->attach_type == BPF_TCX_INGRESS;",
        "	struct net *net = current->nsproxy->net_ns;",
        "	struct bpf_mprog_entry *entry, *entry_new;",
        "	struct bpf_prog *replace_prog = NULL;",
        "	struct net_device *dev;",
        "	int ret;",
        "",
        "	rtnl_lock();",
        "	dev = __dev_get_by_index(net, attr->target_ifindex);",
        "	if (!dev) {",
        "		ret = -ENODEV;",
        "		goto out;",
        "	}",
        "	if (attr->attach_flags & BPF_F_REPLACE) {",
        "		replace_prog = bpf_prog_get_type(attr->replace_bpf_fd,",
        "						 prog->type);",
        "		if (IS_ERR(replace_prog)) {",
        "			ret = PTR_ERR(replace_prog);",
        "			replace_prog = NULL;",
        "			goto out;",
        "		}",
        "	}",
        "	entry = tcx_entry_fetch_or_create(dev, ingress, &created);",
        "	if (!entry) {",
        "		ret = -ENOMEM;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_attach(entry, &entry_new, prog, NULL, replace_prog,",
        "			       attr->attach_flags, attr->relative_fd,",
        "			       attr->expected_revision);",
        "	if (!ret) {",
        "		if (entry != entry_new) {",
        "			tcx_entry_update(dev, entry_new, ingress);",
        "			tcx_entry_sync();",
        "			tcx_skeys_inc(ingress);",
        "		}",
        "		bpf_mprog_commit(entry);",
        "	} else if (created) {",
        "		tcx_entry_free(entry);",
        "	}",
        "out:",
        "	if (replace_prog)",
        "		bpf_prog_put(replace_prog);",
        "	rtnl_unlock();",
        "	return ret;",
        "}",
        "",
        "int tcx_prog_detach(const union bpf_attr *attr, struct bpf_prog *prog)",
        "{",
        "	bool ingress = attr->attach_type == BPF_TCX_INGRESS;",
        "	struct net *net = current->nsproxy->net_ns;",
        "	struct bpf_mprog_entry *entry, *entry_new;",
        "	struct net_device *dev;",
        "	int ret;",
        "",
        "	rtnl_lock();",
        "	dev = __dev_get_by_index(net, attr->target_ifindex);",
        "	if (!dev) {",
        "		ret = -ENODEV;",
        "		goto out;",
        "	}",
        "	entry = tcx_entry_fetch(dev, ingress);",
        "	if (!entry) {",
        "		ret = -ENOENT;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_detach(entry, &entry_new, prog, NULL, attr->attach_flags,",
        "			       attr->relative_fd, attr->expected_revision);",
        "	if (!ret) {",
        "		if (!tcx_entry_is_active(entry_new))",
        "			entry_new = NULL;",
        "		tcx_entry_update(dev, entry_new, ingress);",
        "		tcx_entry_sync();",
        "		tcx_skeys_dec(ingress);",
        "		bpf_mprog_commit(entry);",
        "		if (!entry_new)",
        "			tcx_entry_free(entry);",
        "	}",
        "out:",
        "	rtnl_unlock();",
        "	return ret;",
        "}",
        "",
        "void tcx_uninstall(struct net_device *dev, bool ingress)",
        "{",
        "	struct bpf_mprog_entry *entry, *entry_new = NULL;",
        "	struct bpf_tuple tuple = {};",
        "	struct bpf_mprog_fp *fp;",
        "	struct bpf_mprog_cp *cp;",
        "	bool active;",
        "",
        "	entry = tcx_entry_fetch(dev, ingress);",
        "	if (!entry)",
        "		return;",
        "	active = tcx_entry(entry)->miniq_active;",
        "	if (active)",
        "		bpf_mprog_clear_all(entry, &entry_new);",
        "	tcx_entry_update(dev, entry_new, ingress);",
        "	tcx_entry_sync();",
        "	bpf_mprog_foreach_tuple(entry, fp, cp, tuple) {",
        "		if (tuple.link)",
        "			tcx_link(tuple.link)->dev = NULL;",
        "		else",
        "			bpf_prog_put(tuple.prog);",
        "		tcx_skeys_dec(ingress);",
        "	}",
        "	if (!active)",
        "		tcx_entry_free(entry);",
        "}",
        "",
        "int tcx_prog_query(const union bpf_attr *attr, union bpf_attr __user *uattr)",
        "{",
        "	bool ingress = attr->query.attach_type == BPF_TCX_INGRESS;",
        "	struct net *net = current->nsproxy->net_ns;",
        "	struct net_device *dev;",
        "	int ret;",
        "",
        "	rtnl_lock();",
        "	dev = __dev_get_by_index(net, attr->query.target_ifindex);",
        "	if (!dev) {",
        "		ret = -ENODEV;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_query(attr, uattr, tcx_entry_fetch(dev, ingress));",
        "out:",
        "	rtnl_unlock();",
        "	return ret;",
        "}",
        "",
        "static int tcx_link_prog_attach(struct bpf_link *link, u32 flags, u32 id_or_fd,",
        "				u64 revision)",
        "{",
        "	struct tcx_link *tcx = tcx_link(link);",
        "	bool created, ingress = tcx->location == BPF_TCX_INGRESS;",
        "	struct bpf_mprog_entry *entry, *entry_new;",
        "	struct net_device *dev = tcx->dev;",
        "	int ret;",
        "",
        "	ASSERT_RTNL();",
        "	entry = tcx_entry_fetch_or_create(dev, ingress, &created);",
        "	if (!entry)",
        "		return -ENOMEM;",
        "	ret = bpf_mprog_attach(entry, &entry_new, link->prog, link, NULL, flags,",
        "			       id_or_fd, revision);",
        "	if (!ret) {",
        "		if (entry != entry_new) {",
        "			tcx_entry_update(dev, entry_new, ingress);",
        "			tcx_entry_sync();",
        "			tcx_skeys_inc(ingress);",
        "		}",
        "		bpf_mprog_commit(entry);",
        "	} else if (created) {",
        "		tcx_entry_free(entry);",
        "	}",
        "	return ret;",
        "}",
        "",
        "static void tcx_link_release(struct bpf_link *link)",
        "{",
        "	struct tcx_link *tcx = tcx_link(link);",
        "	bool ingress = tcx->location == BPF_TCX_INGRESS;",
        "	struct bpf_mprog_entry *entry, *entry_new;",
        "	struct net_device *dev;",
        "	int ret = 0;",
        "",
        "	rtnl_lock();",
        "	dev = tcx->dev;",
        "	if (!dev)",
        "		goto out;",
        "	entry = tcx_entry_fetch(dev, ingress);",
        "	if (!entry) {",
        "		ret = -ENOENT;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_detach(entry, &entry_new, link->prog, link, 0, 0, 0);",
        "	if (!ret) {",
        "		if (!tcx_entry_is_active(entry_new))",
        "			entry_new = NULL;",
        "		tcx_entry_update(dev, entry_new, ingress);",
        "		tcx_entry_sync();",
        "		tcx_skeys_dec(ingress);",
        "		bpf_mprog_commit(entry);",
        "		if (!entry_new)",
        "			tcx_entry_free(entry);",
        "		tcx->dev = NULL;",
        "	}",
        "out:",
        "	WARN_ON_ONCE(ret);",
        "	rtnl_unlock();",
        "}",
        "",
        "static int tcx_link_update(struct bpf_link *link, struct bpf_prog *nprog,",
        "			   struct bpf_prog *oprog)",
        "{",
        "	struct tcx_link *tcx = tcx_link(link);",
        "	bool ingress = tcx->location == BPF_TCX_INGRESS;",
        "	struct bpf_mprog_entry *entry, *entry_new;",
        "	struct net_device *dev;",
        "	int ret = 0;",
        "",
        "	rtnl_lock();",
        "	dev = tcx->dev;",
        "	if (!dev) {",
        "		ret = -ENOLINK;",
        "		goto out;",
        "	}",
        "	if (oprog && link->prog != oprog) {",
        "		ret = -EPERM;",
        "		goto out;",
        "	}",
        "	oprog = link->prog;",
        "	if (oprog == nprog) {",
        "		bpf_prog_put(nprog);",
        "		goto out;",
        "	}",
        "	entry = tcx_entry_fetch(dev, ingress);",
        "	if (!entry) {",
        "		ret = -ENOENT;",
        "		goto out;",
        "	}",
        "	ret = bpf_mprog_attach(entry, &entry_new, nprog, link, oprog,",
        "			       BPF_F_REPLACE | BPF_F_ID,",
        "			       link->prog->aux->id, 0);",
        "	if (!ret) {",
        "		WARN_ON_ONCE(entry != entry_new);",
        "		oprog = xchg(&link->prog, nprog);",
        "		bpf_prog_put(oprog);",
        "		bpf_mprog_commit(entry);",
        "	}",
        "out:",
        "	rtnl_unlock();",
        "	return ret;",
        "}",
        "",
        "static void tcx_link_dealloc(struct bpf_link *link)",
        "{",
        "	kfree(tcx_link(link));",
        "}",
        "",
        "static void tcx_link_fdinfo(const struct bpf_link *link, struct seq_file *seq)",
        "{",
        "	const struct tcx_link *tcx = tcx_link(link);",
        "	u32 ifindex = 0;",
        "",
        "	rtnl_lock();",
        "	if (tcx->dev)",
        "		ifindex = tcx->dev->ifindex;",
        "	rtnl_unlock();",
        "",
        "	seq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);",
        "	seq_printf(seq, \"attach_type:\\t%u (%s)\\n\",",
        "		   tcx->location,",
        "		   tcx->location == BPF_TCX_INGRESS ? \"ingress\" : \"egress\");",
        "}",
        "",
        "static int tcx_link_fill_info(const struct bpf_link *link,",
        "			      struct bpf_link_info *info)",
        "{",
        "	const struct tcx_link *tcx = tcx_link(link);",
        "	u32 ifindex = 0;",
        "",
        "	rtnl_lock();",
        "	if (tcx->dev)",
        "		ifindex = tcx->dev->ifindex;",
        "	rtnl_unlock();",
        "",
        "	info->tcx.ifindex = ifindex;",
        "	info->tcx.attach_type = tcx->location;",
        "	return 0;",
        "}",
        "",
        "static int tcx_link_detach(struct bpf_link *link)",
        "{",
        "	tcx_link_release(link);",
        "	return 0;",
        "}",
        "",
        "static const struct bpf_link_ops tcx_link_lops = {",
        "	.release	= tcx_link_release,",
        "	.detach		= tcx_link_detach,",
        "	.dealloc	= tcx_link_dealloc,",
        "	.update_prog	= tcx_link_update,",
        "	.show_fdinfo	= tcx_link_fdinfo,",
        "	.fill_link_info	= tcx_link_fill_info,",
        "};",
        "",
        "static int tcx_link_init(struct tcx_link *tcx,",
        "			 struct bpf_link_primer *link_primer,",
        "			 const union bpf_attr *attr,",
        "			 struct net_device *dev,",
        "			 struct bpf_prog *prog)",
        "{",
        "	bpf_link_init(&tcx->link, BPF_LINK_TYPE_TCX, &tcx_link_lops, prog);",
        "	tcx->location = attr->link_create.attach_type;",
        "	tcx->dev = dev;",
        "	return bpf_link_prime(&tcx->link, link_primer);",
        "}",
        "",
        "int tcx_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)",
        "{",
        "	struct net *net = current->nsproxy->net_ns;",
        "	struct bpf_link_primer link_primer;",
        "	struct net_device *dev;",
        "	struct tcx_link *tcx;",
        "	int ret;",
        "",
        "	rtnl_lock();",
        "	dev = __dev_get_by_index(net, attr->link_create.target_ifindex);",
        "	if (!dev) {",
        "		ret = -ENODEV;",
        "		goto out;",
        "	}",
        "	tcx = kzalloc(sizeof(*tcx), GFP_USER);",
        "	if (!tcx) {",
        "		ret = -ENOMEM;",
        "		goto out;",
        "	}",
        "	ret = tcx_link_init(tcx, &link_primer, attr, dev, prog);",
        "	if (ret) {",
        "		kfree(tcx);",
        "		goto out;",
        "	}",
        "	ret = tcx_link_prog_attach(&tcx->link, attr->link_create.flags,",
        "				   attr->link_create.tcx.relative_fd,",
        "				   attr->link_create.tcx.expected_revision);",
        "	if (ret) {",
        "		tcx->dev = NULL;",
        "		bpf_link_cleanup(&link_primer);",
        "		goto out;",
        "	}",
        "	ret = bpf_link_settle(&link_primer);",
        "out:",
        "	rtnl_unlock();",
        "	return ret;",
        "}"
    ]
  },
  "include_linux_rbtree_augmented_h": {
    path: "include/linux/rbtree_augmented.h",
    covered: [308, 244, 331, 242, 315, 300, 329, 330, 312, 321, 247, 199, 293, 253],
    totalLines: 343,
    coveredCount: 14,
    coveragePct: 4.1,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0-or-later */",
        "/*",
        "  Red Black Trees",
        "  (C) 1999  Andrea Arcangeli <andrea@suse.de>",
        "  (C) 2002  David Woodhouse <dwmw2@infradead.org>",
        "  (C) 2012  Michel Lespinasse <walken@google.com>",
        "",
        "",
        "  linux/include/linux/rbtree_augmented.h",
        "*/",
        "",
        "#ifndef _LINUX_RBTREE_AUGMENTED_H",
        "#define _LINUX_RBTREE_AUGMENTED_H",
        "",
        "#include <linux/compiler.h>",
        "#include <linux/rbtree.h>",
        "#include <linux/rcupdate.h>",
        "",
        "/*",
        " * Please note - only struct rb_augment_callbacks and the prototypes for",
        " * rb_insert_augmented() and rb_erase_augmented() are intended to be public.",
        " * The rest are implementation details you are not expected to depend on.",
        " *",
        " * See Documentation/core-api/rbtree.rst for documentation and samples.",
        " */",
        "",
        "struct rb_augment_callbacks {",
        "	void (*propagate)(struct rb_node *node, struct rb_node *stop);",
        "	void (*copy)(struct rb_node *old, struct rb_node *new);",
        "	void (*rotate)(struct rb_node *old, struct rb_node *new);",
        "};",
        "",
        "extern void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,",
        "	void (*augment_rotate)(struct rb_node *old, struct rb_node *new));",
        "",
        "/*",
        " * Fixup the rbtree and update the augmented information when rebalancing.",
        " *",
        " * On insertion, the user must update the augmented information on the path",
        " * leading to the inserted node, then call rb_link_node() as usual and",
        " * rb_insert_augmented() instead of the usual rb_insert_color() call.",
        " * If rb_insert_augmented() rebalances the rbtree, it will callback into",
        " * a user provided function to update the augmented information on the",
        " * affected subtrees.",
        " */",
        "static inline void",
        "rb_insert_augmented(struct rb_node *node, struct rb_root *root,",
        "		    const struct rb_augment_callbacks *augment)",
        "{",
        "	__rb_insert_augmented(node, root, augment->rotate);",
        "}",
        "",
        "static inline void",
        "rb_insert_augmented_cached(struct rb_node *node,",
        "			   struct rb_root_cached *root, bool newleft,",
        "			   const struct rb_augment_callbacks *augment)",
        "{",
        "	if (newleft)",
        "		root->rb_leftmost = node;",
        "	rb_insert_augmented(node, &root->rb_root, augment);",
        "}",
        "",
        "static __always_inline struct rb_node *",
        "rb_add_augmented_cached(struct rb_node *node, struct rb_root_cached *tree,",
        "			bool (*less)(struct rb_node *, const struct rb_node *),",
        "			const struct rb_augment_callbacks *augment)",
        "{",
        "	struct rb_node **link = &tree->rb_root.rb_node;",
        "	struct rb_node *parent = NULL;",
        "	bool leftmost = true;",
        "",
        "	while (*link) {",
        "		parent = *link;",
        "		if (less(node, parent)) {",
        "			link = &parent->rb_left;",
        "		} else {",
        "			link = &parent->rb_right;",
        "			leftmost = false;",
        "		}",
        "	}",
        "",
        "	rb_link_node(node, parent, link);",
        "	augment->propagate(parent, NULL); /* suboptimal */",
        "	rb_insert_augmented_cached(node, tree, leftmost, augment);",
        "",
        "	return leftmost ? node : NULL;",
        "}",
        "",
        "/*",
        " * Template for declaring augmented rbtree callbacks (generic case)",
        " *",
        " * RBSTATIC:    'static' or empty",
        " * RBNAME:      name of the rb_augment_callbacks structure",
        " * RBSTRUCT:    struct type of the tree nodes",
        " * RBFIELD:     name of struct rb_node field within RBSTRUCT",
        " * RBAUGMENTED: name of field within RBSTRUCT holding data for subtree",
        " * RBCOMPUTE:   name of function that recomputes the RBAUGMENTED data",
        " */",
        "",
        "#define RB_DECLARE_CALLBACKS(RBSTATIC, RBNAME,				\\",
        "			     RBSTRUCT, RBFIELD, RBAUGMENTED, RBCOMPUTE)	\\",
        "static inline void							\\",
        "RBNAME ## _propagate(struct rb_node *rb, struct rb_node *stop)		\\",
        "{									\\",
        "	while (rb != stop) {						\\",
        "		RBSTRUCT *node = rb_entry(rb, RBSTRUCT, RBFIELD);	\\",
        "		if (RBCOMPUTE(node, true))				\\",
        "			break;						\\",
        "		rb = rb_parent(&node->RBFIELD);				\\",
        "	}								\\",
        "}									\\",
        "static inline void							\\",
        "RBNAME ## _copy(struct rb_node *rb_old, struct rb_node *rb_new)		\\",
        "{									\\",
        "	RBSTRUCT *old = rb_entry(rb_old, RBSTRUCT, RBFIELD);		\\",
        "	RBSTRUCT *new = rb_entry(rb_new, RBSTRUCT, RBFIELD);		\\",
        "	new->RBAUGMENTED = old->RBAUGMENTED;				\\",
        "}									\\",
        "static void								\\",
        "RBNAME ## _rotate(struct rb_node *rb_old, struct rb_node *rb_new)	\\",
        "{									\\",
        "	RBSTRUCT *old = rb_entry(rb_old, RBSTRUCT, RBFIELD);		\\",
        "	RBSTRUCT *new = rb_entry(rb_new, RBSTRUCT, RBFIELD);		\\",
        "	new->RBAUGMENTED = old->RBAUGMENTED;				\\",
        "	RBCOMPUTE(old, false);						\\",
        "}									\\",
        "RBSTATIC const struct rb_augment_callbacks RBNAME = {			\\",
        "	.propagate = RBNAME ## _propagate,				\\",
        "	.copy = RBNAME ## _copy,					\\",
        "	.rotate = RBNAME ## _rotate					\\",
        "};",
        "",
        "/*",
        " * Template for declaring augmented rbtree callbacks,",
        " * computing RBAUGMENTED scalar as max(RBCOMPUTE(node)) for all subtree nodes.",
        " *",
        " * RBSTATIC:    'static' or empty",
        " * RBNAME:      name of the rb_augment_callbacks structure",
        " * RBSTRUCT:    struct type of the tree nodes",
        " * RBFIELD:     name of struct rb_node field within RBSTRUCT",
        " * RBTYPE:      type of the RBAUGMENTED field",
        " * RBAUGMENTED: name of RBTYPE field within RBSTRUCT holding data for subtree",
        " * RBCOMPUTE:   name of function that returns the per-node RBTYPE scalar",
        " */",
        "",
        "#define RB_DECLARE_CALLBACKS_MAX(RBSTATIC, RBNAME, RBSTRUCT, RBFIELD,	      \\",
        "				 RBTYPE, RBAUGMENTED, RBCOMPUTE)	      \\",
        "static inline bool RBNAME ## _compute_max(RBSTRUCT *node, bool exit)	      \\",
        "{									      \\",
        "	RBSTRUCT *child;						      \\",
        "	RBTYPE max = RBCOMPUTE(node);					      \\",
        "	if (node->RBFIELD.rb_left) {					      \\",
        "		child = rb_entry(node->RBFIELD.rb_left, RBSTRUCT, RBFIELD);   \\",
        "		if (child->RBAUGMENTED > max)				      \\",
        "			max = child->RBAUGMENTED;			      \\",
        "	}								      \\",
        "	if (node->RBFIELD.rb_right) {					      \\",
        "		child = rb_entry(node->RBFIELD.rb_right, RBSTRUCT, RBFIELD);  \\",
        "		if (child->RBAUGMENTED > max)				      \\",
        "			max = child->RBAUGMENTED;			      \\",
        "	}								      \\",
        "	if (exit && node->RBAUGMENTED == max)				      \\",
        "		return true;						      \\",
        "	node->RBAUGMENTED = max;					      \\",
        "	return false;							      \\",
        "}									      \\",
        "RB_DECLARE_CALLBACKS(RBSTATIC, RBNAME,					      \\",
        "		     RBSTRUCT, RBFIELD, RBAUGMENTED, RBNAME ## _compute_max)",
        "",
        "",
        "#define	RB_RED		0",
        "#define	RB_BLACK	1",
        "",
        "#define __rb_parent(pc)    ((struct rb_node *)(pc & ~3))",
        "",
        "#define __rb_color(pc)     ((pc) & 1)",
        "#define __rb_is_black(pc)  __rb_color(pc)",
        "#define __rb_is_red(pc)    (!__rb_color(pc))",
        "#define rb_color(rb)       __rb_color((rb)->__rb_parent_color)",
        "#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)",
        "#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)",
        "",
        "static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)",
        "{",
        "	rb->__rb_parent_color = rb_color(rb) + (unsigned long)p;",
        "}",
        "",
        "static inline void rb_set_parent_color(struct rb_node *rb,",
        "				       struct rb_node *p, int color)",
        "{",
        "	rb->__rb_parent_color = (unsigned long)p + color;",
        "}",
        "",
        "static inline void",
        "__rb_change_child(struct rb_node *old, struct rb_node *new,",
        "		  struct rb_node *parent, struct rb_root *root)",
        "{",
        "	if (parent) {",
        "		if (parent->rb_left == old)",
        "			WRITE_ONCE(parent->rb_left, new);",
        "		else",
        "			WRITE_ONCE(parent->rb_right, new);",
        "	} else",
        "		WRITE_ONCE(root->rb_node, new);",
        "}",
        "",
        "static inline void",
        "__rb_change_child_rcu(struct rb_node *old, struct rb_node *new,",
        "		      struct rb_node *parent, struct rb_root *root)",
        "{",
        "	if (parent) {",
        "		if (parent->rb_left == old)",
        "			rcu_assign_pointer(parent->rb_left, new);",
        "		else",
        "			rcu_assign_pointer(parent->rb_right, new);",
        "	} else",
        "		rcu_assign_pointer(root->rb_node, new);",
        "}",
        "",
        "extern void __rb_erase_color(struct rb_node *parent, struct rb_root *root,",
        "	void (*augment_rotate)(struct rb_node *old, struct rb_node *new));",
        "",
        "static __always_inline struct rb_node *",
        "__rb_erase_augmented(struct rb_node *node, struct rb_root *root,",
        "		     const struct rb_augment_callbacks *augment)",
        "{",
        "	struct rb_node *child = node->rb_right;",
        "	struct rb_node *tmp = node->rb_left;",
        "	struct rb_node *parent, *rebalance;",
        "	unsigned long pc;",
        "",
        "	if (!tmp) {",
        "		/*",
        "		 * Case 1: node to erase has no more than 1 child (easy!)",
        "		 *",
        "		 * Note that if there is one child it must be red due to 5)",
        "		 * and node must be black due to 4). We adjust colors locally",
        "		 * so as to bypass __rb_erase_color() later on.",
        "		 */",
        "		pc = node->__rb_parent_color;",
        "		parent = __rb_parent(pc);",
        "		__rb_change_child(node, child, parent, root);",
        "		if (child) {",
        "			child->__rb_parent_color = pc;",
        "			rebalance = NULL;",
        "		} else",
        "			rebalance = __rb_is_black(pc) ? parent : NULL;",
        "		tmp = parent;",
        "	} else if (!child) {",
        "		/* Still case 1, but this time the child is node->rb_left */",
        "		tmp->__rb_parent_color = pc = node->__rb_parent_color;",
        "		parent = __rb_parent(pc);",
        "		__rb_change_child(node, tmp, parent, root);",
        "		rebalance = NULL;",
        "		tmp = parent;",
        "	} else {",
        "		struct rb_node *successor = child, *child2;",
        "",
        "		tmp = child->rb_left;",
        "		if (!tmp) {",
        "			/*",
        "			 * Case 2: node's successor is its right child",
        "			 *",
        "			 *    (n)          (s)",
        "			 *    / \\          / \\",
        "			 *  (x) (s)  ->  (x) (c)",
        "			 *        \\",
        "			 *        (c)",
        "			 */",
        "			parent = successor;",
        "			child2 = successor->rb_right;",
        "",
        "			augment->copy(node, successor);",
        "		} else {",
        "			/*",
        "			 * Case 3: node's successor is leftmost under",
        "			 * node's right child subtree",
        "			 *",
        "			 *    (n)          (s)",
        "			 *    / \\          / \\",
        "			 *  (x) (y)  ->  (x) (y)",
        "			 *      /            /",
        "			 *    (p)          (p)",
        "			 *    /            /",
        "			 *  (s)          (c)",
        "			 *    \\",
        "			 *    (c)",
        "			 */",
        "			do {",
        "				parent = successor;",
        "				successor = tmp;",
        "				tmp = tmp->rb_left;",
        "			} while (tmp);",
        "			child2 = successor->rb_right;",
        "			WRITE_ONCE(parent->rb_left, child2);",
        "			WRITE_ONCE(successor->rb_right, child);",
        "			rb_set_parent(child, successor);",
        "",
        "			augment->copy(node, successor);",
        "			augment->propagate(parent, successor);",
        "		}",
        "",
        "		tmp = node->rb_left;",
        "		WRITE_ONCE(successor->rb_left, tmp);",
        "		rb_set_parent(tmp, successor);",
        "",
        "		pc = node->__rb_parent_color;",
        "		tmp = __rb_parent(pc);",
        "		__rb_change_child(node, successor, tmp, root);",
        "",
        "		if (child2) {",
        "			rb_set_parent_color(child2, parent, RB_BLACK);",
        "			rebalance = NULL;",
        "		} else {",
        "			rebalance = rb_is_black(successor) ? parent : NULL;",
        "		}",
        "		successor->__rb_parent_color = pc;",
        "		tmp = successor;",
        "	}",
        "",
        "	augment->propagate(tmp, NULL);",
        "	return rebalance;",
        "}",
        "",
        "static __always_inline void",
        "rb_erase_augmented(struct rb_node *node, struct rb_root *root,",
        "		   const struct rb_augment_callbacks *augment)",
        "{",
        "	struct rb_node *rebalance = __rb_erase_augmented(node, root, augment);",
        "	if (rebalance)",
        "		__rb_erase_color(rebalance, root, augment->rotate);",
        "}",
        "",
        "static __always_inline void",
        "rb_erase_augmented_cached(struct rb_node *node, struct rb_root_cached *root,",
        "			  const struct rb_augment_callbacks *augment)",
        "{",
        "	if (root->rb_leftmost == node)",
        "		root->rb_leftmost = rb_next(node);",
        "	rb_erase_augmented(node, &root->rb_root, augment);",
        "}",
        "",
        "#endif	/* _LINUX_RBTREE_AUGMENTED_H */"
    ]
  },
  "arch_x86_kernel_cpu_mtrr_generic_c": {
    path: "arch/x86/kernel/cpu/mtrr/generic.c",
    covered: [489, 528, 513, 536, 530, 538],
    totalLines: 1079,
    coveredCount: 6,
    coveragePct: 0.6,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "/*",
        " * This only handles 32bit MTRR on 32bit hosts. This is strictly wrong",
        " * because MTRRs can span up to 40 bits (36bits on most modern x86)",
        " */",
        "",
        "#include <linux/export.h>",
        "#include <linux/init.h>",
        "#include <linux/io.h>",
        "#include <linux/mm.h>",
        "#include <linux/cc_platform.h>",
        "#include <asm/processor-flags.h>",
        "#include <asm/cacheinfo.h>",
        "#include <asm/cpufeature.h>",
        "#include <asm/hypervisor.h>",
        "#include <asm/mshyperv.h>",
        "#include <asm/tlbflush.h>",
        "#include <asm/mtrr.h>",
        "#include <asm/msr.h>",
        "#include <asm/memtype.h>",
        "",
        "#include \"mtrr.h\"",
        "",
        "struct fixed_range_block {",
        "	int base_msr;		/* start address of an MTRR block */",
        "	int ranges;		/* number of MTRRs in this block  */",
        "};",
        "",
        "static struct fixed_range_block fixed_range_blocks[] = {",
        "	{ MSR_MTRRfix64K_00000, 1 }, /* one   64k MTRR  */",
        "	{ MSR_MTRRfix16K_80000, 2 }, /* two   16k MTRRs */",
        "	{ MSR_MTRRfix4K_C0000,  8 }, /* eight  4k MTRRs */",
        "	{}",
        "};",
        "",
        "struct cache_map {",
        "	u64 start;",
        "	u64 end;",
        "	u64 flags;",
        "	u64 type:8;",
        "	u64 fixed:1;",
        "};",
        "",
        "bool mtrr_debug;",
        "",
        "static int __init mtrr_param_setup(char *str)",
        "{",
        "	int rc = 0;",
        "",
        "	if (!str)",
        "		return -EINVAL;",
        "	if (!strcmp(str, \"debug\"))",
        "		mtrr_debug = true;",
        "	else",
        "		rc = -EINVAL;",
        "",
        "	return rc;",
        "}",
        "early_param(\"mtrr\", mtrr_param_setup);",
        "",
        "/*",
        " * CACHE_MAP_MAX is the maximum number of memory ranges in cache_map, where",
        " * no 2 adjacent ranges have the same cache mode (those would be merged).",
        " * The number is based on the worst case:",
        " * - no two adjacent fixed MTRRs share the same cache mode",
        " * - one variable MTRR is spanning a huge area with mode WB",
        " * - 255 variable MTRRs with mode UC all overlap with the WB MTRR, creating 2",
        " *   additional ranges each (result like \"ababababa...aba\" with a = WB, b = UC),",
        " *   accounting for MTRR_MAX_VAR_RANGES * 2 - 1 range entries",
        " * - a TOP_MEM2 area (even with overlapping an UC MTRR can't add 2 range entries",
        " *   to the possible maximum, as it always starts at 4GB, thus it can't be in",
        " *   the middle of that MTRR, unless that MTRR starts at 0, which would remove",
        " *   the initial \"a\" from the \"abababa\" pattern above)",
        " * The map won't contain ranges with no matching MTRR (those fall back to the",
        " * default cache mode).",
        " */",
        "#define CACHE_MAP_MAX	(MTRR_NUM_FIXED_RANGES + MTRR_MAX_VAR_RANGES * 2)",
        "",
        "static struct cache_map init_cache_map[CACHE_MAP_MAX] __initdata;",
        "static struct cache_map *cache_map __refdata = init_cache_map;",
        "static unsigned int cache_map_size = CACHE_MAP_MAX;",
        "static unsigned int cache_map_n;",
        "static unsigned int cache_map_fixed;",
        "",
        "static unsigned long smp_changes_mask;",
        "static int mtrr_state_set;",
        "u64 mtrr_tom2;",
        "",
        "struct mtrr_state_type mtrr_state;",
        "EXPORT_SYMBOL_GPL(mtrr_state);",
        "",
        "/* Reserved bits in the high portion of the MTRRphysBaseN MSR. */",
        "u32 phys_hi_rsvd;",
        "",
        "/*",
        " * BIOS is expected to clear MtrrFixDramModEn bit, see for example",
        " * \"BIOS and Kernel Developer's Guide for the AMD Athlon 64 and AMD",
        " * Opteron Processors\" (26094 Rev. 3.30 February 2006), section",
        " * \"13.2.1.2 SYSCFG Register\": \"The MtrrFixDramModEn bit should be set",
        " * to 1 during BIOS initialization of the fixed MTRRs, then cleared to",
        " * 0 for operation.\"",
        " */",
        "static inline void k8_check_syscfg_dram_mod_en(void)",
        "{",
        "	u32 lo, hi;",
        "",
        "	if (!((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) &&",
        "	      (boot_cpu_data.x86 >= 0x0f)))",
        "		return;",
        "",
        "	if (cc_platform_has(CC_ATTR_HOST_SEV_SNP))",
        "		return;",
        "",
        "	rdmsr(MSR_AMD64_SYSCFG, lo, hi);",
        "	if (lo & K8_MTRRFIXRANGE_DRAM_MODIFY) {",
        "		pr_err(FW_WARN \"MTRR: CPU %u: SYSCFG[MtrrFixDramModEn]\"",
        "		       \" not cleared by BIOS, clearing this bit\\n\",",
        "		       smp_processor_id());",
        "		lo &= ~K8_MTRRFIXRANGE_DRAM_MODIFY;",
        "		mtrr_wrmsr(MSR_AMD64_SYSCFG, lo, hi);",
        "	}",
        "}",
        "",
        "/* Get the size of contiguous MTRR range */",
        "static u64 get_mtrr_size(u64 mask)",
        "{",
        "	u64 size;",
        "",
        "	mask |= (u64)phys_hi_rsvd << 32;",
        "	size = -mask;",
        "",
        "	return size;",
        "}",
        "",
        "static u8 get_var_mtrr_state(unsigned int reg, u64 *start, u64 *size)",
        "{",
        "	struct mtrr_var_range *mtrr = mtrr_state.var_ranges + reg;",
        "",
        "	if (!(mtrr->mask_lo & MTRR_PHYSMASK_V))",
        "		return MTRR_TYPE_INVALID;",
        "",
        "	*start = (((u64)mtrr->base_hi) << 32) + (mtrr->base_lo & PAGE_MASK);",
        "	*size = get_mtrr_size((((u64)mtrr->mask_hi) << 32) +",
        "			      (mtrr->mask_lo & PAGE_MASK));",
        "",
        "	return mtrr->base_lo & MTRR_PHYSBASE_TYPE;",
        "}",
        "",
        "static u8 get_effective_type(u8 type1, u8 type2)",
        "{",
        "	if (type1 == MTRR_TYPE_UNCACHABLE || type2 == MTRR_TYPE_UNCACHABLE)",
        "		return MTRR_TYPE_UNCACHABLE;",
        "",
        "	if ((type1 == MTRR_TYPE_WRBACK && type2 == MTRR_TYPE_WRTHROUGH) ||",
        "	    (type1 == MTRR_TYPE_WRTHROUGH && type2 == MTRR_TYPE_WRBACK))",
        "		return MTRR_TYPE_WRTHROUGH;",
        "",
        "	if (type1 != type2)",
        "		return MTRR_TYPE_UNCACHABLE;",
        "",
        "	return type1;",
        "}",
        "",
        "static void rm_map_entry_at(int idx)",
        "{",
        "	cache_map_n--;",
        "	if (cache_map_n > idx) {",
        "		memmove(cache_map + idx, cache_map + idx + 1,",
        "			sizeof(*cache_map) * (cache_map_n - idx));",
        "	}",
        "}",
        "",
        "/*",
        " * Add an entry into cache_map at a specific index.  Merges adjacent entries if",
        " * appropriate.  Return the number of merges for correcting the scan index",
        " * (this is needed as merging will reduce the number of entries, which will",
        " * result in skipping entries in future iterations if the scan index isn't",
        " * corrected).",
        " * Note that the corrected index can never go below -1 (resulting in being 0 in",
        " * the next scan iteration), as \"2\" is returned only if the current index is",
        " * larger than zero.",
        " */",
        "static int add_map_entry_at(u64 start, u64 end, u8 type, int idx)",
        "{",
        "	bool merge_prev = false, merge_next = false;",
        "",
        "	if (start >= end)",
        "		return 0;",
        "",
        "	if (idx > 0) {",
        "		struct cache_map *prev = cache_map + idx - 1;",
        "",
        "		if (!prev->fixed && start == prev->end && type == prev->type)",
        "			merge_prev = true;",
        "	}",
        "",
        "	if (idx < cache_map_n) {",
        "		struct cache_map *next = cache_map + idx;",
        "",
        "		if (!next->fixed && end == next->start && type == next->type)",
        "			merge_next = true;",
        "	}",
        "",
        "	if (merge_prev && merge_next) {",
        "		cache_map[idx - 1].end = cache_map[idx].end;",
        "		rm_map_entry_at(idx);",
        "		return 2;",
        "	}",
        "	if (merge_prev) {",
        "		cache_map[idx - 1].end = end;",
        "		return 1;",
        "	}",
        "	if (merge_next) {",
        "		cache_map[idx].start = start;",
        "		return 1;",
        "	}",
        "",
        "	/* Sanity check: the array should NEVER be too small! */",
        "	if (cache_map_n == cache_map_size) {",
        "		WARN(1, \"MTRR cache mode memory map exhausted!\\n\");",
        "		cache_map_n = cache_map_fixed;",
        "		return 0;",
        "	}",
        "",
        "	if (cache_map_n > idx) {",
        "		memmove(cache_map + idx + 1, cache_map + idx,",
        "			sizeof(*cache_map) * (cache_map_n - idx));",
        "	}",
        "",
        "	cache_map[idx].start = start;",
        "	cache_map[idx].end = end;",
        "	cache_map[idx].type = type;",
        "	cache_map[idx].fixed = 0;",
        "	cache_map_n++;",
        "",
        "	return 0;",
        "}",
        "",
        "/* Clear a part of an entry. Return 1 if start of entry is still valid. */",
        "static int clr_map_range_at(u64 start, u64 end, int idx)",
        "{",
        "	int ret = start != cache_map[idx].start;",
        "	u64 tmp;",
        "",
        "	if (start == cache_map[idx].start && end == cache_map[idx].end) {",
        "		rm_map_entry_at(idx);",
        "	} else if (start == cache_map[idx].start) {",
        "		cache_map[idx].start = end;",
        "	} else if (end == cache_map[idx].end) {",
        "		cache_map[idx].end = start;",
        "	} else {",
        "		tmp = cache_map[idx].end;",
        "		cache_map[idx].end = start;",
        "		add_map_entry_at(end, tmp, cache_map[idx].type, idx + 1);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "/*",
        " * Add MTRR to the map.  The current map is scanned and each part of the MTRR",
        " * either overlapping with an existing entry or with a hole in the map is",
        " * handled separately.",
        " */",
        "static void add_map_entry(u64 start, u64 end, u8 type)",
        "{",
        "	u8 new_type, old_type;",
        "	u64 tmp;",
        "	int i;",
        "",
        "	for (i = 0; i < cache_map_n && start < end; i++) {",
        "		if (start >= cache_map[i].end)",
        "			continue;",
        "",
        "		if (start < cache_map[i].start) {",
        "			/* Region start has no overlap. */",
        "			tmp = min(end, cache_map[i].start);",
        "			i -= add_map_entry_at(start, tmp,  type, i);",
        "			start = tmp;",
        "			continue;",
        "		}",
        "",
        "		new_type = get_effective_type(type, cache_map[i].type);",
        "		old_type = cache_map[i].type;",
        "",
        "		if (cache_map[i].fixed || new_type == old_type) {",
        "			/* Cut off start of new entry. */",
        "			start = cache_map[i].end;",
        "			continue;",
        "		}",
        "",
        "		/* Handle only overlapping part of region. */",
        "		tmp = min(end, cache_map[i].end);",
        "		i += clr_map_range_at(start, tmp, i);",
        "		i -= add_map_entry_at(start, tmp, new_type, i);",
        "		start = tmp;",
        "	}",
        "",
        "	/* Add rest of region after last map entry (rest might be empty). */",
        "	add_map_entry_at(start, end, type, i);",
        "}",
        "",
        "/* Add variable MTRRs to cache map. */",
        "static void map_add_var(void)",
        "{",
        "	u64 start, size;",
        "	unsigned int i;",
        "	u8 type;",
        "",
        "	/*",
        "	 * Add AMD TOP_MEM2 area.  Can't be added in mtrr_build_map(), as it",
        "	 * needs to be added again when rebuilding the map due to potentially",
        "	 * having moved as a result of variable MTRRs for memory below 4GB.",
        "	 */",
        "	if (mtrr_tom2) {",
        "		add_map_entry(BIT_ULL(32), mtrr_tom2, MTRR_TYPE_WRBACK);",
        "		cache_map[cache_map_n - 1].fixed = 1;",
        "	}",
        "",
        "	for (i = 0; i < num_var_ranges; i++) {",
        "		type = get_var_mtrr_state(i, &start, &size);",
        "		if (type != MTRR_TYPE_INVALID)",
        "			add_map_entry(start, start + size, type);",
        "	}",
        "}",
        "",
        "/*",
        " * Rebuild map by replacing variable entries.  Needs to be called when MTRR",
        " * registers are being changed after boot, as such changes could include",
        " * removals of registers, which are complicated to handle without rebuild of",
        " * the map.",
        " */",
        "void generic_rebuild_map(void)",
        "{",
        "	if (mtrr_if != &generic_mtrr_ops)",
        "		return;",
        "",
        "	cache_map_n = cache_map_fixed;",
        "",
        "	map_add_var();",
        "}",
        "",
        "static unsigned int __init get_cache_map_size(void)",
        "{",
        "	return cache_map_fixed + 2 * num_var_ranges + (mtrr_tom2 != 0);",
        "}",
        "",
        "/* Build the cache_map containing the cache modes per memory range. */",
        "void __init mtrr_build_map(void)",
        "{",
        "	u64 start, end, size;",
        "	unsigned int i;",
        "	u8 type;",
        "",
        "	/* Add fixed MTRRs, optimize for adjacent entries with same type. */",
        "	if (mtrr_state.enabled & MTRR_STATE_MTRR_FIXED_ENABLED) {",
        "		/*",
        "		 * Start with 64k size fixed entries, preset 1st one (hence the",
        "		 * loop below is starting with index 1).",
        "		 */",
        "		start = 0;",
        "		end = size = 0x10000;",
        "		type = mtrr_state.fixed_ranges[0];",
        "",
        "		for (i = 1; i < MTRR_NUM_FIXED_RANGES; i++) {",
        "			/* 8 64k entries, then 16 16k ones, rest 4k. */",
        "			if (i == 8 || i == 24)",
        "				size >>= 2;",
        "",
        "			if (mtrr_state.fixed_ranges[i] != type) {",
        "				add_map_entry(start, end, type);",
        "				start = end;",
        "				type = mtrr_state.fixed_ranges[i];",
        "			}",
        "			end += size;",
        "		}",
        "		add_map_entry(start, end, type);",
        "	}",
        "",
        "	/* Mark fixed, they take precedence. */",
        "	for (i = 0; i < cache_map_n; i++)",
        "		cache_map[i].fixed = 1;",
        "	cache_map_fixed = cache_map_n;",
        "",
        "	map_add_var();",
        "",
        "	pr_info(\"MTRR map: %u entries (%u fixed + %u variable; max %u), built from %u variable MTRRs\\n\",",
        "		cache_map_n, cache_map_fixed, cache_map_n - cache_map_fixed,",
        "		get_cache_map_size(), num_var_ranges + (mtrr_tom2 != 0));",
        "",
        "	if (mtrr_debug) {",
        "		for (i = 0; i < cache_map_n; i++) {",
        "			pr_info(\"%3u: %016llx-%016llx %s\\n\", i,",
        "				cache_map[i].start, cache_map[i].end - 1,",
        "				mtrr_attrib_to_str(cache_map[i].type));",
        "		}",
        "	}",
        "}",
        "",
        "/* Copy the cache_map from __initdata memory to dynamically allocated one. */",
        "void __init mtrr_copy_map(void)",
        "{",
        "	unsigned int new_size = get_cache_map_size();",
        "",
        "	if (!mtrr_state.enabled || !new_size) {",
        "		cache_map = NULL;",
        "		return;",
        "	}",
        "",
        "	mutex_lock(&mtrr_mutex);",
        "",
        "	cache_map = kcalloc(new_size, sizeof(*cache_map), GFP_KERNEL);",
        "	if (cache_map) {",
        "		memmove(cache_map, init_cache_map,",
        "			cache_map_n * sizeof(*cache_map));",
        "		cache_map_size = new_size;",
        "	} else {",
        "		mtrr_state.enabled = 0;",
        "		pr_err(\"MTRRs disabled due to allocation failure for lookup map.\\n\");",
        "	}",
        "",
        "	mutex_unlock(&mtrr_mutex);",
        "}",
        "",
        "/**",
        " * mtrr_overwrite_state - set static MTRR state",
        " *",
        " * Used to set MTRR state via different means (e.g. with data obtained from",
        " * a hypervisor).",
        " * Is allowed only for special cases when running virtualized. Must be called",
        " * from the x86_init.hyper.init_platform() hook.  It can be called only once.",
        " * The MTRR state can't be changed afterwards.  To ensure that, X86_FEATURE_MTRR",
        " * is cleared.",
        " *",
        " * @var: MTRR variable range array to use",
        " * @num_var: length of the @var array",
        " * @def_type: default caching type",
        " */",
        "void mtrr_overwrite_state(struct mtrr_var_range *var, unsigned int num_var,",
        "			  mtrr_type def_type)",
        "{",
        "	unsigned int i;",
        "",
        "	/* Only allowed to be called once before mtrr_bp_init(). */",
        "	if (WARN_ON_ONCE(mtrr_state_set))",
        "		return;",
        "",
        "	/* Only allowed when running virtualized. */",
        "	if (!cpu_feature_enabled(X86_FEATURE_HYPERVISOR))",
        "		return;",
        "",
        "	/*",
        "	 * Only allowed for special virtualization cases:",
        "	 * - when running as Hyper-V, SEV-SNP guest using vTOM",
        "	 * - when running as Xen PV guest",
        "	 * - when running as SEV-SNP or TDX guest to avoid unnecessary",
        "	 *   VMM communication/Virtualization exceptions (#VC, #VE)",
        "	 */",
        "	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP) &&",
        "	    !hv_is_isolation_supported() &&",
        "	    !cpu_feature_enabled(X86_FEATURE_XENPV) &&",
        "	    !cpu_feature_enabled(X86_FEATURE_TDX_GUEST))",
        "		return;",
        "",
        "	/* Disable MTRR in order to disable MTRR modifications. */",
        "	setup_clear_cpu_cap(X86_FEATURE_MTRR);",
        "",
        "	if (var) {",
        "		if (num_var > MTRR_MAX_VAR_RANGES) {",
        "			pr_warn(\"Trying to overwrite MTRR state with %u variable entries\\n\",",
        "				num_var);",
        "			num_var = MTRR_MAX_VAR_RANGES;",
        "		}",
        "		for (i = 0; i < num_var; i++)",
        "			mtrr_state.var_ranges[i] = var[i];",
        "		num_var_ranges = num_var;",
        "	}",
        "",
        "	mtrr_state.def_type = def_type;",
        "	mtrr_state.enabled |= MTRR_STATE_MTRR_ENABLED;",
        "",
        "	mtrr_state_set = 1;",
        "}",
        "",
        "static u8 type_merge(u8 type, u8 new_type, u8 *uniform)",
        "{",
        "	u8 effective_type;",
        "",
        "	if (type == MTRR_TYPE_INVALID)",
        "		return new_type;",
        "",
        "	effective_type = get_effective_type(type, new_type);",
        "	if (type != effective_type)",
        "		*uniform = 0;",
        "",
        "	return effective_type;",
        "}",
        "",
        "/**",
        " * mtrr_type_lookup - look up memory type in MTRR",
        " *",
        " * @start: Begin of the physical address range",
        " * @end: End of the physical address range",
        " * @uniform: output argument:",
        " *  - 1: the returned MTRR type is valid for the whole region",
        " *  - 0: otherwise",
        " *",
        " * Return Values:",
        " * MTRR_TYPE_(type)  - The effective MTRR type for the region",
        " * MTRR_TYPE_INVALID - MTRR is disabled",
        " */",
        "u8 mtrr_type_lookup(u64 start, u64 end, u8 *uniform)",
        "{",
        "	u8 type = MTRR_TYPE_INVALID;",
        "	unsigned int i;",
        "",
        "	if (!mtrr_state_set) {",
        "		/* Uniformity is unknown. */",
        "		*uniform = 0;",
        "		return MTRR_TYPE_UNCACHABLE;",
        "	}",
        "",
        "	*uniform = 1;",
        "",
        "	if (!(mtrr_state.enabled & MTRR_STATE_MTRR_ENABLED))",
        "		return MTRR_TYPE_UNCACHABLE;",
        "",
        "	for (i = 0; i < cache_map_n && start < end; i++) {",
        "		/* Region after current map entry? -> continue with next one. */",
        "		if (start >= cache_map[i].end)",
        "			continue;",
        "",
        "		/* Start of region not covered by current map entry? */",
        "		if (start < cache_map[i].start) {",
        "			/* At least some part of region has default type. */",
        "			type = type_merge(type, mtrr_state.def_type, uniform);",
        "			/* End of region not covered, too? -> lookup done. */",
        "			if (end <= cache_map[i].start)",
        "				return type;",
        "		}",
        "",
        "		/* At least part of region covered by map entry. */",
        "		type = type_merge(type, cache_map[i].type, uniform);",
        "",
        "		start = cache_map[i].end;",
        "	}",
        "",
        "	/* End of region past last entry in map? -> use default type. */",
        "	if (start < end)",
        "		type = type_merge(type, mtrr_state.def_type, uniform);",
        "",
        "	return type;",
        "}",
        "",
        "/* Get the MSR pair relating to a var range */",
        "static void",
        "get_mtrr_var_range(unsigned int index, struct mtrr_var_range *vr)",
        "{",
        "	rdmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);",
        "	rdmsr(MTRRphysMask_MSR(index), vr->mask_lo, vr->mask_hi);",
        "}",
        "",
        "/* Fill the MSR pair relating to a var range */",
        "void fill_mtrr_var_range(unsigned int index,",
        "		u32 base_lo, u32 base_hi, u32 mask_lo, u32 mask_hi)",
        "{",
        "	struct mtrr_var_range *vr;",
        "",
        "	vr = mtrr_state.var_ranges;",
        "",
        "	vr[index].base_lo = base_lo;",
        "	vr[index].base_hi = base_hi;",
        "	vr[index].mask_lo = mask_lo;",
        "	vr[index].mask_hi = mask_hi;",
        "}",
        "",
        "static void get_fixed_ranges(mtrr_type *frs)",
        "{",
        "	unsigned int *p = (unsigned int *)frs;",
        "	int i;",
        "",
        "	k8_check_syscfg_dram_mod_en();",
        "",
        "	rdmsr(MSR_MTRRfix64K_00000, p[0], p[1]);",
        "",
        "	for (i = 0; i < 2; i++)",
        "		rdmsr(MSR_MTRRfix16K_80000 + i, p[2 + i * 2], p[3 + i * 2]);",
        "	for (i = 0; i < 8; i++)",
        "		rdmsr(MSR_MTRRfix4K_C0000 + i, p[6 + i * 2], p[7 + i * 2]);",
        "}",
        "",
        "void mtrr_save_fixed_ranges(void *info)",
        "{",
        "	if (boot_cpu_has(X86_FEATURE_MTRR))",
        "		get_fixed_ranges(mtrr_state.fixed_ranges);",
        "}",
        "",
        "static unsigned __initdata last_fixed_start;",
        "static unsigned __initdata last_fixed_end;",
        "static mtrr_type __initdata last_fixed_type;",
        "",
        "static void __init print_fixed_last(void)",
        "{",
        "	if (!last_fixed_end)",
        "		return;",
        "",
        "	pr_info(\"  %05X-%05X %s\\n\", last_fixed_start,",
        "		last_fixed_end - 1, mtrr_attrib_to_str(last_fixed_type));",
        "",
        "	last_fixed_end = 0;",
        "}",
        "",
        "static void __init update_fixed_last(unsigned base, unsigned end,",
        "				     mtrr_type type)",
        "{",
        "	last_fixed_start = base;",
        "	last_fixed_end = end;",
        "	last_fixed_type = type;",
        "}",
        "",
        "static void __init",
        "print_fixed(unsigned base, unsigned step, const mtrr_type *types)",
        "{",
        "	unsigned i;",
        "",
        "	for (i = 0; i < 8; ++i, ++types, base += step) {",
        "		if (last_fixed_end == 0) {",
        "			update_fixed_last(base, base + step, *types);",
        "			continue;",
        "		}",
        "		if (last_fixed_end == base && last_fixed_type == *types) {",
        "			last_fixed_end = base + step;",
        "			continue;",
        "		}",
        "		/* new segments: gap or different type */",
        "		print_fixed_last();",
        "		update_fixed_last(base, base + step, *types);",
        "	}",
        "}",
        "",
        "static void __init print_mtrr_state(void)",
        "{",
        "	unsigned int i;",
        "	int high_width;",
        "",
        "	pr_info(\"MTRR default type: %s\\n\",",
        "		mtrr_attrib_to_str(mtrr_state.def_type));",
        "	if (mtrr_state.have_fixed) {",
        "		pr_info(\"MTRR fixed ranges %sabled:\\n\",",
        "			((mtrr_state.enabled & MTRR_STATE_MTRR_ENABLED) &&",
        "			 (mtrr_state.enabled & MTRR_STATE_MTRR_FIXED_ENABLED)) ?",
        "			 \"en\" : \"dis\");",
        "		print_fixed(0x00000, 0x10000, mtrr_state.fixed_ranges + 0);",
        "		for (i = 0; i < 2; ++i)",
        "			print_fixed(0x80000 + i * 0x20000, 0x04000,",
        "				    mtrr_state.fixed_ranges + (i + 1) * 8);",
        "		for (i = 0; i < 8; ++i)",
        "			print_fixed(0xC0000 + i * 0x08000, 0x01000,",
        "				    mtrr_state.fixed_ranges + (i + 3) * 8);",
        "",
        "		/* tail */",
        "		print_fixed_last();",
        "	}",
        "	pr_info(\"MTRR variable ranges %sabled:\\n\",",
        "		mtrr_state.enabled & MTRR_STATE_MTRR_ENABLED ? \"en\" : \"dis\");",
        "	high_width = (boot_cpu_data.x86_phys_bits - (32 - PAGE_SHIFT) + 3) / 4;",
        "",
        "	for (i = 0; i < num_var_ranges; ++i) {",
        "		if (mtrr_state.var_ranges[i].mask_lo & MTRR_PHYSMASK_V)",
        "			pr_info(\"  %u base %0*X%05X000 mask %0*X%05X000 %s\\n\",",
        "				i,",
        "				high_width,",
        "				mtrr_state.var_ranges[i].base_hi,",
        "				mtrr_state.var_ranges[i].base_lo >> 12,",
        "				high_width,",
        "				mtrr_state.var_ranges[i].mask_hi,",
        "				mtrr_state.var_ranges[i].mask_lo >> 12,",
        "				mtrr_attrib_to_str(mtrr_state.var_ranges[i].base_lo &",
        "						    MTRR_PHYSBASE_TYPE));",
        "		else",
        "			pr_info(\"  %u disabled\\n\", i);",
        "	}",
        "	if (mtrr_tom2)",
        "		pr_info(\"TOM2: %016llx aka %lldM\\n\", mtrr_tom2, mtrr_tom2>>20);",
        "}",
        "",
        "/* Grab all of the MTRR state for this CPU into *state */",
        "bool __init get_mtrr_state(void)",
        "{",
        "	struct mtrr_var_range *vrs;",
        "	unsigned lo, dummy;",
        "	unsigned int i;",
        "",
        "	vrs = mtrr_state.var_ranges;",
        "",
        "	rdmsr(MSR_MTRRcap, lo, dummy);",
        "	mtrr_state.have_fixed = lo & MTRR_CAP_FIX;",
        "",
        "	for (i = 0; i < num_var_ranges; i++)",
        "		get_mtrr_var_range(i, &vrs[i]);",
        "	if (mtrr_state.have_fixed)",
        "		get_fixed_ranges(mtrr_state.fixed_ranges);",
        "",
        "	rdmsr(MSR_MTRRdefType, lo, dummy);",
        "	mtrr_state.def_type = lo & MTRR_DEF_TYPE_TYPE;",
        "	mtrr_state.enabled = (lo & MTRR_DEF_TYPE_ENABLE) >> MTRR_STATE_SHIFT;",
        "",
        "	if (amd_special_default_mtrr()) {",
        "		unsigned low, high;",
        "",
        "		/* TOP_MEM2 */",
        "		rdmsr(MSR_K8_TOP_MEM2, low, high);",
        "		mtrr_tom2 = high;",
        "		mtrr_tom2 <<= 32;",
        "		mtrr_tom2 |= low;",
        "		mtrr_tom2 &= 0xffffff800000ULL;",
        "	}",
        "",
        "	if (mtrr_debug)",
        "		print_mtrr_state();",
        "",
        "	mtrr_state_set = 1;",
        "",
        "	return !!(mtrr_state.enabled & MTRR_STATE_MTRR_ENABLED);",
        "}",
        "",
        "/* Some BIOS's are messed up and don't set all MTRRs the same! */",
        "void __init mtrr_state_warn(void)",
        "{",
        "	unsigned long mask = smp_changes_mask;",
        "",
        "	if (!mask)",
        "		return;",
        "	if (mask & MTRR_CHANGE_MASK_FIXED)",
        "		pr_warn(\"mtrr: your CPUs had inconsistent fixed MTRR settings\\n\");",
        "	if (mask & MTRR_CHANGE_MASK_VARIABLE)",
        "		pr_warn(\"mtrr: your CPUs had inconsistent variable MTRR settings\\n\");",
        "	if (mask & MTRR_CHANGE_MASK_DEFTYPE)",
        "		pr_warn(\"mtrr: your CPUs had inconsistent MTRRdefType settings\\n\");",
        "",
        "	pr_info(\"mtrr: probably your BIOS does not setup all CPUs.\\n\");",
        "	pr_info(\"mtrr: corrected configuration.\\n\");",
        "}",
        "",
        "/*",
        " * Doesn't attempt to pass an error out to MTRR users",
        " * because it's quite complicated in some cases and probably not",
        " * worth it because the best error handling is to ignore it.",
        " */",
        "void mtrr_wrmsr(unsigned msr, unsigned a, unsigned b)",
        "{",
        "	if (wrmsr_safe(msr, a, b) < 0) {",
        "		pr_err(\"MTRR: CPU %u: Writing MSR %x to %x:%x failed\\n\",",
        "			smp_processor_id(), msr, a, b);",
        "	}",
        "}",
        "",
        "/**",
        " * set_fixed_range - checks & updates a fixed-range MTRR if it",
        " *		     differs from the value it should have",
        " * @msr: MSR address of the MTTR which should be checked and updated",
        " * @changed: pointer which indicates whether the MTRR needed to be changed",
        " * @msrwords: pointer to the MSR values which the MSR should have",
        " */",
        "static void set_fixed_range(int msr, bool *changed, unsigned int *msrwords)",
        "{",
        "	unsigned lo, hi;",
        "",
        "	rdmsr(msr, lo, hi);",
        "",
        "	if (lo != msrwords[0] || hi != msrwords[1]) {",
        "		mtrr_wrmsr(msr, msrwords[0], msrwords[1]);",
        "		*changed = true;",
        "	}",
        "}",
        "",
        "/**",
        " * generic_get_free_region - Get a free MTRR.",
        " * @base: The starting (base) address of the region.",
        " * @size: The size (in bytes) of the region.",
        " * @replace_reg: mtrr index to be replaced; set to invalid value if none.",
        " *",
        " * Returns: The index of the region on success, else negative on error.",
        " */",
        "int",
        "generic_get_free_region(unsigned long base, unsigned long size, int replace_reg)",
        "{",
        "	unsigned long lbase, lsize;",
        "	mtrr_type ltype;",
        "	int i, max;",
        "",
        "	max = num_var_ranges;",
        "	if (replace_reg >= 0 && replace_reg < max)",
        "		return replace_reg;",
        "",
        "	for (i = 0; i < max; ++i) {",
        "		mtrr_if->get(i, &lbase, &lsize, &ltype);",
        "		if (lsize == 0)",
        "			return i;",
        "	}",
        "",
        "	return -ENOSPC;",
        "}",
        "",
        "static void generic_get_mtrr(unsigned int reg, unsigned long *base,",
        "			     unsigned long *size, mtrr_type *type)",
        "{",
        "	u32 mask_lo, mask_hi, base_lo, base_hi;",
        "	unsigned int hi;",
        "	u64 tmp, mask;",
        "",
        "	/*",
        "	 * get_mtrr doesn't need to update mtrr_state, also it could be called",
        "	 * from any cpu, so try to print it out directly.",
        "	 */",
        "	get_cpu();",
        "",
        "	rdmsr(MTRRphysMask_MSR(reg), mask_lo, mask_hi);",
        "",
        "	if (!(mask_lo & MTRR_PHYSMASK_V)) {",
        "		/*  Invalid (i.e. free) range */",
        "		*base = 0;",
        "		*size = 0;",
        "		*type = 0;",
        "		goto out_put_cpu;",
        "	}",
        "",
        "	rdmsr(MTRRphysBase_MSR(reg), base_lo, base_hi);",
        "",
        "	/* Work out the shifted address mask: */",
        "	tmp = (u64)mask_hi << 32 | (mask_lo & PAGE_MASK);",
        "	mask = (u64)phys_hi_rsvd << 32 | tmp;",
        "",
        "	/* Expand tmp with high bits to all 1s: */",
        "	hi = fls64(tmp);",
        "	if (hi > 0) {",
        "		tmp |= ~((1ULL<<(hi - 1)) - 1);",
        "",
        "		if (tmp != mask) {",
        "			pr_warn(\"mtrr: your BIOS has configured an incorrect mask, fixing it.\\n\");",
        "			add_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);",
        "			mask = tmp;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * This works correctly if size is a power of two, i.e. a",
        "	 * contiguous range:",
        "	 */",
        "	*size = -mask >> PAGE_SHIFT;",
        "	*base = (u64)base_hi << (32 - PAGE_SHIFT) | base_lo >> PAGE_SHIFT;",
        "	*type = base_lo & MTRR_PHYSBASE_TYPE;",
        "",
        "out_put_cpu:",
        "	put_cpu();",
        "}",
        "",
        "/**",
        " * set_fixed_ranges - checks & updates the fixed-range MTRRs if they",
        " *		      differ from the saved set",
        " * @frs: pointer to fixed-range MTRR values, saved by get_fixed_ranges()",
        " */",
        "static int set_fixed_ranges(mtrr_type *frs)",
        "{",
        "	unsigned long long *saved = (unsigned long long *)frs;",
        "	bool changed = false;",
        "	int block = -1, range;",
        "",
        "	k8_check_syscfg_dram_mod_en();",
        "",
        "	while (fixed_range_blocks[++block].ranges) {",
        "		for (range = 0; range < fixed_range_blocks[block].ranges; range++)",
        "			set_fixed_range(fixed_range_blocks[block].base_msr + range,",
        "					&changed, (unsigned int *)saved++);",
        "	}",
        "",
        "	return changed;",
        "}",
        "",
        "/*",
        " * Set the MSR pair relating to a var range.",
        " * Returns true if changes are made.",
        " */",
        "static bool set_mtrr_var_ranges(unsigned int index, struct mtrr_var_range *vr)",
        "{",
        "	unsigned int lo, hi;",
        "	bool changed = false;",
        "",
        "	rdmsr(MTRRphysBase_MSR(index), lo, hi);",
        "	if ((vr->base_lo & ~MTRR_PHYSBASE_RSVD) != (lo & ~MTRR_PHYSBASE_RSVD)",
        "	    || (vr->base_hi & ~phys_hi_rsvd) != (hi & ~phys_hi_rsvd)) {",
        "",
        "		mtrr_wrmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);",
        "		changed = true;",
        "	}",
        "",
        "	rdmsr(MTRRphysMask_MSR(index), lo, hi);",
        "",
        "	if ((vr->mask_lo & ~MTRR_PHYSMASK_RSVD) != (lo & ~MTRR_PHYSMASK_RSVD)",
        "	    || (vr->mask_hi & ~phys_hi_rsvd) != (hi & ~phys_hi_rsvd)) {",
        "		mtrr_wrmsr(MTRRphysMask_MSR(index), vr->mask_lo, vr->mask_hi);",
        "		changed = true;",
        "	}",
        "	return changed;",
        "}",
        "",
        "static u32 deftype_lo, deftype_hi;",
        "",
        "/**",
        " * set_mtrr_state - Set the MTRR state for this CPU.",
        " *",
        " * NOTE: The CPU must already be in a safe state for MTRR changes, including",
        " *       measures that only a single CPU can be active in set_mtrr_state() in",
        " *       order to not be subject to races for usage of deftype_lo. This is",
        " *       accomplished by taking cache_disable_lock.",
        " * RETURNS: 0 if no changes made, else a mask indicating what was changed.",
        " */",
        "static unsigned long set_mtrr_state(void)",
        "{",
        "	unsigned long change_mask = 0;",
        "	unsigned int i;",
        "",
        "	for (i = 0; i < num_var_ranges; i++) {",
        "		if (set_mtrr_var_ranges(i, &mtrr_state.var_ranges[i]))",
        "			change_mask |= MTRR_CHANGE_MASK_VARIABLE;",
        "	}",
        "",
        "	if (mtrr_state.have_fixed && set_fixed_ranges(mtrr_state.fixed_ranges))",
        "		change_mask |= MTRR_CHANGE_MASK_FIXED;",
        "",
        "	/*",
        "	 * Set_mtrr_restore restores the old value of MTRRdefType,",
        "	 * so to set it we fiddle with the saved value:",
        "	 */",
        "	if ((deftype_lo & MTRR_DEF_TYPE_TYPE) != mtrr_state.def_type ||",
        "	    ((deftype_lo & MTRR_DEF_TYPE_ENABLE) >> MTRR_STATE_SHIFT) != mtrr_state.enabled) {",
        "",
        "		deftype_lo = (deftype_lo & MTRR_DEF_TYPE_DISABLE) |",
        "			     mtrr_state.def_type |",
        "			     (mtrr_state.enabled << MTRR_STATE_SHIFT);",
        "		change_mask |= MTRR_CHANGE_MASK_DEFTYPE;",
        "	}",
        "",
        "	return change_mask;",
        "}",
        "",
        "void mtrr_disable(void)",
        "{",
        "	/* Save MTRR state */",
        "	rdmsr(MSR_MTRRdefType, deftype_lo, deftype_hi);",
        "",
        "	/* Disable MTRRs, and set the default type to uncached */",
        "	mtrr_wrmsr(MSR_MTRRdefType, deftype_lo & MTRR_DEF_TYPE_DISABLE, deftype_hi);",
        "}",
        "",
        "void mtrr_enable(void)",
        "{",
        "	/* Intel (P6) standard MTRRs */",
        "	mtrr_wrmsr(MSR_MTRRdefType, deftype_lo, deftype_hi);",
        "}",
        "",
        "void mtrr_generic_set_state(void)",
        "{",
        "	unsigned long mask, count;",
        "",
        "	/* Actually set the state */",
        "	mask = set_mtrr_state();",
        "",
        "	/* Use the atomic bitops to update the global mask */",
        "	for (count = 0; count < sizeof(mask) * 8; ++count) {",
        "		if (mask & 0x01)",
        "			set_bit(count, &smp_changes_mask);",
        "		mask >>= 1;",
        "	}",
        "}",
        "",
        "/**",
        " * generic_set_mtrr - set variable MTRR register on the local CPU.",
        " *",
        " * @reg: The register to set.",
        " * @base: The base address of the region.",
        " * @size: The size of the region. If this is 0 the region is disabled.",
        " * @type: The type of the region.",
        " *",
        " * Returns nothing.",
        " */",
        "static void generic_set_mtrr(unsigned int reg, unsigned long base,",
        "			     unsigned long size, mtrr_type type)",
        "{",
        "	unsigned long flags;",
        "	struct mtrr_var_range *vr;",
        "",
        "	vr = &mtrr_state.var_ranges[reg];",
        "",
        "	local_irq_save(flags);",
        "	cache_disable();",
        "",
        "	if (size == 0) {",
        "		/*",
        "		 * The invalid bit is kept in the mask, so we simply",
        "		 * clear the relevant mask register to disable a range.",
        "		 */",
        "		mtrr_wrmsr(MTRRphysMask_MSR(reg), 0, 0);",
        "		memset(vr, 0, sizeof(struct mtrr_var_range));",
        "	} else {",
        "		vr->base_lo = base << PAGE_SHIFT | type;",
        "		vr->base_hi = (base >> (32 - PAGE_SHIFT)) & ~phys_hi_rsvd;",
        "		vr->mask_lo = -size << PAGE_SHIFT | MTRR_PHYSMASK_V;",
        "		vr->mask_hi = (-size >> (32 - PAGE_SHIFT)) & ~phys_hi_rsvd;",
        "",
        "		mtrr_wrmsr(MTRRphysBase_MSR(reg), vr->base_lo, vr->base_hi);",
        "		mtrr_wrmsr(MTRRphysMask_MSR(reg), vr->mask_lo, vr->mask_hi);",
        "	}",
        "",
        "	cache_enable();",
        "	local_irq_restore(flags);",
        "}",
        "",
        "int generic_validate_add_page(unsigned long base, unsigned long size,",
        "			      unsigned int type)",
        "{",
        "	unsigned long lbase, last;",
        "",
        "	/*",
        "	 * For Intel PPro stepping <= 7",
        "	 * must be 4 MiB aligned and not touch 0x70000000 -> 0x7003FFFF",
        "	 */",
        "	if (mtrr_if == &generic_mtrr_ops && boot_cpu_data.x86 == 6 &&",
        "	    boot_cpu_data.x86_model == 1 &&",
        "	    boot_cpu_data.x86_stepping <= 7) {",
        "		if (base & ((1 << (22 - PAGE_SHIFT)) - 1)) {",
        "			pr_warn(\"mtrr: base(0x%lx000) is not 4 MiB aligned\\n\", base);",
        "			return -EINVAL;",
        "		}",
        "		if (!(base + size < 0x70000 || base > 0x7003F) &&",
        "		    (type == MTRR_TYPE_WRCOMB",
        "		     || type == MTRR_TYPE_WRBACK)) {",
        "			pr_warn(\"mtrr: writable mtrr between 0x70000000 and 0x7003FFFF may hang the CPU.\\n\");",
        "			return -EINVAL;",
        "		}",
        "	}",
        "",
        "	/*",
        "	 * Check upper bits of base and last are equal and lower bits are 0",
        "	 * for base and 1 for last",
        "	 */",
        "	last = base + size - 1;",
        "	for (lbase = base; !(lbase & 1) && (last & 1);",
        "	     lbase = lbase >> 1, last = last >> 1)",
        "		;",
        "	if (lbase != last) {",
        "		pr_warn(\"mtrr: base(0x%lx000) is not aligned on a size(0x%lx000) boundary\\n\", base, size);",
        "		return -EINVAL;",
        "	}",
        "	return 0;",
        "}",
        "",
        "static int generic_have_wrcomb(void)",
        "{",
        "	unsigned long config, dummy;",
        "	rdmsr(MSR_MTRRcap, config, dummy);",
        "	return config & MTRR_CAP_WC;",
        "}",
        "",
        "int positive_have_wrcomb(void)",
        "{",
        "	return 1;",
        "}",
        "",
        "/*",
        " * Generic structure...",
        " */",
        "const struct mtrr_ops generic_mtrr_ops = {",
        "	.get			= generic_get_mtrr,",
        "	.get_free_region	= generic_get_free_region,",
        "	.set			= generic_set_mtrr,",
        "	.validate_add_page	= generic_validate_add_page,",
        "	.have_wrcomb		= generic_have_wrcomb,",
        "};"
    ]
  },
  "include_linux_rculist_bl_h": {
    path: "include/linux/rculist_bl.h",
    covered: [80, 81],
    totalLines: 101,
    coveredCount: 2,
    coveragePct: 2.0,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_RCULIST_BL_H",
        "#define _LINUX_RCULIST_BL_H",
        "",
        "/*",
        " * RCU-protected bl list version. See include/linux/list_bl.h.",
        " */",
        "#include <linux/list_bl.h>",
        "#include <linux/rcupdate.h>",
        "",
        "static inline void hlist_bl_set_first_rcu(struct hlist_bl_head *h,",
        "					struct hlist_bl_node *n)",
        "{",
        "	LIST_BL_BUG_ON((unsigned long)n & LIST_BL_LOCKMASK);",
        "	LIST_BL_BUG_ON(((unsigned long)h->first & LIST_BL_LOCKMASK) !=",
        "							LIST_BL_LOCKMASK);",
        "	rcu_assign_pointer(h->first,",
        "		(struct hlist_bl_node *)((unsigned long)n | LIST_BL_LOCKMASK));",
        "}",
        "",
        "static inline struct hlist_bl_node *hlist_bl_first_rcu(struct hlist_bl_head *h)",
        "{",
        "	return (struct hlist_bl_node *)",
        "		((unsigned long)rcu_dereference_check(h->first, hlist_bl_is_locked(h)) & ~LIST_BL_LOCKMASK);",
        "}",
        "",
        "/**",
        " * hlist_bl_del_rcu - deletes entry from hash list without re-initialization",
        " * @n: the element to delete from the hash list.",
        " *",
        " * Note: hlist_bl_unhashed() on entry does not return true after this,",
        " * the entry is in an undefined state. It is useful for RCU based",
        " * lockfree traversal.",
        " *",
        " * In particular, it means that we can not poison the forward",
        " * pointers that may still be used for walking the hash list.",
        " *",
        " * The caller must take whatever precautions are necessary",
        " * (such as holding appropriate locks) to avoid racing",
        " * with another list-mutation primitive, such as hlist_bl_add_head_rcu()",
        " * or hlist_bl_del_rcu(), running on this same list.",
        " * However, it is perfectly legal to run concurrently with",
        " * the _rcu list-traversal primitives, such as",
        " * hlist_bl_for_each_entry().",
        " */",
        "static inline void hlist_bl_del_rcu(struct hlist_bl_node *n)",
        "{",
        "	__hlist_bl_del(n);",
        "	n->pprev = LIST_POISON2;",
        "}",
        "",
        "/**",
        " * hlist_bl_add_head_rcu",
        " * @n: the element to add to the hash list.",
        " * @h: the list to add to.",
        " *",
        " * Description:",
        " * Adds the specified element to the specified hlist_bl,",
        " * while permitting racing traversals.",
        " *",
        " * The caller must take whatever precautions are necessary",
        " * (such as holding appropriate locks) to avoid racing",
        " * with another list-mutation primitive, such as hlist_bl_add_head_rcu()",
        " * or hlist_bl_del_rcu(), running on this same list.",
        " * However, it is perfectly legal to run concurrently with",
        " * the _rcu list-traversal primitives, such as",
        " * hlist_bl_for_each_entry_rcu(), used to prevent memory-consistency",
        " * problems on Alpha CPUs.  Regardless of the type of CPU, the",
        " * list-traversal primitive must be guarded by rcu_read_lock().",
        " */",
        "static inline void hlist_bl_add_head_rcu(struct hlist_bl_node *n,",
        "					struct hlist_bl_head *h)",
        "{",
        "	struct hlist_bl_node *first;",
        "",
        "	/* don't need hlist_bl_first_rcu because we're under lock */",
        "	first = hlist_bl_first(h);",
        "",
        "	n->next = first;",
        "	if (first)",
        "		first->pprev = &n->next;",
        "	n->pprev = &h->first;",
        "",
        "	/* need _rcu because we can have concurrent lock free readers */",
        "	hlist_bl_set_first_rcu(h, n);",
        "}",
        "/**",
        " * hlist_bl_for_each_entry_rcu - iterate over rcu list of given type",
        " * @tpos:	the type * to use as a loop cursor.",
        " * @pos:	the &struct hlist_bl_node to use as a loop cursor.",
        " * @head:	the head for your list.",
        " * @member:	the name of the hlist_bl_node within the struct.",
        " *",
        " */",
        "#define hlist_bl_for_each_entry_rcu(tpos, pos, head, member)		\\",
        "	for (pos = hlist_bl_first_rcu(head);				\\",
        "		pos &&							\\",
        "		({ tpos = hlist_bl_entry(pos, typeof(*tpos), member); 1; }); \\",
        "		pos = rcu_dereference_raw(pos->next))",
        "",
        "#endif"
    ]
  },
  "include_linux_uaccess_h": {
    path: "include/linux/uaccess.h",
    covered: [212, 192, 228, 208],
    totalLines: 592,
    coveredCount: 4,
    coveragePct: 0.7,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef __LINUX_UACCESS_H__",
        "#define __LINUX_UACCESS_H__",
        "",
        "#include <linux/fault-inject-usercopy.h>",
        "#include <linux/instrumented.h>",
        "#include <linux/minmax.h>",
        "#include <linux/nospec.h>",
        "#include <linux/sched.h>",
        "#include <linux/thread_info.h>",
        "",
        "#include <asm/uaccess.h>",
        "",
        "/*",
        " * Architectures that support memory tagging (assigning tags to memory regions,",
        " * embedding these tags into addresses that point to these memory regions, and",
        " * checking that the memory and the pointer tags match on memory accesses)",
        " * redefine this macro to strip tags from pointers.",
        " *",
        " * Passing down mm_struct allows to define untagging rules on per-process",
        " * basis.",
        " *",
        " * It's defined as noop for architectures that don't support memory tagging.",
        " */",
        "#ifndef untagged_addr",
        "#define untagged_addr(addr) (addr)",
        "#endif",
        "",
        "#ifndef untagged_addr_remote",
        "#define untagged_addr_remote(mm, addr)	({		\\",
        "	mmap_assert_locked(mm);				\\",
        "	untagged_addr(addr);				\\",
        "})",
        "#endif",
        "",
        "#ifdef masked_user_access_begin",
        " #define can_do_masked_user_access() 1",
        "#else",
        " #define can_do_masked_user_access() 0",
        " #define masked_user_access_begin(src) NULL",
        " #define mask_user_address(src) (src)",
        "#endif",
        "",
        "/*",
        " * Architectures should provide two primitives (raw_copy_{to,from}_user())",
        " * and get rid of their private instances of copy_{to,from}_user() and",
        " * __copy_{to,from}_user{,_inatomic}().",
        " *",
        " * raw_copy_{to,from}_user(to, from, size) should copy up to size bytes and",
        " * return the amount left to copy.  They should assume that access_ok() has",
        " * already been checked (and succeeded); they should *not* zero-pad anything.",
        " * No KASAN or object size checks either - those belong here.",
        " *",
        " * Both of these functions should attempt to copy size bytes starting at from",
        " * into the area starting at to.  They must not fetch or store anything",
        " * outside of those areas.  Return value must be between 0 (everything",
        " * copied successfully) and size (nothing copied).",
        " *",
        " * If raw_copy_{to,from}_user(to, from, size) returns N, size - N bytes starting",
        " * at to must become equal to the bytes fetched from the corresponding area",
        " * starting at from.  All data past to + size - N must be left unmodified.",
        " *",
        " * If copying succeeds, the return value must be 0.  If some data cannot be",
        " * fetched, it is permitted to copy less than had been fetched; the only",
        " * hard requirement is that not storing anything at all (i.e. returning size)",
        " * should happen only when nothing could be copied.  In other words, you don't",
        " * have to squeeze as much as possible - it is allowed, but not necessary.",
        " *",
        " * For raw_copy_from_user() to always points to kernel memory and no faults",
        " * on store should happen.  Interpretation of from is affected by set_fs().",
        " * For raw_copy_to_user() it's the other way round.",
        " *",
        " * Both can be inlined - it's up to architectures whether it wants to bother",
        " * with that.  They should not be used directly; they are used to implement",
        " * the 6 functions (copy_{to,from}_user(), __copy_{to,from}_user_inatomic())",
        " * that are used instead.  Out of those, __... ones are inlined.  Plain",
        " * copy_{to,from}_user() might or might not be inlined.  If you want them",
        " * inlined, have asm/uaccess.h define INLINE_COPY_{TO,FROM}_USER.",
        " *",
        " * NOTE: only copy_from_user() zero-pads the destination in case of short copy.",
        " * Neither __copy_from_user() nor __copy_from_user_inatomic() zero anything",
        " * at all; their callers absolutely must check the return value.",
        " *",
        " * Biarch ones should also provide raw_copy_in_user() - similar to the above,",
        " * but both source and destination are __user pointers (affected by set_fs()",
        " * as usual) and both source and destination can trigger faults.",
        " */",
        "",
        "static __always_inline __must_check unsigned long",
        "__copy_from_user_inatomic(void *to, const void __user *from, unsigned long n)",
        "{",
        "	unsigned long res;",
        "",
        "	instrument_copy_from_user_before(to, from, n);",
        "	check_object_size(to, n, false);",
        "	res = raw_copy_from_user(to, from, n);",
        "	instrument_copy_from_user_after(to, from, n, res);",
        "	return res;",
        "}",
        "",
        "static __always_inline __must_check unsigned long",
        "__copy_from_user(void *to, const void __user *from, unsigned long n)",
        "{",
        "	unsigned long res;",
        "",
        "	might_fault();",
        "	instrument_copy_from_user_before(to, from, n);",
        "	if (should_fail_usercopy())",
        "		return n;",
        "	if(fuzz_copy_from_user_cb((size_t)from, n, 1)){",
        "		return 0;",
        "	}",
        "	check_object_size(to, n, false);",
        "	res = raw_copy_from_user(to, from, n);",
        "	instrument_copy_from_user_after(to, from, n, res);",
        "	return res;",
        "}",
        "",
        "/**",
        " * __copy_to_user_inatomic: - Copy a block of data into user space, with less checking.",
        " * @to:   Destination address, in user space.",
        " * @from: Source address, in kernel space.",
        " * @n:    Number of bytes to copy.",
        " *",
        " * Context: User context only.",
        " *",
        " * Copy data from kernel space to user space.  Caller must check",
        " * the specified block with access_ok() before calling this function.",
        " * The caller should also make sure he pins the user space address",
        " * so that we don't result in page fault and sleep.",
        " */",
        "static __always_inline __must_check unsigned long",
        "__copy_to_user_inatomic(void __user *to, const void *from, unsigned long n)",
        "{",
        "	if (should_fail_usercopy())",
        "		return n;",
        "	if(fuzz_copy_from_user_cb((size_t)to, n, 1)){",
        "		return 0;",
        "	}//# ä¿®æ”¹__copy_to_userå’Œ_copy_from_user",
        "	instrument_copy_to_user(to, from, n);",
        "	check_object_size(from, n, true);",
        "	return raw_copy_to_user(to, from, n);",
        "}",
        "",
        "static __always_inline __must_check unsigned long",
        "__copy_to_user(void __user *to, const void *from, unsigned long n)",
        "{",
        "	might_fault();",
        "	if (should_fail_usercopy())",
        "		return n;",
        "	if(fuzz_copy_from_user_cb((size_t)to, n, 1)){",
        "		return 0;",
        "	}",
        "	instrument_copy_to_user(to, from, n);",
        "	check_object_size(from, n, true);",
        "	return raw_copy_to_user(to, from, n);",
        "}",
        "",
        "/*",
        " * Architectures that #define INLINE_COPY_TO_USER use this function",
        " * directly in the normal copy_to/from_user(), the other ones go",
        " * through an extern _copy_to/from_user(), which expands the same code",
        " * here.",
        " *",
        " * Rust code always uses the extern definition.",
        " */",
        "static inline __must_check unsigned long",
        "_inline_copy_from_user(void *to, const void __user *from, unsigned long n)",
        "{",
        "	unsigned long res = n;",
        "	might_fault();",
        "	if (should_fail_usercopy())",
        "		goto fail;",
        "	if(fuzz_copy_from_user_cb((size_t)to, n, 1)){",
        "		return 0;",
        "	}",
        "	if (can_do_masked_user_access())",
        "		from = mask_user_address(from);",
        "	else {",
        "		if (!access_ok(from, n))",
        "			goto fail;",
        "		/*",
        "		 * Ensure that bad access_ok() speculation will not",
        "		 * lead to nasty side effects *after* the copy is",
        "		 * finished:",
        "		 */",
        "		barrier_nospec();",
        "	}",
        "	instrument_copy_from_user_before(to, from, n);",
        "	res = raw_copy_from_user(to, from, n);",
        "	instrument_copy_from_user_after(to, from, n, res);",
        "	if (likely(!res))",
        "		return 0;",
        "fail:",
        "	memset(to + (n - res), 0, res);",
        "	return res;",
        "}",
        "extern __must_check unsigned long",
        "_copy_from_user(void *, const void __user *, unsigned long);",
        "",
        "static inline __must_check unsigned long",
        "_inline_copy_to_user(void __user *to, const void *from, unsigned long n)",
        "{",
        "	might_fault();",
        "	if (should_fail_usercopy())",
        "		return n;",
        "",
        "	if (access_ok(to, n)) {",
        "		if(fuzz_copy_from_user_cb((size_t)to, n, 1)){",
        "			return 0;",
        "		}",
        "		instrument_copy_to_user(to, from, n);",
        "		n = raw_copy_to_user(to, from, n);",
        "	}",
        "	return n;",
        "}",
        "extern __must_check unsigned long",
        "_copy_to_user(void __user *, const void *, unsigned long);",
        "",
        "static __always_inline unsigned long __must_check",
        "copy_from_user(void *to, const void __user *from, unsigned long n)",
        "{",
        "	if (!check_copy_size(to, n, false))",
        "		return n;",
        "#ifdef INLINE_COPY_FROM_USER",
        "	return _inline_copy_from_user(to, from, n);",
        "#else",
        "	return _copy_from_user(to, from, n);",
        "#endif",
        "}",
        "",
        "static __always_inline unsigned long __must_check",
        "copy_to_user(void __user *to, const void *from, unsigned long n)",
        "{",
        "	if (!check_copy_size(from, n, true))",
        "		return n;",
        "",
        "#ifdef INLINE_COPY_TO_USER",
        "	return _inline_copy_to_user(to, from, n);",
        "#else",
        "	return _copy_to_user(to, from, n);",
        "#endif",
        "}",
        "",
        "#ifndef copy_mc_to_kernel",
        "/*",
        " * Without arch opt-in this generic copy_mc_to_kernel() will not handle",
        " * #MC (or arch equivalent) during source read.",
        " */",
        "static inline unsigned long __must_check",
        "copy_mc_to_kernel(void *dst, const void *src, size_t cnt)",
        "{",
        "	memcpy(dst, src, cnt);",
        "	return 0;",
        "}",
        "#endif",
        "",
        "static __always_inline void pagefault_disabled_inc(void)",
        "{",
        "	current->pagefault_disabled++;",
        "}",
        "",
        "static __always_inline void pagefault_disabled_dec(void)",
        "{",
        "	current->pagefault_disabled--;",
        "}",
        "",
        "/*",
        " * These routines enable/disable the pagefault handler. If disabled, it will",
        " * not take any locks and go straight to the fixup table.",
        " *",
        " * User access methods will not sleep when called from a pagefault_disabled()",
        " * environment.",
        " */",
        "static inline void pagefault_disable(void)",
        "{",
        "	pagefault_disabled_inc();",
        "	/*",
        "	 * make sure to have issued the store before a pagefault",
        "	 * can hit.",
        "	 */",
        "	barrier();",
        "}",
        "",
        "static inline void pagefault_enable(void)",
        "{",
        "	/*",
        "	 * make sure to issue those last loads/stores before enabling",
        "	 * the pagefault handler again.",
        "	 */",
        "	barrier();",
        "	pagefault_disabled_dec();",
        "}",
        "",
        "/*",
        " * Is the pagefault handler disabled? If so, user access methods will not sleep.",
        " */",
        "static inline bool pagefault_disabled(void)",
        "{",
        "	return current->pagefault_disabled != 0;",
        "}",
        "",
        "/*",
        " * The pagefault handler is in general disabled by pagefault_disable() or",
        " * when in irq context (via in_atomic()).",
        " *",
        " * This function should only be used by the fault handlers. Other users should",
        " * stick to pagefault_disabled().",
        " * Please NEVER use preempt_disable() to disable the fault handler. With",
        " * !CONFIG_PREEMPT_COUNT, this is like a NOP. So the handler won't be disabled.",
        " * in_atomic() will report different values based on !CONFIG_PREEMPT_COUNT.",
        " */",
        "#define faulthandler_disabled() (pagefault_disabled() || in_atomic())",
        "",
        "#ifndef CONFIG_ARCH_HAS_SUBPAGE_FAULTS",
        "",
        "/**",
        " * probe_subpage_writeable: probe the user range for write faults at sub-page",
        " *			    granularity (e.g. arm64 MTE)",
        " * @uaddr: start of address range",
        " * @size: size of address range",
        " *",
        " * Returns 0 on success, the number of bytes not probed on fault.",
        " *",
        " * It is expected that the caller checked for the write permission of each",
        " * page in the range either by put_user() or GUP. The architecture port can",
        " * implement a more efficient get_user() probing if the same sub-page faults",
        " * are triggered by either a read or a write.",
        " */",
        "static inline size_t probe_subpage_writeable(char __user *uaddr, size_t size)",
        "{",
        "	return 0;",
        "}",
        "",
        "#endif /* CONFIG_ARCH_HAS_SUBPAGE_FAULTS */",
        "",
        "#ifndef ARCH_HAS_NOCACHE_UACCESS",
        "",
        "static inline __must_check unsigned long",
        "__copy_from_user_inatomic_nocache(void *to, const void __user *from,",
        "				  unsigned long n)",
        "{",
        "	return __copy_from_user_inatomic(to, from, n);",
        "}",
        "",
        "#endif		/* ARCH_HAS_NOCACHE_UACCESS */",
        "",
        "extern __must_check int check_zeroed_user(const void __user *from, size_t size);",
        "",
        "/**",
        " * copy_struct_from_user: copy a struct from userspace",
        " * @dst:   Destination address, in kernel space. This buffer must be @ksize",
        " *         bytes long.",
        " * @ksize: Size of @dst struct.",
        " * @src:   Source address, in userspace.",
        " * @usize: (Alleged) size of @src struct.",
        " *",
        " * Copies a struct from userspace to kernel space, in a way that guarantees",
        " * backwards-compatibility for struct syscall arguments (as long as future",
        " * struct extensions are made such that all new fields are *appended* to the",
        " * old struct, and zeroed-out new fields have the same meaning as the old",
        " * struct).",
        " *",
        " * @ksize is just sizeof(*dst), and @usize should've been passed by userspace.",
        " * The recommended usage is something like the following:",
        " *",
        " *   SYSCALL_DEFINE2(foobar, const struct foo __user *, uarg, size_t, usize)",
        " *   {",
        " *      int err;",
        " *      struct foo karg = {};",
        " *",
        " *      if (usize > PAGE_SIZE)",
        " *        return -E2BIG;",
        " *      if (usize < FOO_SIZE_VER0)",
        " *        return -EINVAL;",
        " *",
        " *      err = copy_struct_from_user(&karg, sizeof(karg), uarg, usize);",
        " *      if (err)",
        " *        return err;",
        " *",
        " *      // ...",
        " *   }",
        " *",
        " * There are three cases to consider:",
        " *  * If @usize == @ksize, then it's copied verbatim.",
        " *  * If @usize < @ksize, then the userspace has passed an old struct to a",
        " *    newer kernel. The rest of the trailing bytes in @dst (@ksize - @usize)",
        " *    are to be zero-filled.",
        " *  * If @usize > @ksize, then the userspace has passed a new struct to an",
        " *    older kernel. The trailing bytes unknown to the kernel (@usize - @ksize)",
        " *    are checked to ensure they are zeroed, otherwise -E2BIG is returned.",
        " *",
        " * Returns (in all cases, some data may have been copied):",
        " *  * -E2BIG:  (@usize > @ksize) and there are non-zero trailing bytes in @src.",
        " *  * -EFAULT: access to userspace failed.",
        " */",
        "static __always_inline __must_check int",
        "copy_struct_from_user(void *dst, size_t ksize, const void __user *src,",
        "		      size_t usize)",
        "{",
        "	size_t size = min(ksize, usize);",
        "	size_t rest = max(ksize, usize) - size;",
        "",
        "	/* Double check if ksize is larger than a known object size. */",
        "	if (WARN_ON_ONCE(ksize > __builtin_object_size(dst, 1)))",
        "		return -E2BIG;",
        "",
        "	/* Deal with trailing bytes. */",
        "	if (usize < ksize) {",
        "		memset(dst + size, 0, rest);",
        "	} else if (usize > ksize) {",
        "		int ret = check_zeroed_user(src + size, rest);",
        "		if (ret <= 0)",
        "			return ret ?: -E2BIG;",
        "	}",
        "	/* Copy the interoperable parts of the struct. */",
        "	if (copy_from_user(dst, src, size))",
        "		return -EFAULT;",
        "	return 0;",
        "}",
        "",
        "/**",
        " * copy_struct_to_user: copy a struct to userspace",
        " * @dst:   Destination address, in userspace. This buffer must be @ksize",
        " *         bytes long.",
        " * @usize: (Alleged) size of @dst struct.",
        " * @src:   Source address, in kernel space.",
        " * @ksize: Size of @src struct.",
        " * @ignored_trailing: Set to %true if there was a non-zero byte in @src that",
        " * userspace cannot see because they are using an smaller struct.",
        " *",
        " * Copies a struct from kernel space to userspace, in a way that guarantees",
        " * backwards-compatibility for struct syscall arguments (as long as future",
        " * struct extensions are made such that all new fields are *appended* to the",
        " * old struct, and zeroed-out new fields have the same meaning as the old",
        " * struct).",
        " *",
        " * Some syscalls may wish to make sure that userspace knows about everything in",
        " * the struct, and if there is a non-zero value that userspce doesn't know",
        " * about, they want to return an error (such as -EMSGSIZE) or have some other",
        " * fallback (such as adding a \"you're missing some information\" flag). If",
        " * @ignored_trailing is non-%NULL, it will be set to %true if there was a",
        " * non-zero byte that could not be copied to userspace (ie. was past @usize).",
        " *",
        " * While unconditionally returning an error in this case is the simplest",
        " * solution, for maximum backward compatibility you should try to only return",
        " * -EMSGSIZE if the user explicitly requested the data that couldn't be copied.",
        " * Note that structure sizes can change due to header changes and simple",
        " * recompilations without code changes(!), so if you care about",
        " * @ignored_trailing you probably want to make sure that any new field data is",
        " * associated with a flag. Otherwise you might assume that a program knows",
        " * about data it does not.",
        " *",
        " * @ksize is just sizeof(*src), and @usize should've been passed by userspace.",
        " * The recommended usage is something like the following:",
        " *",
        " *   SYSCALL_DEFINE2(foobar, struct foo __user *, uarg, size_t, usize)",
        " *   {",
        " *      int err;",
        " *      bool ignored_trailing;",
        " *      struct foo karg = {};",
        " *",
        " *      if (usize > PAGE_SIZE)",
        " *		return -E2BIG;",
        " *      if (usize < FOO_SIZE_VER0)",
        " *		return -EINVAL;",
        " *",
        " *      // ... modify karg somehow ...",
        " *",
        " *      err = copy_struct_to_user(uarg, usize, &karg, sizeof(karg),",
        " *				  &ignored_trailing);",
        " *      if (err)",
        " *		return err;",
        " *      if (ignored_trailing)",
        " *		return -EMSGSIZE:",
        " *",
        " *      // ...",
        " *   }",
        " *",
        " * There are three cases to consider:",
        " *  * If @usize == @ksize, then it's copied verbatim.",
        " *  * If @usize < @ksize, then the kernel is trying to pass userspace a newer",
        " *    struct than it supports. Thus we only copy the interoperable portions",
        " *    (@usize) and ignore the rest (but @ignored_trailing is set to %true if",
        " *    any of the trailing (@ksize - @usize) bytes are non-zero).",
        " *  * If @usize > @ksize, then the kernel is trying to pass userspace an older",
        " *    struct than userspace supports. In order to make sure the",
        " *    unknown-to-the-kernel fields don't contain garbage values, we zero the",
        " *    trailing (@usize - @ksize) bytes.",
        " *",
        " * Returns (in all cases, some data may have been copied):",
        " *  * -EFAULT: access to userspace failed.",
        " */",
        "static __always_inline __must_check int",
        "copy_struct_to_user(void __user *dst, size_t usize, const void *src,",
        "		    size_t ksize, bool *ignored_trailing)",
        "{",
        "	size_t size = min(ksize, usize);",
        "	size_t rest = max(ksize, usize) - size;",
        "",
        "	/* Double check if ksize is larger than a known object size. */",
        "	if (WARN_ON_ONCE(ksize > __builtin_object_size(src, 1)))",
        "		return -E2BIG;",
        "",
        "	/* Deal with trailing bytes. */",
        "	if (usize > ksize) {",
        "		if (clear_user(dst + size, rest))",
        "			return -EFAULT;",
        "	}",
        "	if (ignored_trailing)",
        "		*ignored_trailing = ksize < usize &&",
        "			memchr_inv(src + size, 0, rest) != NULL;",
        "	/* Copy the interoperable parts of the struct. */",
        "	if (copy_to_user(dst, src, size))",
        "		return -EFAULT;",
        "	return 0;",
        "}",
        "",
        "bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size);",
        "",
        "long copy_from_kernel_nofault(void *dst, const void *src, size_t size);",
        "long notrace copy_to_kernel_nofault(void *dst, const void *src, size_t size);",
        "",
        "long copy_from_user_nofault(void *dst, const void __user *src, size_t size);",
        "long notrace copy_to_user_nofault(void __user *dst, const void *src,",
        "		size_t size);",
        "",
        "long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr,",
        "		long count);",
        "",
        "long strncpy_from_user_nofault(char *dst, const void __user *unsafe_addr,",
        "		long count);",
        "long strnlen_user_nofault(const void __user *unsafe_addr, long count);",
        "",
        "#ifndef __get_kernel_nofault",
        "#define __get_kernel_nofault(dst, src, type, label)	\\",
        "do {							\\",
        "	type __user *p = (type __force __user *)(src);	\\",
        "	type data;					\\",
        "	if (__get_user(data, p))			\\",
        "		goto label;				\\",
        "	*(type *)dst = data;				\\",
        "} while (0)",
        "",
        "#define __put_kernel_nofault(dst, src, type, label)	\\",
        "do {							\\",
        "	type __user *p = (type __force __user *)(dst);	\\",
        "	type data = *(type *)src;			\\",
        "	if (__put_user(data, p))			\\",
        "		goto label;				\\",
        "} while (0)",
        "#endif",
        "",
        "/**",
        " * get_kernel_nofault(): safely attempt to read from a location",
        " * @val: read into this variable",
        " * @ptr: address to read from",
        " *",
        " * Returns 0 on success, or -EFAULT.",
        " */",
        "#define get_kernel_nofault(val, ptr) ({				\\",
        "	const typeof(val) *__gk_ptr = (ptr);			\\",
        "	copy_from_kernel_nofault(&(val), __gk_ptr, sizeof(val));\\",
        "})",
        "",
        "#ifndef user_access_begin",
        "#define user_access_begin(ptr,len) access_ok(ptr, len)",
        "#define user_access_end() do { } while (0)",
        "#define unsafe_op_wrap(op, err) do { if (unlikely(op)) goto err; } while (0)",
        "#define unsafe_get_user(x,p,e) unsafe_op_wrap(__get_user(x,p),e)",
        "#define unsafe_put_user(x,p,e) unsafe_op_wrap(__put_user(x,p),e)",
        "#define unsafe_copy_to_user(d,s,l,e) unsafe_op_wrap(__copy_to_user(d,s,l),e)",
        "#define unsafe_copy_from_user(d,s,l,e) unsafe_op_wrap(__copy_from_user(d,s,l),e)",
        "static inline unsigned long user_access_save(void) { return 0UL; }",
        "static inline void user_access_restore(unsigned long flags) { }",
        "#endif",
        "#ifndef user_write_access_begin",
        "#define user_write_access_begin user_access_begin",
        "#define user_write_access_end user_access_end",
        "#endif",
        "#ifndef user_read_access_begin",
        "#define user_read_access_begin user_access_begin",
        "#define user_read_access_end user_access_end",
        "#endif",
        "",
        "#ifdef CONFIG_HARDENED_USERCOPY",
        "void __noreturn usercopy_abort(const char *name, const char *detail,",
        "			       bool to_user, unsigned long offset,",
        "			       unsigned long len);",
        "#endif",
        "",
        "#endif		/* __LINUX_UACCESS_H__ */"
    ]
  },
  "net_core_net_namespace_c": {
    path: "net/core/net_namespace.c",
    covered: [723, 720, 726, 733],
    totalLines: 1509,
    coveredCount: 4,
    coveragePct: 0.3,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-only",
        "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
        "",
        "#include <linux/workqueue.h>",
        "#include <linux/rtnetlink.h>",
        "#include <linux/cache.h>",
        "#include <linux/slab.h>",
        "#include <linux/list.h>",
        "#include <linux/delay.h>",
        "#include <linux/sched.h>",
        "#include <linux/idr.h>",
        "#include <linux/rculist.h>",
        "#include <linux/nsproxy.h>",
        "#include <linux/fs.h>",
        "#include <linux/proc_ns.h>",
        "#include <linux/file.h>",
        "#include <linux/export.h>",
        "#include <linux/user_namespace.h>",
        "#include <linux/net_namespace.h>",
        "#include <linux/sched/task.h>",
        "#include <linux/uidgid.h>",
        "#include <linux/cookie.h>",
        "#include <linux/proc_fs.h>",
        "",
        "#include <net/sock.h>",
        "#include <net/netlink.h>",
        "#include <net/net_namespace.h>",
        "#include <net/netns/generic.h>",
        "",
        "/*",
        " *	Our network namespace constructor/destructor lists",
        " */",
        "",
        "static LIST_HEAD(pernet_list);",
        "static struct list_head *first_device = &pernet_list;",
        "",
        "LIST_HEAD(net_namespace_list);",
        "EXPORT_SYMBOL_GPL(net_namespace_list);",
        "",
        "/* Protects net_namespace_list. Nests iside rtnl_lock() */",
        "DECLARE_RWSEM(net_rwsem);",
        "EXPORT_SYMBOL_GPL(net_rwsem);",
        "",
        "#ifdef CONFIG_KEYS",
        "static struct key_tag init_net_key_domain = { .usage = REFCOUNT_INIT(1) };",
        "#endif",
        "",
        "struct net init_net;",
        "EXPORT_SYMBOL(init_net);",
        "",
        "static bool init_net_initialized;",
        "/*",
        " * pernet_ops_rwsem: protects: pernet_list, net_generic_ids,",
        " * init_net_initialized and first_device pointer.",
        " * This is internal net namespace object. Please, don't use it",
        " * outside.",
        " */",
        "DECLARE_RWSEM(pernet_ops_rwsem);",
        "",
        "#define MIN_PERNET_OPS_ID	\\",
        "	((sizeof(struct net_generic) + sizeof(void *) - 1) / sizeof(void *))",
        "",
        "#define INITIAL_NET_GEN_PTRS	13 /* +1 for len +2 for rcu_head */",
        "",
        "static unsigned int max_gen_ptrs = INITIAL_NET_GEN_PTRS;",
        "",
        "DEFINE_COOKIE(net_cookie);",
        "",
        "static struct net_generic *net_alloc_generic(void)",
        "{",
        "	unsigned int gen_ptrs = READ_ONCE(max_gen_ptrs);",
        "	unsigned int generic_size;",
        "	struct net_generic *ng;",
        "",
        "	generic_size = offsetof(struct net_generic, ptr[gen_ptrs]);",
        "",
        "	ng = kzalloc(generic_size, GFP_KERNEL);",
        "	if (ng)",
        "		ng->s.len = gen_ptrs;",
        "",
        "	return ng;",
        "}",
        "",
        "static int net_assign_generic(struct net *net, unsigned int id, void *data)",
        "{",
        "	struct net_generic *ng, *old_ng;",
        "",
        "	BUG_ON(id < MIN_PERNET_OPS_ID);",
        "",
        "	old_ng = rcu_dereference_protected(net->gen,",
        "					   lockdep_is_held(&pernet_ops_rwsem));",
        "	if (old_ng->s.len > id) {",
        "		old_ng->ptr[id] = data;",
        "		return 0;",
        "	}",
        "",
        "	ng = net_alloc_generic();",
        "	if (!ng)",
        "		return -ENOMEM;",
        "",
        "	/*",
        "	 * Some synchronisation notes:",
        "	 *",
        "	 * The net_generic explores the net->gen array inside rcu",
        "	 * read section. Besides once set the net->gen->ptr[x]",
        "	 * pointer never changes (see rules in netns/generic.h).",
        "	 *",
        "	 * That said, we simply duplicate this array and schedule",
        "	 * the old copy for kfree after a grace period.",
        "	 */",
        "",
        "	memcpy(&ng->ptr[MIN_PERNET_OPS_ID], &old_ng->ptr[MIN_PERNET_OPS_ID],",
        "	       (old_ng->s.len - MIN_PERNET_OPS_ID) * sizeof(void *));",
        "	ng->ptr[id] = data;",
        "",
        "	rcu_assign_pointer(net->gen, ng);",
        "	kfree_rcu(old_ng, s.rcu);",
        "	return 0;",
        "}",
        "",
        "static int ops_init(const struct pernet_operations *ops, struct net *net)",
        "{",
        "	struct net_generic *ng;",
        "	int err = -ENOMEM;",
        "	void *data = NULL;",
        "",
        "	if (ops->id) {",
        "		data = kzalloc(ops->size, GFP_KERNEL);",
        "		if (!data)",
        "			goto out;",
        "",
        "		err = net_assign_generic(net, *ops->id, data);",
        "		if (err)",
        "			goto cleanup;",
        "	}",
        "	err = 0;",
        "	if (ops->init)",
        "		err = ops->init(net);",
        "	if (!err)",
        "		return 0;",
        "",
        "	if (ops->id) {",
        "		ng = rcu_dereference_protected(net->gen,",
        "					       lockdep_is_held(&pernet_ops_rwsem));",
        "		ng->ptr[*ops->id] = NULL;",
        "	}",
        "",
        "cleanup:",
        "	kfree(data);",
        "",
        "out:",
        "	return err;",
        "}",
        "",
        "static void ops_pre_exit_list(const struct pernet_operations *ops,",
        "			      struct list_head *net_exit_list)",
        "{",
        "	struct net *net;",
        "",
        "	if (ops->pre_exit) {",
        "		list_for_each_entry(net, net_exit_list, exit_list)",
        "			ops->pre_exit(net);",
        "	}",
        "}",
        "",
        "static void ops_exit_list(const struct pernet_operations *ops,",
        "			  struct list_head *net_exit_list)",
        "{",
        "	struct net *net;",
        "	if (ops->exit) {",
        "		list_for_each_entry(net, net_exit_list, exit_list) {",
        "			ops->exit(net);",
        "			cond_resched();",
        "		}",
        "	}",
        "	if (ops->exit_batch)",
        "		ops->exit_batch(net_exit_list);",
        "}",
        "",
        "static void ops_free_list(const struct pernet_operations *ops,",
        "			  struct list_head *net_exit_list)",
        "{",
        "	struct net *net;",
        "",
        "	if (ops->id) {",
        "		list_for_each_entry(net, net_exit_list, exit_list)",
        "			kfree(net_generic(net, *ops->id));",
        "	}",
        "}",
        "",
        "/* should be called with nsid_lock held */",
        "static int alloc_netid(struct net *net, struct net *peer, int reqid)",
        "{",
        "	int min = 0, max = 0;",
        "",
        "	if (reqid >= 0) {",
        "		min = reqid;",
        "		max = reqid + 1;",
        "	}",
        "",
        "	return idr_alloc(&net->netns_ids, peer, min, max, GFP_ATOMIC);",
        "}",
        "",
        "/* This function is used by idr_for_each(). If net is equal to peer, the",
        " * function returns the id so that idr_for_each() stops. Because we cannot",
        " * returns the id 0 (idr_for_each() will not stop), we return the magic value",
        " * NET_ID_ZERO (-1) for it.",
        " */",
        "#define NET_ID_ZERO -1",
        "static int net_eq_idr(int id, void *net, void *peer)",
        "{",
        "	if (net_eq(net, peer))",
        "		return id ? : NET_ID_ZERO;",
        "	return 0;",
        "}",
        "",
        "/* Must be called from RCU-critical section or with nsid_lock held */",
        "static int __peernet2id(const struct net *net, struct net *peer)",
        "{",
        "	int id = idr_for_each(&net->netns_ids, net_eq_idr, peer);",
        "",
        "	/* Magic value for id 0. */",
        "	if (id == NET_ID_ZERO)",
        "		return 0;",
        "	if (id > 0)",
        "		return id;",
        "",
        "	return NETNSA_NSID_NOT_ASSIGNED;",
        "}",
        "",
        "static void rtnl_net_notifyid(struct net *net, int cmd, int id, u32 portid,",
        "			      struct nlmsghdr *nlh, gfp_t gfp);",
        "/* This function returns the id of a peer netns. If no id is assigned, one will",
        " * be allocated and returned.",
        " */",
        "int peernet2id_alloc(struct net *net, struct net *peer, gfp_t gfp)",
        "{",
        "	int id;",
        "",
        "	if (refcount_read(&net->ns.count) == 0)",
        "		return NETNSA_NSID_NOT_ASSIGNED;",
        "",
        "	spin_lock_bh(&net->nsid_lock);",
        "	id = __peernet2id(net, peer);",
        "	if (id >= 0) {",
        "		spin_unlock_bh(&net->nsid_lock);",
        "		return id;",
        "	}",
        "",
        "	/* When peer is obtained from RCU lists, we may race with",
        "	 * its cleanup. Check whether it's alive, and this guarantees",
        "	 * we never hash a peer back to net->netns_ids, after it has",
        "	 * just been idr_remove()'d from there in cleanup_net().",
        "	 */",
        "	if (!maybe_get_net(peer)) {",
        "		spin_unlock_bh(&net->nsid_lock);",
        "		return NETNSA_NSID_NOT_ASSIGNED;",
        "	}",
        "",
        "	id = alloc_netid(net, peer, -1);",
        "	spin_unlock_bh(&net->nsid_lock);",
        "",
        "	put_net(peer);",
        "	if (id < 0)",
        "		return NETNSA_NSID_NOT_ASSIGNED;",
        "",
        "	rtnl_net_notifyid(net, RTM_NEWNSID, id, 0, NULL, gfp);",
        "",
        "	return id;",
        "}",
        "EXPORT_SYMBOL_GPL(peernet2id_alloc);",
        "",
        "/* This function returns, if assigned, the id of a peer netns. */",
        "int peernet2id(const struct net *net, struct net *peer)",
        "{",
        "	int id;",
        "",
        "	rcu_read_lock();",
        "	id = __peernet2id(net, peer);",
        "	rcu_read_unlock();",
        "",
        "	return id;",
        "}",
        "EXPORT_SYMBOL(peernet2id);",
        "",
        "/* This function returns true is the peer netns has an id assigned into the",
        " * current netns.",
        " */",
        "bool peernet_has_id(const struct net *net, struct net *peer)",
        "{",
        "	return peernet2id(net, peer) >= 0;",
        "}",
        "",
        "struct net *get_net_ns_by_id(const struct net *net, int id)",
        "{",
        "	struct net *peer;",
        "",
        "	if (id < 0)",
        "		return NULL;",
        "",
        "	rcu_read_lock();",
        "	peer = idr_find(&net->netns_ids, id);",
        "	if (peer)",
        "		peer = maybe_get_net(peer);",
        "	rcu_read_unlock();",
        "",
        "	return peer;",
        "}",
        "EXPORT_SYMBOL_GPL(get_net_ns_by_id);",
        "",
        "static __net_init void preinit_net_sysctl(struct net *net)",
        "{",
        "	net->core.sysctl_somaxconn = SOMAXCONN;",
        "	/* Limits per socket sk_omem_alloc usage.",
        "	 * TCP zerocopy regular usage needs 128 KB.",
        "	 */",
        "	net->core.sysctl_optmem_max = 128 * 1024;",
        "	net->core.sysctl_txrehash = SOCK_TXREHASH_ENABLED;",
        "	net->core.sysctl_tstamp_allow_data = 1;",
        "}",
        "",
        "/* init code that must occur even if setup_net() is not called. */",
        "static __net_init void preinit_net(struct net *net, struct user_namespace *user_ns)",
        "{",
        "	refcount_set(&net->passive, 1);",
        "	refcount_set(&net->ns.count, 1);",
        "	ref_tracker_dir_init(&net->refcnt_tracker, 128, \"net refcnt\");",
        "	ref_tracker_dir_init(&net->notrefcnt_tracker, 128, \"net notrefcnt\");",
        "",
        "	get_random_bytes(&net->hash_mix, sizeof(u32));",
        "	net->dev_base_seq = 1;",
        "	net->user_ns = user_ns;",
        "",
        "	idr_init(&net->netns_ids);",
        "	spin_lock_init(&net->nsid_lock);",
        "	mutex_init(&net->ipv4.ra_mutex);",
        "",
        "#ifdef CONFIG_DEBUG_NET_SMALL_RTNL",
        "	mutex_init(&net->rtnl_mutex);",
        "	lock_set_cmp_fn(&net->rtnl_mutex, rtnl_net_lock_cmp_fn, NULL);",
        "#endif",
        "",
        "	preinit_net_sysctl(net);",
        "}",
        "",
        "/*",
        " * setup_net runs the initializers for the network namespace object.",
        " */",
        "static __net_init int setup_net(struct net *net)",
        "{",
        "	/* Must be called with pernet_ops_rwsem held */",
        "	const struct pernet_operations *ops, *saved_ops;",
        "	LIST_HEAD(net_exit_list);",
        "	LIST_HEAD(dev_kill_list);",
        "	int error = 0;",
        "",
        "	preempt_disable();",
        "	net->net_cookie = gen_cookie_next(&net_cookie);",
        "	preempt_enable();",
        "",
        "	list_for_each_entry(ops, &pernet_list, list) {",
        "		error = ops_init(ops, net);",
        "		if (error < 0)",
        "			goto out_undo;",
        "	}",
        "	down_write(&net_rwsem);",
        "	list_add_tail_rcu(&net->list, &net_namespace_list);",
        "	up_write(&net_rwsem);",
        "out:",
        "	return error;",
        "",
        "out_undo:",
        "	/* Walk through the list backwards calling the exit functions",
        "	 * for the pernet modules whose init functions did not fail.",
        "	 */",
        "	list_add(&net->exit_list, &net_exit_list);",
        "	saved_ops = ops;",
        "	list_for_each_entry_continue_reverse(ops, &pernet_list, list)",
        "		ops_pre_exit_list(ops, &net_exit_list);",
        "",
        "	synchronize_rcu();",
        "",
        "	ops = saved_ops;",
        "	rtnl_lock();",
        "	list_for_each_entry_continue_reverse(ops, &pernet_list, list) {",
        "		if (ops->exit_batch_rtnl)",
        "			ops->exit_batch_rtnl(&net_exit_list, &dev_kill_list);",
        "	}",
        "	unregister_netdevice_many(&dev_kill_list);",
        "	rtnl_unlock();",
        "",
        "	ops = saved_ops;",
        "	list_for_each_entry_continue_reverse(ops, &pernet_list, list)",
        "		ops_exit_list(ops, &net_exit_list);",
        "",
        "	ops = saved_ops;",
        "	list_for_each_entry_continue_reverse(ops, &pernet_list, list)",
        "		ops_free_list(ops, &net_exit_list);",
        "",
        "	rcu_barrier();",
        "	goto out;",
        "}",
        "",
        "#ifdef CONFIG_NET_NS",
        "static struct ucounts *inc_net_namespaces(struct user_namespace *ns)",
        "{",
        "	return inc_ucount(ns, current_euid(), UCOUNT_NET_NAMESPACES);",
        "}",
        "",
        "static void dec_net_namespaces(struct ucounts *ucounts)",
        "{",
        "	dec_ucount(ucounts, UCOUNT_NET_NAMESPACES);",
        "}",
        "",
        "static struct kmem_cache *net_cachep __ro_after_init;",
        "static struct workqueue_struct *netns_wq;",
        "",
        "static struct net *net_alloc(void)",
        "{",
        "	struct net *net = NULL;",
        "	struct net_generic *ng;",
        "",
        "	ng = net_alloc_generic();",
        "	if (!ng)",
        "		goto out;",
        "",
        "	net = kmem_cache_zalloc(net_cachep, GFP_KERNEL);",
        "	if (!net)",
        "		goto out_free;",
        "",
        "#ifdef CONFIG_KEYS",
        "	net->key_domain = kzalloc(sizeof(struct key_tag), GFP_KERNEL);",
        "	if (!net->key_domain)",
        "		goto out_free_2;",
        "	refcount_set(&net->key_domain->usage, 1);",
        "#endif",
        "",
        "	rcu_assign_pointer(net->gen, ng);",
        "out:",
        "	return net;",
        "",
        "#ifdef CONFIG_KEYS",
        "out_free_2:",
        "	kmem_cache_free(net_cachep, net);",
        "	net = NULL;",
        "#endif",
        "out_free:",
        "	kfree(ng);",
        "	goto out;",
        "}",
        "",
        "static LLIST_HEAD(defer_free_list);",
        "",
        "static void net_complete_free(void)",
        "{",
        "	struct llist_node *kill_list;",
        "	struct net *net, *next;",
        "",
        "	/* Get the list of namespaces to free from last round. */",
        "	kill_list = llist_del_all(&defer_free_list);",
        "",
        "	llist_for_each_entry_safe(net, next, kill_list, defer_free_list)",
        "		kmem_cache_free(net_cachep, net);",
        "",
        "}",
        "",
        "void net_passive_dec(struct net *net)",
        "{",
        "	if (refcount_dec_and_test(&net->passive)) {",
        "		kfree(rcu_access_pointer(net->gen));",
        "",
        "		/* There should not be any trackers left there. */",
        "		ref_tracker_dir_exit(&net->notrefcnt_tracker);",
        "",
        "		/* Wait for an extra rcu_barrier() before final free. */",
        "		llist_add(&net->defer_free_list, &defer_free_list);",
        "	}",
        "}",
        "",
        "void net_drop_ns(void *p)",
        "{",
        "	struct net *net = (struct net *)p;",
        "",
        "	if (net)",
        "		net_passive_dec(net);",
        "}",
        "",
        "struct net *copy_net_ns(unsigned long flags,",
        "			struct user_namespace *user_ns, struct net *old_net)",
        "{",
        "	struct ucounts *ucounts;",
        "	struct net *net;",
        "	int rv;",
        "",
        "	if (!(flags & CLONE_NEWNET))",
        "		return get_net(old_net);",
        "",
        "	ucounts = inc_net_namespaces(user_ns);",
        "	if (!ucounts)",
        "		return ERR_PTR(-ENOSPC);",
        "",
        "	net = net_alloc();",
        "	if (!net) {",
        "		rv = -ENOMEM;",
        "		goto dec_ucounts;",
        "	}",
        "",
        "	preinit_net(net, user_ns);",
        "	net->ucounts = ucounts;",
        "	get_user_ns(user_ns);",
        "",
        "	rv = down_read_killable(&pernet_ops_rwsem);",
        "	if (rv < 0)",
        "		goto put_userns;",
        "",
        "	rv = setup_net(net);",
        "",
        "	up_read(&pernet_ops_rwsem);",
        "",
        "	if (rv < 0) {",
        "put_userns:",
        "#ifdef CONFIG_KEYS",
        "		key_remove_domain(net->key_domain);",
        "#endif",
        "		put_user_ns(user_ns);",
        "		net_passive_dec(net);",
        "dec_ucounts:",
        "		dec_net_namespaces(ucounts);",
        "		return ERR_PTR(rv);",
        "	}",
        "	return net;",
        "}",
        "",
        "/**",
        " * net_ns_get_ownership - get sysfs ownership data for @net",
        " * @net: network namespace in question (can be NULL)",
        " * @uid: kernel user ID for sysfs objects",
        " * @gid: kernel group ID for sysfs objects",
        " *",
        " * Returns the uid/gid pair of root in the user namespace associated with the",
        " * given network namespace.",
        " */",
        "void net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid)",
        "{",
        "	if (net) {",
        "		kuid_t ns_root_uid = make_kuid(net->user_ns, 0);",
        "		kgid_t ns_root_gid = make_kgid(net->user_ns, 0);",
        "",
        "		if (uid_valid(ns_root_uid))",
        "			*uid = ns_root_uid;",
        "",
        "		if (gid_valid(ns_root_gid))",
        "			*gid = ns_root_gid;",
        "	} else {",
        "		*uid = GLOBAL_ROOT_UID;",
        "		*gid = GLOBAL_ROOT_GID;",
        "	}",
        "}",
        "EXPORT_SYMBOL_GPL(net_ns_get_ownership);",
        "",
        "static void unhash_nsid(struct net *net, struct net *last)",
        "{",
        "	struct net *tmp;",
        "	/* This function is only called from cleanup_net() work,",
        "	 * and this work is the only process, that may delete",
        "	 * a net from net_namespace_list. So, when the below",
        "	 * is executing, the list may only grow. Thus, we do not",
        "	 * use for_each_net_rcu() or net_rwsem.",
        "	 */",
        "	for_each_net(tmp) {",
        "		int id;",
        "",
        "		spin_lock_bh(&tmp->nsid_lock);",
        "		id = __peernet2id(tmp, net);",
        "		if (id >= 0)",
        "			idr_remove(&tmp->netns_ids, id);",
        "		spin_unlock_bh(&tmp->nsid_lock);",
        "		if (id >= 0)",
        "			rtnl_net_notifyid(tmp, RTM_DELNSID, id, 0, NULL,",
        "					  GFP_KERNEL);",
        "		if (tmp == last)",
        "			break;",
        "	}",
        "	spin_lock_bh(&net->nsid_lock);",
        "	idr_destroy(&net->netns_ids);",
        "	spin_unlock_bh(&net->nsid_lock);",
        "}",
        "",
        "static LLIST_HEAD(cleanup_list);",
        "",
        "static void cleanup_net(struct work_struct *work)",
        "{",
        "	const struct pernet_operations *ops;",
        "	struct net *net, *tmp, *last;",
        "	struct llist_node *net_kill_list;",
        "	LIST_HEAD(net_exit_list);",
        "	LIST_HEAD(dev_kill_list);",
        "",
        "	/* Atomically snapshot the list of namespaces to cleanup */",
        "	net_kill_list = llist_del_all(&cleanup_list);",
        "",
        "	down_read(&pernet_ops_rwsem);",
        "",
        "	/* Don't let anyone else find us. */",
        "	down_write(&net_rwsem);",
        "	llist_for_each_entry(net, net_kill_list, cleanup_list)",
        "		list_del_rcu(&net->list);",
        "	/* Cache last net. After we unlock rtnl, no one new net",
        "	 * added to net_namespace_list can assign nsid pointer",
        "	 * to a net from net_kill_list (see peernet2id_alloc()).",
        "	 * So, we skip them in unhash_nsid().",
        "	 *",
        "	 * Note, that unhash_nsid() does not delete nsid links",
        "	 * between net_kill_list's nets, as they've already",
        "	 * deleted from net_namespace_list. But, this would be",
        "	 * useless anyway, as netns_ids are destroyed there.",
        "	 */",
        "	last = list_last_entry(&net_namespace_list, struct net, list);",
        "	up_write(&net_rwsem);",
        "",
        "	llist_for_each_entry(net, net_kill_list, cleanup_list) {",
        "		unhash_nsid(net, last);",
        "		list_add_tail(&net->exit_list, &net_exit_list);",
        "	}",
        "",
        "	/* Run all of the network namespace pre_exit methods */",
        "	list_for_each_entry_reverse(ops, &pernet_list, list)",
        "		ops_pre_exit_list(ops, &net_exit_list);",
        "",
        "	/*",
        "	 * Another CPU might be rcu-iterating the list, wait for it.",
        "	 * This needs to be before calling the exit() notifiers, so",
        "	 * the rcu_barrier() below isn't sufficient alone.",
        "	 * Also the pre_exit() and exit() methods need this barrier.",
        "	 */",
        "	synchronize_rcu_expedited();",
        "",
        "	rtnl_lock();",
        "	list_for_each_entry_reverse(ops, &pernet_list, list) {",
        "		if (ops->exit_batch_rtnl)",
        "			ops->exit_batch_rtnl(&net_exit_list, &dev_kill_list);",
        "	}",
        "	unregister_netdevice_many(&dev_kill_list);",
        "	rtnl_unlock();",
        "",
        "	/* Run all of the network namespace exit methods */",
        "	list_for_each_entry_reverse(ops, &pernet_list, list)",
        "		ops_exit_list(ops, &net_exit_list);",
        "",
        "	/* Free the net generic variables */",
        "	list_for_each_entry_reverse(ops, &pernet_list, list)",
        "		ops_free_list(ops, &net_exit_list);",
        "",
        "	up_read(&pernet_ops_rwsem);",
        "",
        "	/* Ensure there are no outstanding rcu callbacks using this",
        "	 * network namespace.",
        "	 */",
        "	rcu_barrier();",
        "",
        "	net_complete_free();",
        "",
        "	/* Finally it is safe to free my network namespace structure */",
        "	list_for_each_entry_safe(net, tmp, &net_exit_list, exit_list) {",
        "		list_del_init(&net->exit_list);",
        "		dec_net_namespaces(net->ucounts);",
        "#ifdef CONFIG_KEYS",
        "		key_remove_domain(net->key_domain);",
        "#endif",
        "		put_user_ns(net->user_ns);",
        "		net_passive_dec(net);",
        "	}",
        "}",
        "",
        "/**",
        " * net_ns_barrier - wait until concurrent net_cleanup_work is done",
        " *",
        " * cleanup_net runs from work queue and will first remove namespaces",
        " * from the global list, then run net exit functions.",
        " *",
        " * Call this in module exit path to make sure that all netns",
        " * ->exit ops have been invoked before the function is removed.",
        " */",
        "void net_ns_barrier(void)",
        "{",
        "	down_write(&pernet_ops_rwsem);",
        "	up_write(&pernet_ops_rwsem);",
        "}",
        "EXPORT_SYMBOL(net_ns_barrier);",
        "",
        "static DECLARE_WORK(net_cleanup_work, cleanup_net);",
        "",
        "void __put_net(struct net *net)",
        "{",
        "	ref_tracker_dir_exit(&net->refcnt_tracker);",
        "	/* Cleanup the network namespace in process context */",
        "	if (llist_add(&net->cleanup_list, &cleanup_list))",
        "		queue_work(netns_wq, &net_cleanup_work);",
        "}",
        "EXPORT_SYMBOL_GPL(__put_net);",
        "",
        "/**",
        " * get_net_ns - increment the refcount of the network namespace",
        " * @ns: common namespace (net)",
        " *",
        " * Returns the net's common namespace or ERR_PTR() if ref is zero.",
        " */",
        "struct ns_common *get_net_ns(struct ns_common *ns)",
        "{",
        "	struct net *net;",
        "",
        "	net = maybe_get_net(container_of(ns, struct net, ns));",
        "	if (net)",
        "		return &net->ns;",
        "	return ERR_PTR(-EINVAL);",
        "}",
        "EXPORT_SYMBOL_GPL(get_net_ns);",
        "",
        "struct net *get_net_ns_by_fd(int fd)",
        "{",
        "	CLASS(fd, f)(fd);",
        "",
        "	if (fd_empty(f))",
        "		return ERR_PTR(-EBADF);",
        "",
        "	if (proc_ns_file(fd_file(f))) {",
        "		struct ns_common *ns = get_proc_ns(file_inode(fd_file(f)));",
        "		if (ns->ops == &netns_operations)",
        "			return get_net(container_of(ns, struct net, ns));",
        "	}",
        "",
        "	return ERR_PTR(-EINVAL);",
        "}",
        "EXPORT_SYMBOL_GPL(get_net_ns_by_fd);",
        "#endif",
        "",
        "struct net *get_net_ns_by_pid(pid_t pid)",
        "{",
        "	struct task_struct *tsk;",
        "	struct net *net;",
        "",
        "	/* Lookup the network namespace */",
        "	net = ERR_PTR(-ESRCH);",
        "	rcu_read_lock();",
        "	tsk = find_task_by_vpid(pid);",
        "	if (tsk) {",
        "		struct nsproxy *nsproxy;",
        "		task_lock(tsk);",
        "		nsproxy = tsk->nsproxy;",
        "		if (nsproxy)",
        "			net = get_net(nsproxy->net_ns);",
        "		task_unlock(tsk);",
        "	}",
        "	rcu_read_unlock();",
        "	return net;",
        "}",
        "EXPORT_SYMBOL_GPL(get_net_ns_by_pid);",
        "",
        "static __net_init int net_ns_net_init(struct net *net)",
        "{",
        "#ifdef CONFIG_NET_NS",
        "	net->ns.ops = &netns_operations;",
        "#endif",
        "	return ns_alloc_inum(&net->ns);",
        "}",
        "",
        "static __net_exit void net_ns_net_exit(struct net *net)",
        "{",
        "	ns_free_inum(&net->ns);",
        "}",
        "",
        "static struct pernet_operations __net_initdata net_ns_ops = {",
        "	.init = net_ns_net_init,",
        "	.exit = net_ns_net_exit,",
        "};",
        "",
        "static const struct nla_policy rtnl_net_policy[NETNSA_MAX + 1] = {",
        "	[NETNSA_NONE]		= { .type = NLA_UNSPEC },",
        "	[NETNSA_NSID]		= { .type = NLA_S32 },",
        "	[NETNSA_PID]		= { .type = NLA_U32 },",
        "	[NETNSA_FD]		= { .type = NLA_U32 },",
        "	[NETNSA_TARGET_NSID]	= { .type = NLA_S32 },",
        "};",
        "",
        "static int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh,",
        "			  struct netlink_ext_ack *extack)",
        "{",
        "	struct net *net = sock_net(skb->sk);",
        "	struct nlattr *tb[NETNSA_MAX + 1];",
        "	struct nlattr *nla;",
        "	struct net *peer;",
        "	int nsid, err;",
        "",
        "	err = nlmsg_parse_deprecated(nlh, sizeof(struct rtgenmsg), tb,",
        "				     NETNSA_MAX, rtnl_net_policy, extack);",
        "	if (err < 0)",
        "		return err;",
        "	if (!tb[NETNSA_NSID]) {",
        "		NL_SET_ERR_MSG(extack, \"nsid is missing\");",
        "		return -EINVAL;",
        "	}",
        "	nsid = nla_get_s32(tb[NETNSA_NSID]);",
        "",
        "	if (tb[NETNSA_PID]) {",
        "		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));",
        "		nla = tb[NETNSA_PID];",
        "	} else if (tb[NETNSA_FD]) {",
        "		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));",
        "		nla = tb[NETNSA_FD];",
        "	} else {",
        "		NL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");",
        "		return -EINVAL;",
        "	}",
        "	if (IS_ERR(peer)) {",
        "		NL_SET_BAD_ATTR(extack, nla);",
        "		NL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");",
        "		return PTR_ERR(peer);",
        "	}",
        "",
        "	spin_lock_bh(&net->nsid_lock);",
        "	if (__peernet2id(net, peer) >= 0) {",
        "		spin_unlock_bh(&net->nsid_lock);",
        "		err = -EEXIST;",
        "		NL_SET_BAD_ATTR(extack, nla);",
        "		NL_SET_ERR_MSG(extack,",
        "			       \"Peer netns already has a nsid assigned\");",
        "		goto out;",
        "	}",
        "",
        "	err = alloc_netid(net, peer, nsid);",
        "	spin_unlock_bh(&net->nsid_lock);",
        "	if (err >= 0) {",
        "		rtnl_net_notifyid(net, RTM_NEWNSID, err, NETLINK_CB(skb).portid,",
        "				  nlh, GFP_KERNEL);",
        "		err = 0;",
        "	} else if (err == -ENOSPC && nsid >= 0) {",
        "		err = -EEXIST;",
        "		NL_SET_BAD_ATTR(extack, tb[NETNSA_NSID]);",
        "		NL_SET_ERR_MSG(extack, \"The specified nsid is already used\");",
        "	}",
        "out:",
        "	put_net(peer);",
        "	return err;",
        "}",
        "",
        "static int rtnl_net_get_size(void)",
        "{",
        "	return NLMSG_ALIGN(sizeof(struct rtgenmsg))",
        "	       + nla_total_size(sizeof(s32)) /* NETNSA_NSID */",
        "	       + nla_total_size(sizeof(s32)) /* NETNSA_CURRENT_NSID */",
        "	       ;",
        "}",
        "",
        "struct net_fill_args {",
        "	u32 portid;",
        "	u32 seq;",
        "	int flags;",
        "	int cmd;",
        "	int nsid;",
        "	bool add_ref;",
        "	int ref_nsid;",
        "};",
        "",
        "static int rtnl_net_fill(struct sk_buff *skb, struct net_fill_args *args)",
        "{",
        "	struct nlmsghdr *nlh;",
        "	struct rtgenmsg *rth;",
        "",
        "	nlh = nlmsg_put(skb, args->portid, args->seq, args->cmd, sizeof(*rth),",
        "			args->flags);",
        "	if (!nlh)",
        "		return -EMSGSIZE;",
        "",
        "	rth = nlmsg_data(nlh);",
        "	rth->rtgen_family = AF_UNSPEC;",
        "",
        "	if (nla_put_s32(skb, NETNSA_NSID, args->nsid))",
        "		goto nla_put_failure;",
        "",
        "	if (args->add_ref &&",
        "	    nla_put_s32(skb, NETNSA_CURRENT_NSID, args->ref_nsid))",
        "		goto nla_put_failure;",
        "",
        "	nlmsg_end(skb, nlh);",
        "	return 0;",
        "",
        "nla_put_failure:",
        "	nlmsg_cancel(skb, nlh);",
        "	return -EMSGSIZE;",
        "}",
        "",
        "static int rtnl_net_valid_getid_req(struct sk_buff *skb,",
        "				    const struct nlmsghdr *nlh,",
        "				    struct nlattr **tb,",
        "				    struct netlink_ext_ack *extack)",
        "{",
        "	int i, err;",
        "",
        "	if (!netlink_strict_get_check(skb))",
        "		return nlmsg_parse_deprecated(nlh, sizeof(struct rtgenmsg),",
        "					      tb, NETNSA_MAX, rtnl_net_policy,",
        "					      extack);",
        "",
        "	err = nlmsg_parse_deprecated_strict(nlh, sizeof(struct rtgenmsg), tb,",
        "					    NETNSA_MAX, rtnl_net_policy,",
        "					    extack);",
        "	if (err)",
        "		return err;",
        "",
        "	for (i = 0; i <= NETNSA_MAX; i++) {",
        "		if (!tb[i])",
        "			continue;",
        "",
        "		switch (i) {",
        "		case NETNSA_PID:",
        "		case NETNSA_FD:",
        "		case NETNSA_NSID:",
        "		case NETNSA_TARGET_NSID:",
        "			break;",
        "		default:",
        "			NL_SET_ERR_MSG(extack, \"Unsupported attribute in peer netns getid request\");",
        "			return -EINVAL;",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh,",
        "			  struct netlink_ext_ack *extack)",
        "{",
        "	struct net *net = sock_net(skb->sk);",
        "	struct nlattr *tb[NETNSA_MAX + 1];",
        "	struct net_fill_args fillargs = {",
        "		.portid = NETLINK_CB(skb).portid,",
        "		.seq = nlh->nlmsg_seq,",
        "		.cmd = RTM_NEWNSID,",
        "	};",
        "	struct net *peer, *target = net;",
        "	struct nlattr *nla;",
        "	struct sk_buff *msg;",
        "	int err;",
        "",
        "	err = rtnl_net_valid_getid_req(skb, nlh, tb, extack);",
        "	if (err < 0)",
        "		return err;",
        "	if (tb[NETNSA_PID]) {",
        "		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));",
        "		nla = tb[NETNSA_PID];",
        "	} else if (tb[NETNSA_FD]) {",
        "		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));",
        "		nla = tb[NETNSA_FD];",
        "	} else if (tb[NETNSA_NSID]) {",
        "		peer = get_net_ns_by_id(net, nla_get_s32(tb[NETNSA_NSID]));",
        "		if (!peer)",
        "			peer = ERR_PTR(-ENOENT);",
        "		nla = tb[NETNSA_NSID];",
        "	} else {",
        "		NL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");",
        "		return -EINVAL;",
        "	}",
        "",
        "	if (IS_ERR(peer)) {",
        "		NL_SET_BAD_ATTR(extack, nla);",
        "		NL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");",
        "		return PTR_ERR(peer);",
        "	}",
        "",
        "	if (tb[NETNSA_TARGET_NSID]) {",
        "		int id = nla_get_s32(tb[NETNSA_TARGET_NSID]);",
        "",
        "		target = rtnl_get_net_ns_capable(NETLINK_CB(skb).sk, id);",
        "		if (IS_ERR(target)) {",
        "			NL_SET_BAD_ATTR(extack, tb[NETNSA_TARGET_NSID]);",
        "			NL_SET_ERR_MSG(extack,",
        "				       \"Target netns reference is invalid\");",
        "			err = PTR_ERR(target);",
        "			goto out;",
        "		}",
        "		fillargs.add_ref = true;",
        "		fillargs.ref_nsid = peernet2id(net, peer);",
        "	}",
        "",
        "	msg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);",
        "	if (!msg) {",
        "		err = -ENOMEM;",
        "		goto out;",
        "	}",
        "",
        "	fillargs.nsid = peernet2id(target, peer);",
        "	err = rtnl_net_fill(msg, &fillargs);",
        "	if (err < 0)",
        "		goto err_out;",
        "",
        "	err = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);",
        "	goto out;",
        "",
        "err_out:",
        "	nlmsg_free(msg);",
        "out:",
        "	if (fillargs.add_ref)",
        "		put_net(target);",
        "	put_net(peer);",
        "	return err;",
        "}",
        "",
        "struct rtnl_net_dump_cb {",
        "	struct net *tgt_net;",
        "	struct net *ref_net;",
        "	struct sk_buff *skb;",
        "	struct net_fill_args fillargs;",
        "	int idx;",
        "	int s_idx;",
        "};",
        "",
        "/* Runs in RCU-critical section. */",
        "static int rtnl_net_dumpid_one(int id, void *peer, void *data)",
        "{",
        "	struct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;",
        "	int ret;",
        "",
        "	if (net_cb->idx < net_cb->s_idx)",
        "		goto cont;",
        "",
        "	net_cb->fillargs.nsid = id;",
        "	if (net_cb->fillargs.add_ref)",
        "		net_cb->fillargs.ref_nsid = __peernet2id(net_cb->ref_net, peer);",
        "	ret = rtnl_net_fill(net_cb->skb, &net_cb->fillargs);",
        "	if (ret < 0)",
        "		return ret;",
        "",
        "cont:",
        "	net_cb->idx++;",
        "	return 0;",
        "}",
        "",
        "static int rtnl_valid_dump_net_req(const struct nlmsghdr *nlh, struct sock *sk,",
        "				   struct rtnl_net_dump_cb *net_cb,",
        "				   struct netlink_callback *cb)",
        "{",
        "	struct netlink_ext_ack *extack = cb->extack;",
        "	struct nlattr *tb[NETNSA_MAX + 1];",
        "	int err, i;",
        "",
        "	err = nlmsg_parse_deprecated_strict(nlh, sizeof(struct rtgenmsg), tb,",
        "					    NETNSA_MAX, rtnl_net_policy,",
        "					    extack);",
        "	if (err < 0)",
        "		return err;",
        "",
        "	for (i = 0; i <= NETNSA_MAX; i++) {",
        "		if (!tb[i])",
        "			continue;",
        "",
        "		if (i == NETNSA_TARGET_NSID) {",
        "			struct net *net;",
        "",
        "			net = rtnl_get_net_ns_capable(sk, nla_get_s32(tb[i]));",
        "			if (IS_ERR(net)) {",
        "				NL_SET_BAD_ATTR(extack, tb[i]);",
        "				NL_SET_ERR_MSG(extack,",
        "					       \"Invalid target network namespace id\");",
        "				return PTR_ERR(net);",
        "			}",
        "			net_cb->fillargs.add_ref = true;",
        "			net_cb->ref_net = net_cb->tgt_net;",
        "			net_cb->tgt_net = net;",
        "		} else {",
        "			NL_SET_BAD_ATTR(extack, tb[i]);",
        "			NL_SET_ERR_MSG(extack,",
        "				       \"Unsupported attribute in dump request\");",
        "			return -EINVAL;",
        "		}",
        "	}",
        "",
        "	return 0;",
        "}",
        "",
        "static int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)",
        "{",
        "	struct rtnl_net_dump_cb net_cb = {",
        "		.tgt_net = sock_net(skb->sk),",
        "		.skb = skb,",
        "		.fillargs = {",
        "			.portid = NETLINK_CB(cb->skb).portid,",
        "			.seq = cb->nlh->nlmsg_seq,",
        "			.flags = NLM_F_MULTI,",
        "			.cmd = RTM_NEWNSID,",
        "		},",
        "		.idx = 0,",
        "		.s_idx = cb->args[0],",
        "	};",
        "	int err = 0;",
        "",
        "	if (cb->strict_check) {",
        "		err = rtnl_valid_dump_net_req(cb->nlh, skb->sk, &net_cb, cb);",
        "		if (err < 0)",
        "			goto end;",
        "	}",
        "",
        "	rcu_read_lock();",
        "	idr_for_each(&net_cb.tgt_net->netns_ids, rtnl_net_dumpid_one, &net_cb);",
        "	rcu_read_unlock();",
        "",
        "	cb->args[0] = net_cb.idx;",
        "end:",
        "	if (net_cb.fillargs.add_ref)",
        "		put_net(net_cb.tgt_net);",
        "	return err;",
        "}",
        "",
        "static void rtnl_net_notifyid(struct net *net, int cmd, int id, u32 portid,",
        "			      struct nlmsghdr *nlh, gfp_t gfp)",
        "{",
        "	struct net_fill_args fillargs = {",
        "		.portid = portid,",
        "		.seq = nlh ? nlh->nlmsg_seq : 0,",
        "		.cmd = cmd,",
        "		.nsid = id,",
        "	};",
        "	struct sk_buff *msg;",
        "	int err = -ENOMEM;",
        "",
        "	msg = nlmsg_new(rtnl_net_get_size(), gfp);",
        "	if (!msg)",
        "		goto out;",
        "",
        "	err = rtnl_net_fill(msg, &fillargs);",
        "	if (err < 0)",
        "		goto err_out;",
        "",
        "	rtnl_notify(msg, net, portid, RTNLGRP_NSID, nlh, gfp);",
        "	return;",
        "",
        "err_out:",
        "	nlmsg_free(msg);",
        "out:",
        "	rtnl_set_sk_err(net, RTNLGRP_NSID, err);",
        "}",
        "",
        "#ifdef CONFIG_NET_NS",
        "static void __init netns_ipv4_struct_check(void)",
        "{",
        "	/* TX readonly hotpath cache lines */",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_early_retrans);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_tso_win_divisor);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_tso_rtt_log);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_autocorking);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_min_snd_mss);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_notsent_lowat);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_limit_output_bytes);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_min_rtt_wlen);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_tcp_wmem);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_tx,",
        "				      sysctl_ip_fwd_use_pmtu);",
        "	CACHELINE_ASSERT_GROUP_SIZE(struct netns_ipv4, netns_ipv4_read_tx, 33);",
        "",
        "	/* TXRX readonly hotpath cache lines */",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_txrx,",
        "				      sysctl_tcp_moderate_rcvbuf);",
        "	CACHELINE_ASSERT_GROUP_SIZE(struct netns_ipv4, netns_ipv4_read_txrx, 1);",
        "",
        "	/* RX readonly hotpath cache line */",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_rx,",
        "				      sysctl_ip_early_demux);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_rx,",
        "				      sysctl_tcp_early_demux);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_rx,",
        "				      sysctl_tcp_l3mdev_accept);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_rx,",
        "				      sysctl_tcp_reordering);",
        "	CACHELINE_ASSERT_GROUP_MEMBER(struct netns_ipv4, netns_ipv4_read_rx,",
        "				      sysctl_tcp_rmem);",
        "	CACHELINE_ASSERT_GROUP_SIZE(struct netns_ipv4, netns_ipv4_read_rx, 22);",
        "}",
        "#endif",
        "",
        "static const struct rtnl_msg_handler net_ns_rtnl_msg_handlers[] __initconst = {",
        "	{.msgtype = RTM_NEWNSID, .doit = rtnl_net_newid,",
        "	 .flags = RTNL_FLAG_DOIT_UNLOCKED},",
        "	{.msgtype = RTM_GETNSID, .doit = rtnl_net_getid,",
        "	 .dumpit = rtnl_net_dumpid,",
        "	 .flags = RTNL_FLAG_DOIT_UNLOCKED | RTNL_FLAG_DUMP_UNLOCKED},",
        "};",
        "",
        "void __init net_ns_init(void)",
        "{",
        "	struct net_generic *ng;",
        "",
        "#ifdef CONFIG_NET_NS",
        "	netns_ipv4_struct_check();",
        "	net_cachep = kmem_cache_create(\"net_namespace\", sizeof(struct net),",
        "					SMP_CACHE_BYTES,",
        "					SLAB_PANIC|SLAB_ACCOUNT, NULL);",
        "",
        "	/* Create workqueue for cleanup */",
        "	netns_wq = create_singlethread_workqueue(\"netns\");",
        "	if (!netns_wq)",
        "		panic(\"Could not create netns workq\");",
        "#endif",
        "",
        "	ng = net_alloc_generic();",
        "	if (!ng)",
        "		panic(\"Could not allocate generic netns\");",
        "",
        "	rcu_assign_pointer(init_net.gen, ng);",
        "",
        "#ifdef CONFIG_KEYS",
        "	init_net.key_domain = &init_net_key_domain;",
        "#endif",
        "	preinit_net(&init_net, &init_user_ns);",
        "",
        "	down_write(&pernet_ops_rwsem);",
        "	if (setup_net(&init_net))",
        "		panic(\"Could not setup the initial network namespace\");",
        "",
        "	init_net_initialized = true;",
        "	up_write(&pernet_ops_rwsem);",
        "",
        "	if (register_pernet_subsys(&net_ns_ops))",
        "		panic(\"Could not register network namespace subsystems\");",
        "",
        "	rtnl_register_many(net_ns_rtnl_msg_handlers);",
        "}",
        "",
        "static void free_exit_list(struct pernet_operations *ops, struct list_head *net_exit_list)",
        "{",
        "	ops_pre_exit_list(ops, net_exit_list);",
        "	synchronize_rcu();",
        "",
        "	if (ops->exit_batch_rtnl) {",
        "		LIST_HEAD(dev_kill_list);",
        "",
        "		rtnl_lock();",
        "		ops->exit_batch_rtnl(net_exit_list, &dev_kill_list);",
        "		unregister_netdevice_many(&dev_kill_list);",
        "		rtnl_unlock();",
        "	}",
        "	ops_exit_list(ops, net_exit_list);",
        "",
        "	ops_free_list(ops, net_exit_list);",
        "}",
        "",
        "#ifdef CONFIG_NET_NS",
        "static int __register_pernet_operations(struct list_head *list,",
        "					struct pernet_operations *ops)",
        "{",
        "	struct net *net;",
        "	int error;",
        "	LIST_HEAD(net_exit_list);",
        "",
        "	list_add_tail(&ops->list, list);",
        "	if (ops->init || ops->id) {",
        "		/* We held write locked pernet_ops_rwsem, and parallel",
        "		 * setup_net() and cleanup_net() are not possible.",
        "		 */",
        "		for_each_net(net) {",
        "			error = ops_init(ops, net);",
        "			if (error)",
        "				goto out_undo;",
        "			list_add_tail(&net->exit_list, &net_exit_list);",
        "		}",
        "	}",
        "	return 0;",
        "",
        "out_undo:",
        "	/* If I have an error cleanup all namespaces I initialized */",
        "	list_del(&ops->list);",
        "	free_exit_list(ops, &net_exit_list);",
        "	return error;",
        "}",
        "",
        "static void __unregister_pernet_operations(struct pernet_operations *ops)",
        "{",
        "	struct net *net;",
        "	LIST_HEAD(net_exit_list);",
        "",
        "	list_del(&ops->list);",
        "	/* See comment in __register_pernet_operations() */",
        "	for_each_net(net)",
        "		list_add_tail(&net->exit_list, &net_exit_list);",
        "",
        "	free_exit_list(ops, &net_exit_list);",
        "}",
        "",
        "#else",
        "",
        "static int __register_pernet_operations(struct list_head *list,",
        "					struct pernet_operations *ops)",
        "{",
        "	if (!init_net_initialized) {",
        "		list_add_tail(&ops->list, list);",
        "		return 0;",
        "	}",
        "",
        "	return ops_init(ops, &init_net);",
        "}",
        "",
        "static void __unregister_pernet_operations(struct pernet_operations *ops)",
        "{",
        "	if (!init_net_initialized) {",
        "		list_del(&ops->list);",
        "	} else {",
        "		LIST_HEAD(net_exit_list);",
        "		list_add(&init_net.exit_list, &net_exit_list);",
        "		free_exit_list(ops, &net_exit_list);",
        "	}",
        "}",
        "",
        "#endif /* CONFIG_NET_NS */",
        "",
        "static DEFINE_IDA(net_generic_ids);",
        "",
        "static int register_pernet_operations(struct list_head *list,",
        "				      struct pernet_operations *ops)",
        "{",
        "	int error;",
        "",
        "	if (WARN_ON(!!ops->id ^ !!ops->size))",
        "		return -EINVAL;",
        "",
        "	if (ops->id) {",
        "		error = ida_alloc_min(&net_generic_ids, MIN_PERNET_OPS_ID,",
        "				GFP_KERNEL);",
        "		if (error < 0)",
        "			return error;",
        "		*ops->id = error;",
        "		/* This does not require READ_ONCE as writers already hold",
        "		 * pernet_ops_rwsem. But WRITE_ONCE is needed to protect",
        "		 * net_alloc_generic.",
        "		 */",
        "		WRITE_ONCE(max_gen_ptrs, max(max_gen_ptrs, *ops->id + 1));",
        "	}",
        "	error = __register_pernet_operations(list, ops);",
        "	if (error) {",
        "		rcu_barrier();",
        "		if (ops->id)",
        "			ida_free(&net_generic_ids, *ops->id);",
        "	}",
        "",
        "	return error;",
        "}",
        "",
        "static void unregister_pernet_operations(struct pernet_operations *ops)",
        "{",
        "	__unregister_pernet_operations(ops);",
        "	rcu_barrier();",
        "	if (ops->id)",
        "		ida_free(&net_generic_ids, *ops->id);",
        "}",
        "",
        "/**",
        " *      register_pernet_subsys - register a network namespace subsystem",
        " *	@ops:  pernet operations structure for the subsystem",
        " *",
        " *	Register a subsystem which has init and exit functions",
        " *	that are called when network namespaces are created and",
        " *	destroyed respectively.",
        " *",
        " *	When registered all network namespace init functions are",
        " *	called for every existing network namespace.  Allowing kernel",
        " *	modules to have a race free view of the set of network namespaces.",
        " *",
        " *	When a new network namespace is created all of the init",
        " *	methods are called in the order in which they were registered.",
        " *",
        " *	When a network namespace is destroyed all of the exit methods",
        " *	are called in the reverse of the order with which they were",
        " *	registered.",
        " */",
        "int register_pernet_subsys(struct pernet_operations *ops)",
        "{",
        "	int error;",
        "	down_write(&pernet_ops_rwsem);",
        "	error =  register_pernet_operations(first_device, ops);",
        "	up_write(&pernet_ops_rwsem);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL_GPL(register_pernet_subsys);",
        "",
        "/**",
        " *      unregister_pernet_subsys - unregister a network namespace subsystem",
        " *	@ops: pernet operations structure to manipulate",
        " *",
        " *	Remove the pernet operations structure from the list to be",
        " *	used when network namespaces are created or destroyed.  In",
        " *	addition run the exit method for all existing network",
        " *	namespaces.",
        " */",
        "void unregister_pernet_subsys(struct pernet_operations *ops)",
        "{",
        "	down_write(&pernet_ops_rwsem);",
        "	unregister_pernet_operations(ops);",
        "	up_write(&pernet_ops_rwsem);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_pernet_subsys);",
        "",
        "/**",
        " *      register_pernet_device - register a network namespace device",
        " *	@ops:  pernet operations structure for the subsystem",
        " *",
        " *	Register a device which has init and exit functions",
        " *	that are called when network namespaces are created and",
        " *	destroyed respectively.",
        " *",
        " *	When registered all network namespace init functions are",
        " *	called for every existing network namespace.  Allowing kernel",
        " *	modules to have a race free view of the set of network namespaces.",
        " *",
        " *	When a new network namespace is created all of the init",
        " *	methods are called in the order in which they were registered.",
        " *",
        " *	When a network namespace is destroyed all of the exit methods",
        " *	are called in the reverse of the order with which they were",
        " *	registered.",
        " */",
        "int register_pernet_device(struct pernet_operations *ops)",
        "{",
        "	int error;",
        "	down_write(&pernet_ops_rwsem);",
        "	error = register_pernet_operations(&pernet_list, ops);",
        "	if (!error && (first_device == &pernet_list))",
        "		first_device = &ops->list;",
        "	up_write(&pernet_ops_rwsem);",
        "	return error;",
        "}",
        "EXPORT_SYMBOL_GPL(register_pernet_device);",
        "",
        "/**",
        " *      unregister_pernet_device - unregister a network namespace netdevice",
        " *	@ops: pernet operations structure to manipulate",
        " *",
        " *	Remove the pernet operations structure from the list to be",
        " *	used when network namespaces are created or destroyed.  In",
        " *	addition run the exit method for all existing network",
        " *	namespaces.",
        " */",
        "void unregister_pernet_device(struct pernet_operations *ops)",
        "{",
        "	down_write(&pernet_ops_rwsem);",
        "	if (&ops->list == first_device)",
        "		first_device = first_device->next;",
        "	unregister_pernet_operations(ops);",
        "	up_write(&pernet_ops_rwsem);",
        "}",
        "EXPORT_SYMBOL_GPL(unregister_pernet_device);",
        "",
        "#ifdef CONFIG_NET_NS",
        "static struct ns_common *netns_get(struct task_struct *task)",
        "{",
        "	struct net *net = NULL;",
        "	struct nsproxy *nsproxy;",
        "",
        "	task_lock(task);",
        "	nsproxy = task->nsproxy;",
        "	if (nsproxy)",
        "		net = get_net(nsproxy->net_ns);",
        "	task_unlock(task);",
        "",
        "	return net ? &net->ns : NULL;",
        "}",
        "",
        "static inline struct net *to_net_ns(struct ns_common *ns)",
        "{",
        "	return container_of(ns, struct net, ns);",
        "}",
        "",
        "static void netns_put(struct ns_common *ns)",
        "{",
        "	put_net(to_net_ns(ns));",
        "}",
        "",
        "static int netns_install(struct nsset *nsset, struct ns_common *ns)",
        "{",
        "	struct nsproxy *nsproxy = nsset->nsproxy;",
        "	struct net *net = to_net_ns(ns);",
        "",
        "	if (!ns_capable(net->user_ns, CAP_SYS_ADMIN) ||",
        "	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))",
        "		return -EPERM;",
        "",
        "	put_net(nsproxy->net_ns);",
        "	nsproxy->net_ns = get_net(net);",
        "	return 0;",
        "}",
        "",
        "static struct user_namespace *netns_owner(struct ns_common *ns)",
        "{",
        "	return to_net_ns(ns)->user_ns;",
        "}",
        "",
        "const struct proc_ns_operations netns_operations = {",
        "	.name		= \"net\",",
        "	.type		= CLONE_NEWNET,",
        "	.get		= netns_get,",
        "	.put		= netns_put,",
        "	.install	= netns_install,",
        "	.owner		= netns_owner,",
        "};",
        "#endif"
    ]
  },
  "include_linux_jump_label_h": {
    path: "include/linux/jump_label.h",
    covered: [207],
    totalLines: 540,
    coveredCount: 1,
    coveragePct: 0.2,
    source: [
        "/* SPDX-License-Identifier: GPL-2.0 */",
        "#ifndef _LINUX_JUMP_LABEL_H",
        "#define _LINUX_JUMP_LABEL_H",
        "",
        "/*",
        " * Jump label support",
        " *",
        " * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>",
        " * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra",
        " *",
        " * DEPRECATED API:",
        " *",
        " * The use of 'struct static_key' directly, is now DEPRECATED. In addition",
        " * static_key_{true,false}() is also DEPRECATED. IE DO NOT use the following:",
        " *",
        " * struct static_key false = STATIC_KEY_INIT_FALSE;",
        " * struct static_key true = STATIC_KEY_INIT_TRUE;",
        " * static_key_true()",
        " * static_key_false()",
        " *",
        " * The updated API replacements are:",
        " *",
        " * DEFINE_STATIC_KEY_TRUE(key);",
        " * DEFINE_STATIC_KEY_FALSE(key);",
        " * DEFINE_STATIC_KEY_ARRAY_TRUE(keys, count);",
        " * DEFINE_STATIC_KEY_ARRAY_FALSE(keys, count);",
        " * static_branch_likely()",
        " * static_branch_unlikely()",
        " *",
        " * Jump labels provide an interface to generate dynamic branches using",
        " * self-modifying code. Assuming toolchain and architecture support, if we",
        " * define a \"key\" that is initially false via \"DEFINE_STATIC_KEY_FALSE(key)\",",
        " * an \"if (static_branch_unlikely(&key))\" statement is an unconditional branch",
        " * (which defaults to false - and the true block is placed out of line).",
        " * Similarly, we can define an initially true key via",
        " * \"DEFINE_STATIC_KEY_TRUE(key)\", and use it in the same",
        " * \"if (static_branch_unlikely(&key))\", in which case we will generate an",
        " * unconditional branch to the out-of-line true branch. Keys that are",
        " * initially true or false can be using in both static_branch_unlikely()",
        " * and static_branch_likely() statements.",
        " *",
        " * At runtime we can change the branch target by setting the key",
        " * to true via a call to static_branch_enable(), or false using",
        " * static_branch_disable(). If the direction of the branch is switched by",
        " * these calls then we run-time modify the branch target via a",
        " * no-op -> jump or jump -> no-op conversion. For example, for an",
        " * initially false key that is used in an \"if (static_branch_unlikely(&key))\"",
        " * statement, setting the key to true requires us to patch in a jump",
        " * to the out-of-line of true branch.",
        " *",
        " * In addition to static_branch_{enable,disable}, we can also reference count",
        " * the key or branch direction via static_branch_{inc,dec}. Thus,",
        " * static_branch_inc() can be thought of as a 'make more true' and",
        " * static_branch_dec() as a 'make more false'.",
        " *",
        " * Since this relies on modifying code, the branch modifying functions",
        " * must be considered absolute slow paths (machine wide synchronization etc.).",
        " * OTOH, since the affected branches are unconditional, their runtime overhead",
        " * will be absolutely minimal, esp. in the default (off) case where the total",
        " * effect is a single NOP of appropriate size. The on case will patch in a jump",
        " * to the out-of-line block.",
        " *",
        " * When the control is directly exposed to userspace, it is prudent to delay the",
        " * decrement to avoid high frequency code modifications which can (and do)",
        " * cause significant performance degradation. Struct static_key_deferred and",
        " * static_key_slow_dec_deferred() provide for this.",
        " *",
        " * Lacking toolchain and or architecture support, static keys fall back to a",
        " * simple conditional branch.",
        " *",
        " * Additional babbling in: Documentation/staging/static-keys.rst",
        " */",
        "",
        "#ifndef __ASSEMBLY__",
        "",
        "#include <linux/types.h>",
        "#include <linux/compiler.h>",
        "",
        "extern bool static_key_initialized;",
        "",
        "#define STATIC_KEY_CHECK_USE(key) WARN(!static_key_initialized,		      \\",
        "				    \"%s(): static key '%pS' used before call to jump_label_init()\", \\",
        "				    __func__, (key))",
        "",
        "struct static_key {",
        "	atomic_t enabled;",
        "#ifdef CONFIG_JUMP_LABEL",
        "/*",
        " * Note:",
        " *   To make anonymous unions work with old compilers, the static",
        " *   initialization of them requires brackets. This creates a dependency",
        " *   on the order of the struct with the initializers. If any fields",
        " *   are added, STATIC_KEY_INIT_TRUE and STATIC_KEY_INIT_FALSE may need",
        " *   to be modified.",
        " *",
        " * bit 0 => 1 if key is initially true",
        " *	    0 if initially false",
        " * bit 1 => 1 if points to struct static_key_mod",
        " *	    0 if points to struct jump_entry",
        " */",
        "	union {",
        "		unsigned long type;",
        "		struct jump_entry *entries;",
        "		struct static_key_mod *next;",
        "	};",
        "#endif	/* CONFIG_JUMP_LABEL */",
        "};",
        "",
        "#endif /* __ASSEMBLY__ */",
        "",
        "#ifdef CONFIG_JUMP_LABEL",
        "#include <asm/jump_label.h>",
        "",
        "#ifndef __ASSEMBLY__",
        "#ifdef CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE",
        "",
        "struct jump_entry {",
        "	s32 code;",
        "	s32 target;",
        "	long key;	// key may be far away from the core kernel under KASLR",
        "};",
        "",
        "static inline unsigned long jump_entry_code(const struct jump_entry *entry)",
        "{",
        "	return (unsigned long)&entry->code + entry->code;",
        "}",
        "",
        "static inline unsigned long jump_entry_target(const struct jump_entry *entry)",
        "{",
        "	return (unsigned long)&entry->target + entry->target;",
        "}",
        "",
        "static inline struct static_key *jump_entry_key(const struct jump_entry *entry)",
        "{",
        "	long offset = entry->key & ~3L;",
        "",
        "	return (struct static_key *)((unsigned long)&entry->key + offset);",
        "}",
        "",
        "#else",
        "",
        "static inline unsigned long jump_entry_code(const struct jump_entry *entry)",
        "{",
        "	return entry->code;",
        "}",
        "",
        "static inline unsigned long jump_entry_target(const struct jump_entry *entry)",
        "{",
        "	return entry->target;",
        "}",
        "",
        "static inline struct static_key *jump_entry_key(const struct jump_entry *entry)",
        "{",
        "	return (struct static_key *)((unsigned long)entry->key & ~3UL);",
        "}",
        "",
        "#endif",
        "",
        "static inline bool jump_entry_is_branch(const struct jump_entry *entry)",
        "{",
        "	return (unsigned long)entry->key & 1UL;",
        "}",
        "",
        "static inline bool jump_entry_is_init(const struct jump_entry *entry)",
        "{",
        "	return (unsigned long)entry->key & 2UL;",
        "}",
        "",
        "static inline void jump_entry_set_init(struct jump_entry *entry, bool set)",
        "{",
        "	if (set)",
        "		entry->key |= 2;",
        "	else",
        "		entry->key &= ~2;",
        "}",
        "",
        "static inline int jump_entry_size(struct jump_entry *entry)",
        "{",
        "#ifdef JUMP_LABEL_NOP_SIZE",
        "	return JUMP_LABEL_NOP_SIZE;",
        "#else",
        "	return arch_jump_entry_size(entry);",
        "#endif",
        "}",
        "",
        "#endif",
        "#endif",
        "",
        "#ifndef __ASSEMBLY__",
        "",
        "enum jump_label_type {",
        "	JUMP_LABEL_NOP = 0,",
        "	JUMP_LABEL_JMP,",
        "};",
        "",
        "struct module;",
        "",
        "#ifdef CONFIG_JUMP_LABEL",
        "",
        "#define JUMP_TYPE_FALSE		0UL",
        "#define JUMP_TYPE_TRUE		1UL",
        "#define JUMP_TYPE_LINKED	2UL",
        "#define JUMP_TYPE_MASK		3UL",
        "",
        "static __always_inline bool static_key_false(struct static_key *key)",
        "{",
        "	return arch_static_branch(key, false);",
        "}",
        "",
        "static __always_inline bool static_key_true(struct static_key *key)",
        "{",
        "	return !arch_static_branch(key, true);",
        "}",
        "",
        "extern struct jump_entry __start___jump_table[];",
        "extern struct jump_entry __stop___jump_table[];",
        "",
        "extern void jump_label_init(void);",
        "extern void jump_label_init_ro(void);",
        "extern void jump_label_lock(void);",
        "extern void jump_label_unlock(void);",
        "extern void arch_jump_label_transform(struct jump_entry *entry,",
        "				      enum jump_label_type type);",
        "extern bool arch_jump_label_transform_queue(struct jump_entry *entry,",
        "					    enum jump_label_type type);",
        "extern void arch_jump_label_transform_apply(void);",
        "extern int jump_label_text_reserved(void *start, void *end);",
        "extern bool static_key_slow_inc(struct static_key *key);",
        "extern bool static_key_fast_inc_not_disabled(struct static_key *key);",
        "extern void static_key_slow_dec(struct static_key *key);",
        "extern bool static_key_slow_inc_cpuslocked(struct static_key *key);",
        "extern void static_key_slow_dec_cpuslocked(struct static_key *key);",
        "extern int static_key_count(struct static_key *key);",
        "extern void static_key_enable(struct static_key *key);",
        "extern void static_key_disable(struct static_key *key);",
        "extern void static_key_enable_cpuslocked(struct static_key *key);",
        "extern void static_key_disable_cpuslocked(struct static_key *key);",
        "extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);",
        "",
        "/*",
        " * We should be using ATOMIC_INIT() for initializing .enabled, but",
        " * the inclusion of atomic.h is problematic for inclusion of jump_label.h",
        " * in 'low-level' headers. Thus, we are initializing .enabled with a",
        " * raw value, but have added a BUILD_BUG_ON() to catch any issues in",
        " * jump_label_init() see: kernel/jump_label.c.",
        " */",
        "#define STATIC_KEY_INIT_TRUE					\\",
        "	{ .enabled = { 1 },					\\",
        "	  { .type = JUMP_TYPE_TRUE } }",
        "#define STATIC_KEY_INIT_FALSE					\\",
        "	{ .enabled = { 0 },					\\",
        "	  { .type = JUMP_TYPE_FALSE } }",
        "",
        "#else  /* !CONFIG_JUMP_LABEL */",
        "",
        "#include <linux/atomic.h>",
        "#include <linux/bug.h>",
        "",
        "static __always_inline int static_key_count(struct static_key *key)",
        "{",
        "	return raw_atomic_read(&key->enabled);",
        "}",
        "",
        "static __always_inline void jump_label_init(void)",
        "{",
        "	static_key_initialized = true;",
        "}",
        "",
        "static __always_inline void jump_label_init_ro(void) { }",
        "",
        "static __always_inline bool static_key_false(struct static_key *key)",
        "{",
        "	if (unlikely_notrace(static_key_count(key) > 0))",
        "		return true;",
        "	return false;",
        "}",
        "",
        "static __always_inline bool static_key_true(struct static_key *key)",
        "{",
        "	if (likely_notrace(static_key_count(key) > 0))",
        "		return true;",
        "	return false;",
        "}",
        "",
        "static inline bool static_key_fast_inc_not_disabled(struct static_key *key)",
        "{",
        "	int v;",
        "",
        "	STATIC_KEY_CHECK_USE(key);",
        "	/*",
        "	 * Prevent key->enabled getting negative to follow the same semantics",
        "	 * as for CONFIG_JUMP_LABEL=y, see kernel/jump_label.c comment.",
        "	 */",
        "	v = atomic_read(&key->enabled);",
        "	do {",
        "		if (v < 0 || (v + 1) < 0)",
        "			return false;",
        "	} while (!likely(atomic_try_cmpxchg(&key->enabled, &v, v + 1)));",
        "	return true;",
        "}",
        "#define static_key_slow_inc(key)	static_key_fast_inc_not_disabled(key)",
        "",
        "static inline void static_key_slow_dec(struct static_key *key)",
        "{",
        "	STATIC_KEY_CHECK_USE(key);",
        "	atomic_dec(&key->enabled);",
        "}",
        "",
        "#define static_key_slow_inc_cpuslocked(key) static_key_slow_inc(key)",
        "#define static_key_slow_dec_cpuslocked(key) static_key_slow_dec(key)",
        "",
        "static inline int jump_label_text_reserved(void *start, void *end)",
        "{",
        "	return 0;",
        "}",
        "",
        "static inline void jump_label_lock(void) {}",
        "static inline void jump_label_unlock(void) {}",
        "",
        "static inline void static_key_enable(struct static_key *key)",
        "{",
        "	STATIC_KEY_CHECK_USE(key);",
        "",
        "	if (atomic_read(&key->enabled) != 0) {",
        "		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);",
        "		return;",
        "	}",
        "	atomic_set(&key->enabled, 1);",
        "}",
        "",
        "static inline void static_key_disable(struct static_key *key)",
        "{",
        "	STATIC_KEY_CHECK_USE(key);",
        "",
        "	if (atomic_read(&key->enabled) != 1) {",
        "		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);",
        "		return;",
        "	}",
        "	atomic_set(&key->enabled, 0);",
        "}",
        "",
        "#define static_key_enable_cpuslocked(k)		static_key_enable((k))",
        "#define static_key_disable_cpuslocked(k)	static_key_disable((k))",
        "",
        "#define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }",
        "#define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }",
        "",
        "#endif	/* CONFIG_JUMP_LABEL */",
        "",
        "#define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE",
        "#define jump_label_enabled static_key_enabled",
        "",
        "/* -------------------------------------------------------------------------- */",
        "",
        "/*",
        " * Two type wrappers around static_key, such that we can use compile time",
        " * type differentiation to emit the right code.",
        " *",
        " * All the below code is macros in order to play type games.",
        " */",
        "",
        "struct static_key_true {",
        "	struct static_key key;",
        "};",
        "",
        "struct static_key_false {",
        "	struct static_key key;",
        "};",
        "",
        "#define STATIC_KEY_TRUE_INIT  (struct static_key_true) { .key = STATIC_KEY_INIT_TRUE,  }",
        "#define STATIC_KEY_FALSE_INIT (struct static_key_false){ .key = STATIC_KEY_INIT_FALSE, }",
        "",
        "#define DEFINE_STATIC_KEY_TRUE(name)	\\",
        "	struct static_key_true name = STATIC_KEY_TRUE_INIT",
        "",
        "#define DEFINE_STATIC_KEY_TRUE_RO(name)	\\",
        "	struct static_key_true name __ro_after_init = STATIC_KEY_TRUE_INIT",
        "",
        "#define DECLARE_STATIC_KEY_TRUE(name)	\\",
        "	extern struct static_key_true name",
        "",
        "#define DEFINE_STATIC_KEY_FALSE(name)	\\",
        "	struct static_key_false name = STATIC_KEY_FALSE_INIT",
        "",
        "#define DEFINE_STATIC_KEY_FALSE_RO(name)	\\",
        "	struct static_key_false name __ro_after_init = STATIC_KEY_FALSE_INIT",
        "",
        "#define DECLARE_STATIC_KEY_FALSE(name)	\\",
        "	extern struct static_key_false name",
        "",
        "#define DEFINE_STATIC_KEY_ARRAY_TRUE(name, count)		\\",
        "	struct static_key_true name[count] = {			\\",
        "		[0 ... (count) - 1] = STATIC_KEY_TRUE_INIT,	\\",
        "	}",
        "",
        "#define DEFINE_STATIC_KEY_ARRAY_FALSE(name, count)		\\",
        "	struct static_key_false name[count] = {			\\",
        "		[0 ... (count) - 1] = STATIC_KEY_FALSE_INIT,	\\",
        "	}",
        "",
        "#define _DEFINE_STATIC_KEY_1(name)	DEFINE_STATIC_KEY_TRUE(name)",
        "#define _DEFINE_STATIC_KEY_0(name)	DEFINE_STATIC_KEY_FALSE(name)",
        "#define DEFINE_STATIC_KEY_MAYBE(cfg, name)			\\",
        "	__PASTE(_DEFINE_STATIC_KEY_, IS_ENABLED(cfg))(name)",
        "",
        "#define _DEFINE_STATIC_KEY_RO_1(name)	DEFINE_STATIC_KEY_TRUE_RO(name)",
        "#define _DEFINE_STATIC_KEY_RO_0(name)	DEFINE_STATIC_KEY_FALSE_RO(name)",
        "#define DEFINE_STATIC_KEY_MAYBE_RO(cfg, name)			\\",
        "	__PASTE(_DEFINE_STATIC_KEY_RO_, IS_ENABLED(cfg))(name)",
        "",
        "#define _DECLARE_STATIC_KEY_1(name)	DECLARE_STATIC_KEY_TRUE(name)",
        "#define _DECLARE_STATIC_KEY_0(name)	DECLARE_STATIC_KEY_FALSE(name)",
        "#define DECLARE_STATIC_KEY_MAYBE(cfg, name)			\\",
        "	__PASTE(_DECLARE_STATIC_KEY_, IS_ENABLED(cfg))(name)",
        "",
        "extern bool ____wrong_branch_error(void);",
        "",
        "#define static_key_enabled(x)							\\",
        "({										\\",
        "	if (!__builtin_types_compatible_p(typeof(*x), struct static_key) &&	\\",
        "	    !__builtin_types_compatible_p(typeof(*x), struct static_key_true) &&\\",
        "	    !__builtin_types_compatible_p(typeof(*x), struct static_key_false))	\\",
        "		____wrong_branch_error();					\\",
        "	static_key_count((struct static_key *)x) > 0;				\\",
        "})",
        "",
        "#ifdef CONFIG_JUMP_LABEL",
        "",
        "/*",
        " * Combine the right initial value (type) with the right branch order",
        " * to generate the desired result.",
        " *",
        " *",
        " * type\\branch|	likely (1)	      |	unlikely (0)",
        " * -----------+-----------------------+------------------",
        " *            |                       |",
        " *  true (1)  |	   ...		      |	   ...",
        " *            |    NOP		      |	   JMP L",
        " *            |    <br-stmts>	      |	1: ...",
        " *            |	L: ...		      |",
        " *            |			      |",
        " *            |			      |	L: <br-stmts>",
        " *            |			      |	   jmp 1b",
        " *            |                       |",
        " * -----------+-----------------------+------------------",
        " *            |                       |",
        " *  false (0) |	   ...		      |	   ...",
        " *            |    JMP L	      |	   NOP",
        " *            |    <br-stmts>	      |	1: ...",
        " *            |	L: ...		      |",
        " *            |			      |",
        " *            |			      |	L: <br-stmts>",
        " *            |			      |	   jmp 1b",
        " *            |                       |",
        " * -----------+-----------------------+------------------",
        " *",
        " * The initial value is encoded in the LSB of static_key::entries,",
        " * type: 0 = false, 1 = true.",
        " *",
        " * The branch type is encoded in the LSB of jump_entry::key,",
        " * branch: 0 = unlikely, 1 = likely.",
        " *",
        " * This gives the following logic table:",
        " *",
        " *	enabled	type	branch	  instuction",
        " * -----------------------------+-----------",
        " *	0	0	0	| NOP",
        " *	0	0	1	| JMP",
        " *	0	1	0	| NOP",
        " *	0	1	1	| JMP",
        " *",
        " *	1	0	0	| JMP",
        " *	1	0	1	| NOP",
        " *	1	1	0	| JMP",
        " *	1	1	1	| NOP",
        " *",
        " * Which gives the following functions:",
        " *",
        " *   dynamic: instruction = enabled ^ branch",
        " *   static:  instruction = type ^ branch",
        " *",
        " * See jump_label_type() / jump_label_init_type().",
        " */",
        "",
        "#define static_branch_likely(x)							\\",
        "({										\\",
        "	bool branch;								\\",
        "	if (__builtin_types_compatible_p(typeof(*x), struct static_key_true))	\\",
        "		branch = !arch_static_branch(&(x)->key, true);			\\",
        "	else if (__builtin_types_compatible_p(typeof(*x), struct static_key_false)) \\",
        "		branch = !arch_static_branch_jump(&(x)->key, true);		\\",
        "	else									\\",
        "		branch = ____wrong_branch_error();				\\",
        "	likely_notrace(branch);								\\",
        "})",
        "",
        "#define static_branch_unlikely(x)						\\",
        "({										\\",
        "	bool branch;								\\",
        "	if (__builtin_types_compatible_p(typeof(*x), struct static_key_true))	\\",
        "		branch = arch_static_branch_jump(&(x)->key, false);		\\",
        "	else if (__builtin_types_compatible_p(typeof(*x), struct static_key_false)) \\",
        "		branch = arch_static_branch(&(x)->key, false);			\\",
        "	else									\\",
        "		branch = ____wrong_branch_error();				\\",
        "	unlikely_notrace(branch);							\\",
        "})",
        "",
        "#else /* !CONFIG_JUMP_LABEL */",
        "",
        "#define static_branch_likely(x)		likely_notrace(static_key_enabled(&(x)->key))",
        "#define static_branch_unlikely(x)	unlikely_notrace(static_key_enabled(&(x)->key))",
        "",
        "#endif /* CONFIG_JUMP_LABEL */",
        "",
        "#define static_branch_maybe(config, x)					\\",
        "	(IS_ENABLED(config) ? static_branch_likely(x)			\\",
        "			    : static_branch_unlikely(x))",
        "",
        "/*",
        " * Advanced usage; refcount, branch is enabled when: count != 0",
        " */",
        "",
        "#define static_branch_inc(x)		static_key_slow_inc(&(x)->key)",
        "#define static_branch_dec(x)		static_key_slow_dec(&(x)->key)",
        "#define static_branch_inc_cpuslocked(x)	static_key_slow_inc_cpuslocked(&(x)->key)",
        "#define static_branch_dec_cpuslocked(x)	static_key_slow_dec_cpuslocked(&(x)->key)",
        "",
        "/*",
        " * Normal usage; boolean enable/disable.",
        " */",
        "",
        "#define static_branch_enable(x)			static_key_enable(&(x)->key)",
        "#define static_branch_disable(x)		static_key_disable(&(x)->key)",
        "#define static_branch_enable_cpuslocked(x)	static_key_enable_cpuslocked(&(x)->key)",
        "#define static_branch_disable_cpuslocked(x)	static_key_disable_cpuslocked(&(x)->key)",
        "",
        "#endif /* __ASSEMBLY__ */",
        "",
        "#endif	/* _LINUX_JUMP_LABEL_H */"
    ]
  },
  "net_core_dev_c": {
    path: "net/core/dev.c",
    covered: [878, 10927, 10954, 883, 10966, 920, 916, 906, 10951, 882],
    totalLines: 12363,
    coveredCount: 10,
    coveragePct: 0.1,
    source: [
        "// SPDX-License-Identifier: GPL-2.0-or-later",
        "/*",
        " *      NET3    Protocol independent device support routines.",
        " *",
        " *	Derived from the non IP parts of dev.c 1.0.19",
        " *              Authors:	Ross Biro",
        " *				Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>",
        " *				Mark Evans, <evansmp@uhura.aston.ac.uk>",
        " *",
        " *	Additional Authors:",
        " *		Florian la Roche <rzsfl@rz.uni-sb.de>",
        " *		Alan Cox <gw4pts@gw4pts.ampr.org>",
        " *		David Hinds <dahinds@users.sourceforge.net>",
        " *		Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>",
        " *		Adam Sulmicki <adam@cfar.umd.edu>",
        " *              Pekka Riikonen <priikone@poesidon.pspt.fi>",
        " *",
        " *	Changes:",
        " *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set",
        " *                                      to 2 if register_netdev gets called",
        " *                                      before net_dev_init & also removed a",
        " *                                      few lines of code in the process.",
        " *		Alan Cox	:	device private ioctl copies fields back.",
        " *		Alan Cox	:	Transmit queue code does relevant",
        " *					stunts to keep the queue safe.",
        " *		Alan Cox	:	Fixed double lock.",
        " *		Alan Cox	:	Fixed promisc NULL pointer trap",
        " *		????????	:	Support the full private ioctl range",
        " *		Alan Cox	:	Moved ioctl permission check into",
        " *					drivers",
        " *		Tim Kordas	:	SIOCADDMULTI/SIOCDELMULTI",
        " *		Alan Cox	:	100 backlog just doesn't cut it when",
        " *					you start doing multicast video 8)",
        " *		Alan Cox	:	Rewrote net_bh and list manager.",
        " *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.",
        " *		Alan Cox	:	Took out transmit every packet pass",
        " *					Saved a few bytes in the ioctl handler",
        " *		Alan Cox	:	Network driver sets packet type before",
        " *					calling netif_rx. Saves a function",
        " *					call a packet.",
        " *		Alan Cox	:	Hashed net_bh()",
        " *		Richard Kooijman:	Timestamp fixes.",
        " *		Alan Cox	:	Wrong field in SIOCGIFDSTADDR",
        " *		Alan Cox	:	Device lock protection.",
        " *              Alan Cox        :       Fixed nasty side effect of device close",
        " *					changes.",
        " *		Rudi Cilibrasi	:	Pass the right thing to",
        " *					set_mac_address()",
        " *		Dave Miller	:	32bit quantity for the device lock to",
        " *					make it work out on a Sparc.",
        " *		Bjorn Ekwall	:	Added KERNELD hack.",
        " *		Alan Cox	:	Cleaned up the backlog initialise.",
        " *		Craig Metz	:	SIOCGIFCONF fix if space for under",
        " *					1 device.",
        " *	    Thomas Bogendoerfer :	Return ENODEV for dev_open, if there",
        " *					is no device open function.",
        " *		Andi Kleen	:	Fix error reporting for SIOCGIFCONF",
        " *	    Michael Chastain	:	Fix signed/unsigned for SIOCGIFCONF",
        " *		Cyrus Durgin	:	Cleaned for KMOD",
        " *		Adam Sulmicki   :	Bug Fix : Network Device Unload",
        " *					A network device unload needs to purge",
        " *					the backlog queue.",
        " *	Paul Rusty Russell	:	SIOCSIFNAME",
        " *              Pekka Riikonen  :	Netdev boot-time settings code",
        " *              Andrew Morton   :       Make unregister_netdevice wait",
        " *                                      indefinitely on dev->refcnt",
        " *              J Hadi Salim    :       - Backlog queue sampling",
        " *				        - netif_rx() feedback",
        " */",
        "",
        "#include <linux/uaccess.h>",
        "#include <linux/bitmap.h>",
        "#include <linux/capability.h>",
        "#include <linux/cpu.h>",
        "#include <linux/types.h>",
        "#include <linux/kernel.h>",
        "#include <linux/hash.h>",
        "#include <linux/slab.h>",
        "#include <linux/sched.h>",
        "#include <linux/sched/isolation.h>",
        "#include <linux/sched/mm.h>",
        "#include <linux/smpboot.h>",
        "#include <linux/mutex.h>",
        "#include <linux/rwsem.h>",
        "#include <linux/string.h>",
        "#include <linux/mm.h>",
        "#include <linux/socket.h>",
        "#include <linux/sockios.h>",
        "#include <linux/errno.h>",
        "#include <linux/interrupt.h>",
        "#include <linux/if_ether.h>",
        "#include <linux/netdevice.h>",
        "#include <linux/etherdevice.h>",
        "#include <linux/ethtool.h>",
        "#include <linux/skbuff.h>",
        "#include <linux/kthread.h>",
        "#include <linux/bpf.h>",
        "#include <linux/bpf_trace.h>",
        "#include <net/net_namespace.h>",
        "#include <net/sock.h>",
        "#include <net/busy_poll.h>",
        "#include <linux/rtnetlink.h>",
        "#include <linux/stat.h>",
        "#include <net/dsa.h>",
        "#include <net/dst.h>",
        "#include <net/dst_metadata.h>",
        "#include <net/gro.h>",
        "#include <net/pkt_sched.h>",
        "#include <net/pkt_cls.h>",
        "#include <net/checksum.h>",
        "#include <net/xfrm.h>",
        "#include <net/tcx.h>",
        "#include <linux/highmem.h>",
        "#include <linux/init.h>",
        "#include <linux/module.h>",
        "#include <linux/netpoll.h>",
        "#include <linux/rcupdate.h>",
        "#include <linux/delay.h>",
        "#include <net/iw_handler.h>",
        "#include <asm/current.h>",
        "#include <linux/audit.h>",
        "#include <linux/dmaengine.h>",
        "#include <linux/err.h>",
        "#include <linux/ctype.h>",
        "#include <linux/if_arp.h>",
        "#include <linux/if_vlan.h>",
        "#include <linux/ip.h>",
        "#include <net/ip.h>",
        "#include <net/mpls.h>",
        "#include <linux/ipv6.h>",
        "#include <linux/in.h>",
        "#include <linux/jhash.h>",
        "#include <linux/random.h>",
        "#include <trace/events/napi.h>",
        "#include <trace/events/net.h>",
        "#include <trace/events/skb.h>",
        "#include <trace/events/qdisc.h>",
        "#include <trace/events/xdp.h>",
        "#include <linux/inetdevice.h>",
        "#include <linux/cpu_rmap.h>",
        "#include <linux/static_key.h>",
        "#include <linux/hashtable.h>",
        "#include <linux/vmalloc.h>",
        "#include <linux/if_macvlan.h>",
        "#include <linux/errqueue.h>",
        "#include <linux/hrtimer.h>",
        "#include <linux/netfilter_netdev.h>",
        "#include <linux/crash_dump.h>",
        "#include <linux/sctp.h>",
        "#include <net/udp_tunnel.h>",
        "#include <linux/net_namespace.h>",
        "#include <linux/indirect_call_wrapper.h>",
        "#include <net/devlink.h>",
        "#include <linux/pm_runtime.h>",
        "#include <linux/prandom.h>",
        "#include <linux/once_lite.h>",
        "#include <net/netdev_rx_queue.h>",
        "#include <net/page_pool/types.h>",
        "#include <net/page_pool/helpers.h>",
        "#include <net/rps.h>",
        "#include <linux/phy_link_topology.h>",
        "",
        "#include \"dev.h\"",
        "#include \"devmem.h\"",
        "#include \"net-sysfs.h\"",
        "",
        "static DEFINE_SPINLOCK(ptype_lock);",
        "struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;",
        "",
        "static int netif_rx_internal(struct sk_buff *skb);",
        "static int call_netdevice_notifiers_extack(unsigned long val,",
        "					   struct net_device *dev,",
        "					   struct netlink_ext_ack *extack);",
        "",
        "static DEFINE_MUTEX(ifalias_mutex);",
        "",
        "/* protects napi_hash addition/deletion and napi_gen_id */",
        "static DEFINE_SPINLOCK(napi_hash_lock);",
        "",
        "static unsigned int napi_gen_id = NR_CPUS;",
        "static DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);",
        "",
        "static DECLARE_RWSEM(devnet_rename_sem);",
        "",
        "static inline void dev_base_seq_inc(struct net *net)",
        "{",
        "	unsigned int val = net->dev_base_seq + 1;",
        "",
        "	WRITE_ONCE(net->dev_base_seq, val ?: 1);",
        "}",
        "",
        "static inline struct hlist_head *dev_name_hash(struct net *net, const char *name)",
        "{",
        "	unsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));",
        "",
        "	return &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];",
        "}",
        "",
        "static inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)",
        "{",
        "	return &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];",
        "}",
        "",
        "#ifndef CONFIG_PREEMPT_RT",
        "",
        "static DEFINE_STATIC_KEY_FALSE(use_backlog_threads_key);",
        "",
        "static int __init setup_backlog_napi_threads(char *arg)",
        "{",
        "	static_branch_enable(&use_backlog_threads_key);",
        "	return 0;",
        "}",
        "early_param(\"thread_backlog_napi\", setup_backlog_napi_threads);",
        "",
        "static bool use_backlog_threads(void)",
        "{",
        "	return static_branch_unlikely(&use_backlog_threads_key);",
        "}",
        "",
        "#else",
        "",
        "static bool use_backlog_threads(void)",
        "{",
        "	return true;",
        "}",
        "",
        "#endif",
        "",
        "static inline void backlog_lock_irq_save(struct softnet_data *sd,",
        "					 unsigned long *flags)",
        "{",
        "	if (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())",
        "		spin_lock_irqsave(&sd->input_pkt_queue.lock, *flags);",
        "	else",
        "		local_irq_save(*flags);",
        "}",
        "",
        "static inline void backlog_lock_irq_disable(struct softnet_data *sd)",
        "{",
        "	if (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())",
        "		spin_lock_irq(&sd->input_pkt_queue.lock);",
        "	else",
        "		local_irq_disable();",
        "}",
        "",
        "static inline void backlog_unlock_irq_restore(struct softnet_data *sd,",
        "					      unsigned long *flags)",
        "{",
        "	if (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())",
        "		spin_unlock_irqrestore(&sd->input_pkt_queue.lock, *flags);",
        "	else",
        "		local_irq_restore(*flags);",
        "}",
        "",
        "static inline void backlog_unlock_irq_enable(struct softnet_data *sd)",
        "{",
        "	if (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())",
        "		spin_unlock_irq(&sd->input_pkt_queue.lock);",
        "	else",
        "		local_irq_enable();",
        "}",
        "",
        "static struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,",
        "						       const char *name)",
        "{",
        "	struct netdev_name_node *name_node;",
        "",
        "	name_node = kmalloc(sizeof(*name_node), GFP_KERNEL);",
        "	if (!name_node)",
        "		return NULL;",
        "	INIT_HLIST_NODE(&name_node->hlist);",
        "	name_node->dev = dev;",
        "	name_node->name = name;",
        "	return name_node;",
        "}",
        "",
        "static struct netdev_name_node *",
        "netdev_name_node_head_alloc(struct net_device *dev)",
        "{",
        "	struct netdev_name_node *name_node;",
        "",
        "	name_node = netdev_name_node_alloc(dev, dev->name);",
        "	if (!name_node)",
        "		return NULL;",
        "	INIT_LIST_HEAD(&name_node->list);",
        "	return name_node;",
        "}",
        "",
        "static void netdev_name_node_free(struct netdev_name_node *name_node)",
        "{",
        "	kfree(name_node);",
        "}",
        "",
        "static void netdev_name_node_add(struct net *net,",
        "				 struct netdev_name_node *name_node)",
        "{",
        "	hlist_add_head_rcu(&name_node->hlist,",
        "			   dev_name_hash(net, name_node->name));",
        "}",
        "",
        "static void netdev_name_node_del(struct netdev_name_node *name_node)",
        "{",
        "	hlist_del_rcu(&name_node->hlist);",
        "}",
        "",
        "static struct netdev_name_node *netdev_name_node_lookup(struct net *net,",
        "							const char *name)",
        "{",
        "	struct hlist_head *head = dev_name_hash(net, name);",
        "	struct netdev_name_node *name_node;",
        "",
        "	hlist_for_each_entry(name_node, head, hlist)",
        "		if (!strcmp(name_node->name, name))",
        "			return name_node;",
        "	return NULL;",
        "}",
        "",
        "static struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,",
        "							    const char *name)",
        "{",
        "	struct hlist_head *head = dev_name_hash(net, name);",
        "	struct netdev_name_node *name_node;",
        "",
        "	hlist_for_each_entry_rcu(name_node, head, hlist)",
        "		if (!strcmp(name_node->name, name))",
        "			return name_node;",
        "	return NULL;",
        "}",
        "",
        "bool netdev_name_in_use(struct net *net, const char *name)",
        "{",
        "	return netdev_name_node_lookup(net, name);",
        "}",
        "EXPORT_SYMBOL(netdev_name_in_use);",
        "",
        "int netdev_name_node_alt_create(struct net_device *dev, const char *name)",
        "{",
        "	struct netdev_name_node *name_node;",
        "	struct net *net = dev_net(dev);",
        "",
        "	name_node = netdev_name_node_lookup(net, name);",
        "	if (name_node)",
        "		return -EEXIST;",
        "	name_node = netdev_name_node_alloc(dev, name);",
        "	if (!name_node)",
        "		return -ENOMEM;",
        "	netdev_name_node_add(net, name_node);",
        "	/* The node that holds dev->name acts as a head of per-device list. */",
        "	list_add_tail_rcu(&name_node->list, &dev->name_node->list);",
        "",
        "	return 0;",
        "}",
        "",
        "static void netdev_name_node_alt_free(struct rcu_head *head)",
        "{",
        "	struct netdev_name_node *name_node =",
        "		container_of(head, struct netdev_name_node, rcu);",
        "",
        "	kfree(name_node->name);",
        "	netdev_name_node_free(name_node);",
        "}",
        "",
        "static void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)",
        "{",
        "	netdev_name_node_del(name_node);",
        "	list_del(&name_node->list);",
        "	call_rcu(&name_node->rcu, netdev_name_node_alt_free);",
        "}",
        "",
        "int netdev_name_node_alt_destroy(struct net_device *dev, const char *name)",
        "{",
        "	struct netdev_name_node *name_node;",
        "	struct net *net = dev_net(dev);",
        "",
        "	name_node = netdev_name_node_lookup(net, name);",
        "	if (!name_node)",
        "		return -ENOENT;",
        "	/* lookup might have found our primary name or a name belonging",
        "	 * to another device.",
        "	 */",
        "	if (name_node == dev->name_node || name_node->dev != dev)",
        "		return -EINVAL;",
        "",
        "	__netdev_name_node_alt_destroy(name_node);",
        "	return 0;",
        "}",
        "",
        "static void netdev_name_node_alt_flush(struct net_device *dev)",
        "{",
        "	struct netdev_name_node *name_node, *tmp;",
        "",
        "	list_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list) {",
        "		list_del(&name_node->list);",
        "		netdev_name_node_alt_free(&name_node->rcu);",
        "	}",
        "}",
        "",
        "/* Device list insertion */",
        "static void list_netdevice(struct net_device *dev)",
        "{",
        "	struct netdev_name_node *name_node;",
        "	struct net *net = dev_net(dev);",
        "",
        "	ASSERT_RTNL();",
        "",
        "	list_add_tail_rcu(&dev->dev_list, &net->dev_base_head);",
        "	netdev_name_node_add(net, dev->name_node);",
        "	hlist_add_head_rcu(&dev->index_hlist,",
        "			   dev_index_hash(net, dev->ifindex));",
        "",
        "	netdev_for_each_altname(dev, name_node)",
        "		netdev_name_node_add(net, name_node);",
        "",
        "	/* We reserved the ifindex, this can't fail */",
        "	WARN_ON(xa_store(&net->dev_by_index, dev->ifindex, dev, GFP_KERNEL));",
        "",
        "	dev_base_seq_inc(net);",
        "}",
        "",
        "/* Device list removal",
        " * caller must respect a RCU grace period before freeing/reusing dev",
        " */",
        "static void unlist_netdevice(struct net_device *dev)",
        "{",
        "	struct netdev_name_node *name_node;",
        "	struct net *net = dev_net(dev);",
        "",
        "	ASSERT_RTNL();",
        "",
        "	xa_erase(&net->dev_by_index, dev->ifindex);",
        "",
        "	netdev_for_each_altname(dev, name_node)",
        "		netdev_name_node_del(name_node);",
        "",
        "	/* Unlink dev from the device chain */",
        "	list_del_rcu(&dev->dev_list);",
        "	netdev_name_node_del(dev->name_node);",
        "	hlist_del_rcu(&dev->index_hlist);",
        "",
        "	dev_base_seq_inc(dev_net(dev));",
        "}",
        "",
        "/*",
        " *	Our notifier list",
        " */",
        "",
        "static RAW_NOTIFIER_HEAD(netdev_chain);",
        "",
        "/*",
        " *	Device drivers call our routines to queue packets here. We empty the",
        " *	queue in the local softnet handler.",
        " */",
        "",
        "DEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data) = {",
        "	.process_queue_bh_lock = INIT_LOCAL_LOCK(process_queue_bh_lock),",
        "};",
        "EXPORT_PER_CPU_SYMBOL(softnet_data);",
        "",
        "/* Page_pool has a lockless array/stack to alloc/recycle pages.",
        " * PP consumers must pay attention to run APIs in the appropriate context",
        " * (e.g. NAPI context).",
        " */",
        "static DEFINE_PER_CPU(struct page_pool *, system_page_pool);",
        "",
        "#ifdef CONFIG_LOCKDEP",
        "/*",
        " * register_netdevice() inits txq->_xmit_lock and sets lockdep class",
        " * according to dev->type",
        " */",
        "static const unsigned short netdev_lock_type[] = {",
        "	 ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,",
        "	 ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,",
        "	 ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,",
        "	 ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,",
        "	 ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,",
        "	 ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,",
        "	 ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,",
        "	 ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,",
        "	 ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,",
        "	 ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,",
        "	 ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,",
        "	 ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,",
        "	 ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,",
        "	 ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,",
        "	 ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};",
        "",
        "static const char *const netdev_lock_name[] = {",
        "	\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",",
        "	\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",",
        "	\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",",
        "	\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",",
        "	\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",",
        "	\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",",
        "	\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",",
        "	\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",",
        "	\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",",
        "	\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",",
        "	\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",",
        "	\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",",
        "	\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",",
        "	\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",",
        "	\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};",
        "",
        "static struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];",
        "static struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];",
        "",
        "static inline unsigned short netdev_lock_pos(unsigned short dev_type)",
        "{",
        "	int i;",
        "",
        "	for (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)",
        "		if (netdev_lock_type[i] == dev_type)",
        "			return i;",
        "	/* the last key is used by default */",
        "	return ARRAY_SIZE(netdev_lock_type) - 1;",
        "}",
        "",
        "static inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,",
        "						 unsigned short dev_type)",
        "{",
        "	int i;",
        "",
        "	i = netdev_lock_pos(dev_type);",
        "	lockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],",
        "				   netdev_lock_name[i]);",
        "}",
        "",
        "static inline void netdev_set_addr_lockdep_class(struct net_device *dev)",
        "{",
        "	int i;",
        "",
        "	i = netdev_lock_pos(dev->type);",
        "	lockdep_set_class_and_name(&dev->addr_list_lock,",
        "				   &netdev_addr_lock_key[i],",
        "				   netdev_lock_name[i]);",
        "}",
        "#else",
        "static inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,",
        "						 unsigned short dev_type)",
        "{",
        "}",
        "",
        "static inline void netdev_set_addr_lockdep_class(struct net_device *dev)",
        "{",
        "}",
        "#endif",
        "",
        "/*******************************************************************************",
        " *",
        " *		Protocol management and registration routines",
        " *",
        " *******************************************************************************/",
        "",
        "",
        "/*",
        " *	Add a protocol ID to the list. Now that the input handler is",
        " *	smarter we can dispense with all the messy stuff that used to be",
        " *	here.",
        " *",
        " *	BEWARE!!! Protocol handlers, mangling input packets,",
        " *	MUST BE last in hash buckets and checking protocol handlers",
        " *	MUST start from promiscuous ptype_all chain in net_bh.",
        " *	It is true now, do not change it.",
        " *	Explanation follows: if protocol handler, mangling packet, will",
        " *	be the first on list, it is not able to sense, that packet",
        " *	is cloned and should be copied-on-write, so that it will",
        " *	change it and subsequent readers will get broken packet.",
        " *							--ANK (980803)",
        " */",
        "",
        "static inline struct list_head *ptype_head(const struct packet_type *pt)",
        "{",
        "	if (pt->type == htons(ETH_P_ALL))",
        "		return pt->dev ? &pt->dev->ptype_all : &net_hotdata.ptype_all;",
        "	else",
        "		return pt->dev ? &pt->dev->ptype_specific :",
        "				 &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];",
        "}",
        "",
        "/**",
        " *	dev_add_pack - add packet handler",
        " *	@pt: packet type declaration",
        " *",
        " *	Add a protocol handler to the networking stack. The passed &packet_type",
        " *	is linked into kernel lists and may not be freed until it has been",
        " *	removed from the kernel lists.",
        " *",
        " *	This call does not sleep therefore it can not",
        " *	guarantee all CPU's that are in middle of receiving packets",
        " *	will see the new packet type (until the next received packet).",
        " */",
        "",
        "void dev_add_pack(struct packet_type *pt)",
        "{",
        "	struct list_head *head = ptype_head(pt);",
        "",
        "	spin_lock(&ptype_lock);",
        "	list_add_rcu(&pt->list, head);",
        "	spin_unlock(&ptype_lock);",
        "}",
        "EXPORT_SYMBOL(dev_add_pack);",
        "",
        "/**",
        " *	__dev_remove_pack	 - remove packet handler",
        " *	@pt: packet type declaration",
        " *",
        " *	Remove a protocol handler that was previously added to the kernel",
        " *	protocol handlers by dev_add_pack(). The passed &packet_type is removed",
        " *	from the kernel lists and can be freed or reused once this function",
        " *	returns.",
        " *",
        " *      The packet type might still be in use by receivers",
        " *	and must not be freed until after all the CPU's have gone",
        " *	through a quiescent state.",
        " */",
        "void __dev_remove_pack(struct packet_type *pt)",
        "{",
        "	struct list_head *head = ptype_head(pt);",
        "	struct packet_type *pt1;",
        "",
        "	spin_lock(&ptype_lock);",
        "",
        "	list_for_each_entry(pt1, head, list) {",
        "		if (pt == pt1) {",
        "			list_del_rcu(&pt->list);",
        "			goto out;",
        "		}",
        "	}",
        "",
        "	pr_warn(\"dev_remove_pack: %p not found\\n\", pt);",
        "out:",
        "	spin_unlock(&ptype_lock);",
        "}",
        "EXPORT_SYMBOL(__dev_remove_pack);",
        "",
        "/**",
        " *	dev_remove_pack	 - remove packet handler",
        " *	@pt: packet type declaration",
        " *",
        " *	Remove a protocol handler that was previously added to the kernel",
        " *	protocol handlers by dev_add_pack(). The passed &packet_type is removed",
        " *	from the kernel lists and can be freed or reused once this function",
        " *	returns.",
        " *",
        " *	This call sleeps to guarantee that no CPU is looking at the packet",
        " *	type after return.",
        " */",
        "void dev_remove_pack(struct packet_type *pt)",
        "{",
        "	__dev_remove_pack(pt);",
        "",
        "	synchronize_net();",
        "}",
        "EXPORT_SYMBOL(dev_remove_pack);",
        "",
        "",
        "/*******************************************************************************",
        " *",
        " *			    Device Interface Subroutines",
        " *",
        " *******************************************************************************/",
        "",
        "/**",
        " *	dev_get_iflink	- get 'iflink' value of a interface",
        " *	@dev: targeted interface",
        " *",
        " *	Indicates the ifindex the interface is linked to.",
        " *	Physical interfaces have the same 'ifindex' and 'iflink' values.",
        " */",
        "",
        "int dev_get_iflink(const struct net_device *dev)",
        "{",
        "	if (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)",
        "		return dev->netdev_ops->ndo_get_iflink(dev);",
        "",
        "	return READ_ONCE(dev->ifindex);",
        "}",
        "EXPORT_SYMBOL(dev_get_iflink);",
        "",
        "/**",
        " *	dev_fill_metadata_dst - Retrieve tunnel egress information.",
        " *	@dev: targeted interface",
        " *	@skb: The packet.",
        " *",
        " *	For better visibility of tunnel traffic OVS needs to retrieve",
        " *	egress tunnel information for a packet. Following API allows",
        " *	user to get this info.",
        " */",
        "int dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)",
        "{",
        "	struct ip_tunnel_info *info;",
        "",
        "	if (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)",
        "		return -EINVAL;",
        "",
        "	info = skb_tunnel_info_unclone(skb);",
        "	if (!info)",
        "		return -ENOMEM;",
        "	if (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))",
        "		return -EINVAL;",
        "",
        "	return dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);",
        "}",
        "EXPORT_SYMBOL_GPL(dev_fill_metadata_dst);",
        "",
        "static struct net_device_path *dev_fwd_path(struct net_device_path_stack *stack)",
        "{",
        "	int k = stack->num_paths++;",
        "",
        "	if (WARN_ON_ONCE(k >= NET_DEVICE_PATH_STACK_MAX))",
        "		return NULL;",
        "",
        "	return &stack->path[k];",
        "}",
        "",
        "int dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,",
        "			  struct net_device_path_stack *stack)",
        "{",
        "	const struct net_device *last_dev;",
        "	struct net_device_path_ctx ctx = {",
        "		.dev	= dev,",
        "	};",
        "	struct net_device_path *path;",
        "	int ret = 0;",
        "",
        "	memcpy(ctx.daddr, daddr, sizeof(ctx.daddr));",
        "	stack->num_paths = 0;",
        "	while (ctx.dev && ctx.dev->netdev_ops->ndo_fill_forward_path) {",
        "		last_dev = ctx.dev;",
        "		path = dev_fwd_path(stack);",
        "		if (!path)",
        "			return -1;",
        "",
        "		memset(path, 0, sizeof(struct net_device_path));",
        "		ret = ctx.dev->netdev_ops->ndo_fill_forward_path(&ctx, path);",
        "		if (ret < 0)",
        "			return -1;",
        "",
        "		if (WARN_ON_ONCE(last_dev == ctx.dev))",
        "			return -1;",
        "	}",
        "",
        "	if (!ctx.dev)",
        "		return ret;",
        "",
        "	path = dev_fwd_path(stack);",
        "	if (!path)",
        "		return -1;",
        "	path->type = DEV_PATH_ETHERNET;",
        "	path->dev = ctx.dev;",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL_GPL(dev_fill_forward_path);",
        "",
        "/* must be called under rcu_read_lock(), as we dont take a reference */",
        "static struct napi_struct *napi_by_id(unsigned int napi_id)",
        "{",
        "	unsigned int hash = napi_id % HASH_SIZE(napi_hash);",
        "	struct napi_struct *napi;",
        "",
        "	hlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)",
        "		if (napi->napi_id == napi_id)",
        "			return napi;",
        "",
        "	return NULL;",
        "}",
        "",
        "/* must be called under rcu_read_lock(), as we dont take a reference */",
        "struct napi_struct *netdev_napi_by_id(struct net *net, unsigned int napi_id)",
        "{",
        "	struct napi_struct *napi;",
        "",
        "	napi = napi_by_id(napi_id);",
        "	if (!napi)",
        "		return NULL;",
        "",
        "	if (WARN_ON_ONCE(!napi->dev))",
        "		return NULL;",
        "	if (!net_eq(net, dev_net(napi->dev)))",
        "		return NULL;",
        "",
        "	return napi;",
        "}",
        "",
        "/**",
        " *	__dev_get_by_name	- find a device by its name",
        " *	@net: the applicable net namespace",
        " *	@name: name to find",
        " *",
        " *	Find an interface by name. Must be called under RTNL semaphore.",
        " *	If the name is found a pointer to the device is returned.",
        " *	If the name is not found then %NULL is returned. The",
        " *	reference counters are not incremented so the caller must be",
        " *	careful with locks.",
        " */",
        "",
        "struct net_device *__dev_get_by_name(struct net *net, const char *name)",
        "{",
        "	struct netdev_name_node *node_name;",
        "",
        "	node_name = netdev_name_node_lookup(net, name);",
        "	return node_name ? node_name->dev : NULL;",
        "}",
        "EXPORT_SYMBOL(__dev_get_by_name);",
        "",
        "/**",
        " * dev_get_by_name_rcu	- find a device by its name",
        " * @net: the applicable net namespace",
        " * @name: name to find",
        " *",
        " * Find an interface by name.",
        " * If the name is found a pointer to the device is returned.",
        " * If the name is not found then %NULL is returned.",
        " * The reference counters are not incremented so the caller must be",
        " * careful with locks. The caller must hold RCU lock.",
        " */",
        "",
        "struct net_device *dev_get_by_name_rcu(struct net *net, const char *name)",
        "{",
        "	struct netdev_name_node *node_name;",
        "",
        "	node_name = netdev_name_node_lookup_rcu(net, name);",
        "	return node_name ? node_name->dev : NULL;",
        "}",
        "EXPORT_SYMBOL(dev_get_by_name_rcu);",
        "",
        "/* Deprecated for new users, call netdev_get_by_name() instead */",
        "struct net_device *dev_get_by_name(struct net *net, const char *name)",
        "{",
        "	struct net_device *dev;",
        "",
        "	rcu_read_lock();",
        "	dev = dev_get_by_name_rcu(net, name);",
        "	dev_hold(dev);",
        "	rcu_read_unlock();",
        "	return dev;",
        "}",
        "EXPORT_SYMBOL(dev_get_by_name);",
        "",
        "/**",
        " *	netdev_get_by_name() - find a device by its name",
        " *	@net: the applicable net namespace",
        " *	@name: name to find",
        " *	@tracker: tracking object for the acquired reference",
        " *	@gfp: allocation flags for the tracker",
        " *",
        " *	Find an interface by name. This can be called from any",
        " *	context and does its own locking. The returned handle has",
        " *	the usage count incremented and the caller must use netdev_put() to",
        " *	release it when it is no longer needed. %NULL is returned if no",
        " *	matching device is found.",
        " */",
        "struct net_device *netdev_get_by_name(struct net *net, const char *name,",
        "				      netdevice_tracker *tracker, gfp_t gfp)",
        "{",
        "	struct net_device *dev;",
        "",
        "	dev = dev_get_by_name(net, name);",
        "	if (dev)",
        "		netdev_tracker_alloc(dev, tracker, gfp);",
        "	return dev;",
        "}",
        "EXPORT_SYMBOL(netdev_get_by_name);",
        "",
        "/**",
        " *	__dev_get_by_index - find a device by its ifindex",
        " *	@net: the applicable net namespace",
        " *	@ifindex: index of device",
        " *",
        " *	Search for an interface by index. Returns %NULL if the device",
        " *	is not found or a pointer to the device. The device has not",
        " *	had its reference counter increased so the caller must be careful",
        " *	about locking. The caller must hold the RTNL semaphore.",
        " */",
        "",
        "struct net_device *__dev_get_by_index(struct net *net, int ifindex)",
        "{",
        "	struct net_device *dev;",
        "	struct hlist_head *head = dev_index_hash(net, ifindex);",
        "",
        "	hlist_for_each_entry(dev, head, index_hlist)",
        "		if (dev->ifindex == ifindex)",
        "			return dev;",
        "",
        "	return NULL;",
        "}",
        "EXPORT_SYMBOL(__dev_get_by_index);",
        "",
        "/**",
        " *	dev_get_by_index_rcu - find a device by its ifindex",
        " *	@net: the applicable net namespace",
        " *	@ifindex: index of device",
        " *",
        " *	Search for an interface by index. Returns %NULL if the device",
        " *	is not found or a pointer to the device. The device has not",
        " *	had its reference counter increased so the caller must be careful",
        " *	about locking. The caller must hold RCU lock.",
        " */",
        "",
        "struct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)",
        "{",
        "	struct net_device *dev;",
        "	struct hlist_head *head = dev_index_hash(net, ifindex);",
        "",
        "	hlist_for_each_entry_rcu(dev, head, index_hlist)",
        "		if (dev->ifindex == ifindex)",
        "			return dev;",
        "",
        "	return NULL;",
        "}",
        "EXPORT_SYMBOL(dev_get_by_index_rcu);",
        "",
        "/* Deprecated for new users, call netdev_get_by_index() instead */",
        "struct net_device *dev_get_by_index(struct net *net, int ifindex)",
        "{",
        "	struct net_device *dev;",
        "",
        "	rcu_read_lock();",
        "	dev = dev_get_by_index_rcu(net, ifindex);",
        "	dev_hold(dev);",
        "	rcu_read_unlock();",
        "	return dev;",
        "}",
        "EXPORT_SYMBOL(dev_get_by_index);",
        "",
        "/**",
        " *	netdev_get_by_index() - find a device by its ifindex",
        " *	@net: the applicable net namespace",
        " *	@ifindex: index of device",
        " *	@tracker: tracking object for the acquired reference",
        " *	@gfp: allocation flags for the tracker",
        " *",
        " *	Search for an interface by index. Returns NULL if the device",
        " *	is not found or a pointer to the device. The device returned has",
        " *	had a reference added and the pointer is safe until the user calls",
        " *	netdev_put() to indicate they have finished with it.",
        " */",
        "struct net_device *netdev_get_by_index(struct net *net, int ifindex,",
        "				       netdevice_tracker *tracker, gfp_t gfp)",
        "{",
        "	struct net_device *dev;",
        "",
        "	dev = dev_get_by_index(net, ifindex);",
        "	if (dev)",
        "		netdev_tracker_alloc(dev, tracker, gfp);",
        "	return dev;",
        "}",
        "EXPORT_SYMBOL(netdev_get_by_index);",
        "",
        "/**",
        " *	dev_get_by_napi_id - find a device by napi_id",
        " *	@napi_id: ID of the NAPI struct",
        " *",
        " *	Search for an interface by NAPI ID. Returns %NULL if the device",
        " *	is not found or a pointer to the device. The device has not had",
        " *	its reference counter increased so the caller must be careful",
        " *	about locking. The caller must hold RCU lock.",
        " */",
        "",
        "struct net_device *dev_get_by_napi_id(unsigned int napi_id)",
        "{",
        "	struct napi_struct *napi;",
        "",
        "	WARN_ON_ONCE(!rcu_read_lock_held());",
        "",
        "	if (napi_id < MIN_NAPI_ID)",
        "		return NULL;",
        "",
        "	napi = napi_by_id(napi_id);",
        "",
        "	return napi ? napi->dev : NULL;",
        "}",
        "EXPORT_SYMBOL(dev_get_by_napi_id);",
        "",
        "static DEFINE_SEQLOCK(netdev_rename_lock);",
        "",
        "void netdev_copy_name(struct net_device *dev, char *name)",
        "{",
        "	unsigned int seq;",
        "",
        "	do {",
        "		seq = read_seqbegin(&netdev_rename_lock);",
        "		strscpy(name, dev->name, IFNAMSIZ);",
        "	} while (read_seqretry(&netdev_rename_lock, seq));",
        "}",
        "",
        "/**",
        " *	netdev_get_name - get a netdevice name, knowing its ifindex.",
        " *	@net: network namespace",
        " *	@name: a pointer to the buffer where the name will be stored.",
        " *	@ifindex: the ifindex of the interface to get the name from.",
        " */",
        "int netdev_get_name(struct net *net, char *name, int ifindex)",
        "{",
        "	struct net_device *dev;",
        "	int ret;",
        "",
        "	rcu_read_lock();",
        "",
        "	dev = dev_get_by_index_rcu(net, ifindex);",
        "	if (!dev) {",
        "		ret = -ENODEV;",
        "		goto out;",
        "	}",
        "",
        "	netdev_copy_name(dev, name);",
        "",
        "	ret = 0;",
        "out:",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "",
        "static bool dev_addr_cmp(struct net_device *dev, unsigned short type,",
        "			 const char *ha)",
        "{",
        "	return dev->type == type && !memcmp(dev->dev_addr, ha, dev->addr_len);",
        "}",
        "",
        "/**",
        " *	dev_getbyhwaddr_rcu - find a device by its hardware address",
        " *	@net: the applicable net namespace",
        " *	@type: media type of device",
        " *	@ha: hardware address",
        " *",
        " *	Search for an interface by MAC address. Returns NULL if the device",
        " *	is not found or a pointer to the device.",
        " *	The caller must hold RCU.",
        " *	The returned device has not had its ref count increased",
        " *	and the caller must therefore be careful about locking",
        " *",
        " */",
        "",
        "struct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,",
        "				       const char *ha)",
        "{",
        "	struct net_device *dev;",
        "",
        "	for_each_netdev_rcu(net, dev)",
        "		if (dev_addr_cmp(dev, type, ha))",
        "			return dev;",
        "",
        "	return NULL;",
        "}",
        "EXPORT_SYMBOL(dev_getbyhwaddr_rcu);",
        "",
        "/**",
        " * dev_getbyhwaddr() - find a device by its hardware address",
        " * @net: the applicable net namespace",
        " * @type: media type of device",
        " * @ha: hardware address",
        " *",
        " * Similar to dev_getbyhwaddr_rcu(), but the owner needs to hold",
        " * rtnl_lock.",
        " *",
        " * Context: rtnl_lock() must be held.",
        " * Return: pointer to the net_device, or NULL if not found",
        " */",
        "struct net_device *dev_getbyhwaddr(struct net *net, unsigned short type,",
        "				   const char *ha)",
        "{",
        "	struct net_device *dev;",
        "",
        "	ASSERT_RTNL();",
        "	for_each_netdev(net, dev)",
        "		if (dev_addr_cmp(dev, type, ha))",
        "			return dev;",
        "",
        "	return NULL;",
        "}",
        "EXPORT_SYMBOL(dev_getbyhwaddr);",
        "",
        "struct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)",
        "{",
        "	struct net_device *dev, *ret = NULL;",
        "",
        "	rcu_read_lock();",
        "	for_each_netdev_rcu(net, dev)",
        "		if (dev->type == type) {",
        "			dev_hold(dev);",
        "			ret = dev;",
        "			break;",
        "		}",
        "	rcu_read_unlock();",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(dev_getfirstbyhwtype);",
        "",
        "/**",
        " *	__dev_get_by_flags - find any device with given flags",
        " *	@net: the applicable net namespace",
        " *	@if_flags: IFF_* values",
        " *	@mask: bitmask of bits in if_flags to check",
        " *",
        " *	Search for any interface with the given flags. Returns NULL if a device",
        " *	is not found or a pointer to the device. Must be called inside",
        " *	rtnl_lock(), and result refcount is unchanged.",
        " */",
        "",
        "struct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,",
        "				      unsigned short mask)",
        "{",
        "	struct net_device *dev, *ret;",
        "",
        "	ASSERT_RTNL();",
        "",
        "	ret = NULL;",
        "	for_each_netdev(net, dev) {",
        "		if (((dev->flags ^ if_flags) & mask) == 0) {",
        "			ret = dev;",
        "			break;",
        "		}",
        "	}",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(__dev_get_by_flags);",
        "",
        "/**",
        " *	dev_valid_name - check if name is okay for network device",
        " *	@name: name string",
        " *",
        " *	Network device names need to be valid file names to",
        " *	allow sysfs to work.  We also disallow any kind of",
        " *	whitespace.",
        " */",
        "bool dev_valid_name(const char *name)",
        "{",
        "	if (*name == '\\0')",
        "		return false;",
        "	if (strnlen(name, IFNAMSIZ) == IFNAMSIZ)",
        "		return false;",
        "	if (!strcmp(name, \".\") || !strcmp(name, \"..\"))",
        "		return false;",
        "",
        "	while (*name) {",
        "		if (*name == '/' || *name == ':' || isspace(*name))",
        "			return false;",
        "		name++;",
        "	}",
        "	return true;",
        "}",
        "EXPORT_SYMBOL(dev_valid_name);",
        "",
        "/**",
        " *	__dev_alloc_name - allocate a name for a device",
        " *	@net: network namespace to allocate the device name in",
        " *	@name: name format string",
        " *	@res: result name string",
        " *",
        " *	Passed a format string - eg \"lt%d\" it will try and find a suitable",
        " *	id. It scans list of devices to build up a free map, then chooses",
        " *	the first empty slot. The caller must hold the dev_base or rtnl lock",
        " *	while allocating the name and adding the device in order to avoid",
        " *	duplicates.",
        " *	Limited to bits_per_byte * page size devices (ie 32K on most platforms).",
        " *	Returns the number of the unit assigned or a negative errno code.",
        " */",
        "",
        "static int __dev_alloc_name(struct net *net, const char *name, char *res)",
        "{",
        "	int i = 0;",
        "	const char *p;",
        "	const int max_netdevices = 8*PAGE_SIZE;",
        "	unsigned long *inuse;",
        "	struct net_device *d;",
        "	char buf[IFNAMSIZ];",
        "",
        "	/* Verify the string as this thing may have come from the user.",
        "	 * There must be one \"%d\" and no other \"%\" characters.",
        "	 */",
        "	p = strchr(name, '%');",
        "	if (!p || p[1] != 'd' || strchr(p + 2, '%'))",
        "		return -EINVAL;",
        "",
        "	/* Use one page as a bit array of possible slots */",
        "	inuse = bitmap_zalloc(max_netdevices, GFP_ATOMIC);",
        "	if (!inuse)",
        "		return -ENOMEM;",
        "",
        "	for_each_netdev(net, d) {",
        "		struct netdev_name_node *name_node;",
        "",
        "		netdev_for_each_altname(d, name_node) {",
        "			if (!sscanf(name_node->name, name, &i))",
        "				continue;",
        "			if (i < 0 || i >= max_netdevices)",
        "				continue;",
        "",
        "			/* avoid cases where sscanf is not exact inverse of printf */",
        "			snprintf(buf, IFNAMSIZ, name, i);",
        "			if (!strncmp(buf, name_node->name, IFNAMSIZ))",
        "				__set_bit(i, inuse);",
        "		}",
        "		if (!sscanf(d->name, name, &i))",
        "			continue;",
        "		if (i < 0 || i >= max_netdevices)",
        "			continue;",
        "",
        "		/* avoid cases where sscanf is not exact inverse of printf */",
        "		snprintf(buf, IFNAMSIZ, name, i);",
        "		if (!strncmp(buf, d->name, IFNAMSIZ))",
        "			__set_bit(i, inuse);",
        "	}",
        "",
        "	i = find_first_zero_bit(inuse, max_netdevices);",
        "	bitmap_free(inuse);",
        "	if (i == max_netdevices)",
        "		return -ENFILE;",
        "",
        "	/* 'res' and 'name' could overlap, use 'buf' as an intermediate buffer */",
        "	strscpy(buf, name, IFNAMSIZ);",
        "	snprintf(res, IFNAMSIZ, buf, i);",
        "	return i;",
        "}",
        "",
        "/* Returns negative errno or allocated unit id (see __dev_alloc_name()) */",
        "static int dev_prep_valid_name(struct net *net, struct net_device *dev,",
        "			       const char *want_name, char *out_name,",
        "			       int dup_errno)",
        "{",
        "	if (!dev_valid_name(want_name))",
        "		return -EINVAL;",
        "",
        "	if (strchr(want_name, '%'))",
        "		return __dev_alloc_name(net, want_name, out_name);",
        "",
        "	if (netdev_name_in_use(net, want_name))",
        "		return -dup_errno;",
        "	if (out_name != want_name)",
        "		strscpy(out_name, want_name, IFNAMSIZ);",
        "	return 0;",
        "}",
        "",
        "/**",
        " *	dev_alloc_name - allocate a name for a device",
        " *	@dev: device",
        " *	@name: name format string",
        " *",
        " *	Passed a format string - eg \"lt%d\" it will try and find a suitable",
        " *	id. It scans list of devices to build up a free map, then chooses",
        " *	the first empty slot. The caller must hold the dev_base or rtnl lock",
        " *	while allocating the name and adding the device in order to avoid",
        " *	duplicates.",
        " *	Limited to bits_per_byte * page size devices (ie 32K on most platforms).",
        " *	Returns the number of the unit assigned or a negative errno code.",
        " */",
        "",
        "int dev_alloc_name(struct net_device *dev, const char *name)",
        "{",
        "	return dev_prep_valid_name(dev_net(dev), dev, name, dev->name, ENFILE);",
        "}",
        "EXPORT_SYMBOL(dev_alloc_name);",
        "",
        "static int dev_get_valid_name(struct net *net, struct net_device *dev,",
        "			      const char *name)",
        "{",
        "	int ret;",
        "",
        "	ret = dev_prep_valid_name(net, dev, name, dev->name, EEXIST);",
        "	return ret < 0 ? ret : 0;",
        "}",
        "",
        "/**",
        " *	dev_change_name - change name of a device",
        " *	@dev: device",
        " *	@newname: name (or format string) must be at least IFNAMSIZ",
        " *",
        " *	Change name of a device, can pass format strings \"eth%d\".",
        " *	for wildcarding.",
        " */",
        "int dev_change_name(struct net_device *dev, const char *newname)",
        "{",
        "	unsigned char old_assign_type;",
        "	char oldname[IFNAMSIZ];",
        "	int err = 0;",
        "	int ret;",
        "	struct net *net;",
        "",
        "	ASSERT_RTNL();",
        "	BUG_ON(!dev_net(dev));",
        "",
        "	net = dev_net(dev);",
        "",
        "	down_write(&devnet_rename_sem);",
        "",
        "	if (strncmp(newname, dev->name, IFNAMSIZ) == 0) {",
        "		up_write(&devnet_rename_sem);",
        "		return 0;",
        "	}",
        "",
        "	memcpy(oldname, dev->name, IFNAMSIZ);",
        "",
        "	write_seqlock_bh(&netdev_rename_lock);",
        "	err = dev_get_valid_name(net, dev, newname);",
        "	write_sequnlock_bh(&netdev_rename_lock);",
        "",
        "	if (err < 0) {",
        "		up_write(&devnet_rename_sem);",
        "		return err;",
        "	}",
        "",
        "	if (oldname[0] && !strchr(oldname, '%'))",
        "		netdev_info(dev, \"renamed from %s%s\\n\", oldname,",
        "			    dev->flags & IFF_UP ? \" (while UP)\" : \"\");",
        "",
        "	old_assign_type = dev->name_assign_type;",
        "	WRITE_ONCE(dev->name_assign_type, NET_NAME_RENAMED);",
        "",
        "rollback:",
        "	ret = device_rename(&dev->dev, dev->name);",
        "	if (ret) {",
        "		write_seqlock_bh(&netdev_rename_lock);",
        "		memcpy(dev->name, oldname, IFNAMSIZ);",
        "		write_sequnlock_bh(&netdev_rename_lock);",
        "		WRITE_ONCE(dev->name_assign_type, old_assign_type);",
        "		up_write(&devnet_rename_sem);",
        "		return ret;",
        "	}",
        "",
        "	up_write(&devnet_rename_sem);",
        "",
        "	netdev_adjacent_rename_links(dev, oldname);",
        "",
        "	netdev_name_node_del(dev->name_node);",
        "",
        "	synchronize_net();",
        "",
        "	netdev_name_node_add(net, dev->name_node);",
        "",
        "	ret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);",
        "	ret = notifier_to_errno(ret);",
        "",
        "	if (ret) {",
        "		/* err >= 0 after dev_alloc_name() or stores the first errno */",
        "		if (err >= 0) {",
        "			err = ret;",
        "			down_write(&devnet_rename_sem);",
        "			write_seqlock_bh(&netdev_rename_lock);",
        "			memcpy(dev->name, oldname, IFNAMSIZ);",
        "			write_sequnlock_bh(&netdev_rename_lock);",
        "			memcpy(oldname, newname, IFNAMSIZ);",
        "			WRITE_ONCE(dev->name_assign_type, old_assign_type);",
        "			old_assign_type = NET_NAME_RENAMED;",
        "			goto rollback;",
        "		} else {",
        "			netdev_err(dev, \"name change rollback failed: %d\\n\",",
        "				   ret);",
        "		}",
        "	}",
        "",
        "	return err;",
        "}",
        "",
        "/**",
        " *	dev_set_alias - change ifalias of a device",
        " *	@dev: device",
        " *	@alias: name up to IFALIASZ",
        " *	@len: limit of bytes to copy from info",
        " *",
        " *	Set ifalias for a device,",
        " */",
        "int dev_set_alias(struct net_device *dev, const char *alias, size_t len)",
        "{",
        "	struct dev_ifalias *new_alias = NULL;",
        "",
        "	if (len >= IFALIASZ)",
        "		return -EINVAL;",
        "",
        "	if (len) {",
        "		new_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);",
        "		if (!new_alias)",
        "			return -ENOMEM;",
        "",
        "		memcpy(new_alias->ifalias, alias, len);",
        "		new_alias->ifalias[len] = 0;",
        "	}",
        "",
        "	mutex_lock(&ifalias_mutex);",
        "	new_alias = rcu_replace_pointer(dev->ifalias, new_alias,",
        "					mutex_is_locked(&ifalias_mutex));",
        "	mutex_unlock(&ifalias_mutex);",
        "",
        "	if (new_alias)",
        "		kfree_rcu(new_alias, rcuhead);",
        "",
        "	return len;",
        "}",
        "EXPORT_SYMBOL(dev_set_alias);",
        "",
        "/**",
        " *	dev_get_alias - get ifalias of a device",
        " *	@dev: device",
        " *	@name: buffer to store name of ifalias",
        " *	@len: size of buffer",
        " *",
        " *	get ifalias for a device.  Caller must make sure dev cannot go",
        " *	away,  e.g. rcu read lock or own a reference count to device.",
        " */",
        "int dev_get_alias(const struct net_device *dev, char *name, size_t len)",
        "{",
        "	const struct dev_ifalias *alias;",
        "	int ret = 0;",
        "",
        "	rcu_read_lock();",
        "	alias = rcu_dereference(dev->ifalias);",
        "	if (alias)",
        "		ret = snprintf(name, len, \"%s\", alias->ifalias);",
        "	rcu_read_unlock();",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " *	netdev_features_change - device changes features",
        " *	@dev: device to cause notification",
        " *",
        " *	Called to indicate a device has changed features.",
        " */",
        "void netdev_features_change(struct net_device *dev)",
        "{",
        "	call_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);",
        "}",
        "EXPORT_SYMBOL(netdev_features_change);",
        "",
        "/**",
        " *	netdev_state_change - device changes state",
        " *	@dev: device to cause notification",
        " *",
        " *	Called to indicate a device has changed state. This function calls",
        " *	the notifier chains for netdev_chain and sends a NEWLINK message",
        " *	to the routing socket.",
        " */",
        "void netdev_state_change(struct net_device *dev)",
        "{",
        "	if (dev->flags & IFF_UP) {",
        "		struct netdev_notifier_change_info change_info = {",
        "			.info.dev = dev,",
        "		};",
        "",
        "		call_netdevice_notifiers_info(NETDEV_CHANGE,",
        "					      &change_info.info);",
        "		rtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL, 0, NULL);",
        "	}",
        "}",
        "EXPORT_SYMBOL(netdev_state_change);",
        "",
        "/**",
        " * __netdev_notify_peers - notify network peers about existence of @dev,",
        " * to be called when rtnl lock is already held.",
        " * @dev: network device",
        " *",
        " * Generate traffic such that interested network peers are aware of",
        " * @dev, such as by generating a gratuitous ARP. This may be used when",
        " * a device wants to inform the rest of the network about some sort of",
        " * reconfiguration such as a failover event or virtual machine",
        " * migration.",
        " */",
        "void __netdev_notify_peers(struct net_device *dev)",
        "{",
        "	ASSERT_RTNL();",
        "	call_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);",
        "	call_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);",
        "}",
        "EXPORT_SYMBOL(__netdev_notify_peers);",
        "",
        "/**",
        " * netdev_notify_peers - notify network peers about existence of @dev",
        " * @dev: network device",
        " *",
        " * Generate traffic such that interested network peers are aware of",
        " * @dev, such as by generating a gratuitous ARP. This may be used when",
        " * a device wants to inform the rest of the network about some sort of",
        " * reconfiguration such as a failover event or virtual machine",
        " * migration.",
        " */",
        "void netdev_notify_peers(struct net_device *dev)",
        "{",
        "	rtnl_lock();",
        "	__netdev_notify_peers(dev);",
        "	rtnl_unlock();",
        "}",
        "EXPORT_SYMBOL(netdev_notify_peers);",
        "",
        "static int napi_threaded_poll(void *data);",
        "",
        "static int napi_kthread_create(struct napi_struct *n)",
        "{",
        "	int err = 0;",
        "",
        "	/* Create and wake up the kthread once to put it in",
        "	 * TASK_INTERRUPTIBLE mode to avoid the blocked task",
        "	 * warning and work with loadavg.",
        "	 */",
        "	n->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",",
        "				n->dev->name, n->napi_id);",
        "	if (IS_ERR(n->thread)) {",
        "		err = PTR_ERR(n->thread);",
        "		pr_err(\"kthread_run failed with err %d\\n\", err);",
        "		n->thread = NULL;",
        "	}",
        "",
        "	return err;",
        "}",
        "",
        "static int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)",
        "{",
        "	const struct net_device_ops *ops = dev->netdev_ops;",
        "	int ret;",
        "",
        "	ASSERT_RTNL();",
        "	dev_addr_check(dev);",
        "",
        "	if (!netif_device_present(dev)) {",
        "		/* may be detached because parent is runtime-suspended */",
        "		if (dev->dev.parent)",
        "			pm_runtime_resume(dev->dev.parent);",
        "		if (!netif_device_present(dev))",
        "			return -ENODEV;",
        "	}",
        "",
        "	/* Block netpoll from trying to do any rx path servicing.",
        "	 * If we don't do this there is a chance ndo_poll_controller",
        "	 * or ndo_poll may be running while we open the device",
        "	 */",
        "	netpoll_poll_disable(dev);",
        "",
        "	ret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);",
        "	ret = notifier_to_errno(ret);",
        "	if (ret)",
        "		return ret;",
        "",
        "	set_bit(__LINK_STATE_START, &dev->state);",
        "",
        "	if (ops->ndo_validate_addr)",
        "		ret = ops->ndo_validate_addr(dev);",
        "",
        "	if (!ret && ops->ndo_open)",
        "		ret = ops->ndo_open(dev);",
        "",
        "	netpoll_poll_enable(dev);",
        "",
        "	if (ret)",
        "		clear_bit(__LINK_STATE_START, &dev->state);",
        "	else {",
        "		dev->flags |= IFF_UP;",
        "		dev_set_rx_mode(dev);",
        "		dev_activate(dev);",
        "		add_device_randomness(dev->dev_addr, dev->addr_len);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "/**",
        " *	dev_open	- prepare an interface for use.",
        " *	@dev: device to open",
        " *	@extack: netlink extended ack",
        " *",
        " *	Takes a device from down to up state. The device's private open",
        " *	function is invoked and then the multicast lists are loaded. Finally",
        " *	the device is moved into the up state and a %NETDEV_UP message is",
        " *	sent to the netdev notifier chain.",
        " *",
        " *	Calling this function on an active interface is a nop. On a failure",
        " *	a negative errno code is returned.",
        " */",
        "int dev_open(struct net_device *dev, struct netlink_ext_ack *extack)",
        "{",
        "	int ret;",
        "",
        "	if (dev->flags & IFF_UP)",
        "		return 0;",
        "",
        "	ret = __dev_open(dev, extack);",
        "	if (ret < 0)",
        "		return ret;",
        "",
        "	rtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);",
        "	call_netdevice_notifiers(NETDEV_UP, dev);",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(dev_open);",
        "",
        "static void __dev_close_many(struct list_head *head)",
        "{",
        "	struct net_device *dev;",
        "",
        "	ASSERT_RTNL();",
        "	might_sleep();",
        "",
        "	list_for_each_entry(dev, head, close_list) {",
        "		/* Temporarily disable netpoll until the interface is down */",
        "		netpoll_poll_disable(dev);",
        "",
        "		call_netdevice_notifiers(NETDEV_GOING_DOWN, dev);",
        "",
        "		clear_bit(__LINK_STATE_START, &dev->state);",
        "",
        "		/* Synchronize to scheduled poll. We cannot touch poll list, it",
        "		 * can be even on different cpu. So just clear netif_running().",
        "		 *",
        "		 * dev->stop() will invoke napi_disable() on all of it's",
        "		 * napi_struct instances on this device.",
        "		 */",
        "		smp_mb__after_atomic(); /* Commit netif_running(). */",
        "	}",
        "",
        "	dev_deactivate_many(head);",
        "",
        "	list_for_each_entry(dev, head, close_list) {",
        "		const struct net_device_ops *ops = dev->netdev_ops;",
        "",
        "		/*",
        "		 *	Call the device specific close. This cannot fail.",
        "		 *	Only if device is UP",
        "		 *",
        "		 *	We allow it to be called even after a DETACH hot-plug",
        "		 *	event.",
        "		 */",
        "		if (ops->ndo_stop)",
        "			ops->ndo_stop(dev);",
        "",
        "		dev->flags &= ~IFF_UP;",
        "		netpoll_poll_enable(dev);",
        "	}",
        "}",
        "",
        "static void __dev_close(struct net_device *dev)",
        "{",
        "	LIST_HEAD(single);",
        "",
        "	list_add(&dev->close_list, &single);",
        "	__dev_close_many(&single);",
        "	list_del(&single);",
        "}",
        "",
        "void dev_close_many(struct list_head *head, bool unlink)",
        "{",
        "	struct net_device *dev, *tmp;",
        "",
        "	/* Remove the devices that don't need to be closed */",
        "	list_for_each_entry_safe(dev, tmp, head, close_list)",
        "		if (!(dev->flags & IFF_UP))",
        "			list_del_init(&dev->close_list);",
        "",
        "	__dev_close_many(head);",
        "",
        "	list_for_each_entry_safe(dev, tmp, head, close_list) {",
        "		rtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);",
        "		call_netdevice_notifiers(NETDEV_DOWN, dev);",
        "		if (unlink)",
        "			list_del_init(&dev->close_list);",
        "	}",
        "}",
        "EXPORT_SYMBOL(dev_close_many);",
        "",
        "/**",
        " *	dev_close - shutdown an interface.",
        " *	@dev: device to shutdown",
        " *",
        " *	This function moves an active device into down state. A",
        " *	%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device",
        " *	is then deactivated and finally a %NETDEV_DOWN is sent to the notifier",
        " *	chain.",
        " */",
        "void dev_close(struct net_device *dev)",
        "{",
        "	if (dev->flags & IFF_UP) {",
        "		LIST_HEAD(single);",
        "",
        "		list_add(&dev->close_list, &single);",
        "		dev_close_many(&single, true);",
        "		list_del(&single);",
        "	}",
        "}",
        "EXPORT_SYMBOL(dev_close);",
        "",
        "",
        "/**",
        " *	dev_disable_lro - disable Large Receive Offload on a device",
        " *	@dev: device",
        " *",
        " *	Disable Large Receive Offload (LRO) on a net device.  Must be",
        " *	called under RTNL.  This is needed if received packets may be",
        " *	forwarded to another interface.",
        " */",
        "void dev_disable_lro(struct net_device *dev)",
        "{",
        "	struct net_device *lower_dev;",
        "	struct list_head *iter;",
        "",
        "	dev->wanted_features &= ~NETIF_F_LRO;",
        "	netdev_update_features(dev);",
        "",
        "	if (unlikely(dev->features & NETIF_F_LRO))",
        "		netdev_WARN(dev, \"failed to disable LRO!\\n\");",
        "",
        "	netdev_for_each_lower_dev(dev, lower_dev, iter)",
        "		dev_disable_lro(lower_dev);",
        "}",
        "EXPORT_SYMBOL(dev_disable_lro);",
        "",
        "/**",
        " *	dev_disable_gro_hw - disable HW Generic Receive Offload on a device",
        " *	@dev: device",
        " *",
        " *	Disable HW Generic Receive Offload (GRO_HW) on a net device.  Must be",
        " *	called under RTNL.  This is needed if Generic XDP is installed on",
        " *	the device.",
        " */",
        "static void dev_disable_gro_hw(struct net_device *dev)",
        "{",
        "	dev->wanted_features &= ~NETIF_F_GRO_HW;",
        "	netdev_update_features(dev);",
        "",
        "	if (unlikely(dev->features & NETIF_F_GRO_HW))",
        "		netdev_WARN(dev, \"failed to disable GRO_HW!\\n\");",
        "}",
        "",
        "const char *netdev_cmd_to_name(enum netdev_cmd cmd)",
        "{",
        "#define N(val) 						\\",
        "	case NETDEV_##val:				\\",
        "		return \"NETDEV_\" __stringify(val);",
        "	switch (cmd) {",
        "	N(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)",
        "	N(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)",
        "	N(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)",
        "	N(POST_INIT) N(PRE_UNINIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN)",
        "	N(CHANGEUPPER) N(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA)",
        "	N(BONDING_INFO) N(PRECHANGEUPPER) N(CHANGELOWERSTATE)",
        "	N(UDP_TUNNEL_PUSH_INFO) N(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)",
        "	N(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)",
        "	N(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)",
        "	N(PRE_CHANGEADDR) N(OFFLOAD_XSTATS_ENABLE) N(OFFLOAD_XSTATS_DISABLE)",
        "	N(OFFLOAD_XSTATS_REPORT_USED) N(OFFLOAD_XSTATS_REPORT_DELTA)",
        "	N(XDP_FEAT_CHANGE)",
        "	}",
        "#undef N",
        "	return \"UNKNOWN_NETDEV_EVENT\";",
        "}",
        "EXPORT_SYMBOL_GPL(netdev_cmd_to_name);",
        "",
        "static int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,",
        "				   struct net_device *dev)",
        "{",
        "	struct netdev_notifier_info info = {",
        "		.dev = dev,",
        "	};",
        "",
        "	return nb->notifier_call(nb, val, &info);",
        "}",
        "",
        "static int call_netdevice_register_notifiers(struct notifier_block *nb,",
        "					     struct net_device *dev)",
        "{",
        "	int err;",
        "",
        "	err = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);",
        "	err = notifier_to_errno(err);",
        "	if (err)",
        "		return err;",
        "",
        "	if (!(dev->flags & IFF_UP))",
        "		return 0;",
        "",
        "	call_netdevice_notifier(nb, NETDEV_UP, dev);",
        "	return 0;",
        "}",
        "",
        "static void call_netdevice_unregister_notifiers(struct notifier_block *nb,",
        "						struct net_device *dev)",
        "{",
        "	if (dev->flags & IFF_UP) {",
        "		call_netdevice_notifier(nb, NETDEV_GOING_DOWN,",
        "					dev);",
        "		call_netdevice_notifier(nb, NETDEV_DOWN, dev);",
        "	}",
        "	call_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);",
        "}",
        "",
        "static int call_netdevice_register_net_notifiers(struct notifier_block *nb,",
        "						 struct net *net)",
        "{",
        "	struct net_device *dev;",
        "	int err;",
        "",
        "	for_each_netdev(net, dev) {",
        "		err = call_netdevice_register_notifiers(nb, dev);",
        "		if (err)",
        "			goto rollback;",
        "	}",
        "	return 0;",
        "",
        "rollback:",
        "	for_each_netdev_continue_reverse(net, dev)",
        "		call_netdevice_unregister_notifiers(nb, dev);",
        "	return err;",
        "}",
        "",
        "static void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,",
        "						    struct net *net)",
        "{",
        "	struct net_device *dev;",
        "",
        "	for_each_netdev(net, dev)",
        "		call_netdevice_unregister_notifiers(nb, dev);",
        "}",
        "",
        "static int dev_boot_phase = 1;",
        "",
        "/**",
        " * register_netdevice_notifier - register a network notifier block",
        " * @nb: notifier",
        " *",
        " * Register a notifier to be called when network device events occur.",
        " * The notifier passed is linked into the kernel structures and must",
        " * not be reused until it has been unregistered. A negative errno code",
        " * is returned on a failure.",
        " *",
        " * When registered all registration and up events are replayed",
        " * to the new notifier to allow device to have a race free",
        " * view of the network device list.",
        " */",
        "",
        "int register_netdevice_notifier(struct notifier_block *nb)",
        "{",
        "	struct net *net;",
        "	int err;",
        "",
        "	/* Close race with setup_net() and cleanup_net() */",
        "	down_write(&pernet_ops_rwsem);",
        "	rtnl_lock();",
        "	err = raw_notifier_chain_register(&netdev_chain, nb);",
        "	if (err)",
        "		goto unlock;",
        "	if (dev_boot_phase)",
        "		goto unlock;",
        "	for_each_net(net) {",
        "		err = call_netdevice_register_net_notifiers(nb, net);",
        "		if (err)",
        "			goto rollback;",
        "	}",
        "",
        "unlock:",
        "	rtnl_unlock();",
        "	up_write(&pernet_ops_rwsem);",
        "	return err;",
        "",
        "rollback:",
        "	for_each_net_continue_reverse(net)",
        "		call_netdevice_unregister_net_notifiers(nb, net);",
        "",
        "	raw_notifier_chain_unregister(&netdev_chain, nb);",
        "	goto unlock;",
        "}",
        "EXPORT_SYMBOL(register_netdevice_notifier);",
        "",
        "/**",
        " * unregister_netdevice_notifier - unregister a network notifier block",
        " * @nb: notifier",
        " *",
        " * Unregister a notifier previously registered by",
        " * register_netdevice_notifier(). The notifier is unlinked into the",
        " * kernel structures and may then be reused. A negative errno code",
        " * is returned on a failure.",
        " *",
        " * After unregistering unregister and down device events are synthesized",
        " * for all devices on the device list to the removed notifier to remove",
        " * the need for special case cleanup code.",
        " */",
        "",
        "int unregister_netdevice_notifier(struct notifier_block *nb)",
        "{",
        "	struct net *net;",
        "	int err;",
        "",
        "	/* Close race with setup_net() and cleanup_net() */",
        "	down_write(&pernet_ops_rwsem);",
        "	rtnl_lock();",
        "	err = raw_notifier_chain_unregister(&netdev_chain, nb);",
        "	if (err)",
        "		goto unlock;",
        "",
        "	for_each_net(net)",
        "		call_netdevice_unregister_net_notifiers(nb, net);",
        "",
        "unlock:",
        "	rtnl_unlock();",
        "	up_write(&pernet_ops_rwsem);",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(unregister_netdevice_notifier);",
        "",
        "static int __register_netdevice_notifier_net(struct net *net,",
        "					     struct notifier_block *nb,",
        "					     bool ignore_call_fail)",
        "{",
        "	int err;",
        "",
        "	err = raw_notifier_chain_register(&net->netdev_chain, nb);",
        "	if (err)",
        "		return err;",
        "	if (dev_boot_phase)",
        "		return 0;",
        "",
        "	err = call_netdevice_register_net_notifiers(nb, net);",
        "	if (err && !ignore_call_fail)",
        "		goto chain_unregister;",
        "",
        "	return 0;",
        "",
        "chain_unregister:",
        "	raw_notifier_chain_unregister(&net->netdev_chain, nb);",
        "	return err;",
        "}",
        "",
        "static int __unregister_netdevice_notifier_net(struct net *net,",
        "					       struct notifier_block *nb)",
        "{",
        "	int err;",
        "",
        "	err = raw_notifier_chain_unregister(&net->netdev_chain, nb);",
        "	if (err)",
        "		return err;",
        "",
        "	call_netdevice_unregister_net_notifiers(nb, net);",
        "	return 0;",
        "}",
        "",
        "/**",
        " * register_netdevice_notifier_net - register a per-netns network notifier block",
        " * @net: network namespace",
        " * @nb: notifier",
        " *",
        " * Register a notifier to be called when network device events occur.",
        " * The notifier passed is linked into the kernel structures and must",
        " * not be reused until it has been unregistered. A negative errno code",
        " * is returned on a failure.",
        " *",
        " * When registered all registration and up events are replayed",
        " * to the new notifier to allow device to have a race free",
        " * view of the network device list.",
        " */",
        "",
        "int register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)",
        "{",
        "	int err;",
        "",
        "	rtnl_lock();",
        "	err = __register_netdevice_notifier_net(net, nb, false);",
        "	rtnl_unlock();",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(register_netdevice_notifier_net);",
        "",
        "/**",
        " * unregister_netdevice_notifier_net - unregister a per-netns",
        " *                                     network notifier block",
        " * @net: network namespace",
        " * @nb: notifier",
        " *",
        " * Unregister a notifier previously registered by",
        " * register_netdevice_notifier_net(). The notifier is unlinked from the",
        " * kernel structures and may then be reused. A negative errno code",
        " * is returned on a failure.",
        " *",
        " * After unregistering unregister and down device events are synthesized",
        " * for all devices on the device list to the removed notifier to remove",
        " * the need for special case cleanup code.",
        " */",
        "",
        "int unregister_netdevice_notifier_net(struct net *net,",
        "				      struct notifier_block *nb)",
        "{",
        "	int err;",
        "",
        "	rtnl_lock();",
        "	err = __unregister_netdevice_notifier_net(net, nb);",
        "	rtnl_unlock();",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(unregister_netdevice_notifier_net);",
        "",
        "static void __move_netdevice_notifier_net(struct net *src_net,",
        "					  struct net *dst_net,",
        "					  struct notifier_block *nb)",
        "{",
        "	__unregister_netdevice_notifier_net(src_net, nb);",
        "	__register_netdevice_notifier_net(dst_net, nb, true);",
        "}",
        "",
        "int register_netdevice_notifier_dev_net(struct net_device *dev,",
        "					struct notifier_block *nb,",
        "					struct netdev_net_notifier *nn)",
        "{",
        "	int err;",
        "",
        "	rtnl_lock();",
        "	err = __register_netdevice_notifier_net(dev_net(dev), nb, false);",
        "	if (!err) {",
        "		nn->nb = nb;",
        "		list_add(&nn->list, &dev->net_notifier_list);",
        "	}",
        "	rtnl_unlock();",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(register_netdevice_notifier_dev_net);",
        "",
        "int unregister_netdevice_notifier_dev_net(struct net_device *dev,",
        "					  struct notifier_block *nb,",
        "					  struct netdev_net_notifier *nn)",
        "{",
        "	int err;",
        "",
        "	rtnl_lock();",
        "	list_del(&nn->list);",
        "	err = __unregister_netdevice_notifier_net(dev_net(dev), nb);",
        "	rtnl_unlock();",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);",
        "",
        "static void move_netdevice_notifiers_dev_net(struct net_device *dev,",
        "					     struct net *net)",
        "{",
        "	struct netdev_net_notifier *nn;",
        "",
        "	list_for_each_entry(nn, &dev->net_notifier_list, list)",
        "		__move_netdevice_notifier_net(dev_net(dev), net, nn->nb);",
        "}",
        "",
        "/**",
        " *	call_netdevice_notifiers_info - call all network notifier blocks",
        " *	@val: value passed unmodified to notifier function",
        " *	@info: notifier information data",
        " *",
        " *	Call all network notifier blocks.  Parameters and return value",
        " *	are as for raw_notifier_call_chain().",
        " */",
        "",
        "int call_netdevice_notifiers_info(unsigned long val,",
        "				  struct netdev_notifier_info *info)",
        "{",
        "	struct net *net = dev_net(info->dev);",
        "	int ret;",
        "",
        "	ASSERT_RTNL();",
        "",
        "	/* Run per-netns notifier block chain first, then run the global one.",
        "	 * Hopefully, one day, the global one is going to be removed after",
        "	 * all notifier block registrators get converted to be per-netns.",
        "	 */",
        "	ret = raw_notifier_call_chain(&net->netdev_chain, val, info);",
        "	if (ret & NOTIFY_STOP_MASK)",
        "		return ret;",
        "	return raw_notifier_call_chain(&netdev_chain, val, info);",
        "}",
        "",
        "/**",
        " *	call_netdevice_notifiers_info_robust - call per-netns notifier blocks",
        " *	                                       for and rollback on error",
        " *	@val_up: value passed unmodified to notifier function",
        " *	@val_down: value passed unmodified to the notifier function when",
        " *	           recovering from an error on @val_up",
        " *	@info: notifier information data",
        " *",
        " *	Call all per-netns network notifier blocks, but not notifier blocks on",
        " *	the global notifier chain. Parameters and return value are as for",
        " *	raw_notifier_call_chain_robust().",
        " */",
        "",
        "static int",
        "call_netdevice_notifiers_info_robust(unsigned long val_up,",
        "				     unsigned long val_down,",
        "				     struct netdev_notifier_info *info)",
        "{",
        "	struct net *net = dev_net(info->dev);",
        "",
        "	ASSERT_RTNL();",
        "",
        "	return raw_notifier_call_chain_robust(&net->netdev_chain,",
        "					      val_up, val_down, info);",
        "}",
        "",
        "static int call_netdevice_notifiers_extack(unsigned long val,",
        "					   struct net_device *dev,",
        "					   struct netlink_ext_ack *extack)",
        "{",
        "	struct netdev_notifier_info info = {",
        "		.dev = dev,",
        "		.extack = extack,",
        "	};",
        "",
        "	return call_netdevice_notifiers_info(val, &info);",
        "}",
        "",
        "/**",
        " *	call_netdevice_notifiers - call all network notifier blocks",
        " *      @val: value passed unmodified to notifier function",
        " *      @dev: net_device pointer passed unmodified to notifier function",
        " *",
        " *	Call all network notifier blocks.  Parameters and return value",
        " *	are as for raw_notifier_call_chain().",
        " */",
        "",
        "int call_netdevice_notifiers(unsigned long val, struct net_device *dev)",
        "{",
        "	return call_netdevice_notifiers_extack(val, dev, NULL);",
        "}",
        "EXPORT_SYMBOL(call_netdevice_notifiers);",
        "",
        "/**",
        " *	call_netdevice_notifiers_mtu - call all network notifier blocks",
        " *	@val: value passed unmodified to notifier function",
        " *	@dev: net_device pointer passed unmodified to notifier function",
        " *	@arg: additional u32 argument passed to the notifier function",
        " *",
        " *	Call all network notifier blocks.  Parameters and return value",
        " *	are as for raw_notifier_call_chain().",
        " */",
        "static int call_netdevice_notifiers_mtu(unsigned long val,",
        "					struct net_device *dev, u32 arg)",
        "{",
        "	struct netdev_notifier_info_ext info = {",
        "		.info.dev = dev,",
        "		.ext.mtu = arg,",
        "	};",
        "",
        "	BUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);",
        "",
        "	return call_netdevice_notifiers_info(val, &info.info);",
        "}",
        "",
        "#ifdef CONFIG_NET_INGRESS",
        "static DEFINE_STATIC_KEY_FALSE(ingress_needed_key);",
        "",
        "void net_inc_ingress_queue(void)",
        "{",
        "	static_branch_inc(&ingress_needed_key);",
        "}",
        "EXPORT_SYMBOL_GPL(net_inc_ingress_queue);",
        "",
        "void net_dec_ingress_queue(void)",
        "{",
        "	static_branch_dec(&ingress_needed_key);",
        "}",
        "EXPORT_SYMBOL_GPL(net_dec_ingress_queue);",
        "#endif",
        "",
        "#ifdef CONFIG_NET_EGRESS",
        "static DEFINE_STATIC_KEY_FALSE(egress_needed_key);",
        "",
        "void net_inc_egress_queue(void)",
        "{",
        "	static_branch_inc(&egress_needed_key);",
        "}",
        "EXPORT_SYMBOL_GPL(net_inc_egress_queue);",
        "",
        "void net_dec_egress_queue(void)",
        "{",
        "	static_branch_dec(&egress_needed_key);",
        "}",
        "EXPORT_SYMBOL_GPL(net_dec_egress_queue);",
        "#endif",
        "",
        "#ifdef CONFIG_NET_CLS_ACT",
        "DEFINE_STATIC_KEY_FALSE(tcf_sw_enabled_key);",
        "EXPORT_SYMBOL(tcf_sw_enabled_key);",
        "#endif",
        "",
        "DEFINE_STATIC_KEY_FALSE(netstamp_needed_key);",
        "EXPORT_SYMBOL(netstamp_needed_key);",
        "#ifdef CONFIG_JUMP_LABEL",
        "static atomic_t netstamp_needed_deferred;",
        "static atomic_t netstamp_wanted;",
        "static void netstamp_clear(struct work_struct *work)",
        "{",
        "	int deferred = atomic_xchg(&netstamp_needed_deferred, 0);",
        "	int wanted;",
        "",
        "	wanted = atomic_add_return(deferred, &netstamp_wanted);",
        "	if (wanted > 0)",
        "		static_branch_enable(&netstamp_needed_key);",
        "	else",
        "		static_branch_disable(&netstamp_needed_key);",
        "}",
        "static DECLARE_WORK(netstamp_work, netstamp_clear);",
        "#endif",
        "",
        "void net_enable_timestamp(void)",
        "{",
        "#ifdef CONFIG_JUMP_LABEL",
        "	int wanted = atomic_read(&netstamp_wanted);",
        "",
        "	while (wanted > 0) {",
        "		if (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted + 1))",
        "			return;",
        "	}",
        "	atomic_inc(&netstamp_needed_deferred);",
        "	schedule_work(&netstamp_work);",
        "#else",
        "	static_branch_inc(&netstamp_needed_key);",
        "#endif",
        "}",
        "EXPORT_SYMBOL(net_enable_timestamp);",
        "",
        "void net_disable_timestamp(void)",
        "{",
        "#ifdef CONFIG_JUMP_LABEL",
        "	int wanted = atomic_read(&netstamp_wanted);",
        "",
        "	while (wanted > 1) {",
        "		if (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted - 1))",
        "			return;",
        "	}",
        "	atomic_dec(&netstamp_needed_deferred);",
        "	schedule_work(&netstamp_work);",
        "#else",
        "	static_branch_dec(&netstamp_needed_key);",
        "#endif",
        "}",
        "EXPORT_SYMBOL(net_disable_timestamp);",
        "",
        "static inline void net_timestamp_set(struct sk_buff *skb)",
        "{",
        "	skb->tstamp = 0;",
        "	skb->tstamp_type = SKB_CLOCK_REALTIME;",
        "	if (static_branch_unlikely(&netstamp_needed_key))",
        "		skb->tstamp = ktime_get_real();",
        "}",
        "",
        "#define net_timestamp_check(COND, SKB)				\\",
        "	if (static_branch_unlikely(&netstamp_needed_key)) {	\\",
        "		if ((COND) && !(SKB)->tstamp)			\\",
        "			(SKB)->tstamp = ktime_get_real();	\\",
        "	}							\\",
        "",
        "bool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)",
        "{",
        "	return __is_skb_forwardable(dev, skb, true);",
        "}",
        "EXPORT_SYMBOL_GPL(is_skb_forwardable);",
        "",
        "static int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,",
        "			      bool check_mtu)",
        "{",
        "	int ret = ____dev_forward_skb(dev, skb, check_mtu);",
        "",
        "	if (likely(!ret)) {",
        "		skb->protocol = eth_type_trans(skb, dev);",
        "		skb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);",
        "	}",
        "",
        "	return ret;",
        "}",
        "",
        "int __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)",
        "{",
        "	return __dev_forward_skb2(dev, skb, true);",
        "}",
        "EXPORT_SYMBOL_GPL(__dev_forward_skb);",
        "",
        "/**",
        " * dev_forward_skb - loopback an skb to another netif",
        " *",
        " * @dev: destination network device",
        " * @skb: buffer to forward",
        " *",
        " * return values:",
        " *	NET_RX_SUCCESS	(no congestion)",
        " *	NET_RX_DROP     (packet was dropped, but freed)",
        " *",
        " * dev_forward_skb can be used for injecting an skb from the",
        " * start_xmit function of one device into the receive queue",
        " * of another device.",
        " *",
        " * The receiving device may be in another namespace, so",
        " * we have to clear all information in the skb that could",
        " * impact namespace isolation.",
        " */",
        "int dev_forward_skb(struct net_device *dev, struct sk_buff *skb)",
        "{",
        "	return __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);",
        "}",
        "EXPORT_SYMBOL_GPL(dev_forward_skb);",
        "",
        "int dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)",
        "{",
        "	return __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);",
        "}",
        "",
        "static inline int deliver_skb(struct sk_buff *skb,",
        "			      struct packet_type *pt_prev,",
        "			      struct net_device *orig_dev)",
        "{",
        "	if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))",
        "		return -ENOMEM;",
        "	refcount_inc(&skb->users);",
        "	return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);",
        "}",
        "",
        "static inline void deliver_ptype_list_skb(struct sk_buff *skb,",
        "					  struct packet_type **pt,",
        "					  struct net_device *orig_dev,",
        "					  __be16 type,",
        "					  struct list_head *ptype_list)",
        "{",
        "	struct packet_type *ptype, *pt_prev = *pt;",
        "",
        "	list_for_each_entry_rcu(ptype, ptype_list, list) {",
        "		if (ptype->type != type)",
        "			continue;",
        "		if (pt_prev)",
        "			deliver_skb(skb, pt_prev, orig_dev);",
        "		pt_prev = ptype;",
        "	}",
        "	*pt = pt_prev;",
        "}",
        "",
        "static inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)",
        "{",
        "	if (!ptype->af_packet_priv || !skb->sk)",
        "		return false;",
        "",
        "	if (ptype->id_match)",
        "		return ptype->id_match(ptype, skb->sk);",
        "	else if ((struct sock *)ptype->af_packet_priv == skb->sk)",
        "		return true;",
        "",
        "	return false;",
        "}",
        "",
        "/**",
        " * dev_nit_active - return true if any network interface taps are in use",
        " *",
        " * @dev: network device to check for the presence of taps",
        " */",
        "bool dev_nit_active(struct net_device *dev)",
        "{",
        "	return !list_empty(&net_hotdata.ptype_all) ||",
        "	       !list_empty(&dev->ptype_all);",
        "}",
        "EXPORT_SYMBOL_GPL(dev_nit_active);",
        "",
        "/*",
        " *	Support routine. Sends outgoing frames to any network",
        " *	taps currently in use.",
        " */",
        "",
        "void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)",
        "{",
        "	struct list_head *ptype_list = &net_hotdata.ptype_all;",
        "	struct packet_type *ptype, *pt_prev = NULL;",
        "	struct sk_buff *skb2 = NULL;",
        "",
        "	rcu_read_lock();",
        "again:",
        "	list_for_each_entry_rcu(ptype, ptype_list, list) {",
        "		if (READ_ONCE(ptype->ignore_outgoing))",
        "			continue;",
        "",
        "		/* Never send packets back to the socket",
        "		 * they originated from - MvS (miquels@drinkel.ow.org)",
        "		 */",
        "		if (skb_loop_sk(ptype, skb))",
        "			continue;",
        "",
        "		if (pt_prev) {",
        "			deliver_skb(skb2, pt_prev, skb->dev);",
        "			pt_prev = ptype;",
        "			continue;",
        "		}",
        "",
        "		/* need to clone skb, done only once */",
        "		skb2 = skb_clone(skb, GFP_ATOMIC);",
        "		if (!skb2)",
        "			goto out_unlock;",
        "",
        "		net_timestamp_set(skb2);",
        "",
        "		/* skb->nh should be correctly",
        "		 * set by sender, so that the second statement is",
        "		 * just protection against buggy protocols.",
        "		 */",
        "		skb_reset_mac_header(skb2);",
        "",
        "		if (skb_network_header(skb2) < skb2->data ||",
        "		    skb_network_header(skb2) > skb_tail_pointer(skb2)) {",
        "			net_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",",
        "					     ntohs(skb2->protocol),",
        "					     dev->name);",
        "			skb_reset_network_header(skb2);",
        "		}",
        "",
        "		skb2->transport_header = skb2->network_header;",
        "		skb2->pkt_type = PACKET_OUTGOING;",
        "		pt_prev = ptype;",
        "	}",
        "",
        "	if (ptype_list == &net_hotdata.ptype_all) {",
        "		ptype_list = &dev->ptype_all;",
        "		goto again;",
        "	}",
        "out_unlock:",
        "	if (pt_prev) {",
        "		if (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))",
        "			pt_prev->func(skb2, skb->dev, pt_prev, skb->dev);",
        "		else",
        "			kfree_skb(skb2);",
        "	}",
        "	rcu_read_unlock();",
        "}",
        "EXPORT_SYMBOL_GPL(dev_queue_xmit_nit);",
        "",
        "/**",
        " * netif_setup_tc - Handle tc mappings on real_num_tx_queues change",
        " * @dev: Network device",
        " * @txq: number of queues available",
        " *",
        " * If real_num_tx_queues is changed the tc mappings may no longer be",
        " * valid. To resolve this verify the tc mapping remains valid and if",
        " * not NULL the mapping. With no priorities mapping to this",
        " * offset/count pair it will no longer be used. In the worst case TC0",
        " * is invalid nothing can be done so disable priority mappings. If is",
        " * expected that drivers will fix this mapping if they can before",
        " * calling netif_set_real_num_tx_queues.",
        " */",
        "static void netif_setup_tc(struct net_device *dev, unsigned int txq)",
        "{",
        "	int i;",
        "	struct netdev_tc_txq *tc = &dev->tc_to_txq[0];",
        "",
        "	/* If TC0 is invalidated disable TC mapping */",
        "	if (tc->offset + tc->count > txq) {",
        "		netdev_warn(dev, \"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");",
        "		dev->num_tc = 0;",
        "		return;",
        "	}",
        "",
        "	/* Invalidated prio to tc mappings set to TC0 */",
        "	for (i = 1; i < TC_BITMASK + 1; i++) {",
        "		int q = netdev_get_prio_tc_map(dev, i);",
        "",
        "		tc = &dev->tc_to_txq[q];",
        "		if (tc->offset + tc->count > txq) {",
        "			netdev_warn(dev, \"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",",
        "				    i, q);",
        "			netdev_set_prio_tc_map(dev, i, 0);",
        "		}",
        "	}",
        "}",
        "",
        "int netdev_txq_to_tc(struct net_device *dev, unsigned int txq)",
        "{",
        "	if (dev->num_tc) {",
        "		struct netdev_tc_txq *tc = &dev->tc_to_txq[0];",
        "		int i;",
        "",
        "		/* walk through the TCs and see if it falls into any of them */",
        "		for (i = 0; i < TC_MAX_QUEUE; i++, tc++) {",
        "			if ((txq - tc->offset) < tc->count)",
        "				return i;",
        "		}",
        "",
        "		/* didn't find it, just return -1 to indicate no match */",
        "		return -1;",
        "	}",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netdev_txq_to_tc);",
        "",
        "#ifdef CONFIG_XPS",
        "static struct static_key xps_needed __read_mostly;",
        "static struct static_key xps_rxqs_needed __read_mostly;",
        "static DEFINE_MUTEX(xps_map_mutex);",
        "#define xmap_dereference(P)		\\",
        "	rcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))",
        "",
        "static bool remove_xps_queue(struct xps_dev_maps *dev_maps,",
        "			     struct xps_dev_maps *old_maps, int tci, u16 index)",
        "{",
        "	struct xps_map *map = NULL;",
        "	int pos;",
        "",
        "	map = xmap_dereference(dev_maps->attr_map[tci]);",
        "	if (!map)",
        "		return false;",
        "",
        "	for (pos = map->len; pos--;) {",
        "		if (map->queues[pos] != index)",
        "			continue;",
        "",
        "		if (map->len > 1) {",
        "			map->queues[pos] = map->queues[--map->len];",
        "			break;",
        "		}",
        "",
        "		if (old_maps)",
        "			RCU_INIT_POINTER(old_maps->attr_map[tci], NULL);",
        "		RCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);",
        "		kfree_rcu(map, rcu);",
        "		return false;",
        "	}",
        "",
        "	return true;",
        "}",
        "",
        "static bool remove_xps_queue_cpu(struct net_device *dev,",
        "				 struct xps_dev_maps *dev_maps,",
        "				 int cpu, u16 offset, u16 count)",
        "{",
        "	int num_tc = dev_maps->num_tc;",
        "	bool active = false;",
        "	int tci;",
        "",
        "	for (tci = cpu * num_tc; num_tc--; tci++) {",
        "		int i, j;",
        "",
        "		for (i = count, j = offset; i--; j++) {",
        "			if (!remove_xps_queue(dev_maps, NULL, tci, j))",
        "				break;",
        "		}",
        "",
        "		active |= i < 0;",
        "	}",
        "",
        "	return active;",
        "}",
        "",
        "static void reset_xps_maps(struct net_device *dev,",
        "			   struct xps_dev_maps *dev_maps,",
        "			   enum xps_map_type type)",
        "{",
        "	static_key_slow_dec_cpuslocked(&xps_needed);",
        "	if (type == XPS_RXQS)",
        "		static_key_slow_dec_cpuslocked(&xps_rxqs_needed);",
        "",
        "	RCU_INIT_POINTER(dev->xps_maps[type], NULL);",
        "",
        "	kfree_rcu(dev_maps, rcu);",
        "}",
        "",
        "static void clean_xps_maps(struct net_device *dev, enum xps_map_type type,",
        "			   u16 offset, u16 count)",
        "{",
        "	struct xps_dev_maps *dev_maps;",
        "	bool active = false;",
        "	int i, j;",
        "",
        "	dev_maps = xmap_dereference(dev->xps_maps[type]);",
        "	if (!dev_maps)",
        "		return;",
        "",
        "	for (j = 0; j < dev_maps->nr_ids; j++)",
        "		active |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);",
        "	if (!active)",
        "		reset_xps_maps(dev, dev_maps, type);",
        "",
        "	if (type == XPS_CPUS) {",
        "		for (i = offset + (count - 1); count--; i--)",
        "			netdev_queue_numa_node_write(",
        "				netdev_get_tx_queue(dev, i), NUMA_NO_NODE);",
        "	}",
        "}",
        "",
        "static void netif_reset_xps_queues(struct net_device *dev, u16 offset,",
        "				   u16 count)",
        "{",
        "	if (!static_key_false(&xps_needed))",
        "		return;",
        "",
        "	cpus_read_lock();",
        "	mutex_lock(&xps_map_mutex);",
        "",
        "	if (static_key_false(&xps_rxqs_needed))",
        "		clean_xps_maps(dev, XPS_RXQS, offset, count);",
        "",
        "	clean_xps_maps(dev, XPS_CPUS, offset, count);",
        "",
        "	mutex_unlock(&xps_map_mutex);",
        "	cpus_read_unlock();",
        "}",
        "",
        "static void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)",
        "{",
        "	netif_reset_xps_queues(dev, index, dev->num_tx_queues - index);",
        "}",
        "",
        "static struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,",
        "				      u16 index, bool is_rxqs_map)",
        "{",
        "	struct xps_map *new_map;",
        "	int alloc_len = XPS_MIN_MAP_ALLOC;",
        "	int i, pos;",
        "",
        "	for (pos = 0; map && pos < map->len; pos++) {",
        "		if (map->queues[pos] != index)",
        "			continue;",
        "		return map;",
        "	}",
        "",
        "	/* Need to add tx-queue to this CPU's/rx-queue's existing map */",
        "	if (map) {",
        "		if (pos < map->alloc_len)",
        "			return map;",
        "",
        "		alloc_len = map->alloc_len * 2;",
        "	}",
        "",
        "	/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's",
        "	 *  map",
        "	 */",
        "	if (is_rxqs_map)",
        "		new_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);",
        "	else",
        "		new_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,",
        "				       cpu_to_node(attr_index));",
        "	if (!new_map)",
        "		return NULL;",
        "",
        "	for (i = 0; i < pos; i++)",
        "		new_map->queues[i] = map->queues[i];",
        "	new_map->alloc_len = alloc_len;",
        "	new_map->len = pos;",
        "",
        "	return new_map;",
        "}",
        "",
        "/* Copy xps maps at a given index */",
        "static void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,",
        "			      struct xps_dev_maps *new_dev_maps, int index,",
        "			      int tc, bool skip_tc)",
        "{",
        "	int i, tci = index * dev_maps->num_tc;",
        "	struct xps_map *map;",
        "",
        "	/* copy maps belonging to foreign traffic classes */",
        "	for (i = 0; i < dev_maps->num_tc; i++, tci++) {",
        "		if (i == tc && skip_tc)",
        "			continue;",
        "",
        "		/* fill in the new device map from the old device map */",
        "		map = xmap_dereference(dev_maps->attr_map[tci]);",
        "		RCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);",
        "	}",
        "}",
        "",
        "/* Must be called under cpus_read_lock */",
        "int __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,",
        "			  u16 index, enum xps_map_type type)",
        "{",
        "	struct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;",
        "	const unsigned long *online_mask = NULL;",
        "	bool active = false, copy = false;",
        "	int i, j, tci, numa_node_id = -2;",
        "	int maps_sz, num_tc = 1, tc = 0;",
        "	struct xps_map *map, *new_map;",
        "	unsigned int nr_ids;",
        "",
        "	WARN_ON_ONCE(index >= dev->num_tx_queues);",
        "",
        "	if (dev->num_tc) {",
        "		/* Do not allow XPS on subordinate device directly */",
        "		num_tc = dev->num_tc;",
        "		if (num_tc < 0)",
        "			return -EINVAL;",
        "",
        "		/* If queue belongs to subordinate dev use its map */",
        "		dev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;",
        "",
        "		tc = netdev_txq_to_tc(dev, index);",
        "		if (tc < 0)",
        "			return -EINVAL;",
        "	}",
        "",
        "	mutex_lock(&xps_map_mutex);",
        "",
        "	dev_maps = xmap_dereference(dev->xps_maps[type]);",
        "	if (type == XPS_RXQS) {",
        "		maps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);",
        "		nr_ids = dev->num_rx_queues;",
        "	} else {",
        "		maps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);",
        "		if (num_possible_cpus() > 1)",
        "			online_mask = cpumask_bits(cpu_online_mask);",
        "		nr_ids = nr_cpu_ids;",
        "	}",
        "",
        "	if (maps_sz < L1_CACHE_BYTES)",
        "		maps_sz = L1_CACHE_BYTES;",
        "",
        "	/* The old dev_maps could be larger or smaller than the one we're",
        "	 * setting up now, as dev->num_tc or nr_ids could have been updated in",
        "	 * between. We could try to be smart, but let's be safe instead and only",
        "	 * copy foreign traffic classes if the two map sizes match.",
        "	 */",
        "	if (dev_maps &&",
        "	    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)",
        "		copy = true;",
        "",
        "	/* allocate memory for queue storage */",
        "	for (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),",
        "	     j < nr_ids;) {",
        "		if (!new_dev_maps) {",
        "			new_dev_maps = kzalloc(maps_sz, GFP_KERNEL);",
        "			if (!new_dev_maps) {",
        "				mutex_unlock(&xps_map_mutex);",
        "				return -ENOMEM;",
        "			}",
        "",
        "			new_dev_maps->nr_ids = nr_ids;",
        "			new_dev_maps->num_tc = num_tc;",
        "		}",
        "",
        "		tci = j * num_tc + tc;",
        "		map = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;",
        "",
        "		map = expand_xps_map(map, j, index, type == XPS_RXQS);",
        "		if (!map)",
        "			goto error;",
        "",
        "		RCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);",
        "	}",
        "",
        "	if (!new_dev_maps)",
        "		goto out_no_new_maps;",
        "",
        "	if (!dev_maps) {",
        "		/* Increment static keys at most once per type */",
        "		static_key_slow_inc_cpuslocked(&xps_needed);",
        "		if (type == XPS_RXQS)",
        "			static_key_slow_inc_cpuslocked(&xps_rxqs_needed);",
        "	}",
        "",
        "	for (j = 0; j < nr_ids; j++) {",
        "		bool skip_tc = false;",
        "",
        "		tci = j * num_tc + tc;",
        "		if (netif_attr_test_mask(j, mask, nr_ids) &&",
        "		    netif_attr_test_online(j, online_mask, nr_ids)) {",
        "			/* add tx-queue to CPU/rx-queue maps */",
        "			int pos = 0;",
        "",
        "			skip_tc = true;",
        "",
        "			map = xmap_dereference(new_dev_maps->attr_map[tci]);",
        "			while ((pos < map->len) && (map->queues[pos] != index))",
        "				pos++;",
        "",
        "			if (pos == map->len)",
        "				map->queues[map->len++] = index;",
        "#ifdef CONFIG_NUMA",
        "			if (type == XPS_CPUS) {",
        "				if (numa_node_id == -2)",
        "					numa_node_id = cpu_to_node(j);",
        "				else if (numa_node_id != cpu_to_node(j))",
        "					numa_node_id = -1;",
        "			}",
        "#endif",
        "		}",
        "",
        "		if (copy)",
        "			xps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,",
        "					  skip_tc);",
        "	}",
        "",
        "	rcu_assign_pointer(dev->xps_maps[type], new_dev_maps);",
        "",
        "	/* Cleanup old maps */",
        "	if (!dev_maps)",
        "		goto out_no_old_maps;",
        "",
        "	for (j = 0; j < dev_maps->nr_ids; j++) {",
        "		for (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {",
        "			map = xmap_dereference(dev_maps->attr_map[tci]);",
        "			if (!map)",
        "				continue;",
        "",
        "			if (copy) {",
        "				new_map = xmap_dereference(new_dev_maps->attr_map[tci]);",
        "				if (map == new_map)",
        "					continue;",
        "			}",
        "",
        "			RCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);",
        "			kfree_rcu(map, rcu);",
        "		}",
        "	}",
        "",
        "	old_dev_maps = dev_maps;",
        "",
        "out_no_old_maps:",
        "	dev_maps = new_dev_maps;",
        "	active = true;",
        "",
        "out_no_new_maps:",
        "	if (type == XPS_CPUS)",
        "		/* update Tx queue numa node */",
        "		netdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),",
        "					     (numa_node_id >= 0) ?",
        "					     numa_node_id : NUMA_NO_NODE);",
        "",
        "	if (!dev_maps)",
        "		goto out_no_maps;",
        "",
        "	/* removes tx-queue from unused CPUs/rx-queues */",
        "	for (j = 0; j < dev_maps->nr_ids; j++) {",
        "		tci = j * dev_maps->num_tc;",
        "",
        "		for (i = 0; i < dev_maps->num_tc; i++, tci++) {",
        "			if (i == tc &&",
        "			    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&",
        "			    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))",
        "				continue;",
        "",
        "			active |= remove_xps_queue(dev_maps,",
        "						   copy ? old_dev_maps : NULL,",
        "						   tci, index);",
        "		}",
        "	}",
        "",
        "	if (old_dev_maps)",
        "		kfree_rcu(old_dev_maps, rcu);",
        "",
        "	/* free map if not active */",
        "	if (!active)",
        "		reset_xps_maps(dev, dev_maps, type);",
        "",
        "out_no_maps:",
        "	mutex_unlock(&xps_map_mutex);",
        "",
        "	return 0;",
        "error:",
        "	/* remove any maps that we added */",
        "	for (j = 0; j < nr_ids; j++) {",
        "		for (i = num_tc, tci = j * num_tc; i--; tci++) {",
        "			new_map = xmap_dereference(new_dev_maps->attr_map[tci]);",
        "			map = copy ?",
        "			      xmap_dereference(dev_maps->attr_map[tci]) :",
        "			      NULL;",
        "			if (new_map && new_map != map)",
        "				kfree(new_map);",
        "		}",
        "	}",
        "",
        "	mutex_unlock(&xps_map_mutex);",
        "",
        "	kfree(new_dev_maps);",
        "	return -ENOMEM;",
        "}",
        "EXPORT_SYMBOL_GPL(__netif_set_xps_queue);",
        "",
        "int netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,",
        "			u16 index)",
        "{",
        "	int ret;",
        "",
        "	cpus_read_lock();",
        "	ret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);",
        "	cpus_read_unlock();",
        "",
        "	return ret;",
        "}",
        "EXPORT_SYMBOL(netif_set_xps_queue);",
        "",
        "#endif",
        "static void netdev_unbind_all_sb_channels(struct net_device *dev)",
        "{",
        "	struct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];",
        "",
        "	/* Unbind any subordinate channels */",
        "	while (txq-- != &dev->_tx[0]) {",
        "		if (txq->sb_dev)",
        "			netdev_unbind_sb_channel(dev, txq->sb_dev);",
        "	}",
        "}",
        "",
        "void netdev_reset_tc(struct net_device *dev)",
        "{",
        "#ifdef CONFIG_XPS",
        "	netif_reset_xps_queues_gt(dev, 0);",
        "#endif",
        "	netdev_unbind_all_sb_channels(dev);",
        "",
        "	/* Reset TC configuration of device */",
        "	dev->num_tc = 0;",
        "	memset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));",
        "	memset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));",
        "}",
        "EXPORT_SYMBOL(netdev_reset_tc);",
        "",
        "int netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)",
        "{",
        "	if (tc >= dev->num_tc)",
        "		return -EINVAL;",
        "",
        "#ifdef CONFIG_XPS",
        "	netif_reset_xps_queues(dev, offset, count);",
        "#endif",
        "	dev->tc_to_txq[tc].count = count;",
        "	dev->tc_to_txq[tc].offset = offset;",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netdev_set_tc_queue);",
        "",
        "int netdev_set_num_tc(struct net_device *dev, u8 num_tc)",
        "{",
        "	if (num_tc > TC_MAX_QUEUE)",
        "		return -EINVAL;",
        "",
        "#ifdef CONFIG_XPS",
        "	netif_reset_xps_queues_gt(dev, 0);",
        "#endif",
        "	netdev_unbind_all_sb_channels(dev);",
        "",
        "	dev->num_tc = num_tc;",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netdev_set_num_tc);",
        "",
        "void netdev_unbind_sb_channel(struct net_device *dev,",
        "			      struct net_device *sb_dev)",
        "{",
        "	struct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];",
        "",
        "#ifdef CONFIG_XPS",
        "	netif_reset_xps_queues_gt(sb_dev, 0);",
        "#endif",
        "	memset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));",
        "	memset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));",
        "",
        "	while (txq-- != &dev->_tx[0]) {",
        "		if (txq->sb_dev == sb_dev)",
        "			txq->sb_dev = NULL;",
        "	}",
        "}",
        "EXPORT_SYMBOL(netdev_unbind_sb_channel);",
        "",
        "int netdev_bind_sb_channel_queue(struct net_device *dev,",
        "				 struct net_device *sb_dev,",
        "				 u8 tc, u16 count, u16 offset)",
        "{",
        "	/* Make certain the sb_dev and dev are already configured */",
        "	if (sb_dev->num_tc >= 0 || tc >= dev->num_tc)",
        "		return -EINVAL;",
        "",
        "	/* We cannot hand out queues we don't have */",
        "	if ((offset + count) > dev->real_num_tx_queues)",
        "		return -EINVAL;",
        "",
        "	/* Record the mapping */",
        "	sb_dev->tc_to_txq[tc].count = count;",
        "	sb_dev->tc_to_txq[tc].offset = offset;",
        "",
        "	/* Provide a way for Tx queue to find the tc_to_txq map or",
        "	 * XPS map for itself.",
        "	 */",
        "	while (count--)",
        "		netdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netdev_bind_sb_channel_queue);",
        "",
        "int netdev_set_sb_channel(struct net_device *dev, u16 channel)",
        "{",
        "	/* Do not use a multiqueue device to represent a subordinate channel */",
        "	if (netif_is_multiqueue(dev))",
        "		return -ENODEV;",
        "",
        "	/* We allow channels 1 - 32767 to be used for subordinate channels.",
        "	 * Channel 0 is meant to be \"native\" mode and used only to represent",
        "	 * the main root device. We allow writing 0 to reset the device back",
        "	 * to normal mode after being used as a subordinate channel.",
        "	 */",
        "	if (channel > S16_MAX)",
        "		return -EINVAL;",
        "",
        "	dev->num_tc = -channel;",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netdev_set_sb_channel);",
        "",
        "/*",
        " * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues",
        " * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.",
        " */",
        "int netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)",
        "{",
        "	bool disabling;",
        "	int rc;",
        "",
        "	disabling = txq < dev->real_num_tx_queues;",
        "",
        "	if (txq < 1 || txq > dev->num_tx_queues)",
        "		return -EINVAL;",
        "",
        "	if (dev->reg_state == NETREG_REGISTERED ||",
        "	    dev->reg_state == NETREG_UNREGISTERING) {",
        "		ASSERT_RTNL();",
        "",
        "		rc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,",
        "						  txq);",
        "		if (rc)",
        "			return rc;",
        "",
        "		if (dev->num_tc)",
        "			netif_setup_tc(dev, txq);",
        "",
        "		net_shaper_set_real_num_tx_queues(dev, txq);",
        "",
        "		dev_qdisc_change_real_num_tx(dev, txq);",
        "",
        "		dev->real_num_tx_queues = txq;",
        "",
        "		if (disabling) {",
        "			synchronize_net();",
        "			qdisc_reset_all_tx_gt(dev, txq);",
        "#ifdef CONFIG_XPS",
        "			netif_reset_xps_queues_gt(dev, txq);",
        "#endif",
        "		}",
        "	} else {",
        "		dev->real_num_tx_queues = txq;",
        "	}",
        "",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netif_set_real_num_tx_queues);",
        "",
        "#ifdef CONFIG_SYSFS",
        "/**",
        " *	netif_set_real_num_rx_queues - set actual number of RX queues used",
        " *	@dev: Network device",
        " *	@rxq: Actual number of RX queues",
        " *",
        " *	This must be called either with the rtnl_lock held or before",
        " *	registration of the net device.  Returns 0 on success, or a",
        " *	negative error code.  If called before registration, it always",
        " *	succeeds.",
        " */",
        "int netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)",
        "{",
        "	int rc;",
        "",
        "	if (rxq < 1 || rxq > dev->num_rx_queues)",
        "		return -EINVAL;",
        "",
        "	if (dev->reg_state == NETREG_REGISTERED) {",
        "		ASSERT_RTNL();",
        "",
        "		rc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,",
        "						  rxq);",
        "		if (rc)",
        "			return rc;",
        "	}",
        "",
        "	dev->real_num_rx_queues = rxq;",
        "	return 0;",
        "}",
        "EXPORT_SYMBOL(netif_set_real_num_rx_queues);",
        "#endif",
        "",
        "/**",
        " *	netif_set_real_num_queues - set actual number of RX and TX queues used",
        " *	@dev: Network device",
        " *	@txq: Actual number of TX queues",
        " *	@rxq: Actual number of RX queues",
        " *",
        " *	Set the real number of both TX and RX queues.",
        " *	Does nothing if the number of queues is already correct.",
        " */",
        "int netif_set_real_num_queues(struct net_device *dev,",
        "			      unsigned int txq, unsigned int rxq)",
        "{",
        "	unsigned int old_rxq = dev->real_num_rx_queues;",
        "	int err;",
        "",
        "	if (txq < 1 || txq > dev->num_tx_queues ||",
        "	    rxq < 1 || rxq > dev->num_rx_queues)",
        "		return -EINVAL;",
        "",
        "	/* Start from increases, so the error path only does decreases -",
        "	 * decreases can't fail.",
        "	 */",
        "	if (rxq > dev->real_num_rx_queues) {",
        "		err = netif_set_real_num_rx_queues(dev, rxq);",
        "		if (err)",
        "			return err;",
        "	}",
        "	if (txq > dev->real_num_tx_queues) {",
        "		err = netif_set_real_num_tx_queues(dev, txq);",
        "		if (err)",
        "			goto undo_rx;",
        "	}",
        "	if (rxq < dev->real_num_rx_queues)",
        "		WARN_ON(netif_set_real_num_rx_queues(dev, rxq));",
        "	if (txq < dev->real_num_tx_queues)",
        "		WARN_ON(netif_set_real_num_tx_queues(dev, txq));",
        "",
        "	return 0;",
        "undo_rx:",
        "	WARN_ON(netif_set_real_num_rx_queues(dev, old_rxq));",
        "	return err;",
        "}",
        "EXPORT_SYMBOL(netif_set_real_num_queues);",
        "",
        "/**",
        " * netif_set_tso_max_size() - set the max size of TSO frames supported",
        " * @dev:	netdev to update",
        " * @size:	max skb->len of a TSO frame",
        " *",
        " * Set the limit on the size of TSO super-frames the device can handle.",
        " * Unless explicitly set the stack will assume the value of",
        " * %GSO_LEGACY_MAX_SIZE.",
        " */",
        "void netif_set_tso_max_size(struct net_device *dev, unsigned int size)",
        "{",
        "	dev->tso_max_size = min(GSO_MAX_SIZE, size);",
        "	if (size < READ_ONCE(dev->gso_max_size))",
        "		netif_set_gso_max_size(dev, size);",
        "	if (size < READ_ONCE(dev->gso_ipv4_max_size))",
        "		netif_set_gso_ipv4_max_size(dev, size);",
        "}",
        "EXPORT_SYMBOL(netif_set_tso_max_size);",
        "",
        "/**",
        " * netif_set_tso_max_segs() - set the max number of segs supported for TSO",
        " * @dev:	netdev to update",
        " * @segs:	max number of TCP segments",
        " *",
        " * Set the limit on the number of TCP segments the device can generate from",
        " * a single TSO super-frame.",
        " * Unless explicitly set the stack will assume the value of %GSO_MAX_SEGS.",
        " */",
        "void netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)",
        "{",
        "	dev->tso_max_segs = segs;",
        "	if (segs < READ_ONCE(dev->gso_max_segs))",
        "		netif_set_gso_max_segs(dev, segs);",
        "}",
        "EXPORT_SYMBOL(netif_set_tso_max_segs);",
        "",
        "/**",
        " * netif_inherit_tso_max() - copy all TSO limits from a lower device to an upper",
        " * @to:		netdev to update",
        " * @from:	netdev from which to copy the limits",
        " */",
        "void netif_inherit_tso_max(struct net_device *to, const struct net_device *from)",
        "{",
        "	netif_set_tso_max_size(to, from->tso_max_size);",
        "	netif_set_tso_max_segs(to, from->tso_max_segs);",
        "}",
        "EXPORT_SYMBOL(netif_inherit_tso_max);",
        "",
        "/**",
        " * netif_get_num_default_rss_queues - default number of RSS queues",
        " *",
        " * Default value is the number of physical cores if there are only 1 or 2, or",
        " * divided by 2 if there are more.",
        " */",
        "int netif_get_num_default_rss_queues(void)",
        "{",
        "	cpumask_var_t cpus;",
        "	int cpu, count = 0;",
        "",
        "	if (unlikely(is_kdump_kernel() || !zalloc_cpumask_var(&cpus, GFP_KERNEL)))",
        "		return 1;",
        "",
        "	cpumask_copy(cpus, cpu_online_mask);",
        "	for_each_cpu(cpu, cpus) {",
        "		++count;",
        "		cpumask_andnot(cpus, cpus, topology_sibling_cpumask(cpu));",
        "	}",
        "	free_cpumask_var(cpus);",
        "",
        "	return count > 2 ? DIV_ROUND_UP(count, 2) : count;",
        "}",
        "EXPORT_SYMBOL(netif_get_num_default_rss_queues);",
        "",
        "static void __netif_reschedule(struct Qdisc *q)",
        "{",
        "	struct softnet_data *sd;",
        "	unsigned long flags;",
        "",
        "	local_irq_save(flags);",
        "	sd = this_cpu_ptr(&softnet_data);",
        "	q->next_sched = NULL;",
        "	*sd->output_queue_tailp = q;",
        "	sd->output_queue_tailp = &q->next_sched;",
        "	raise_softirq_irqoff(NET_TX_SOFTIRQ);",
        "	local_irq_restore(flags);",
        "}",
        "",
        "void __netif_schedule(struct Qdisc *q)",
        "{",
        "	if (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))",
        "		__netif_reschedule(q);",
        "}",
        "EXPORT_SYMBOL(__netif_schedule);",
        "",
        "struct dev_kfree_skb_cb {",
        "	enum skb_drop_reason reason;",
        "};",
        "",
        "static struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)",
        "{",
        "	return (struct dev_kfree_skb_cb *)skb->cb;",
        "}",
        "",
        "void netif_schedule_queue(struct netdev_queue *txq)",
        "{",
        "	rcu_read_lock();",
        "	if (!netif_xmit_stopped(txq)) {",
        "		struct Qdisc *q = rcu_dereference(txq->qdisc);",
        "",
        "		__netif_schedule(q);",
        "	}",
        "	rcu_read_unlock();",
        "}",
        "EXPORT_SYMBOL(netif_schedule_queue);",
        "",
        "void netif_tx_wake_queue(struct netdev_queue *dev_queue)",
        "{",
        "	if (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {",
        "		struct Qdisc *q;",
        "",
        "		rcu_read_lock();",
        "		q = rcu_dereference(dev_queue->qdisc);",
        "		__netif_schedule(q);",
        "		rcu_read_unlock();",
        "	}",
        "}",
        "EXPORT_SYMBOL(netif_tx_wake_queue);",
        "",
        "void dev_kfree_skb_irq_reason(struct sk_buff *skb, enum skb_drop_reason reason)",
        "{",
        "	unsigned long flags;",
        "",
        "	if (unlikely(!skb))",
        "		return;",
        "",
        "	if (likely(refcount_read(&skb->users) == 1)) {",
        "		smp_rmb();",
        "		refcount_set(&skb->users, 0);",
        "	} else if (likely(!refcount_dec_and_test(&skb->users))) {",
        "		return;",
        "	}",
        "	get_kfree_skb_cb(skb)->reason = reason;",
        "	local_irq_save(flags);",
        "	skb->next = __this_cpu_read(softnet_data.completion_queue);",
        "	__this_cpu_write(softnet_data.completion_queue, skb);",
        "	raise_softirq_irqoff(NET_TX_SOFTIRQ);",
        "	local_irq_restore(flags);",
        "}",
        "EXPORT_SYMBOL(dev_kfree_skb_irq_reason);",
        "",
        "void dev_kfree_skb_any_reason(struct sk_buff *skb, enum skb_drop_reason reason)",
        "{",
        "	if (in_hardirq() || irqs_disabled())",
        "		dev_kfree_skb_irq_reason(skb, reason);",
        "	else",
        "		kfree_skb_reason(skb, reason);",
        "}",
        "EXPORT_SYMBOL(dev_kfree_skb_any_reason);",
        "",
        "",
        "/**",
        " * netif_device_detach - mark device as removed",
        " * @dev: network device",
        " *",
        " * Mark device as removed from system and therefore no longer available.",
        " */",
        "void netif_device_detach(struct net_device *dev)",
        "{",
        "	if (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&",
        "	    netif_running(dev)) {",
        "		netif_tx_stop_all_queues(dev);",
        "	}",
        "}",
        "EXPORT_SYMBOL(netif_device_detach);",
        "",
        "/**",
        " * netif_device_attach - mark device as attached",
        " * @dev: network device",
        " *",
        " * Mark device as attached from system and restart if needed.",
        " */",
        "void netif_device_attach(struct net_device *dev)",
        "{",
        "	if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&",
        "	    netif_running(dev)) {",
        "		netif_tx_wake_all_queues(dev);",
        "		__netdev_watchdog_up(dev);",
        "	}",
        "}",
        "EXPORT_SYMBOL(netif_device_attach);",
        "",
        "/*",
        " * Returns a Tx hash based on the given packet descriptor a Tx queues' number",
        " * to be used as a distribution range.",
        " */",
        "static u16 skb_tx_hash(const struct net_device *dev,",
        "		       const struct net_device *sb_dev,",
        "		       struct sk_buff *skb)",
        "{",
        "	u32 hash;",
        "	u16 qoffset = 0;",
        "	u16 qcount = dev->real_num_tx_queues;",
        "",
        "	if (dev->num_tc) {",
        "		u8 tc = netdev_get_prio_tc_map(dev, skb->priority);",
        "",
        "		qoffset = sb_dev->tc_to_txq[tc].offset;",
        "		qcount = sb_dev->tc_to_txq[tc].count;",
        "		if (unlikely(!qcount)) {",
        "			net_warn_ratelimited(\"%s: invalid qcount, qoffset %u for tc %u\\n\",",
        "					     sb_dev->name, qoffset, tc);",
        "			qoffset = 0;",
        "			qcount = dev->real_num_tx_queues;",
        "		}",
        "	}",
        "",
        "	if (skb_rx_queue_recorded(skb)) {",
        "		DEBUG_NET_WARN_ON_ONCE(qcount == 0);",
        "		hash = skb_get_rx_queue(skb);",
        "		if (hash >= qoffset)",
        "			hash -= qoffset;",
        "		while (unlikely(hash >= qcount))",
        "			hash -= qcount;",
        "		return hash + qoffset;",
        "	}",
        "",
        "	return (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;",
        "}",
        "",
        "void skb_warn_bad_offload(const struct sk_buff *skb)",
        "{",
        "	static const netdev_features_t null_features;",
        "	struct net_device *dev = skb->dev;",
        "	const char *name = \"\";",
        "",
        "	if (!net_ratelimit())",
        "		return;",
        "",
        "	if (dev) {",
        "		if (dev->dev.parent)",
        "			name = dev_driver_string(dev->dev.parent);",
        "		else",
        "			name = netdev_name(dev);",
        "	}",
        "	skb_dump(KERN_WARNING, skb, false);",
        "	WARN(1, \"%s: caps=(%pNF, %pNF)\\n\",",
        "	     name, dev ? &dev->features : &null_features,",
        "	     skb->sk ? &skb->sk->sk_route_caps : &null_features);",
        "}",
        "",
        "/*",
        " * Invalidate hardware checksum when packet is to be mangled, and",
        " * complete checksum manually on outgoing path.",
        " */",
        "int skb_checksum_help(struct sk_buff *skb)",
        "{",
        "	__wsum csum;",
        "	int ret = 0, offset;",
        "",
        "	if (skb->ip_summed == CHECKSUM_COMPLETE)",
        "		goto out_set_summed;",
        "",
        "	if (unlikely(skb_is_gso(skb))) {",
        "		skb_warn_bad_offload(skb);",
        "		return -EINVAL;",
        "	}",
        "",
        "	if (!skb_frags_readable(skb)) {",
        "		return -EFAULT;",
        "	}",
        "",
        "	/* Before computing a checksum, we should make sure no frag could",
        "	 * be modified by an external entity : checksum could be wrong.",
        "	 */",
        "	if (skb_has_shared_frag(skb)) {",
        "		ret = __skb_linearize(skb);",
        "		if (ret)",
        "			goto out;",
        "	}",
        "",
        "	offset = skb_checksum_start_offset(skb);",
        "	ret = -EINVAL;",
        "	if (unlikely(offset >= skb_headlen(skb))) {",
        "		DO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);",
        "		WARN_ONCE(true, \"offset (%d) >= skb_headlen() (%u)\\n\",",
        "			  offset, skb_headlen(skb));",
        "		goto out;",
        "	}",
        "	csum = skb_checksum(skb, offset, skb->len - offset, 0);",
        "",
        "	offset += skb->csum_offset;",
        "	if (unlikely(offset + sizeof(__sum16) > skb_headlen(skb))) {",
        "		D